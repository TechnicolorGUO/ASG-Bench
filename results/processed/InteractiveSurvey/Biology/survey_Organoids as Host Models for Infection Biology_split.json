{
  "outline": [
    [
      1,
      "A Survey of Organoids as Host Models for Infection Biology"
    ],
    [
      1,
      "1 Abstract"
    ],
    [
      1,
      "2 Introduction"
    ],
    [
      1,
      "3 Observational Astronomy and Cosmology"
    ],
    [
      2,
      "3.1 Observational Techniques and Instrumentation"
    ],
    [
      3,
      "3.1.1 Multi-wavelength and Spectroscopic Analysis"
    ],
    [
      3,
      "3.1.2 Transit and Interferometry Methods"
    ],
    [
      2,
      "3.2 Cosmological and Galactic Structure Studies"
    ],
    [
      3,
      "3.2.1 Galaxy Evolution and Mergers"
    ],
    [
      3,
      "3.2.2 Stellar Populations and Cluster Analysis"
    ],
    [
      2,
      "3.3 Exoplanetary and AGN Investigations"
    ],
    [
      3,
      "3.3.1 Exoplanet Detection and Characterization"
    ],
    [
      3,
      "3.3.2 AGN Activity and Spectral Modeling"
    ],
    [
      1,
      "4 Quantum and Material Physics Simulations"
    ],
    [
      2,
      "4.1 Quantum Spin and Coherence Studies"
    ],
    [
      3,
      "4.1.1 Hyperfine Interactions and Decoherence"
    ],
    [
      3,
      "4.1.2 Spin Control and Manipulation"
    ],
    [
      2,
      "4.2 Material Properties and Electronic Behavior"
    ],
    [
      3,
      "4.2.1 Band Structure and Transport Phenomena"
    ],
    [
      3,
      "4.2.2 Polaritonic and Coherent Systems"
    ],
    [
      2,
      "4.3 Computational and Theoretical Modeling"
    ],
    [
      3,
      "4.3.1 Many-body and Nonlinear Dynamics"
    ],
    [
      3,
      "4.3.2 Topological and Quantum Simulation"
    ],
    [
      1,
      "5 LLM Security and Systematic Evaluation"
    ],
    [
      2,
      "5.1 System Resilience and Data Flow"
    ],
    [
      3,
      "5.1.1 End-to-end Data Movement Analysis"
    ],
    [
      3,
      "5.1.2 Operational Resilience Metrics"
    ],
    [
      2,
      "5.2 Authentication and Security Mechanisms"
    ],
    [
      3,
      "5.2.1 Proof-based Authentication Frameworks"
    ],
    [
      3,
      "5.2.2 Secure Agent Execution and Workloads"
    ],
    [
      2,
      "5.3 Attack Taxonomy and Benchmarking"
    ],
    [
      3,
      "5.3.1 Multi-Component Attack Classification"
    ],
    [
      3,
      "5.3.2 Real-world Robustness Evaluation"
    ],
    [
      1,
      "6 Future Directions"
    ],
    [
      1,
      "7 Conclusion"
    ],
    [
      1,
      "References"
    ]
  ],
  "content": [
    {
      "heading": "A Survey of Organoids as Host Models for Infection Biology",
      "level": 1,
      "content": "",
      "stats": {
        "char_count": 0,
        "word_count": 0,
        "sentence_count": 0,
        "line_count": 1
      }
    },
    {
      "heading": "1 Abstract",
      "level": 1,
      "content": "Organoids have emerged as powerful in vitro models that closely mimic the complexity of human tissues and organs, offering significant advantages over traditional two-dimensional cell cultures. These three-dimensional structures, derived from stem cells, replicate the architecture, function, and genetic diversity of their parental tissues, making them invaluable for studying biological processes and disease mechanisms. This survey paper provides a comprehensive overview of the application of organoids as host models for infection biology, focusing on their ability to replicate in vivo conditions and investigate host-pathogen interactions. The review highlights the strengths of organoid-based systems in studying pathogen entry, replication, and immune responses, as well as their utility in developing novel therapeutic strategies. It also addresses the challenges, including scalability and reproducibility, that hinder their broader adoption. By synthesizing current knowledge and identifying research gaps, this paper aims to guide future advancements in organoid technology and its application in infection research, ultimately contributing to a deeper understanding of infectious diseases and the development of more effective interventions.",
      "stats": {
        "char_count": 1255,
        "word_count": 165,
        "sentence_count": 6,
        "line_count": 1
      }
    },
    {
      "heading": "2 Introduction",
      "level": 1,
      "content": "The rapid advancement of biological research has led to the emergence of innovative in vitro models that more accurately reflect the complexity of human tissues and organs. Among these, organoids have gained significant attention as three-dimensional (3D) structures derived from stem cells that recapitulate the architecture, function, and genetic diversity of their parental tissues. These models have become indispensable tools in various fields, including developmental biology, disease modeling, and drug discovery. The ability of organoids to mimic the microenvironment of native tissues, including cell-cell interactions, extracellular matrix composition, and tissue-specific functions, makes them particularly valuable for studying complex biological processes. As organoid technology continues to evolve, it has become a focal point for researchers seeking to understand the molecular mechanisms underlying tissue homeostasis, regeneration, and pathogenesis. The increasing use of organoids in both basic and translational research has led to a growing body of literature, necessitating a comprehensive review to consolidate current knowledge and identify future research directions.\n\nThis survey paper focuses on the application of organoids as host models for infection biology, a rapidly expanding area that leverages the unique properties of organoids to study host-pathogen interactions in a physiologically relevant context. Unlike traditional two-dimensional (2D) cell cultures, which often fail to capture the complexity of tissue architecture and cellular heterogeneity, organoids provide a more accurate representation of in vivo conditions. This makes them particularly suitable for investigating the mechanisms of pathogen entry, replication, and immune response within a tissue-like environment. The use of organoids as infection models has been instrumental in studying a wide range of pathogens, including viruses, bacteria, and parasites, and has contributed to the development of novel therapeutic strategies. By systematically reviewing the current literature, this paper aims to highlight the strengths and limitations of organoid-based infection models, as well as their potential for advancing our understanding of infectious diseases.\n\nThe content of this survey paper is structured to provide a comprehensive overview of the key developments in organoid-based infection models. The first section presents an in-depth analysis of the methodologies used to generate and characterize organoids, emphasizing their suitability as host systems for studying pathogen interactions. This includes a discussion of the different types of organoids, such as intestinal, liver, and lung organoids, and their respective applications in infection studies. The second section explores the various pathogens that have been investigated using organoid models, highlighting the insights gained from these studies and the challenges encountered in their implementation. Finally, the third section addresses the current limitations of organoid-based infection models, such as scalability, reproducibility, and the need for more advanced genetic and functional tools, and discusses potential strategies for overcoming these barriers.\n\nThe contributions of this survey paper are manifold. First, it provides a structured and comprehensive review of the literature on organoids as host models for infection biology, offering a critical synthesis of the current state of the field. Second, it identifies key research gaps and emerging trends, which can guide future investigations and technological developments. Third, it highlights the potential of organoid models to transform our understanding of host-pathogen interactions, particularly in the context of complex diseases and emerging pathogens. By consolidating existing knowledge and pointing toward future directions, this paper serves as a valuable resource for researchers, clinicians, and policymakers engaged in the study of infectious diseases. The insights presented here are expected to contribute to the advancement of infection biology research and the development of more effective diagnostic and therapeutic strategies.",
      "stats": {
        "char_count": 4195,
        "word_count": 571,
        "sentence_count": 22,
        "line_count": 7
      }
    },
    {
      "heading": "3.1.1 Multi-wavelength and Spectroscopic Analysis",
      "level": 3,
      "content": "Multi-wavelength and spectroscopic analysis has become a cornerstone in understanding the physical properties and evolutionary paths of high-redshift galaxies, particularly in the context of the early Universe. The James Webb Space Telescope (JWST) has enabled unprecedented insights through its ability to capture data across a broad range of wavelengths, from the ultraviolet to the mid-infrared. This capability allows for a detailed characterization of galaxy spectral energy distributions (SEDs), enabling the determination of key parameters such as stellar mass, star formation rate, and dust content. By combining photometric data with spectroscopic observations, researchers can disentangle the contributions of different stellar populations and identify emission and absorption features that reveal the internal dynamics and chemical composition of these distant systems.\n\nSpectroscopic analysis, particularly with instruments like JWST/NIRSpec, has provided critical information on the kinematics and ionization states of gas in high-redshift galaxies. The detection of strong emission lines, such as [1] and [2], along with the identification of broad absorption features, has allowed for the study of outflows and feedback mechanisms that influence galaxy evolution. These observations are essential for understanding the interplay between star formation, black hole activity, and the surrounding interstellar medium. Additionally, the ability to resolve extended ionized gas emission within galaxies has offered insights into the spatial distribution of star-forming regions and the influence of environmental factors on galaxy growth.\n\nThe integration of multi-wavelength data with high-resolution spectroscopy has also been instrumental in addressing discrepancies between observational findings and theoretical models. For instance, the high number densities of massive, quenched galaxies observed at $z > 3$ challenge the predictions of the ΛCDM model, suggesting the need for revised scenarios of galaxy formation. By leveraging the full spectral coverage and sensitivity of JWST, researchers can more accurately model the physical conditions of these galaxies, leading to a deeper understanding of their formation mechanisms and the role of environmental effects in their evolution [3]. This combined approach continues to refine our knowledge of the early Universe and the processes that shaped its structure.",
      "stats": {
        "char_count": 2429,
        "word_count": 333,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "3.1.2 Transit and Interferometry Methods",
      "level": 3,
      "content": "Transit and interferometry methods have become essential tools in modern astrophysical research, enabling the detection and characterization of celestial objects with high precision. Transit methods, primarily used in exoplanet detection, rely on the periodic dimming of a star's light as a planet passes in front of it. This technique has been instrumental in identifying thousands of exoplanets, particularly through missions like Kepler and TESS, which have provided extensive datasets for statistical analysis. The method's strength lies in its ability to determine planetary radii and orbital periods, though it is limited by the need for precise alignment and the inability to directly measure mass without additional techniques such as radial velocity. The combination of transit data with other observational methods enhances the accuracy of planetary and stellar parameter estimation.\n\nInterferometry, on the other hand, leverages the principles of wave interference to achieve high angular resolution, making it particularly valuable for studying compact or distant objects. Optical and radio interferometers, such as the Very Large Telescope Interferometer (VLTI) and the Atacama Large Millimeter Array (ALMA), allow for the imaging of structures that would otherwise be unresolved by single-aperture telescopes. This method is crucial for probing the environments of active galactic nuclei, binary star systems, and the immediate surroundings of young stellar objects. By combining signals from multiple telescopes, interferometry can achieve resolutions that surpass those of individual instruments, enabling the study of fine details in astrophysical phenomena. Its application is especially significant in the context of high-redshift observations, where resolving small-scale structures is critical for understanding galaxy formation and evolution.\n\nBoth transit and interferometry methods face unique challenges that influence their effectiveness and applicability. Transit methods are subject to selection biases, such as the preference for short-period planets and the difficulty in detecting planets around active stars. Interferometry, while powerful, requires complex instrumentation and data processing, and is often limited by atmospheric conditions in optical wavelengths. Despite these limitations, ongoing technological advancements and the development of new observational strategies continue to expand the capabilities of these techniques. As future missions and instruments are deployed, the synergy between transit and interferometry methods will play a vital role in addressing fundamental questions in astrophysics, from the nature of exoplanetary systems to the structure of the early Universe.",
      "stats": {
        "char_count": 2728,
        "word_count": 373,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "3.2.1 Galaxy Evolution and Mergers",
      "level": 3,
      "content": "Galaxy evolution and mergers play a central role in shaping the structural and dynamical properties of galaxies across cosmic time. Observations from the James Webb Space Telescope (JWST) have revealed an unexpectedly large population of massive, quenched galaxies at high redshifts (z > 3), with stellar masses exceeding 10^10 M☉ and minimal star formation [4]. These findings challenge traditional hierarchical models of galaxy formation, as such massive structures appear too early in the Universe's history to be explained by standard star formation efficiencies. The high number densities of these galaxies suggest that early mergers and rapid assembly processes may have been more prevalent than previously assumed, prompting a reevaluation of the timescales and mechanisms driving galaxy growth in the early Universe.\n\nThe role of mergers in galaxy evolution is further highlighted by the observed differences in properties between galaxies in isolated and non-isolated environments. Galaxies in dense group or cluster environments often exhibit distinct star formation rates, morphologies, and kinematic characteristics compared to their isolated counterparts. This indicates that environmental factors, such as interactions with neighboring galaxies and the influence of large-scale structure, significantly impact the evolutionary paths of galaxies. JWST observations have provided critical insights into how mergers and environmental effects contribute to the quenching of star formation and the formation of massive, passive galaxies at early cosmic epochs, offering a more nuanced picture of galaxy assembly.\n\nThe implications of these findings extend beyond individual galaxies, influencing our understanding of the broader cosmological framework. The ΛCDM model, while successful in many aspects, faces challenges in explaining the rapid formation of massive galaxies at high redshifts. Mergers and environmental effects may provide key mechanisms for addressing these discrepancies, suggesting that the interplay between dark matter dynamics, gas accretion, and galaxy interactions is essential for a complete picture of cosmic structure formation. Future studies will need to integrate multi-wavelength observations with high-resolution simulations to further explore the complex processes driving galaxy evolution and mergers in the early Universe.",
      "stats": {
        "char_count": 2366,
        "word_count": 327,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "3.2.2 Stellar Populations and Cluster Analysis",
      "level": 3,
      "content": "Stellar populations and cluster analysis play a crucial role in understanding the evolutionary pathways of galaxies, particularly in the context of high-redshift systems such as Massive, Quenched Galaxies (MQGs). These galaxies, observed at $ z > 3 $, exhibit stellar masses exceeding $ 10^{10} M_{\\odot} $ and show minimal signs of ongoing star formation [4]. The analysis of their stellar populations provides insights into their formation mechanisms, including the efficiency of star formation and the processes that led to their quenching. Cluster analysis further helps in identifying environmental effects that may have influenced their evolution, such as interactions with neighboring galaxies or the influence of dark matter halos. These studies are essential for testing cosmological models, particularly the ΛCDM framework, which struggles to explain the rapid formation of such massive structures in the early Universe.\n\nThe high number densities of MQGs observed by the James Webb Space Telescope (JWST) challenge existing models, as they suggest that these galaxies formed and evolved more rapidly than predicted [4]. This has prompted a reevaluation of the star formation efficiencies and the role of environmental factors in their development. Cluster analysis of these galaxies reveals that their properties, such as stellar mass and star formation rates, differ significantly from those of isolated galaxies, indicating that the surrounding environment plays a key role in shaping their evolutionary paths [5]. By comparing galaxies in different cluster environments, researchers can better understand the interplay between internal and external processes that drive galaxy evolution, particularly in the early Universe.\n\nFurther investigation into stellar populations and cluster analysis is necessary to refine our understanding of galaxy formation and evolution [5]. This includes examining the metallicity, age distributions, and kinematic properties of stars within these systems, as well as the structural characteristics of their host clusters. Such analyses can provide critical constraints on the formation mechanisms of MQGs and their role in the broader context of cosmic structure formation. Additionally, they help in identifying potential discrepancies between observations and theoretical models, guiding future research and improving our comprehension of the early Universe.",
      "stats": {
        "char_count": 2407,
        "word_count": 342,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "3.3.1 Exoplanet Detection and Characterization",
      "level": 3,
      "content": "The detection and characterization of exoplanets have advanced significantly with the development of high-precision observational techniques and space-based missions. Transit photometry remains one of the most effective methods, allowing for the identification of planetary candidates by measuring periodic dips in stellar brightness. This method has been instrumental in discovering thousands of exoplanets, particularly through missions like Kepler and TESS. However, it is limited to detecting planets with orbital planes aligned with the observer's line of sight. Radial velocity measurements, which track stellar motion induced by planetary gravitational pull, complement transit data by providing mass estimates and orbital parameters. These techniques, when combined, enable a more comprehensive understanding of exoplanetary systems, including their orbital architectures and potential habitability.\n\nModern exoplanet characterization extends beyond basic detection to include atmospheric analysis and the study of planetary magnetic fields [6]. Observations of stellar chromospheric lines and X-ray emission can reveal interactions between exoplanets and their host stars, offering insights into planetary magnetic properties [6]. Additionally, the study of transiting exoplanet atmospheres through transmission spectroscopy has enabled the detection of molecular species and the characterization of atmospheric composition. These methods are critical for assessing the potential for life and understanding the diversity of exoplanetary environments. The integration of multi-wavelength observations, including radio and infrared, further enhances the ability to probe planetary atmospheres and detect signs of planetary activity.\n\nRecent advancements in instrumentation, such as the James Webb Space Telescope (JWST), have revolutionized exoplanet studies by providing unprecedented sensitivity and resolution. JWST's capabilities allow for the detection of faint signals from exoplanetary systems, including the study of their thermal emission and atmospheric composition. This has led to the identification of new exoplanet types and the refinement of existing models. Furthermore, the development of direct imaging techniques and high-contrast spectroscopy has expanded the range of detectable exoplanets, particularly those in wide orbits. These innovations continue to shape the field, enabling more detailed and accurate characterizations of exoplanets and their host systems.",
      "stats": {
        "char_count": 2492,
        "word_count": 324,
        "sentence_count": 16,
        "line_count": 5
      }
    },
    {
      "heading": "3.3.2 AGN Activity and Spectral Modeling",
      "level": 3,
      "content": "AGN activity plays a central role in shaping the evolution of galaxies, particularly in high-redshift environments where the interplay between supermassive black holes and their host galaxies becomes more pronounced [7]. Spectral modeling of AGN is essential for disentangling the contributions of accretion processes, star formation, and dust emission, especially in the context of the early Universe [8]. Observational data from the James Webb Space Telescope (JWST) have revealed a population of massive, quenched galaxies at $z > 3$ with stellar masses exceeding $10^{10} M_{\\odot}$, yet showing negligible star formation [4]. These objects challenge conventional models of galaxy formation, as their existence implies either unusually high star formation efficiencies or alternative mechanisms, such as AGN feedback, that rapidly quench star formation. Spectral energy distribution (SED) modeling of such systems is crucial for understanding the relative contributions of AGN and stellar emission, particularly in the mid-infrared, where AGN tori and dust emission dominate.\n\nSpectral modeling of AGN involves a combination of empirical and theoretical approaches, including photoionization models, radiative transfer simulations, and SED fitting techniques [8]. These methods are used to infer key properties such as black hole mass, accretion rate, and the geometry of the surrounding torus. For high-redshift AGN, the challenge is compounded by the limited wavelength coverage and the need to account for redshifted features. The use of instruments like the Near-Infrared Spectrograph (NIRSpec) on JWST allows for the detection of emission lines and continuum features that are critical for distinguishing AGN from star-forming galaxies [3]. Additionally, the presence of broad and narrow emission lines, along with the shape of the continuum, provides insights into the physical conditions of the AGN environment, including the presence of outflows and the nature of the host galaxy.\n\nRecent advances in spectral modeling have enabled more accurate characterization of AGN activity in the early Universe, particularly in the context of the so-called \"canyon\" feature observed in the mid-infrared color space of galaxy clusters [7]. This feature suggests a rapid transition from star-forming to quiescent states, which may be driven by AGN feedback mechanisms. The application of detailed photoionization models and radiative transfer simulations has provided new constraints on the properties of AGN, including their luminosity, black hole mass, and the density of the surrounding gas [8]. These models are essential for interpreting the observed spectral features and for testing theoretical predictions about the co-evolution of AGN and their host galaxies [7]. As observational data continue to improve, spectral modeling will remain a vital tool for unraveling the complex processes that govern AGN activity and galaxy evolution.",
      "stats": {
        "char_count": 2942,
        "word_count": 424,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "4.1.1 Hyperfine Interactions and Decoherence",
      "level": 3,
      "content": "Hyperfine interactions arise from the coupling between nuclear spins and electron spins, playing a critical role in determining the coherence properties of quantum systems. In systems with nuclear spin $ I = 1/2 $, each electronic level splits into two hyperfine-structure (HFS) states, simplifying the spectroscopic fingerprint and enabling precise control over qubit initialization and manipulation. The HFS shift, determined by the formula $ \\Delta \\nu_F = \\frac{A}{2} [9] $, reflects the interaction strength between the nuclear and electronic angular momenta. These interactions are particularly significant in quantum memory and spin qubit systems, where they influence the lifetime and stability of quantum states. The coupling tensor, which encapsulates the anisotropic nature of hyperfine interactions, is essential for modeling and optimizing the performance of such systems.\n\nDecoherence, driven by hyperfine interactions, is a major obstacle in maintaining quantum coherence over extended periods. The coupling between nuclear and electronic spins introduces random phase fluctuations, leading to dephasing and loss of quantum information. This effect is exacerbated in systems with a large number of nuclear spins, as the cumulative interaction leads to a complex and unpredictable environment. Strategies to mitigate hyperfine-induced decoherence include encoding in decoherence-free subspaces, utilizing spectator qubits, and employing dynamic decoupling techniques [10]. These methods aim to suppress the influence of nuclear spin fluctuations, thereby extending the coherence time of quantum states. Additionally, the directional dependence of hyperfine couplings allows for tunable control, enabling the optimization of system parameters to minimize decoherence effects.\n\nThe interplay between hyperfine interactions and decoherence is a central theme in the study of quantum systems, particularly in the context of spin qubits and quantum memory. The anisotropic nature of hyperfine couplings necessitates detailed modeling to predict and control the dynamics of quantum states. In systems where the nuclear spin is $ I = 1/2 $, the reduced number of HFS states simplifies the spectroscopic analysis, but the underlying interactions remain a key factor in determining the system's performance. Understanding and mitigating the effects of hyperfine interactions is crucial for the development of robust quantum technologies, as it directly impacts the fidelity and stability of quantum operations. This section highlights the fundamental role of hyperfine interactions in shaping the coherence properties of quantum systems and the ongoing efforts to overcome their challenges.",
      "stats": {
        "char_count": 2695,
        "word_count": 376,
        "sentence_count": 16,
        "line_count": 5
      }
    },
    {
      "heading": "4.1.2 Spin Control and Manipulation",
      "level": 3,
      "content": "Spin control and manipulation in two-dimensional magnetic systems have emerged as a critical area of research, driven by the need for scalable and controllable quantum devices. In materials such as 2H-RuC$_2$, the interplay between spin, valley, and orbital degrees of freedom enables unique opportunities for manipulating magnetic states through external stimuli. The intrinsic valley Zeeman splitting and out-of-plane magnetization provide a platform for valley-polarized spin states, which can be tuned via electric or magnetic fields. This control is essential for realizing valleytronic and spintronic applications, where the ability to switch and read spin states with high fidelity is paramount. Theoretical models based on density functional theory and effective Hamiltonians have been instrumental in predicting the spin dynamics and response to external perturbations in such systems.\n\nThe manipulation of spin states in these materials often involves the use of optical and electromagnetic techniques, such as circularly polarized light, which can induce transitions between spin states through valley-selective excitation. The absorption of right-circularly polarized light at the energy gap $2|m|$ depends on the sign of $v_x v_y m$, offering a means to control spin polarization through light-matter interactions. This phenomenon is closely tied to the topological properties of the system, where the sign of the mass term determines the valley Chern number and hence the response to electromagnetic radiation. Additionally, the presence of bound states and multi-spinon excitations introduces complex dynamics that can be harnessed for quantum information processing, such as in the realization of non-Abelian anyons for topological quantum computation.\n\nRecent advances in experimental techniques, including time-resolved spectroscopy and ultrafast pump-probe measurements, have enabled detailed investigations into spin relaxation and coherent control mechanisms. These studies reveal the role of spin-orbit coupling, magnetic anisotropy, and interfacial effects in determining the lifetime and coherence of spin states. The development of novel materials with tailored spin properties, such as those exhibiting fractional quantum Hall-like behavior without external magnetic fields, further expands the possibilities for spin-based quantum technologies. As research progresses, the integration of spin control strategies with topological and correlated electron systems promises to unlock new paradigms in quantum information science.",
      "stats": {
        "char_count": 2552,
        "word_count": 347,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "4.2.1 Band Structure and Transport Phenomena",
      "level": 3,
      "content": "The band structure of two-dimensional (2D) Dirac materials plays a central role in determining their electronic and transport properties. When inversion and time-reversal symmetries are broken, these materials exhibit unique characteristics that align with theoretical models, particularly when the incident light frequency approaches the energy gap. The energy bands of such systems depend on the magnitudes of the Dirac mass and velocities, but the electronic eigenstates are highly sensitive to their signs [11]. This sensitivity leads to distinct band-geometric properties, such as the Berry curvature distribution in the valence and conduction bands, which significantly influence the material's topological and optical responses. Understanding these band features is essential for predicting and interpreting the material's behavior under various external conditions.\n\nTransport phenomena in 2D Dirac materials are closely tied to their band structure and the associated geometric properties. The Raman response, for instance, is influenced by the signs of the Dirac mass and velocities, as demonstrated by theoretical predictions of specific symmetries in the Raman tensor components. These findings highlight the importance of considering both the magnitude and sign of material parameters when analyzing transport and optical properties. Additionally, the interplay between the Dirac mass, velocities, and external fields can lead to non-trivial effects, such as modified conductivity and quantized Hall responses, which are critical for applications in spintronics and quantum computing.\n\nThe study of band structure and transport in 2D Dirac materials also extends to the effects of proximity coupling and interface transparency in heterostructures. For example, in FCISC junctions involving MoTe₂ and NbSe₂, the induced superconducting gaps and their interplay with the intrinsic band structure influence the system's transport behavior. These effects are further complicated by the presence of fractional quantum Hall states and the emergence of non-Abelian statistics in certain configurations. Overall, a comprehensive understanding of the band structure and its impact on transport is vital for both fundamental research and the development of novel electronic and quantum devices.",
      "stats": {
        "char_count": 2297,
        "word_count": 320,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "4.2.2 Polaritonic and Coherent Systems",
      "level": 3,
      "content": "Polaritonic and coherent systems represent a critical area of research at the intersection of quantum optics, condensed matter physics, and nonlinear dynamics. These systems arise from the strong coupling between light and matter, leading to hybrid quasiparticles known as polaritons, which exhibit both photonic and electronic characteristics. In such systems, the interplay between coherent light-matter interactions and the underlying material properties gives rise to novel phenomena, including coherent oscillations, superradiance, and nontrivial band structures. The study of these systems often involves advanced spectroscopic techniques, such as two-dimensional coherent spectroscopy, which can probe higher-order response functions and reveal dynamical information not accessible through linear response methods [12]. This capability is particularly valuable in complex quantum systems where traditional approaches may fall short.\n\nTheoretical and experimental investigations into polaritonic systems have highlighted the importance of phase coherence and the role of symmetry in shaping the system's response. For instance, the phase differences between Raman tensor elements, though not directly observable, play a crucial role in determining the overall optical response and selection rules. These phase effects become particularly significant when considering more realistic models of electronic structure, as opposed to simplified low-energy effective theories. The inclusion of realistic band structures and interactions can lead to the emergence of new coherent phenomena, such as the formation of bound states or the modification of existing selection rules. This suggests that the behavior of polaritonic systems is highly sensitive to the underlying electronic and structural details, necessitating a more comprehensive theoretical framework.\n\nMoreover, the integration of superconducting proximity effects with fractional quantum Hall systems has opened new avenues for exploring exotic states of matter, such as fractional topological superconductors [13]. These systems combine the robustness of topological order with the unique properties of superconductivity, offering a platform for realizing non-Abelian anyons. The coherence of such systems is essential for maintaining the topological protection of these quasiparticles, making the study of polaritonic and coherent effects a central theme in the development of next-generation quantum devices. By leveraging the interplay between light, matter, and topology, researchers are paving the way for novel quantum technologies with potential applications in quantum computing and information processing.",
      "stats": {
        "char_count": 2677,
        "word_count": 356,
        "sentence_count": 14,
        "line_count": 5
      }
    },
    {
      "heading": "4.3.1 Many-body and Nonlinear Dynamics",
      "level": 3,
      "content": "The section on many-body and nonlinear dynamics explores the complex interplay of interactions and collective behavior in quantum systems, particularly in the context of topological phases and strongly correlated materials. These systems exhibit rich dynamics due to the presence of multiple particles, which can lead to emergent phenomena such as fractionalized excitations, collective oscillations, and nontrivial correlations. Theoretical frameworks, such as the tight-binding model and first-principles calculations, are employed to capture the essential features of these interactions, including the role of symmetry breaking and the emergence of nontrivial band structures. The inclusion of nonlinear effects, such as those arising from higher-order perturbations and external fields, further enriches the dynamical landscape, necessitating advanced analytical and numerical techniques for their description.\n\nNonlinear dynamics in these systems often manifest through the response to external stimuli, such as electromagnetic fields or strain, leading to phenomena like harmonic generation, parametric amplification, and nonlinear transport. These effects are critical for understanding the behavior of materials under extreme conditions and for developing novel quantum devices. The study of such dynamics is further complicated by the presence of many-body correlations, which can significantly alter the system's response compared to single-particle predictions. The interplay between nonlinear effects and many-body interactions is particularly pronounced in systems with strong coupling, such as those found in fractional Chern insulators and superconducting heterostructures, where the resulting dynamics can give rise to exotic quantum states.\n\nThe analysis of many-body and nonlinear dynamics also involves the derivation of effective theories that capture the essential physics at different energy scales. These theories often rely on renormalization group techniques, K-matrix formalisms, and conformal field theory to describe the low-energy behavior and critical phenomena. By incorporating superconducting and backscattering perturbations, researchers can explore how these systems evolve under various conditions and identify the emergence of topological order. Such studies are essential for advancing our understanding of quantum materials and for guiding the design of future quantum technologies based on these principles.",
      "stats": {
        "char_count": 2447,
        "word_count": 323,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "4.3.2 Topological and Quantum Simulation",
      "level": 3,
      "content": "Topological and quantum simulation have emerged as critical tools for exploring exotic quantum phases and realizing fault-tolerant quantum computation. These simulations leverage the interplay between topology and quantum coherence to model systems with non-trivial band structures, such as topological insulators and superconductors. By incorporating symmetry-breaking terms and non-Abelian anyons, researchers can investigate the robustness of topological invariants under various perturbations. The study of such systems often involves the calculation of topological invariants like the Chern number, which depends on the signs of the Dirac mass and velocities, providing a direct link between microscopic parameters and macroscopic properties. These simulations are essential for understanding how topological protection manifests in real materials and how it can be harnessed for quantum information processing.\n\nRecent advances in theoretical frameworks have enabled more accurate and realistic simulations of topological systems, moving beyond simplified low-energy models. First-principles calculations on materials such as monolayer 2H-RuC₂ have provided deeper insights into the electronic structure and its impact on topological properties. These methods account for the full complexity of the band structure, including spin-orbit coupling and electron-electron interactions, which are crucial for capturing the true behavior of topological phases. Additionally, the development of efficient numerical techniques, such as the tight-binding model with broken inversion and time-reversal symmetries, has allowed for the precise calculation of quantities like the Raman tensor, which can reveal important information about the system's symmetry and topology.\n\nQuantum simulation also plays a pivotal role in the realization of universal topological quantum computation, particularly through the braiding of non-Abelian anyons [13]. This approach eliminates the need for magic state distillation, offering a more efficient pathway for fault-tolerant quantum operations. Theoretical studies have explored the edge theories of such systems, mapping them to parafermion conformal field theories, and have proposed architectures for implementing quantum computations using Fibonacci anyons. These developments highlight the potential of topological quantum simulation not only for fundamental research but also for the design of next-generation quantum technologies.",
      "stats": {
        "char_count": 2469,
        "word_count": 323,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "5.1.1 End-to-end Data Movement Analysis",
      "level": 3,
      "content": "End-to-end data movement analysis is critical in understanding the flow and efficiency of information within complex systems, particularly in environments where multiple components interact through APIs, tools, and services. This analysis involves tracing data from its origin through various processing stages, including model execution, tool invocation, and network transmission. The primary goal is to identify bottlenecks, latency sources, and inefficiencies that may hinder performance. By examining the entire data lifecycle, researchers and practitioners can optimize system design, improve resource allocation, and enhance overall throughput. This section explores the methodologies and challenges involved in capturing and analyzing end-to-end data movement, with a focus on real-world scenarios and multi-turn interactions.\n\nThe complexity of end-to-end data movement is amplified by the integration of diverse components such as large language models, external tools, and networked services. Each step in the data flow introduces potential delays, dependencies, and variability, making it difficult to predict or control performance. For instance, tool latencies can vary significantly based on external factors, while model execution times may be affected by computational load and resource availability. Additionally, the interaction between different system layers—such as the model, host, and network—requires careful coordination to ensure data integrity and consistency. This section delves into the technical challenges of tracking and measuring data movement across these layers, emphasizing the need for robust monitoring and analysis techniques.\n\nTo address these challenges, various frameworks and methodologies have been developed to capture and analyze end-to-end data movement. These include performance benchmarks, trace analysis, and latency profiling, which provide insights into system behavior under different workloads. The integration of these tools enables a comprehensive view of data flow, highlighting areas for optimization and improvement. Furthermore, the analysis of data movement is essential for evaluating the effectiveness of proposed solutions, such as multi-path transfer techniques and model execution strategies. By focusing on end-to-end performance, this section aims to provide a foundational understanding of data movement dynamics and their impact on system efficiency.",
      "stats": {
        "char_count": 2422,
        "word_count": 323,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "5.1.2 Operational Resilience Metrics",
      "level": 3,
      "content": "Operational resilience metrics are critical for evaluating the robustness and reliability of systems under stress, particularly in environments where failures or attacks can have cascading effects. These metrics typically encompass a range of quantitative and qualitative measures, including system uptime, recovery time, error rates, and the ability to maintain functionality during disruptions. In the context of agent-based systems, operational resilience is further complicated by the interplay between orchestration layers, tool APIs, and external services. The resilience of such systems is often determined by how effectively they can detect, isolate, and recover from faults, especially when black-box services are treated as trusted components. This necessitates a structured approach to metric design that accounts for both internal and external failure modes.\n\nThe evaluation of operational resilience involves a combination of empirical testing, simulation, and theoretical modeling to capture the dynamic nature of system behavior [14]. Metrics such as defense success rate and task success rate provide insights into the balance between system performance and security, as demonstrated in benchmarking studies where models exhibit trade-offs between these dimensions. Additionally, operational resilience metrics must consider the complexity introduced by multi-turn interactions, distributed architectures, and the potential for compounding vulnerabilities. These factors require the development of scalable and adaptable metrics that can accurately reflect the system's ability to withstand and recover from various types of disruptions, including those arising from tool poisoning or parameter manipulation.\n\nTo ensure the effectiveness of operational resilience metrics, they must be grounded in practical scenarios and validated through real-world case studies [14]. This includes analyzing the impact of system configurations, service dependencies, and operational practices on resilience outcomes. Metrics should also be designed to support decision-making at different levels, from granular technical assessments to high-level executive summaries. By integrating resilience metrics into the system design and operational workflows, organizations can proactively identify weaknesses, optimize resource allocation, and enhance the overall reliability of their systems in the face of evolving threats and uncertainties.",
      "stats": {
        "char_count": 2438,
        "word_count": 322,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "5.2.1 Proof-based Authentication Frameworks",
      "level": 3,
      "content": "Proof-based authentication frameworks leverage formal verification and cryptographic proofs to ensure the integrity and authenticity of agent computations [15]. These frameworks are particularly relevant in environments where trust in the execution environment is limited, such as in decentralized or adversarial settings. By constructing mathematical proofs that validate the correctness of computations, these systems enable remote verification of an agent's behavior without requiring full transparency of its internal state. This approach is especially valuable in scenarios where the computation must be authenticated without exposing sensitive data or proprietary algorithms. The use of proof systems allows for a high degree of assurance, as the correctness of the computation is derived from formal logic rather than relying solely on the trustworthiness of the execution environment.\n\nIn the context of agent-based systems, proof-based authentication frameworks often integrate with verifiable computation techniques, such as zero-knowledge proofs or succinct non-interactive arguments of knowledge (SNARKs). These methods enable agents to demonstrate that they have executed specific tasks correctly, while maintaining privacy over the inputs and intermediate steps. This is particularly useful in applications like secure multi-party computation, where multiple parties collaborate without revealing their private data. The framework typically involves a prover that generates a proof of computation and a verifier that checks the proof's validity. The efficiency and scalability of these systems depend on the underlying proof system and the complexity of the computations being verified, making them well-suited for scenarios where computational integrity is paramount.\n\nThe integration of proof-based authentication into agent systems also raises important considerations regarding performance, scalability, and ease of deployment. While these frameworks provide strong security guarantees, they can introduce additional computational overhead and complexity. To address these challenges, researchers have explored optimizations such as proof composition, parallel verification, and efficient proof generation techniques. These advancements aim to make proof-based authentication more practical for real-world applications, particularly in environments where agents operate in constrained or dynamic settings [15]. Overall, proof-based authentication frameworks represent a critical component in the broader landscape of secure and trustworthy agent systems.",
      "stats": {
        "char_count": 2572,
        "word_count": 335,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "5.2.2 Secure Agent Execution and Workloads",
      "level": 3,
      "content": "Secure agent execution and workloads represent a critical area of focus in the development of trustworthy and autonomous systems. As agents become more integrated into complex, distributed environments, ensuring the integrity and confidentiality of their execution becomes paramount. This section explores the mechanisms and challenges involved in securing agent execution, emphasizing the need for verifiable execution traces and robust authentication layers. Techniques such as VET (Verifiable Execution Traces) introduce compositional authentication frameworks that bind agent outputs to execution traces consistent with predefined trust assumptions, thereby mitigating risks of tampering and impersonation. These approaches are essential for maintaining the reliability of agents in environments where execution is mediated by untrusted hosts, ensuring that outputs remain authentic and verifiable.\n\nThe evaluation of secure agent execution involves assessing the overhead introduced by authentication mechanisms, particularly in realistic workloads involving API-based LLM agents [15]. Web Proofs, for instance, demonstrate that authentication can be achieved with modest overhead, typically below three times that of direct API calls, especially in scenarios where transcript privacy is beneficial [15]. This is crucial for maintaining performance while ensuring security. However, the integration of such mechanisms into existing frameworks presents challenges, including the need for efficient synchronization, dynamic scheduling, and minimal latency. The complexity of multi-step reasoning and coordination across multiple servers further complicates the secure execution of agent workloads, necessitating optimized strategies for handling tool-heavy and resource-intensive tasks.\n\nAddressing these challenges requires a combination of theoretical analysis and practical implementation. Techniques such as speculative tool execution offer promising solutions by breaking the sequential nature of tool calls and reducing overhead in agent workflows [16]. Additionally, advancements in intra-server memory access, such as MMA, provide new avenues for optimizing data transfer and maintaining execution integrity. These developments underscore the importance of designing secure, efficient, and scalable agent execution environments that can withstand evolving threats while supporting complex, real-world applications. The ongoing refinement of these methods is essential for achieving host-independent autonomy and ensuring the trustworthiness of agent-based systems [15].",
      "stats": {
        "char_count": 2580,
        "word_count": 329,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "5.3.1 Multi-Component Attack Classification",
      "level": 3,
      "content": "Multi-component attack classification (MCP) involves the systematic categorization of attacks that exploit multiple components within an AI-driven system, such as large language models (LLMs) and their associated infrastructure. These attacks often span the server, host, and user levels, leveraging vulnerabilities in the orchestration layer, tool APIs, and underlying system components. A unified taxonomy of 20 MCP attack types has been developed to consolidate prior work and clarify key attack categories, enabling more structured analysis and defense strategies [17]. This classification is critical for understanding how different components interact and where potential weaknesses may arise, particularly in environments where inference and tool APIs are treated as trusted black-box services.\n\nThe evaluation of MCP attacks requires benchmarks that reflect realistic scenarios and multi-turn reasoning workflows [17]. MCP-SafetyBench, built upon the MCP-Universe benchmark, addresses this need by providing tasks that simulate real-world conditions across five domains. This benchmark supports comprehensive safety evaluations, enabling researchers to assess the robustness of LLM agents against a wide range of multi-component threats [17]. By incorporating real-world MCP servers and structured attack scenarios, MCP-SafetyBench facilitates the development of more resilient AI systems and helps identify gaps in current defense mechanisms.\n\nRecent studies highlight the importance of analyzing attacks that exploit the orchestration layer, such as those involving prompt manipulation, tool wiring, and glue code vulnerabilities. These attacks often go undetected due to the opaque nature of black-box APIs and the complexity of multi-component interactions. The identification of notarized TLS transcript proofs as a potential defense mechanism underscores the need for compositional approaches in securing distributed AI systems. As the threat landscape evolves, continuous refinement of attack classifications and evaluation frameworks remains essential for ensuring the safety and reliability of AI-driven infrastructures.",
      "stats": {
        "char_count": 2137,
        "word_count": 283,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "5.3.2 Real-world Robustness Evaluation",
      "level": 3,
      "content": "The section on real-world robustness evaluation focuses on assessing the resilience of large language model (LLM) agents in practical deployment scenarios. Given that many agent systems rely on third-party black-box APIs for inference and tool execution, the orchestration layer on the host becomes a critical attack surface [15]. This evaluation framework aims to systematically test how well agents can withstand malicious manipulations across different components, including the server, host, and user interfaces. By simulating real-world conditions, the assessment ensures that agents can maintain functionality and safety even when faced with adversarial inputs or compromised external services.\n\nMCP-SafetyBench serves as a comprehensive benchmark for evaluating agent robustness against multi-component attacks (MCP) [17]. It spans five key domains—browser automation, financial analysis, location navigation, repository management, and web search—each with distinct attack vectors. The benchmark includes 20 attack types, capturing the multi-turn nature of real-world interactions. Through systematic evaluations, it reveals that no model achieves both high task success and strong defense, highlighting the trade-off between performance and safety. This insight underscores the need for more robust and adaptive agent architectures that can balance these competing objectives.\n\nThe evaluation also highlights the challenges in deploying safe and reliable agent systems. Issues such as tool execution latency, incorrect ordering of operations, and the inability to enforce dependency constraints in distributed environments complicate robustness. Furthermore, the lack of visibility into network and infrastructure states limits the effectiveness of load-balancing and fault-tolerance mechanisms. These findings emphasize the importance of designing agent systems with built-in resilience, transparent orchestration, and enhanced monitoring capabilities to ensure reliable performance in complex, real-world settings.",
      "stats": {
        "char_count": 2025,
        "word_count": 263,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "6 Future Directions",
      "level": 1,
      "content": "Despite significant advances in the development and application of organoid-based infection models, several limitations and gaps remain that hinder their broader utility and translational potential. One major limitation is the limited scalability and reproducibility of organoid cultures, which can vary significantly between different batches and sources. This variability complicates the standardization of experiments and the generation of consistent, reliable results. Additionally, the current organoid systems often lack the full complexity of native tissues, including the integration of immune cells, vasculature, and microbial communities, which are essential for accurately modeling host-pathogen interactions. Furthermore, the development of advanced genetic and functional tools to manipulate and monitor organoid systems in real time remains an ongoing challenge, restricting the depth of mechanistic insights that can be obtained.\n\nTo address these limitations, future research should focus on improving the consistency and scalability of organoid generation through the development of standardized protocols and the use of bioreactors or microfluidic systems that enable large-scale production. Additionally, efforts should be directed toward integrating more diverse cell types, such as immune cells and endothelial cells, to better recapitulate the complexity of in vivo tissues. The development of more sophisticated genetic engineering tools, including CRISPR-based screens and single-cell omics techniques, will also be critical for enhancing the functional characterization of organoids. Furthermore, the integration of organoid models with advanced imaging and computational modeling approaches will enable more precise and dynamic analysis of pathogen-host interactions, providing deeper insights into disease mechanisms.\n\nThe proposed future work has the potential to significantly advance the field of infection biology by enabling more accurate and physiologically relevant models for studying host-pathogen interactions. Enhanced organoid systems with improved reproducibility and complexity will facilitate the discovery of novel therapeutic targets and the development of more effective antiviral and antibacterial strategies. Moreover, the integration of multi-omics and computational approaches will allow for a systems-level understanding of infection processes, which is essential for addressing complex diseases and emerging pathogens. By overcoming current limitations and expanding the capabilities of organoid-based models, future research can contribute to the development of personalized medicine, precision diagnostics, and innovative treatment options, ultimately improving public health outcomes.",
      "stats": {
        "char_count": 2738,
        "word_count": 352,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "7 Conclusion",
      "level": 1,
      "content": "The survey paper has provided a comprehensive analysis of the application of organoids as host models for infection biology, highlighting their potential to revolutionize the study of host-pathogen interactions. Organoids, as three-dimensional in vitro models derived from stem cells, offer a physiologically relevant system that closely mimics the complexity of native tissues. This has enabled researchers to investigate pathogen entry, replication, and immune responses in a context that is more representative of in vivo conditions than traditional two-dimensional cell cultures. The review has outlined the methodologies for generating and characterizing organoids, discussed the range of pathogens studied using these models, and identified key challenges such as scalability, reproducibility, and the need for advanced genetic and functional tools. These findings underscore the transformative potential of organoid-based infection models in advancing our understanding of infectious diseases and in facilitating the development of novel therapeutic strategies.\n\nThe significance of this survey lies in its systematic synthesis of current knowledge, which serves as a critical resource for researchers, clinicians, and policymakers in the field of infection biology. By identifying research gaps and emerging trends, the paper provides a roadmap for future investigations and technological innovations. The insights presented here are not only relevant to basic research but also have direct implications for translational applications, including drug discovery and personalized medicine. The growing body of literature on organoid-based infection models demonstrates their increasing importance in addressing complex biological questions, and this survey contributes to the broader effort of consolidating and disseminating this knowledge. The paper emphasizes the need for interdisciplinary collaboration and continued investment in the development of more sophisticated organoid systems to overcome existing limitations and unlock new possibilities in infection research.\n\nIn conclusion, the study of organoids as infection models represents a promising frontier in biomedical research, with the potential to significantly impact our understanding of infectious diseases and the development of effective interventions. While challenges remain, the progress made in this field highlights the importance of continued exploration and innovation. Researchers are encouraged to build upon the findings presented here, integrating new technologies and methodologies to enhance the utility and applicability of organoid models. By fostering a collaborative and forward-thinking approach, the scientific community can further advance the use of organoids in infection biology, ultimately contributing to improved public health outcomes and more effective therapeutic strategies.",
      "stats": {
        "char_count": 2880,
        "word_count": 384,
        "sentence_count": 14,
        "line_count": 5
      }
    }
  ],
  "references": [
    {
      "text": "[1] Unknown PDF",
      "number": null,
      "title": "unknown pdf"
    },
    {
      "text": "[3] The AGN nature of strong CIII emitters in the Early Universe with JWST",
      "number": null,
      "title": "the agn nature of strong ciii emitters in the early universe with jwst"
    },
    {
      "text": "[4] Unveiling the properties and origin of massive quenched galaxies at $z ge2$ in the COLIBRE hydrodyna",
      "number": null,
      "title": "unveiling the properties and origin of massive quenched galaxies at $z ge2$ in the colibre hydrodyna"
    },
    {
      "text": "[5] Galaxy evolution in compact groups - III. Structural analysis of galaxies and dynamical state of non",
      "number": null,
      "title": "galaxy evolution in compact groups - iii"
    },
    {
      "text": "[6] A Radio Search for Star-Planet Interaction in TOI-540 and SPECULOOS-3",
      "number": null,
      "title": "a radio search for star-planet interaction in toi-540 and speculoos-3"
    },
    {
      "text": "[7] Connecting current and future dual AGN searches to LISA and PTA gravitational wave detections",
      "number": null,
      "title": "connecting current and future dual agn searches to lisa and pta gravitational wave detections"
    },
    {
      "text": "[8] From ASTRID to BRAHMA -- The role of overmassive black holes in little red dots in cosmological simu",
      "number": null,
      "title": "from astrid to brahma -- the role of overmassive black holes in little red dots in cosmological simu"
    },
    {
      "text": "[10] Silicon T centre hyperfine structure and memory protection schemes",
      "number": null,
      "title": "silicon t centre hyperfine structure and memory protection schemes"
    },
    {
      "text": "[11] Tight-binding and density-functional study of the Raman tensor in two-dimensional massive Dirac ferm",
      "number": null,
      "title": "tight-binding and density-functional study of the raman tensor in two-dimensional massive dirac ferm"
    },
    {
      "text": "[12] Two-dimensional coherent spectroscopy of CoNb$ 2$O$ 6$",
      "number": null,
      "title": "two-dimensional coherent spectroscopy of conb$ 2$o$ 6$"
    },
    {
      "text": "[13] Engineering Fractional Topological Superconductors  Numerical Bogoliubov-de Gennes Analysis for Para",
      "number": null,
      "title": "engineering fractional topological superconductors numerical bogoliubov-de gennes analysis for para"
    },
    {
      "text": "[14] Assessing Resilience in Authoritative DNS Infrastructure Supporting Government Services",
      "number": null,
      "title": "assessing resilience in authoritative dns infrastructure supporting government services"
    },
    {
      "text": "[15] VET Your Agent  Towards Host-Independent Autonomy via Verifiable Execution Traces",
      "number": null,
      "title": "vet your agent towards host-independent autonomy via verifiable execution traces"
    },
    {
      "text": "[16] Optimizing Agentic Language Model Inference via Speculative Tool Calls",
      "number": null,
      "title": "optimizing agentic language model inference via speculative tool calls"
    },
    {
      "text": "[17] MCP-SafetyBench  A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Serv",
      "number": null,
      "title": "mcp-safetybench a benchmark for safety evaluation of large language models with real-world mcp serv"
    }
  ],
  "metadata": {
    "source_file": "results\\original\\InteractiveSurvey\\Biology\\survey_Organoids as Host Models for Infection Biology_split.json",
    "processed_date": "2025-12-30T20:33:39.080563",
    "config": {
      "normalize_outline": true,
      "normalize_content": true,
      "normalize_references": true
    }
  }
}