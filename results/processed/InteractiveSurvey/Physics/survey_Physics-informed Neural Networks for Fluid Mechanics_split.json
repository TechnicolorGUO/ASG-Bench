{
  "outline": [
    [
      1,
      "A Survey of Physics-informed Neural Networks for Fluid Mechanics"
    ],
    [
      1,
      "1 Abstract"
    ],
    [
      1,
      "2 Introduction"
    ],
    [
      1,
      "3 Physics-Informed Deep Learning Architectures"
    ],
    [
      2,
      "3.1 Architecture Design with Physical Constraints"
    ],
    [
      3,
      "3.1.1 Integration of Domain Knowledge in Neural Frameworks"
    ],
    [
      3,
      "3.1.2 Self-Supervised Optimization for Local Adaptation"
    ],
    [
      2,
      "3.2 Hybrid Modeling with Multi-Stage and Physics-Embedded Networks"
    ],
    [
      3,
      "3.2.1 Multi-Stage Neural Networks for Complex Dynamics"
    ],
    [
      3,
      "3.2.2 Embedding of Physical Laws in Learning Processes"
    ],
    [
      2,
      "3.3 End-to-End Learning with Physical Consistency"
    ],
    [
      3,
      "3.3.1 Joint Optimization of Sensitivity and Parameter Estimation"
    ],
    [
      3,
      "3.3.2 Differentiable Physics for Inverse Design"
    ],
    [
      1,
      "4 Data-Driven Physical Modeling"
    ],
    [
      2,
      "4.1 Neural Network Architectures for PDE Solutions"
    ],
    [
      3,
      "4.1.1 Activation Function Impact on Convergence"
    ],
    [
      3,
      "4.1.2 Gradient Normalization and Precision Optimization"
    ],
    [
      2,
      "4.2 Physics-Informed Learning for Inverse Problems"
    ],
    [
      3,
      "4.2.1 Fault Estimation and Control with PINNs"
    ],
    [
      3,
      "4.2.2 Reconstruction of Physical Systems from Boundary Data"
    ],
    [
      2,
      "4.3 Parameter Identification and Model Enhancement"
    ],
    [
      3,
      "4.3.1 Inverse Problems in Elasticity and Rheology"
    ],
    [
      3,
      "4.3.2 Conservation Laws in Neural Training"
    ],
    [
      1,
      "5 Graph and Neural Network Integration for Scientific Simulation"
    ],
    [
      2,
      "5.1 Graph-Based Generative Models for Physical Systems"
    ],
    [
      3,
      "5.1.1 Multiscale GNNs for Flow Field Generation"
    ],
    [
      3,
      "5.1.2 Hypergraph Convolutions for Conservation Laws"
    ],
    [
      2,
      "5.2 Neural Network Augmentation of Numerical Solvers"
    ],
    [
      3,
      "5.2.1 Differentiable Solvers with Neural Correction"
    ],
    [
      3,
      "5.2.2 Surrogate Modeling for Complex Dynamics"
    ],
    [
      2,
      "5.3 Graph Construction and Feature Engineering"
    ],
    [
      3,
      "5.3.1 Systematic Framework for Graph-Based ML"
    ],
    [
      3,
      "5.3.2 Comparative Analysis with Traditional Methods"
    ],
    [
      1,
      "6 Future Directions"
    ],
    [
      1,
      "7 Conclusion"
    ],
    [
      1,
      "References"
    ]
  ],
  "content": [
    {
      "heading": "A Survey of Physics-informed Neural Networks for Fluid Mechanics",
      "level": 1,
      "content": "",
      "stats": {
        "char_count": 0,
        "word_count": 0,
        "sentence_count": 0,
        "line_count": 1
      }
    },
    {
      "heading": "1 Abstract",
      "level": 1,
      "content": "The integration of physical principles into machine learning has emerged as a critical approach to enhance the accuracy, interpretability, and reliability of models in scientific computing, particularly in fluid mechanics. This survey paper provides a comprehensive overview of the application of Physics-Informed Neural Networks (PINNs) to fluid dynamics, focusing on their design, training strategies, and performance in modeling complex fluid behaviors. The paper highlights key developments, including the integration of physical laws into neural architectures, the use of hybrid models combining data-driven and physics-based components, and the application of self-supervised optimization techniques to improve local adaptability. It also discusses the role of multi-stage neural networks in capturing complex dynamics and the use of differentiable physics for inverse design. The contributions of this survey include a structured review of current methodologies, an analysis of how physical constraints improve model reliability, and an identification of open challenges and future research directions. By synthesizing recent advancements and offering a critical evaluation of existing techniques, this work aims to provide a valuable reference for researchers and practitioners at the intersection of machine learning and fluid mechanics.",
      "stats": {
        "char_count": 1346,
        "word_count": 182,
        "sentence_count": 6,
        "line_count": 1
      }
    },
    {
      "heading": "2 Introduction",
      "level": 1,
      "content": "The integration of physical principles into machine learning has become a cornerstone of modern scientific computing, particularly in the realm of fluid mechanics [1]. As computational models grow in complexity and data availability fluctuates, the need for robust, interpretable, and physically consistent models has never been more pressing. Traditional numerical methods, while effective in many scenarios, often struggle with high-dimensional, nonlinear, and data-scarce problems. In response, machine learning techniques—particularly deep learning—have emerged as powerful tools for approximating and solving complex physical systems [2]. However, purely data-driven models lack the ability to enforce physical constraints, leading to solutions that may be statistically accurate but physically implausible. To address this limitation, researchers have developed frameworks that embed physical laws directly into the learning process, ensuring that models not only fit data but also respect the underlying physics [3]. This convergence of machine learning and physics-based modeling has given rise to a new class of methods, including Physics-Informed Neural Networks (PINNs), which are increasingly being applied to fluid dynamics and related domains [4].\n\nThis survey paper focuses on the application of Physics-Informed Neural Networks (PINNs) to fluid mechanics, exploring their design, training strategies, and performance in modeling complex fluid behaviors [5]. The paper provides a comprehensive overview of the key developments in this field, highlighting the integration of physical laws into neural architectures, the use of hybrid models that combine deep learning with traditional numerical solvers, and the application of self-supervised optimization techniques to improve local adaptability. It also examines the role of multi-stage neural networks in capturing complex dynamics, the embedding of physical constraints in learning processes, and the use of differentiable physics for inverse design. Through these discussions, the paper aims to provide a clear and structured understanding of the current state of research on PINNs in fluid mechanics, as well as the challenges and opportunities that lie ahead.\n\nThe content of this survey spans a broad range of topics, beginning with an exploration of how domain knowledge is integrated into neural network architectures to ensure physical consistency. This includes the use of physics-informed loss functions, the incorporation of conservation laws, and the design of hybrid models that combine data-driven and physics-based components. The paper also delves into the development of self-supervised optimization strategies that enhance the adaptability of models to local variations in the system. A detailed discussion follows on the use of multi-stage neural networks to decompose complex dynamics into sequential stages, enabling more accurate and interpretable modeling of fluid behavior. Additionally, the paper examines the role of differentiable physics in embedding physical laws directly into learning processes, allowing for the efficient solution of inverse problems and parameter identification. Finally, it addresses the challenges of ensuring physical consistency in neural training, including the impact of activation functions on convergence and the role of gradient normalization in improving stability.\n\nThe contributions of this survey paper are threefold. First, it provides a structured and comprehensive review of the current state of research on Physics-Informed Neural Networks in fluid mechanics, offering insights into the key methodologies, challenges, and applications [1]. Second, it highlights the integration of physical constraints into neural architectures, demonstrating how such approaches can improve the reliability and interpretability of machine learning models in scientific domains [1]. Third, it identifies key research directions and open challenges, providing a foundation for future work in the field. By synthesizing the latest advancements and offering a critical analysis of the strengths and limitations of existing methods, this survey aims to serve as a valuable resource for researchers and practitioners working at the intersection of machine learning and fluid dynamics.",
      "stats": {
        "char_count": 4296,
        "word_count": 596,
        "sentence_count": 22,
        "line_count": 7
      }
    },
    {
      "heading": "3.1.1 Integration of Domain Knowledge in Neural Frameworks",
      "level": 3,
      "content": "The integration of domain knowledge into neural frameworks has emerged as a critical strategy to enhance the interpretability, generalizability, and physical consistency of machine learning models. This approach involves embedding prior physical or geometric understanding directly into the architecture or training process, often through inductive biases, constraint-based learning, or physics-informed loss functions. By doing so, models can leverage the structure of the underlying physical laws, such as conservation principles or differential equations, to guide the learning process. This is particularly valuable in scenarios where data is scarce or noisy, as it provides a mechanism to regularize the model and ensure that predictions align with known scientific principles. Such integration is not limited to specific domains but has found applications in fluid dynamics, structural mechanics, and even particle physics, demonstrating its broad relevance.\n\nA key technique in this domain is the use of Physics-Informed Neural Networks (PINNs), which directly encode governing equations into the neural network’s loss function [5]. This allows the model to learn solutions that inherently satisfy the physical constraints, even when trained on limited or imperfect data. Another approach involves the use of hybrid models that combine deep learning with traditional numerical solvers, where the neural network acts as a surrogate for parts of the simulation. These frameworks benefit from the flexibility of neural networks while retaining the accuracy and reliability of physics-based models [1]. Additionally, recent advancements have explored the use of differentiable programming to embed domain-specific knowledge, such as spatial correlations or temporal dependencies, into the learning pipeline. This enables models to capture complex relationships that are otherwise difficult to infer purely from data.\n\nDespite these advances, challenges remain in effectively translating domain knowledge into neural architectures without compromising model performance or increasing computational complexity. The balance between incorporating prior information and maintaining the model's ability to generalize is delicate, requiring careful design of the learning objective and network structure. Furthermore, the interpretability of such models is often limited, as the integration of domain knowledge can obscure the underlying decision-making process. Ongoing research aims to address these issues by developing more transparent and scalable methods for incorporating physical and geometric priors. As the field continues to evolve, the integration of domain knowledge is expected to play an increasingly central role in the development of robust, reliable, and interpretable machine learning systems.",
      "stats": {
        "char_count": 2808,
        "word_count": 385,
        "sentence_count": 16,
        "line_count": 5
      }
    },
    {
      "heading": "3.1.2 Self-Supervised Optimization for Local Adaptation",
      "level": 3,
      "content": "Self-supervised optimization for local adaptation leverages the inherent structure of temporal data to refine model predictions without explicit supervision. This approach capitalizes on the self-attention mechanism, which implicitly reconstructs historical dependencies and emulates complex dynamical behaviors through contextual patterns [6]. By processing temporal sequences, the transformer architecture enables the model to autonomously discover and adapt to local variations in the data, making it particularly effective for systems with strong historical dependence. The optimization process is guided by the internal consistency of the data, allowing the model to adjust its parameters in a way that aligns with the underlying physical or statistical laws governing the system.\n\nIn the context of physics-informed machine learning, self-supervised optimization integrates domain-specific constraints into the training process, ensuring that the model adheres to known physical principles while adapting to local conditions [3]. This is achieved by embedding governing equations into the loss function, which acts as a self-supervised signal during training. The resulting framework not only improves the model's generalization capabilities but also enhances its ability to handle noisy or incomplete data. Furthermore, by focusing on local adaptation, the model can dynamically adjust its behavior in response to changing conditions, making it more robust and versatile for real-world applications.\n\nThe effectiveness of self-supervised optimization for local adaptation is further enhanced through the use of adaptive sampling strategies and curriculum learning. These techniques allow the model to prioritize regions of high uncertainty or complexity, ensuring that the optimization process is both efficient and effective. By iteratively refining its predictions based on local feedback, the model can achieve higher accuracy and stability, even in the presence of non-linearities and high-dimensional data. This approach represents a significant advancement in the development of data-driven models that are both physically consistent and capable of adapting to local variations in the system dynamics.",
      "stats": {
        "char_count": 2214,
        "word_count": 301,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "3.2.1 Multi-Stage Neural Networks for Complex Dynamics",
      "level": 3,
      "content": "Multi-stage neural networks have emerged as a powerful paradigm for modeling complex dynamics, particularly in systems where traditional numerical methods face significant limitations. These architectures decompose the learning process into sequential stages, each responsible for capturing distinct aspects of the system's behavior, such as temporal dependencies, spatial correlations, or nonlinear interactions. By structuring the network in a hierarchical manner, multi-stage models can effectively handle high-dimensional and multi-scale phenomena, enabling more accurate and interpretable representations of physical processes. This approach is especially beneficial in scenarios where the system's state variables are not fully accessible, as the staged design allows for the integration of domain-specific knowledge and constraints at each level of abstraction.\n\nThe design of multi-stage neural networks often incorporates physics-informed components, ensuring that the learned models adhere to fundamental physical laws while maintaining the flexibility of data-driven approaches [1]. This is achieved through the embedding of governing equations, such as partial differential equations, as soft constraints within the training process. Additionally, the modular nature of these networks facilitates the incorporation of differentiable modules, such as clustering, fitting, or optimization layers, which can be jointly trained to enhance the model's ability to capture complex dynamics. This integration of physical priors not only improves the accuracy of predictions but also enhances the generalization capabilities of the model across varying conditions and discretization levels.\n\nRecent advancements have demonstrated the effectiveness of multi-stage neural networks in a wide range of applications, from fluid dynamics to structural analysis and particle physics. These models have shown particular promise in addressing challenges such as spectral bias, training instability, and the need for high-fidelity simulations. By leveraging the strengths of deep learning and physics-based modeling, multi-stage architectures provide a robust and scalable framework for simulating and analyzing complex systems. Their ability to handle large-scale, nonlinear, and multi-factor coupling phenomena makes them a compelling choice for next-generation computational methods in scientific and engineering domains.",
      "stats": {
        "char_count": 2417,
        "word_count": 315,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "3.2.2 Embedding of Physical Laws in Learning Processes",
      "level": 3,
      "content": "The embedding of physical laws within learning processes represents a critical advancement in the fusion of machine learning and scientific modeling [1]. By integrating fundamental principles such as conservation laws, symmetry constraints, and thermodynamic relationships into the training objectives of neural networks, these approaches ensure that models not only fit data but also adhere to the underlying physical reality. This is achieved through the use of differentiable physics models, where the governing equations are directly incorporated as loss terms or constraints. This strategy not only enhances the interpretability of the models but also improves their generalization capabilities, especially in data-scarce regimes where purely data-driven methods may fail. The incorporation of physical knowledge acts as a strong inductive bias, guiding the learning process towards physically plausible solutions.\n\nA key methodology in this domain is the use of Physics-Informed Neural Networks (PINNs), which embed partial differential equations (PDEs) directly into the neural network architecture [4]. This allows the model to learn solutions that satisfy the governing equations, even when the training data is sparse or noisy. The integration of physical laws in this manner ensures that the model’s predictions are consistent with the known behavior of the system, making them particularly useful for problems in fluid dynamics, structural mechanics, and other domains governed by complex physical interactions. Furthermore, this approach enables the estimation of unknown parameters within the equations, offering a powerful tool for inverse problems and parameter identification. By leveraging the expressive power of neural networks while maintaining fidelity to physical principles, these models bridge the gap between data-driven learning and traditional physics-based simulations [7].\n\nBeyond PINNs, other strategies such as hybrid architectures and differentiable programming have emerged to further enhance the integration of physical laws into learning processes. These frameworks allow for the seamless incorporation of domain-specific knowledge, enabling models to respect constraints such as energy conservation or material properties. The use of differentiable modules for physical operations, such as clustering or fitting, facilitates end-to-end optimization, ensuring that the entire learning pipeline adheres to the underlying physics. This not only improves the accuracy of the models but also enhances their robustness and adaptability to new scenarios. As the field continues to evolve, the embedding of physical laws is becoming an essential component in the development of reliable, interpretable, and generalizable machine learning models for scientific and engineering applications [8].",
      "stats": {
        "char_count": 2822,
        "word_count": 389,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "3.3.1 Joint Optimization of Sensitivity and Parameter Estimation",
      "level": 3,
      "content": "The joint optimization of sensitivity and parameter estimation is a critical aspect of enhancing the performance and reliability of data-driven models, particularly in complex systems where physical constraints and parameter uncertainties play significant roles. This approach involves simultaneously optimizing model parameters and their associated sensitivities, which quantify how changes in parameters affect model outputs. By integrating these two objectives, the resulting models can better capture the underlying physical dynamics, even when faced with noisy or incomplete data. This synergy allows for more accurate predictions and improved robustness, especially in scenarios where parameter estimation is challenging due to high-dimensional or nonlinear relationships.\n\nThe integration of sensitivity analysis into parameter estimation processes enables the model to dynamically adjust its learning strategy based on the importance of different parameters. This is particularly beneficial in systems governed by partial differential equations or other complex physical laws, where the interplay between parameters can significantly influence the system's behavior. Techniques such as automatic differentiation facilitate the efficient computation of sensitivities, which in turn guide the optimization process. This joint optimization framework not only enhances the model's ability to fit observed data but also ensures that the estimated parameters are physically meaningful and consistent with the governing equations of the system.\n\nFurthermore, the joint optimization of sensitivity and parameter estimation is essential for applications requiring real-time decision-making or adaptive control. By embedding sensitivity information directly into the parameter estimation process, the model can quickly respond to changes in the system's state or external conditions. This capability is particularly valuable in fields such as fluid dynamics, structural health monitoring, and control systems, where timely and accurate parameter updates are crucial. Overall, this approach bridges the gap between data-driven learning and physics-based modeling, offering a powerful tool for improving the accuracy, reliability, and interpretability of predictive models [9].",
      "stats": {
        "char_count": 2273,
        "word_count": 299,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "3.3.2 Differentiable Physics for Inverse Design",
      "level": 3,
      "content": "Differentiable physics has emerged as a transformative approach in inverse design, enabling the integration of physical laws directly into machine learning frameworks. By making the underlying physical models differentiable, this paradigm allows for gradient-based optimization of design parameters, which is essential for solving complex inverse problems. Traditional numerical methods often struggle with computational cost and scalability, particularly in high-dimensional or multi-scale systems. Differentiable physics addresses these limitations by providing a seamless interface between physical simulations and learning algorithms, facilitating efficient parameter estimation and design optimization. This approach leverages the structure of partial differential equations (PDEs) to guide the learning process, ensuring that the resulting models remain consistent with the governing physical principles.\n\nThe application of differentiable physics in inverse design spans a wide range of domains, from fluid dynamics to materials science and particle physics. In these contexts, the ability to backpropagate gradients through physical simulations enables the automated discovery of optimal system configurations. For instance, in computational fluid dynamics, differentiable solvers allow for the direct optimization of flow fields to achieve desired performance metrics. Similarly, in materials design, differentiable physics can be used to tailor microstructures for specific mechanical or thermal properties. These capabilities are further enhanced by the integration of neural networks, which can approximate complex physical behaviors while maintaining differentiability. This synergy between physics and machine learning opens new avenues for solving inverse problems that were previously intractable with conventional methods.\n\nRecent advances in differentiable physics have also focused on improving the accuracy and efficiency of gradient-based optimization. Techniques such as physics-informed neural networks (PINNs) and differentiable solvers have demonstrated remarkable success in capturing the essential dynamics of physical systems while enabling end-to-end learning [5]. These models incorporate physical constraints as soft penalties in the loss function, ensuring that the learned representations adhere to the laws of nature. Moreover, the development of hybrid architectures that combine data-driven and physics-based components has further expanded the scope of inverse design. By leveraging the strengths of both paradigms, differentiable physics provides a robust and scalable framework for tackling complex, real-world design challenges.",
      "stats": {
        "char_count": 2668,
        "word_count": 342,
        "sentence_count": 16,
        "line_count": 5
      }
    },
    {
      "heading": "4.1.1 Activation Function Impact on Convergence",
      "level": 3,
      "content": "The choice of activation function significantly influences the convergence behavior of Physics-Informed Neural Networks (PINNs) [10]. Activation functions introduce non-linearities that enable neural networks to approximate complex functions, but their specific forms affect the optimization landscape and the ability of the network to learn the underlying physical laws. Commonly used activation functions, such as hyperbolic tangent, sigmoid, and ReLU, have distinct properties that can either facilitate or hinder the training process [10]. For instance, functions with bounded outputs, like tanh, may lead to vanishing gradients, while unbounded functions, like ReLU, can cause exploding gradients in certain scenarios. These characteristics directly impact the stability and speed of convergence during training, particularly in problems governed by partial differential equations where accurate representation of solution gradients is crucial.\n\nEmpirical studies have shown that the performance of activation functions varies across different problem domains and network architectures. Some activation functions may converge faster in low-dimensional problems but struggle with high-frequency solution fields, while others may offer better generalization at the cost of slower training [10]. The interplay between the activation function and the loss function is also critical, as the non-linear transformations affect how the network enforces physical constraints. For example, the use of sinusoidal or swish activation functions has been shown to improve the representation of high-frequency components, which can enhance the accuracy of PINN solutions. However, no single activation function consistently outperforms others across all scenarios, highlighting the need for problem-specific tuning.\n\nThe impact of activation functions on convergence is further complicated by the stochastic nature of the optimization process and the inherent non-convexity of the loss landscape. While theoretical guarantees for convergence in PINNs remain limited, practical insights suggest that the selection of activation functions should be guided by the characteristics of the problem at hand. This includes the smoothness of the solution, the presence of sharp gradients, and the need for accurate enforcement of physical constraints. As such, the design of activation functions in PINNs is a critical factor in achieving reliable and efficient convergence, particularly in complex physical systems governed by partial differential equations.",
      "stats": {
        "char_count": 2540,
        "word_count": 345,
        "sentence_count": 14,
        "line_count": 5
      }
    },
    {
      "heading": "4.1.2 Gradient Normalization and Precision Optimization",
      "level": 3,
      "content": "Gradient normalization and precision optimization play a critical role in enhancing the stability and accuracy of physics-informed neural networks (PINNs) when solving partial differential equations (PDEs) [4]. Traditional optimization approaches often struggle with high-gradient regions, where the loss function becomes dominated by large residuals, leading to unstable training and poor convergence. Gradient normalization addresses this by scaling the PDE residuals relative to the local gradients of the solution, effectively rebalancing the optimization landscape [11]. This technique ensures that regions with critical physical behavior, such as singularities or sharp transitions, are not overshadowed by high-gradient areas, thereby improving the overall accuracy of the solution. By normalizing the gradients, the optimization process becomes more robust and capable of capturing fine-scale features that are essential for physical consistency.\n\nPrecision optimization further refines the solution by employing advanced numerical methods and high-precision computational frameworks. Standard optimizers like Adam may provide a good initial approximation, but they often lack the precision required for complex PDEs with stringent accuracy demands. To achieve near-machine precision, specialized algorithms such as full-matrix Gauss-Newton or L-BFGS are utilized, which offer superior convergence properties. These methods, combined with high-precision arithmetic, enable the accurate resolution of high-gradient and unstable regions, ensuring that the solution adheres closely to the governing physical laws. Additionally, precision optimization is essential for handling problems with self-similar structures or infinite domains, where small numerical errors can propagate and significantly affect the solution's validity.\n\nThe integration of gradient normalization and precision optimization is particularly beneficial in scenarios involving inverse problems, noisy data, or complex geometries, where maintaining physical consistency is paramount. These techniques not only enhance the reliability of the neural network's predictions but also ensure that the solution remains mathematically and physically sound. By carefully balancing the loss function and refining the optimization process, the resulting models achieve higher fidelity and better generalization, making them suitable for a wide range of scientific and engineering applications. This synergy between gradient normalization and precision optimization represents a significant advancement in the field of PINNs, enabling more accurate and robust solutions to challenging PDE-based problems.",
      "stats": {
        "char_count": 2668,
        "word_count": 344,
        "sentence_count": 14,
        "line_count": 5
      }
    },
    {
      "heading": "4.2.1 Fault Estimation and Control with PINNs",
      "level": 3,
      "content": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for fault estimation and control by integrating physical laws directly into the learning process [5]. Unlike conventional neural networks that rely solely on data, PINNs enforce consistency with governing differential equations through their loss functions, enabling them to infer fault characteristics even in the presence of noisy or incomplete data. This capability is particularly valuable in fault estimation, where the goal is to detect and quantify anomalies in system behavior. By embedding physical constraints, PINNs can provide more reliable and interpretable fault estimates, ensuring that the learned models remain consistent with the underlying system dynamics. This makes them well-suited for applications in complex, real-world systems where traditional data-driven methods may fail due to lack of physical consistency.\n\nIn the context of control, PINNs facilitate the development of physics-aware fault-tolerant control strategies by providing real-time estimates of fault severity and system states [12]. These estimates can be integrated into observer-controller frameworks, where the PINN acts as a dynamic model that informs both the observer and the controller about the system's health. This synergy allows for adaptive control laws that adjust to varying operational conditions and fault scenarios. For instance, in thermal energy systems, PINNs have been used to estimate actuator faults and implement control strategies that minimize valve actuation while preventing overtemperature [12]. The integration of PINNs with sliding mode control (SMC) or other robust control techniques further enhances the system's resilience, ensuring stability even under uncertain or faulty conditions.\n\nThe application of PINNs to fault estimation and control also addresses key limitations of traditional methods. Conventional neural networks often produce physically inconsistent results due to the absence of explicit physical constraints, whereas PINNs inherently encode these constraints through their training process. Additionally, PINNs can operate effectively under varying operational modes where fault signatures may be ambiguous or non-unique. This adaptability, combined with their ability to handle complex system geometries and high-frequency solution fields, makes them a promising approach for fault estimation and control in a wide range of engineering applications. Their flexibility and robustness position PINNs as a critical component in the development of next-generation intelligent control systems.",
      "stats": {
        "char_count": 2605,
        "word_count": 358,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "4.2.2 Reconstruction of Physical Systems from Boundary Data",
      "level": 3,
      "content": "Reconstruction of physical systems from boundary data is a critical challenge in computational science, particularly when dealing with complex, nonlinear, or high-dimensional systems. Traditional numerical methods, such as the finite-volume method (FVM), often struggle with maintaining accuracy and conservation properties in such scenarios. Physics-Informed Neural Networks (PINNs) offer a promising alternative by integrating physical laws directly into the learning process, enabling the reconstruction of system behavior from limited or noisy boundary observations [5]. This approach allows for the inference of internal states and parameters, even in cases where direct measurements are unavailable. By embedding governing equations as constraints within the neural network training, PINNs provide a flexible and robust framework for solving inverse problems, making them particularly suitable for applications involving unknown or spatially varying parameters [13].\n\nThe application of PINNs to boundary data reconstruction is particularly effective in scenarios where the system exhibits complex dynamics, such as in fluid dynamics, elasticity, or relativistic systems. These networks are capable of learning implicit functional representations that approximate the solution to partial differential equations, while simultaneously enforcing physical constraints such as conservation laws and boundary conditions [1]. This dual capability ensures that the reconstructed models not only fit the observed data but also adhere to the underlying physical principles. In particular, PINNs have demonstrated success in scenarios involving singularities, nonlinearity, and moving boundaries—domains where conventional numerical methods often fail. By leveraging the expressive power of neural networks, PINNs enable accurate and stable reconstructions even in the presence of noisy or sparse boundary data.\n\nThe reconstruction process using PINNs is inherently data-driven and unsupervised, making it well-suited for problems where direct measurements of the system's internal state are impractical or impossible. This is especially relevant in fields such as holographic duality, where the goal is to infer bulk spacetime properties from boundary observables. The ability to reconstruct physical systems from boundary data opens new avenues for understanding complex phenomena and developing computational strategies for strongly coupled systems. Furthermore, the adaptability of PINNs allows them to be applied across diverse domains, from engineering to the natural sciences, where the interplay of nonlinear dynamics and boundary conditions plays a central role. This versatility underscores the significance of PINNs in advancing the field of physical system reconstruction.",
      "stats": {
        "char_count": 2779,
        "word_count": 371,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "4.3.1 Inverse Problems in Elasticity and Rheology",
      "level": 3,
      "content": "Inverse problems in elasticity and rheology involve the estimation of material properties or constitutive parameters from observed data, often requiring the solution of partial differential equations (PDEs) with limited or noisy measurements. These problems are inherently ill-posed, meaning that small perturbations in the data can lead to large variations in the solution. In the context of rheology, the challenge is further compounded by the nonlinear and history-dependent behavior of materials, particularly in granular media. Physics-informed neural networks (PINNs) have emerged as a promising tool for addressing such inverse problems, as they incorporate physical laws directly into the learning process [5]. By training neural networks to satisfy both the governing PDEs and available data, PINNs provide a flexible framework for parameter identification, even in the presence of complex geometries and heterogeneous material responses [14].\n\nThe $\\mu(I)$-rheology model, which describes the flow behavior of granular materials based on the inertial number $I$, has been widely used in continuum simulations [2]. However, the determination of its parameters, such as $\\mu_s$ and $\\mu_2$, remains a significant challenge due to the difficulty of direct experimental measurements. Synthetic data generated from granular column collapse simulations offer a controlled environment for evaluating the performance of PINNs in inferring these parameters. This approach allows for the systematic assessment of reconstruction accuracy and the identification of error sources, such as noise or model inadequacy. The use of PINNs in this context not only enhances the interpretability of the results but also enables the incorporation of physical constraints that ensure the solutions remain consistent with the underlying rheological model.\n\nWhile PINNs have shown success in forward simulations, their application to inverse problems in elasticity and rheology requires careful consideration of stability and uniqueness [4]. Unlike unique continuation problems, parameter identification often involves estimating spatially varying properties, which can lead to non-unique or unstable solutions. Theoretical guarantees for such inverse problems are still limited, but recent advances in stability analysis provide a foundation for developing more robust methods [4]. By integrating physics-informed constraints and leveraging the smoothness of neural network solutions, PINNs offer a powerful approach to tackle these challenges [4]. This work highlights the potential of PINNs in advancing the understanding of complex material behaviors, particularly in systems where traditional methods struggle to provide accurate and reliable results.",
      "stats": {
        "char_count": 2741,
        "word_count": 378,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "4.3.2 Conservation Laws in Neural Training",
      "level": 3,
      "content": "Conservation laws play a critical role in ensuring the physical consistency of numerical simulations, particularly in fields such as fluid dynamics and relativistic hydrodynamics. In traditional numerical methods, such as finite-volume schemes, conservation is enforced through flux calculations at cell interfaces, which guarantees that global quantities like mass, momentum, and energy remain preserved. However, maintaining the conservation of angular momentum alongside linear and energy momentum presents significant challenges due to the non-trivial coupling between these quantities. This complexity is further exacerbated in high-dimensional and relativistic systems, where the interplay between different conservation laws becomes more intricate. The integration of these laws into neural training frameworks requires careful design to avoid discretization errors and ensure physical fidelity.\n\nPhysics-Informed Neural Networks (PINNs) offer a promising approach to incorporate conservation laws directly into the training process by embedding governing equations into the loss function [5]. This allows the network to learn solutions that inherently respect the underlying physical principles, such as conservation of momentum and energy. Unlike conventional numerical methods that rely on explicit discretization, PINNs operate in a continuous space, making them particularly suitable for problems where conservation is paramount. By introducing terms in the loss function that enforce conservation laws, PINNs can achieve accurate solutions even in complex scenarios where traditional methods struggle. This capability is especially valuable in relativistic simulations, where maintaining total angular momentum conservation is essential for physical accuracy [13].\n\nThe application of conservation laws in neural training also raises questions about the stability and convergence of the learning process. While PINNs have shown success in capturing physical behaviors, the non-linear nature of neural networks can lead to challenges in ensuring that conservation laws are strictly upheld. Techniques such as adaptive loss weighting, gradient normalization, and the use of smooth activation functions have been explored to enhance the accuracy of conservation enforcement. Additionally, the integration of physical constraints into the network architecture, such as through input transformations or specialized layer designs, can further improve the adherence to conservation laws. These strategies highlight the importance of embedding physical priors into the training process to ensure that neural networks produce solutions that are not only accurate but also physically meaningful.",
      "stats": {
        "char_count": 2698,
        "word_count": 360,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "5.1.1 Multiscale GNNs for Flow Field Generation",
      "level": 3,
      "content": "Multiscale Graph Neural Networks (GNNs) have emerged as a powerful tool for modeling complex physical systems, particularly in the context of flow field generation [15]. These architectures leverage the inherent graph structure of fluid domains, enabling the modeling of interactions at multiple spatial scales. By incorporating hierarchical message passing mechanisms, multiscale GNNs can capture both local and global dynamics, making them well-suited for simulating turbulent flows and complex fluid-structure interactions [16]. This capability is especially valuable in scenarios where high-resolution data is sparse or computationally expensive to obtain, as the models can generalize across different scales and topologies.\n\nRecent advancements in multiscale GNNs have focused on integrating physical constraints and domain-specific knowledge into the learning process. This includes the use of physics-informed losses that enforce conservation laws, such as mass and momentum conservation, ensuring that generated flow fields adhere to fundamental physical principles. Additionally, these models often employ hierarchical architectures that operate on different levels of resolution, allowing for efficient computation while maintaining accuracy. By combining these techniques, multiscale GNNs can generate realistic flow fields that are both physically consistent and computationally feasible, even for large and complex domains.\n\nThe application of multiscale GNNs to flow field generation has shown promising results in various domains, including atmospheric modeling, cardiovascular simulations, and industrial fluid dynamics. These models are particularly effective in scenarios requiring real-time or near-real-time predictions, where traditional computational fluid dynamics (CFD) methods may be too slow or resource-intensive. As the field continues to evolve, further research is needed to improve the scalability, interpretability, and generalizability of these models, ensuring their broad applicability across diverse scientific and engineering challenges.",
      "stats": {
        "char_count": 2075,
        "word_count": 269,
        "sentence_count": 11,
        "line_count": 5
      }
    },
    {
      "heading": "5.1.2 Hypergraph Convolutions for Conservation Laws",
      "level": 3,
      "content": "Hypergraph convolutions offer a promising framework for modeling conservation laws by leveraging higher-order interactions that go beyond the pairwise relationships captured by traditional graph neural networks (GNNs). Unlike standard GNNs, which aggregate information through edges connecting two nodes, hypergraph convolutions use hyperedges to connect multiple nodes simultaneously, enabling the representation of complex physical couplings [15]. This structure is particularly advantageous in scenarios involving multi-node interactions, such as those found in fluid dynamics and plasma simulations, where local conservation laws and geometric fidelity are critical. By incorporating finite-element shape functions into hypergraph convolutions, models can preserve the essential properties of the underlying physical systems, making them well-suited for long-horizon predictions and high-fidelity simulations.\n\nThe application of hypergraph convolutions to conservation laws involves defining convolution operations that respect the intrinsic structure of the problem. These operations aggregate information at the element level, mimicking the local stiffness assembly process of finite element methods. This approach ensures that the model maintains geometric fidelity and local conservation, which are essential for accurately capturing the dynamics of physical systems. Furthermore, hypergraph convolutions can be designed to be rotation-equivariant, allowing the model to generalize across different configurations while maintaining physical consistency. This property is particularly valuable in problems where the orientation of the system may vary, such as in complex fluid flows or deformable structures.\n\nRecent advancements in hypergraph-based models have demonstrated their effectiveness in capturing multi-node physical interactions that standard GNNs cannot represent directly [15]. By extending the expressivity of GNNs through hyperedges, these models can better represent the intricate dependencies present in conservation laws. This increased expressivity enables more accurate and interpretable predictions, especially in scenarios where the interactions between multiple nodes play a significant role. As a result, hypergraph convolutions provide a robust and scalable approach for modeling conservation laws, offering a powerful tool for scientific machine learning and computational physics.",
      "stats": {
        "char_count": 2416,
        "word_count": 307,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "5.2.1 Differentiable Solvers with Neural Correction",
      "level": 3,
      "content": "Differentiable solvers with neural correction represent a significant advancement in integrating machine learning with traditional numerical methods, enabling the seamless incorporation of physical constraints into deep learning frameworks. These solvers are designed to maintain differentiability throughout the computational pipeline, allowing gradients to be backpropagated through the numerical operations. This property is crucial for optimizing model parameters using gradient-based methods, ensuring that the solver's outputs align with the desired physical behavior. By embedding domain-specific knowledge directly into the loss function, differentiable solvers facilitate the training of models that are both accurate and physically consistent, making them particularly suitable for complex scientific simulations [3].\n\nNeural correction mechanisms further enhance the performance of differentiable solvers by introducing learned adjustments to the numerical solutions [17]. These corrections are typically implemented through additional neural network components that refine the solver's outputs, addressing discrepancies caused by approximations or simplifications in the underlying physical models. For instance, in fluid dynamics, neural correction modules can account for small-scale kinetic effects that are often neglected in conventional solvers. This approach not only improves the accuracy of the simulations but also maintains the computational efficiency of the solver, making it feasible for real-time or large-scale applications. The integration of such correction layers allows for a more robust and adaptable modeling framework.\n\nThe synergy between differentiable solvers and neural correction has demonstrated promising results in various domains, including fluid-structure interaction and turbulent flow simulations. By combining the strengths of traditional numerical methods with the flexibility of deep learning, these approaches enable the development of models that can generalize across diverse scenarios while adhering to physical laws [14]. The ability to refine predictions through learned corrections also enhances interpretability, as the neural components can provide insights into the sources of error and the nature of the physical phenomena being modeled. Overall, differentiable solvers with neural correction offer a powerful tool for advancing computational science, bridging the gap between data-driven learning and physics-based modeling.",
      "stats": {
        "char_count": 2486,
        "word_count": 321,
        "sentence_count": 13,
        "line_count": 5
      }
    },
    {
      "heading": "5.2.2 Surrogate Modeling for Complex Dynamics",
      "level": 3,
      "content": "Surrogate modeling has emerged as a critical approach for addressing the computational challenges associated with simulating complex dynamical systems, particularly in fields such as fluid dynamics and structural mechanics. Traditional high-fidelity solvers, while accurate, often incur prohibitive computational costs, limiting their applicability to real-time or large-scale problems. Surrogate models, including data-driven techniques like graph neural networks (GNNs) and physics-informed neural networks (PINNs), offer a viable alternative by learning compact representations of system behavior [7]. These models can predict outcomes based on input parameters or initial conditions, enabling faster simulations without sacrificing essential physical insights. By leveraging structured or unstructured data, surrogate models facilitate efficient exploration of design spaces and support tasks such as optimization and control.\n\nRecent advancements in surrogate modeling have focused on integrating domain-specific knowledge to enhance predictive accuracy and generalizability. Techniques such as physics-informed neural networks embed governing equations directly into the learning process, ensuring that model outputs adhere to physical constraints [5]. Similarly, reduced-order modeling (ROM) approaches, including proper orthogonal decomposition (POD), project high-dimensional data onto a lower-dimensional subspace, significantly reducing computational complexity [18]. These methods are particularly effective when combined with machine learning, where models are trained to map input parameters to reduced coefficients, which are then used to reconstruct full-field solutions. This synergy between data-driven and physics-based modeling has shown promise in applications ranging from cardiovascular simulations to turbulent flow predictions, where capturing intricate dynamics remains a challenge.\n\nDespite these advances, surrogate modeling for complex dynamics still faces several challenges, including the need for robust uncertainty quantification, scalability to high-dimensional problems, and adaptability to varying system configurations. Current methods often struggle with generalization across different geometries or operating conditions, necessitating retraining or fine-tuning. Additionally, the integration of surrogate models into iterative workflows, such as digital twins or urban design optimization, remains an open area of research [19]. Ongoing efforts aim to develop more efficient training strategies, incorporate uncertainty-aware frameworks, and enhance model interpretability, ensuring that surrogate models can reliably support decision-making in complex, real-world scenarios.",
      "stats": {
        "char_count": 2715,
        "word_count": 331,
        "sentence_count": 14,
        "line_count": 5
      }
    },
    {
      "heading": "5.3.1 Systematic Framework for Graph-Based ML",
      "level": 3,
      "content": "A systematic framework for graph-based machine learning (ML) is essential to harness the power of graph structures in modeling complex systems. This framework typically involves defining graph representations of data, where nodes and edges encode relevant features and relationships. Graph Neural Networks (GNNs) form the core of this approach, enabling the learning of node and graph-level representations through message passing mechanisms [5]. These mechanisms allow the model to aggregate information from neighboring nodes, capturing dependencies that are critical in domains such as social networks, molecular structures, and power systems. The design of the graph structure and the choice of message-passing functions are pivotal in determining the effectiveness of the model.\n\nThe framework also includes strategies for training and optimizing graph-based models, ensuring they generalize well across different graph configurations and data distributions. Techniques such as graph pooling, attention mechanisms, and hierarchical aggregation are employed to enhance model performance and scalability. Additionally, the integration of domain-specific knowledge into the graph construction process can significantly improve the interpretability and accuracy of the models. This is particularly important in applications like fluid dynamics and power systems, where the underlying physical laws and topological constraints must be respected. The systematic approach ensures that the models are not only data-driven but also grounded in the structural properties of the problem at hand.\n\nFinally, the framework addresses the challenges of computational efficiency and model robustness. By leveraging parallel computing and GPU acceleration, graph-based models can handle large-scale graphs without incurring excessive computational costs. Furthermore, the framework incorporates methods to mitigate overfitting and improve generalization, such as regularization techniques and data augmentation. These elements collectively contribute to a robust and scalable system for graph-based ML, making it a viable solution for a wide range of complex, real-world problems.",
      "stats": {
        "char_count": 2167,
        "word_count": 291,
        "sentence_count": 14,
        "line_count": 5
      }
    },
    {
      "heading": "5.3.2 Comparative Analysis with Traditional Methods",
      "level": 3,
      "content": "The comparative analysis between the proposed method and traditional approaches reveals significant improvements in both performance and efficiency. Traditional methods, such as those based on computational fluid dynamics (CFD), often require extensive computational resources and are limited by their reliance on uniform grids, which restrict their applicability to complex geometries. In contrast, the proposed approach, which integrates explainable AI techniques like Grad-CAM, not only enhances model interpretability but also achieves near-optimal performance with a minimal cost gap [20]. This efficiency is particularly beneficial in clinical and real-time applications where rapid decision-making is essential.\n\nWhen evaluating traditional dimensionality reduction techniques such as Principal Component Analysis (PCA) and Proper Orthogonal Decomposition (POD), the proposed method demonstrates superior generalization and constraint satisfaction [18]. While these conventional methods rely on singular value decomposition to extract orthogonal modes, they often struggle with high-dimensional data and are sensitive to noise. The proposed approach, however, leverages machine learning to adaptively capture complex patterns, leading to more robust and reliable predictions. This adaptability is crucial in scenarios where data variability is high, such as in hemodynamic assessments or urban flow simulations.\n\nFurthermore, the integration of explainable AI in the proposed method addresses a critical limitation of traditional models: their lack of transparency. Traditional methods often operate as black boxes, making it difficult to validate their predictions or understand their decision-making processes. By incorporating Grad-CAM, the model provides visual explanations that enhance trust and facilitate clinical or operational validation. This transparency, combined with improved performance and efficiency, positions the proposed method as a viable alternative to conventional approaches in a variety of applications.",
      "stats": {
        "char_count": 2036,
        "word_count": 264,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "6 Future Directions",
      "level": 1,
      "content": "Despite significant progress in the integration of physical principles into machine learning models, several limitations and gaps remain in the current research on Physics-Informed Neural Networks (PINNs) for fluid mechanics. One major limitation is the difficulty in ensuring the robustness and generalizability of these models across diverse and complex physical scenarios. Many existing approaches rely on specific assumptions about the underlying physics or the structure of the problem, which may not hold in more realistic or high-dimensional settings. Additionally, the enforcement of physical constraints during training often introduces computational challenges, such as increased training time and instability, particularly when dealing with high-frequency or non-smooth solutions. Furthermore, the interpretability of PINNs remains a concern, as the integration of physical laws into the learning process can obscure the model's decision-making mechanisms, making it difficult to validate or trust the results in critical applications.\n\nTo address these challenges, future research should focus on developing more scalable and stable training strategies that can handle complex physical systems while maintaining computational efficiency. This includes exploring advanced optimization techniques, such as adaptive gradient normalization and physics-informed regularization, to improve convergence and stability. Additionally, there is a need for more interpretable PINN architectures that can provide insights into how physical constraints influence model predictions. Another promising direction is the development of hybrid models that combine PINNs with traditional numerical solvers, enabling the use of physics-based approximations to guide the learning process and improve accuracy. These models could also benefit from the integration of multi-stage neural networks, which allow for the decomposition of complex dynamics into sequential stages, thereby enhancing the model's ability to capture long-term dependencies and nonlinear interactions.\n\nThe potential impact of these future research directions is substantial, as they could significantly enhance the reliability, efficiency, and applicability of PINNs in fluid mechanics and related domains. Improved training strategies and more interpretable models would make PINNs more accessible to practitioners and researchers, facilitating their adoption in real-world applications. The development of hybrid and multi-stage architectures could lead to more accurate and robust simulations, particularly in scenarios where data is scarce or computational resources are limited. Furthermore, the integration of physical constraints into learning processes could enable the solution of previously intractable inverse problems, such as parameter identification and system reconstruction from limited observations. Overall, these advancements would contribute to the broader goal of creating machine learning models that are not only data-driven but also grounded in the fundamental principles of physics, paving the way for more accurate, trustworthy, and generalizable scientific computing tools.",
      "stats": {
        "char_count": 3162,
        "word_count": 417,
        "sentence_count": 15,
        "line_count": 5
      }
    },
    {
      "heading": "7 Conclusion",
      "level": 1,
      "content": "This survey paper provides a comprehensive overview of the integration of physical principles into deep learning frameworks, with a specific focus on Physics-Informed Neural Networks (PINNs) and their applications in fluid mechanics. The study highlights the critical role of embedding physical laws, such as conservation equations and differential constraints, into the training process of neural networks to ensure that models are not only data-driven but also physically consistent. Key developments in the field include the use of hybrid models that combine deep learning with traditional numerical solvers, the application of self-supervised optimization techniques for local adaptation, and the deployment of multi-stage neural networks to decompose complex dynamics. Additionally, the paper explores the use of differentiable physics for inverse design, the impact of activation functions on convergence, and the importance of gradient normalization and precision optimization in enhancing model stability. These findings collectively demonstrate the potential of physics-informed deep learning to address challenges in high-dimensional, nonlinear, and data-scarce systems, offering a robust alternative to conventional numerical methods.\n\nThe significance of this survey lies in its structured analysis of the current state of research on PINNs in fluid mechanics, providing a critical synthesis of methodologies, challenges, and opportunities. By emphasizing the integration of physical constraints into neural architectures, the paper underscores the importance of interpretability and reliability in machine learning models used for scientific and engineering applications. This work also serves as a foundation for future research, identifying key areas that require further exploration, such as the development of more scalable and interpretable models, the enhancement of physical consistency in training, and the improvement of generalization across diverse problem domains. The survey contributes to the growing body of knowledge at the intersection of machine learning and physics-based modeling, offering insights that can guide the design of more accurate and efficient computational tools for complex physical systems.\n\nAs the field continues to evolve, it is essential to foster interdisciplinary collaboration between machine learning researchers, physicists, and engineers to address the remaining challenges and unlock the full potential of physics-informed deep learning. Future work should focus on improving the scalability and adaptability of these models, particularly in real-world applications where data quality and computational resources may be limited. Additionally, there is a need for more rigorous theoretical analysis to establish the convergence properties and stability of these methods. By building on the advancements presented in this survey, researchers can continue to push the boundaries of what is possible in scientific computing, paving the way for more accurate, interpretable, and physically grounded machine learning models. The integration of physical principles into deep learning is not just a technical innovation but a transformative approach that has the potential to revolutionize how we model and understand complex physical systems.",
      "stats": {
        "char_count": 3294,
        "word_count": 452,
        "sentence_count": 14,
        "line_count": 5
      }
    }
  ],
  "references": [
    {
      "text": "[1] AdS Deep-Learning made easy II  neural network-based approaches to holography and inverse problems",
      "number": null,
      "title": "ads deep-learning made easy ii neural network-based approaches to holography and inverse problems"
    },
    {
      "text": "[2] Rheological Parameter Identification in Granular Materials Using Physics-Informed Neural Networks",
      "number": null,
      "title": "rheological parameter identification in granular materials using physics-informed neural networks"
    },
    {
      "text": "[3] Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement",
      "number": null,
      "title": "test time training for ac power flow surrogates via physics and operational constraint refinement"
    },
    {
      "text": "[4] On Parameter Identification in Three-Dimensional Elasticity and Discretisation with Physics-Informed",
      "number": null,
      "title": "on parameter identification in three-dimensional elasticity and discretisation with physics-informed"
    },
    {
      "text": "[5] Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constr",
      "number": null,
      "title": "refining graphical neural network predictions using flow matching for optimal power flow with constr"
    },
    {
      "text": "[6] Can Transformers overcome the lack of data in the simulation of history-dependent flows",
      "number": null,
      "title": "can transformers overcome the lack of data in the simulation of history-dependent flows"
    },
    {
      "text": "[7] Parallelizable Complex Neural Dynamics Models for PMSM Temperature Estimation with Hardware Accelera",
      "number": null,
      "title": "parallelizable complex neural dynamics models for pmsm temperature estimation with hardware accelera"
    },
    {
      "text": "[8] A Physics-Informed U-net-LSTM Network for Data-Driven Seismic Response Modeling of Structures",
      "number": null,
      "title": "a physics-informed u-net-lstm network for data-driven seismic response modeling of structures"
    },
    {
      "text": "[9] Neural Scaling Laws for Learning-based Identification of Nonlinear Systems",
      "number": null,
      "title": "neural scaling laws for learning-based identification of nonlinear systems"
    },
    {
      "text": "[10] On the failure of ReLU activation for physics-informed machine learning",
      "number": null,
      "title": "on the failure of relu activation for physics-informed machine learning"
    },
    {
      "text": "[11] Resolving Sharp Gradients of Unstable Singularities to Machine Precision via Neural Networks",
      "number": null,
      "title": "resolving sharp gradients of unstable singularities to machine precision via neural networks"
    },
    {
      "text": "[12] Fault-Tolerant Control of Steam Temperature in HRSG Superheater under Actuator Fault Using a Sliding",
      "number": null,
      "title": "fault-tolerant control of steam temperature in hrsg superheater under actuator fault using a sliding"
    },
    {
      "text": "[13] Achieving angular-momentum conservation with physics-informed neural networks in computational relat",
      "number": null,
      "title": "achieving angular-momentum conservation with physics-informed neural networks in computational relat"
    },
    {
      "text": "[14] A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation",
      "number": null,
      "title": "a physics informed machine learning framework for optimal sensor placement and parameter estimation"
    },
    {
      "text": "[15] An Efficient and Accurate Surrogate Modeling of Flapping Dynamics in Inverted Elastic Foils using Hy",
      "number": null,
      "title": "an efficient and accurate surrogate modeling of flapping dynamics in inverted elastic foils using hy"
    },
    {
      "text": "[16] Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment",
      "number": null,
      "title": "graph deep learning for intracranial aneurysm blood flow simulation and risk assessment"
    },
    {
      "text": "[17] Accelerating high-order energy-stable discontinous Galerkin solver using auto-differentiation and ne",
      "number": null,
      "title": "accelerating high-order energy-stable discontinous galerkin solver using auto-differentiation and ne"
    },
    {
      "text": "[18] A data-driven framework to identify restenosis-prone regions in femoral arteries from geometric and",
      "number": null,
      "title": "a data-driven framework to identify restenosis-prone regions in femoral arteries from geometric and"
    },
    {
      "text": "[19] Generative Urban Flow Modeling  From Geometry to Airflow with Graph Diffusion",
      "number": null,
      "title": "generative urban flow modeling from geometry to airflow with graph diffusion"
    },
    {
      "text": "[20] A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad",
      "number": null,
      "title": "a clinically interpretable deep cnn framework for early chronic kidney disease prediction using grad"
    }
  ],
  "metadata": {
    "source_file": "results\\original\\InteractiveSurvey\\Physics\\survey_Physics-informed Neural Networks for Fluid Mechanics_split.json",
    "processed_date": "2025-12-30T20:33:40.736147",
    "config": {
      "normalize_outline": true,
      "normalize_content": true,
      "normalize_references": true
    }
  }
}