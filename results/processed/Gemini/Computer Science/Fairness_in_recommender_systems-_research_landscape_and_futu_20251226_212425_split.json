{
  "outline": [
    [
      1,
      "Literature Review: Fairness in recommender systems- research landscape and future directions."
    ],
    [
      1,
      "Fairness in Recommender Systems: Research Landscape and Future Directions"
    ],
    [
      2,
      "1. Introduction"
    ],
    [
      2,
      "2. Key Concepts and Definitions"
    ],
    [
      3,
      "2.1. Taxonomy of Fairness in RS"
    ],
    [
      4,
      "2.1.1. Stakeholder Perspectives"
    ],
    [
      4,
      "2.1.2. Granularity: Group vs. Individual"
    ],
    [
      3,
      "2.2. Key Metrics"
    ],
    [
      2,
      "3. Historical Development and Milestones"
    ],
    [
      3,
      "3.1. The Era of Accuracy and Bias Discovery (Pre-2015)"
    ],
    [
      3,
      "3.2. The Rise of Fairness-Aware Machine Learning (2015–2019)"
    ],
    [
      3,
      "3.3. The Trustworthy RS Paradigm (2020–Present)"
    ],
    [
      2,
      "4. Current State-of-the-Art Methods and Techniques"
    ],
    [
      3,
      "4.1. Pre-processing Methods"
    ],
    [
      3,
      "4.2. In-processing Methods"
    ],
    [
      3,
      "4.3. Post-processing Methods"
    ],
    [
      3,
      "4.4. LLM-Based Fairness"
    ],
    [
      2,
      "5. Applications and Case Studies"
    ],
    [
      3,
      "5.1. Hiring and Recruitment: LinkedIn"
    ],
    [
      3,
      "5.2. The Gig Economy: Uber"
    ],
    [
      3,
      "5.3. Accommodation and Hospitality: Airbnb"
    ],
    [
      3,
      "5.4. Microfinance: Kiva"
    ],
    [
      2,
      "6. Challenges and Open Problems"
    ],
    [
      3,
      "6.1. The Accuracy-Fairness Trade-off"
    ],
    [
      3,
      "6.2. Intersectionality"
    ],
    [
      3,
      "6.3. Dynamic and Long-Term Fairness"
    ],
    [
      3,
      "6.4. Data Privacy vs. Fairness Verification"
    ],
    [
      2,
      "7. Future Research Directions"
    ],
    [
      3,
      "7.1. Fairness in Large Language Model (LLM)-Based RS"
    ],
    [
      3,
      "7.2. Regulatory Compliance and Auditing"
    ],
    [
      3,
      "7.3. User-Centric Fairness and Perception"
    ],
    [
      3,
      "7.4. Benchmarking and Reproducibility"
    ],
    [
      2,
      "8. Conclusion"
    ],
    [
      2,
      "References"
    ]
  ],
  "content": [
    {
      "heading": "Literature Review: Fairness in recommender systems- research landscape and future directions.",
      "level": 1,
      "content": "*Generated on: 2025-12-26 21:24:25*\n*Progress: [31/50]*\n*Saved to: /Users/joanna/Desktop/Literature_Reviews/Fairness_in_recommender_systems-_research_landscape_and_futu_20251226_212425.md*\n---",
      "stats": {
        "char_count": 192,
        "word_count": 10,
        "sentence_count": 2,
        "line_count": 4
      }
    },
    {
      "heading": "Fairness in Recommender Systems: Research Landscape and Future Directions",
      "level": 1,
      "content": "**Key Points**\n*   **Paradigm Shift:** The field of Recommender Systems (RS) is undergoing a fundamental shift from purely accuracy-driven metrics (e.g., RMSE, NDCG) to \"trustworthy\" AI, with fairness as a central pillar alongside diversity and robustness.\n*   **Multi-Sided Nature:** Fairness in RS is complex because it involves multiple stakeholders. It must balance **Consumer Fairness** (users receiving unbiased recommendations) and **Provider Fairness** (items/creators receiving equitable exposure), which often conflict.\n*   **Methodological Evolution:** Techniques have evolved from simple post-processing re-ranking to sophisticated in-processing methods, including **Adversarial Learning** (to unlearn sensitive attributes), **Causal Inference** (to de-confound popularity bias), and **Graph Neural Networks** (to correct structural bias).\n*   **Emerging Frontiers:** The integration of **Large Language Models (LLMs)** introduces new challenges regarding semantic bias and intersectionality, while new legal frameworks like the **EU Digital Services Act (DSA)** and **AI Act** are moving fairness from a theoretical goal to a compliance requirement.\n\n---",
      "stats": {
        "char_count": 1167,
        "word_count": 147,
        "sentence_count": 8,
        "line_count": 7
      }
    },
    {
      "heading": "1. Introduction",
      "level": 2,
      "content": "Recommender systems (RS) have become the primary information filtering mechanism of the digital age, governing user experiences across e-commerce, social media, streaming platforms, and employment markets [cite: 1, 2]. Historically, the primary objective of these systems was to maximize utility—defined by metrics such as click-through rate (CTR), conversion, or rating prediction accuracy [cite: 2, 3]. However, a growing body of literature has demonstrated that a singular focus on utility often exacerbates systemic biases, leading to unfair outcomes for specific user demographics or item providers [cite: 4, 5].\n\nThe motivation for fairness research in RS stems from both ethical imperatives and practical sustainability. Unfair recommendations can perpetuate societal discrimination (e.g., gender bias in job recommendations) and degrade the long-term health of a platform by alienating minority user groups or niche content creators [cite: 2, 6]. Consequently, the research landscape has expanded to include \"Trustworthy RS,\" where fairness is treated not as a constraint but as a core objective [cite: 7].\n\nThis paper provides a comprehensive systematic literature review of fairness in recommender systems. It synthesizes the historical development of the field, categorizes state-of-the-art methodologies (including adversarial and causal approaches), analyzes real-world applications in high-stakes domains like hiring and the gig economy, and identifies critical research gaps regarding intersectionality and Large Language Models (LLMs).",
      "stats": {
        "char_count": 1551,
        "word_count": 212,
        "sentence_count": 10,
        "line_count": 5
      }
    },
    {
      "heading": "2. Key Concepts and Definitions",
      "level": 2,
      "content": "To navigate the literature, it is essential to distinguish between *bias* (a statistical property) and *fairness* (a normative value). While bias refers to the discrepancy between model predictions and ground truth (often caused by data artifacts), fairness reflects the social requirement that the system should treat individuals or groups equitably [cite: 2, 6].",
      "stats": {
        "char_count": 364,
        "word_count": 53,
        "sentence_count": 2,
        "line_count": 1
      }
    },
    {
      "heading": "2.1. Taxonomy of Fairness in RS",
      "level": 3,
      "content": "The literature generally classifies fairness in RS along three orthogonal dimensions: the stakeholder (subject), the granularity (level), and the optimization objective [cite: 8, 9].",
      "stats": {
        "char_count": 182,
        "word_count": 24,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "2.1.1. Stakeholder Perspectives",
      "level": 4,
      "content": "*   **User Fairness (C-Fairness):** Focuses on the consumers of the recommendations. It ensures that different user groups (e.g., grouped by gender or race) receive recommendations of comparable quality. For example, a medical RS should not provide less accurate health suggestions to minority groups [cite: 8, 10].\n*   **Item/Provider Fairness (P-Fairness):** Focuses on the items or their creators. It ensures equitable exposure or attention for items. This is critical in two-sided marketplaces (e.g., Airbnb, Uber, Spotify), where the livelihood of providers depends on algorithmic visibility [cite: 8, 11].\n*   **Multi-Sided Fairness:** Attempts to satisfy the fairness requirements of both users and providers simultaneously. This is computationally challenging as C-fairness and P-fairness are often zero-sum games; improving exposure for niche items (P-fairness) may temporarily reduce recommendation relevance for users (C-fairness) [cite: 9, 12].",
      "stats": {
        "char_count": 956,
        "word_count": 131,
        "sentence_count": 12,
        "line_count": 3
      }
    },
    {
      "heading": "2.1.2. Granularity: Group vs. Individual",
      "level": 4,
      "content": "*   **Group Fairness:** Requires that the system treats defined groups (protected classes) equally. Common metrics include *Demographic Parity* (independence between recommendation and sensitive attribute) and *Equalized Odds* (equal error rates across groups) [cite: 13, 14].\n*   **Individual Fairness:** Based on the principle that \"similar individuals should be treated similarly.\" This requires a distance metric to define similarity between users or items, which is often difficult to operationalize in sparse data environments [cite: 8, 13].",
      "stats": {
        "char_count": 547,
        "word_count": 74,
        "sentence_count": 4,
        "line_count": 2
      }
    },
    {
      "heading": "2.2. Key Metrics",
      "level": 3,
      "content": "Research has adapted metrics from classification tasks to ranking tasks:\n*   **Exposure-based Metrics:** Measure the visibility of items. For example, *equality of exposure* demands that the exposure of an item group is proportional to its relevance or size [cite: 4, 10].\n*   **Utility-based Metrics:** Measure the satisfaction of users. Differences in NDCG or Recall across user groups indicate unfairness [cite: 10].\n*   **Intersectionality:** Recent work argues that single-axis metrics (e.g., race *or* gender) are insufficient. *Intersectional fairness* considers subgroups formed by combining multiple sensitive attributes (e.g., Black women), which often face compounded discrimination [cite: 9, 15].",
      "stats": {
        "char_count": 708,
        "word_count": 96,
        "sentence_count": 10,
        "line_count": 4
      }
    },
    {
      "heading": "3. Historical Development and Milestones",
      "level": 2,
      "content": "The evolution of fairness in RS can be traced through distinct phases, moving from awareness of bias to sophisticated mitigation strategies.",
      "stats": {
        "char_count": 140,
        "word_count": 21,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "3.1. The Era of Accuracy and Bias Discovery (Pre-2015)",
      "level": 3,
      "content": "Early RS research focused almost exclusively on accuracy (RMSE, MAE) and matrix factorization techniques [cite: 3, 16]. During this period, \"bias\" typically referred to statistical deviations like *popularity bias*—the tendency of algorithms to over-recommend popular items at the expense of the \"long tail\" [cite: 2, 17]. While not initially framed as a social justice issue, this laid the groundwork for understanding how algorithms distort information distribution.",
      "stats": {
        "char_count": 468,
        "word_count": 66,
        "sentence_count": 3,
        "line_count": 1
      }
    },
    {
      "heading": "3.2. The Rise of Fairness-Aware Machine Learning (2015–2019)",
      "level": 3,
      "content": "As machine learning began impacting high-stakes domains, researchers translated fairness concepts from classification to recommendation. Seminal works began to address specific biases, such as gender discrimination in career-related ads and the \"filter bubble\" effect [cite: 2, 18]. This era saw the introduction of post-processing methods (re-ranking) as the primary intervention strategy [cite: 19].",
      "stats": {
        "char_count": 401,
        "word_count": 53,
        "sentence_count": 3,
        "line_count": 1
      }
    },
    {
      "heading": "3.3. The Trustworthy RS Paradigm (2020–Present)",
      "level": 3,
      "content": "The field has shifted toward \"Trustworthy RS,\" where fairness is integrated into the model architecture itself.\n*   **Causal Revolution:** Researchers began applying causal inference to distinguish between correlation (user clicks) and causation (user preference), addressing biases like *position bias* and *conformity bias* [cite: 20, 21].\n*   **Legal and Regulatory Pressure:** The introduction of frameworks like the EU AI Act and the Digital Services Act (DSA) has forced the industry to move beyond theoretical metrics to auditable transparency [cite: 22, 23].\n*   **Generative AI:** The advent of LLMs in 2023–2024 has introduced \"Generative Recommendation,\" necessitating new definitions of fairness related to semantic bias and stereotype propagation in generated text [cite: 24, 25].",
      "stats": {
        "char_count": 793,
        "word_count": 111,
        "sentence_count": 4,
        "line_count": 4
      }
    },
    {
      "heading": "4. Current State-of-the-Art Methods and Techniques",
      "level": 2,
      "content": "Contemporary approaches to fairness in RS are categorized by the stage of the pipeline they intervene in: Pre-processing, In-processing, and Post-processing.",
      "stats": {
        "char_count": 157,
        "word_count": 21,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "4.1. Pre-processing Methods",
      "level": 3,
      "content": "These techniques modify the training data to remove bias before the model sees it.\n*   **Data Balancing:** Techniques involve up-sampling minority groups or down-sampling majority groups to achieve statistical parity in the dataset [cite: 19].\n*   **Counterfactual Data Augmentation:** Recent methods generate \"fake\" interaction data to simulate a balanced world. For instance, if a user interacted with a popular item, a counterfactual sample might simulate an interaction with a less popular but similar item to mitigate popularity bias [cite: 10].",
      "stats": {
        "char_count": 550,
        "word_count": 80,
        "sentence_count": 4,
        "line_count": 3
      }
    },
    {
      "heading": "4.2. In-processing Methods",
      "level": 3,
      "content": "These methods modify the learning algorithm to optimize for fairness alongside accuracy.\n*   **Adversarial Learning:** This is a dominant approach where a \"generator\" (the recommender) tries to produce embeddings that are useful for prediction but from which a \"discriminator\" cannot predict the sensitive attribute (e.g., gender). If the discriminator fails, the embeddings are considered fair [cite: 18, 19, 26].\n*   **Regularization:** Fairness constraints are added directly to the loss function. For example, a term penalizing the correlation between predicted scores and sensitive attributes allows the model to learn a trade-off between accuracy and fairness [cite: 19].\n*   **Causal Inference:** Causal graphs are used to model the generation of interactions. Techniques like *Inverse Propensity Weighting (IPW)* and *Backdoor Adjustment* are used to de-confound the effect of item popularity or display position from true user preference [cite: 20, 27]. This is particularly effective for \"long-tail\" item fairness [cite: 17].\n*   **Fair Graph Neural Networks (GNNs):** GNNs are prone to amplifying bias because they aggregate information from neighbors (homophily). State-of-the-art Fair GNNs use topology rewiring or fair message-passing mechanisms to prevent the propagation of sensitive attributes across the graph [cite: 17, 28].",
      "stats": {
        "char_count": 1343,
        "word_count": 189,
        "sentence_count": 12,
        "line_count": 5
      }
    },
    {
      "heading": "4.3. Post-processing Methods",
      "level": 3,
      "content": "These techniques re-rank the output of a trained model.\n*   **Fair Ranking:** Algorithms like *Detriment* or integer linear programming are used to permute the top-$k$ recommendations to satisfy constraints (e.g., \"at least 30% of recommended candidates must be female\") [cite: 29, 30]. While flexible and model-agnostic, these methods often incur a higher cost in accuracy compared to in-processing methods [cite: 19].",
      "stats": {
        "char_count": 419,
        "word_count": 61,
        "sentence_count": 5,
        "line_count": 2
      }
    },
    {
      "heading": "4.4. LLM-Based Fairness",
      "level": 3,
      "content": "With the rise of Large Language Models (LLMs) as recommenders, new methods are emerging.\n*   **Fairness via Prompting:** Researchers are exploring \"fair prompts\" that explicitly instruct the LLM to consider diversity and equity when generating recommendations [cite: 31].\n*   **LLMs as Auditors:** LLMs are used to evaluate the fairness of other models by analyzing the semantic content of recommendations for stereotypes, acting as a \"fairness recognizer\" [cite: 31, 32].",
      "stats": {
        "char_count": 472,
        "word_count": 69,
        "sentence_count": 3,
        "line_count": 3
      }
    },
    {
      "heading": "5. Applications and Case Studies",
      "level": 2,
      "content": "The theoretical advancements in fairness are increasingly being deployed in industrial settings, particularly in two-sided marketplaces.",
      "stats": {
        "char_count": 136,
        "word_count": 16,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "5.1. Hiring and Recruitment: LinkedIn",
      "level": 3,
      "content": "LinkedIn provided a landmark case study in deploying fairness-aware ranking in talent search. Their system uses a re-ranking algorithm to ensuring that the gender distribution of top-ranked candidates matches the distribution of the qualified talent pool. This \"representative ranking\" approach improved fairness metrics (e.g., exposure for female candidates) without statistically significant degradation in business metrics (hiring efficiency), demonstrating that the accuracy-fairness trade-off is not always a zero-sum game [cite: 29, 30].",
      "stats": {
        "char_count": 543,
        "word_count": 71,
        "sentence_count": 5,
        "line_count": 1
      }
    },
    {
      "heading": "5.2. The Gig Economy: Uber",
      "level": 3,
      "content": "In ride-hailing platforms like Uber, fairness is often framed as **income equality** among drivers (provider fairness). Algorithms that purely optimize for rider wait times may systematically disadvantage drivers in remote areas or those who refuse unprofitable rides. Research has proposed \"income-aware\" matching algorithms and income redistribution schemes to reduce the Gini coefficient of driver earnings while maintaining platform efficiency [cite: 33, 34]. However, investigations have also highlighted \"algorithmic gamblification,\" where dynamic pricing opacity makes pay unpredictable, raising concerns about the fairness of the algorithm's transparency [cite: 35, 36].",
      "stats": {
        "char_count": 678,
        "word_count": 88,
        "sentence_count": 4,
        "line_count": 1
      }
    },
    {
      "heading": "5.3. Accommodation and Hospitality: Airbnb",
      "level": 3,
      "content": "Airbnb's search ranking algorithm balances guest relevance (conversion probability) with host equity. A notable intervention was the \"New Listing Boost,\" designed to solve the cold-start problem and give new hosts a fair chance against established \"Superhosts\" [cite: 37, 38]. However, recent analyses suggest that removing such boosts or heavily weighting past reviews can entrench a \"rich-get-richer\" dynamic, making it difficult for new or marginalized hosts to gain visibility [cite: 38].",
      "stats": {
        "char_count": 492,
        "word_count": 70,
        "sentence_count": 3,
        "line_count": 1
      }
    },
    {
      "heading": "5.4. Microfinance: Kiva",
      "level": 3,
      "content": "Kiva, a peer-to-peer lending platform, utilizes RS to match lenders with borrowers. Unlike commercial platforms, Kiva's objective includes social good. Research here has focused on **fairness-aware re-ranking** to ensure that loans from underrepresented regions or sectors receive adequate exposure, preventing funds from concentrating solely on \"popular\" types of borrowers. Studies show that fairness constraints can significantly improve loan distribution equality with minimal loss in total funding volume [cite: 39, 40].",
      "stats": {
        "char_count": 525,
        "word_count": 70,
        "sentence_count": 4,
        "line_count": 1
      }
    },
    {
      "heading": "6. Challenges and Open Problems",
      "level": 2,
      "content": "Despite significant progress, several critical challenges remain unresolved.",
      "stats": {
        "char_count": 76,
        "word_count": 8,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "6.1. The Accuracy-Fairness Trade-off",
      "level": 3,
      "content": "A persistent debate in the literature is the nature of the trade-off between utility and fairness. While some studies (e.g., LinkedIn) show they can coexist, theoretical work suggests that in many cases, enforcing fairness constraints (like demographic parity) mathematically necessitates a drop in predictive accuracy (Pareto optimality) [cite: 14, 41]. Optimizing this frontier remains a key technical challenge.",
      "stats": {
        "char_count": 414,
        "word_count": 58,
        "sentence_count": 5,
        "line_count": 1
      }
    },
    {
      "heading": "6.2. Intersectionality",
      "level": 3,
      "content": "Most existing methods address single-axis fairness (e.g., gender *or* race). However, real-world discrimination is often intersectional. A system might be fair to \"women\" and \"Black people\" independently but grossly unfair to \"Black women.\" Modeling intersectional fairness is difficult due to data sparsity—splitting users into fine-grained subgroups reduces the data available for learning, exacerbating the \"cold-start\" problem for the very groups that need protection [cite: 9, 15, 42].",
      "stats": {
        "char_count": 490,
        "word_count": 67,
        "sentence_count": 6,
        "line_count": 1
      }
    },
    {
      "heading": "6.3. Dynamic and Long-Term Fairness",
      "level": 3,
      "content": "Most fairness evaluations are static (one-time ranking). However, RS operate in a feedback loop. A small bias in exposure today leads to fewer interactions for the disadvantaged item, which leads to even lower exposure tomorrow (the \"Matthew Effect\"). Research into **long-term fairness** using simulation environments and reinforcement learning is nascent but critical to understanding how biases compound over time [cite: 43, 44].",
      "stats": {
        "char_count": 432,
        "word_count": 62,
        "sentence_count": 4,
        "line_count": 1
      }
    },
    {
      "heading": "6.4. Data Privacy vs. Fairness Verification",
      "level": 3,
      "content": "To measure fairness (e.g., demographic parity), the system needs to know the sensitive attributes of users (race, gender). However, privacy regulations (GDPR) and user reluctance often result in this data being missing or noisy. Developing \"blind\" fairness methods or robust proxies that do not violate privacy is a significant open problem [cite: 18].",
      "stats": {
        "char_count": 352,
        "word_count": 53,
        "sentence_count": 5,
        "line_count": 1
      }
    },
    {
      "heading": "7. Future Research Directions",
      "level": 2,
      "content": "The future of fairness in RS will likely be defined by the integration of generative AI and stricter regulatory compliance.",
      "stats": {
        "char_count": 123,
        "word_count": 20,
        "sentence_count": 1,
        "line_count": 1
      }
    },
    {
      "heading": "7.1. Fairness in Large Language Model (LLM)-Based RS",
      "level": 3,
      "content": "As RS evolve from \"retrieval\" to \"generation\" (e.g., ChatGPT recommending an itinerary), the definition of fairness must expand. Future research must address **semantic fairness**—ensuring that the *language* used to describe recommendations is free from stereotypes. Furthermore, \"hallucination\" in LLMs can invent non-existent items or attributes, posing a new type of reliability risk that disproportionately affects underrepresented entities [cite: 24, 25, 32].",
      "stats": {
        "char_count": 465,
        "word_count": 61,
        "sentence_count": 5,
        "line_count": 1
      }
    },
    {
      "heading": "7.2. Regulatory Compliance and Auditing",
      "level": 3,
      "content": "The **EU Digital Services Act (DSA)** and **AI Act** explicitly mandate that Very Large Online Platforms (VLOPs) assess and mitigate systemic risks, including algorithmic discrimination [cite: 22, 45]. Future research will likely focus on **auditing frameworks**—standardized, third-party tools to verify compliance without exposing proprietary code. This moves the field from \"self-regulation\" to legal accountability [cite: 23, 46].",
      "stats": {
        "char_count": 434,
        "word_count": 57,
        "sentence_count": 3,
        "line_count": 1
      }
    },
    {
      "heading": "7.3. User-Centric Fairness and Perception",
      "level": 3,
      "content": "There is a gap between mathematical fairness and user perception. Do users *perceive* a fair ranking as better? Research suggests that explaining fairness (e.g., \"We showed you this diverse list because...\") can improve user trust, but poorly implemented fairness can be seen as \"forced\" or irrelevant. Future work must bridge the gap between algorithmic metrics and Human-Computer Interaction (HCI) [cite: 47, 48].",
      "stats": {
        "char_count": 415,
        "word_count": 62,
        "sentence_count": 7,
        "line_count": 1
      }
    },
    {
      "heading": "7.4. Benchmarking and Reproducibility",
      "level": 3,
      "content": "The field suffers from a lack of standardized benchmarks. While repositories like **BARS** and **FairnessRecSys** are emerging, there is a need for widely accepted datasets that include verified sensitive attributes to allow for rigorous cross-model comparison [cite: 49, 50, 51].",
      "stats": {
        "char_count": 280,
        "word_count": 40,
        "sentence_count": 2,
        "line_count": 1
      }
    },
    {
      "heading": "8. Conclusion",
      "level": 2,
      "content": "Fairness in recommender systems has matured from a niche sub-topic to a fundamental requirement of algorithmic design. The research landscape has successfully identified the sources of bias and developed a robust taxonomy of fairness definitions (user vs. item, individual vs. group). Methodologically, the field has advanced from simple re-ranking heuristics to complex adversarial and causal learning frameworks that treat fairness as a core optimization objective.\n\nHowever, the \"solution\" to unfairness is not merely technical. Case studies from LinkedIn, Uber, and Airbnb demonstrate that fairness is deeply contextual—what works for hiring may not work for ride-sharing. As the field moves forward, the integration of LLMs and the enforcement of regulations like the DSA will require a more interdisciplinary approach, combining computer science with law, sociology, and ethics. The ultimate goal is to transition from systems that merely \"optimize engagement\" to systems that sustain a fair and equitable digital ecosystem for all stakeholders.",
      "stats": {
        "char_count": 1051,
        "word_count": 151,
        "sentence_count": 9,
        "line_count": 3
      }
    }
  ],
  "references": [
    {
      "text": "Fairness and Diversity in Recommender Systems: A Survey",
      "number": null,
      "title": "Fairness and Diversity in Recommender Systems: A Survey"
    },
    {
      "text": "A Survey on the Fairness of Recommender Systems",
      "number": null,
      "title": "A Survey on the Fairness of Recommender Systems"
    },
    {
      "text": "The Taxonomy of Fairness Notions in Recommender Systems",
      "number": null,
      "title": "The Taxonomy of Fairness Notions in Recommender Systems"
    },
    {
      "text": "Fairness in Recommender Systems: Taxonomy and Dimensions",
      "number": null,
      "title": "Fairness in Recommender Systems: Taxonomy and Dimensions"
    },
    {
      "text": "Fairness in Recommender Systems: Evaluation Approaches and Assurance Strategies",
      "number": null,
      "title": "Fairness in Recommender Systems: Evaluation Approaches and Assurance Strategies"
    },
    {
      "text": "Fairness in Recommender Systems: Research Landscape and Future Directions",
      "number": null,
      "title": "Fairness in Recommender Systems: Research Landscape and Future Directions"
    },
    {
      "text": "Fairness in Recommender Systems",
      "number": null,
      "title": "Fairness in Recommender Systems"
    },
    {
      "text": "Fairness Metrics: Demographic Parity, Equal Accuracy, Equalized Odds",
      "number": null,
      "title": "Fairness Metrics: Demographic Parity, Equal Accuracy, Equalized Odds"
    },
    {
      "text": "Benchmarks for Implicit-Feedback based Top-N Recommendation Algorithms",
      "number": null,
      "title": "Benchmarks for Implicit-Feedback based Top-N Recommendation Algorithms"
    },
    {
      "text": "Singular Value Decomposition in Recommender Systems",
      "number": null,
      "title": "Singular Value Decomposition in Recommender Systems"
    },
    {
      "text": "A Survey on Fairness-Aware Recommender Systems",
      "number": null,
      "title": "A Survey on Fairness-Aware Recommender Systems"
    },
    {
      "text": "Feature Fairness and Adversarial Training in Recommender Systems",
      "number": null,
      "title": "Feature Fairness and Adversarial Training in Recommender Systems"
    },
    {
      "text": "Trustworthy Recommender Systems",
      "number": null,
      "title": "Trustworthy Recommender Systems"
    },
    {
      "text": "Joint Fairness in Recommender Systems",
      "number": null,
      "title": "Joint Fairness in Recommender Systems"
    },
    {
      "text": "Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations",
      "number": null,
      "title": "Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations"
    },
    {
      "text": "Item-side Fairness of Large Language Model-based Recommendation System",
      "number": null,
      "title": "Item-side Fairness of Large Language Model-based Recommendation System"
    },
    {
      "text": "Fairness in Recommendation Systems: LLMs as Fairness Recognizers",
      "number": null,
      "title": "Fairness in Recommendation Systems: LLMs as Fairness Recognizers"
    },
    {
      "text": "Fairness-Aware Graph Neural Networks: A Survey",
      "number": null,
      "title": "Fairness-Aware Graph Neural Networks: A Survey"
    },
    {
      "text": "Graph-based Recommendation and Popularity Bias",
      "number": null,
      "title": "Graph-based Recommendation and Popularity Bias"
    },
    {
      "text": "Causal Inference in Recommender Systems: A Survey",
      "number": null,
      "title": "Causal Inference in Recommender Systems: A Survey"
    },
    {
      "text": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
      "number": null,
      "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking"
    },
    {
      "text": "Journey Ranker: Multi-task Learning for Airbnb Search Ranking",
      "number": null,
      "title": "Journey Ranker: Multi-task Learning for Airbnb Search Ranking"
    },
    {
      "text": "Fairness-Aware Loan Recommendation for Microfinance Services",
      "number": null,
      "title": "Fairness-Aware Loan Recommendation for Microfinance Services"
    },
    {
      "text": "Adaptive Recommendation Framework at Kiva Microfunds",
      "number": null,
      "title": "Adaptive Recommendation Framework at Kiva Microfunds"
    },
    {
      "text": "Personalized Fairness-aware Re-ranking for Microlending",
      "number": null,
      "title": "Personalized Fairness-aware Re-ranking for Microlending"
    },
    {
      "text": "Understanding Fairness in Recommender Systems: A Healthcare Perspective",
      "number": null,
      "title": "Understanding Fairness in Recommender Systems: A Healthcare Perspective"
    },
    {
      "text": "Understanding Fairness Metrics in Recommender Systems: A Healthcare Perspective",
      "number": null,
      "title": "Understanding Fairness Metrics in Recommender Systems: A Healthcare Perspective"
    },
    {
      "text": "Public Understanding of Fairness in Healthcare Recommender Systems",
      "number": null,
      "title": "Public Understanding of Fairness in Healthcare Recommender Systems"
    },
    {
      "text": "Long-term Fairness in Healthcare ML",
      "number": null,
      "title": "Long-term Fairness in Healthcare ML"
    },
    {
      "text": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search",
      "number": null,
      "title": "Fairness-Aware Ranking in Search & Recommendation Systems with Application to LinkedIn Talent Search"
    },
    {
      "text": "Uber Eats Recommender System and Multi-sided Fairness",
      "number": null,
      "title": "Uber Eats Recommender System and Multi-sided Fairness"
    },
    {
      "text": "Reducing Inequality in Ride-Pooling Platforms",
      "number": null,
      "title": "Reducing Inequality in Ride-Pooling Platforms"
    },
    {
      "text": "Consumer Fairness in Recommender Systems: Contextualizing Definitions and Mitigations",
      "number": null,
      "title": "Consumer Fairness in Recommender Systems: Contextualizing Definitions and Mitigations"
    },
    {
      "text": "Towards Open Benchmarking for Recommender Systems",
      "number": null,
      "title": "Towards Open Benchmarking for Recommender Systems"
    },
    {
      "text": "Fair Fairness Benchmark (FFB)",
      "number": null,
      "title": "Fair Fairness Benchmark (FFB)"
    },
    {
      "text": "Digital Services Act: Impact on Platforms",
      "number": null,
      "title": "Digital Services Act: Impact on Platforms"
    },
    {
      "text": "How the Digital Services Act Addresses Platform Recommender Systems",
      "number": null,
      "title": "How the Digital Services Act Addresses Platform Recommender Systems"
    },
    {
      "text": "Fairness and Unlawful Bias in the EU AI Act",
      "number": null,
      "title": "Fairness and Unlawful Bias in the EU AI Act"
    },
    {
      "text": "Regulating High-Reach AI and Recommender Systems",
      "number": null,
      "title": "Regulating High-Reach AI and Recommender Systems"
    },
    {
      "text": "Rental Scale-Up",
      "number": null,
      "title": "Rental Scale-Up"
    },
    {
      "text": "Beyond Booking",
      "number": null,
      "title": "Beyond Booking"
    },
    {
      "text": "Fairness-Aware Ranking in Search & Recommendation Systems",
      "number": null,
      "title": "Fairness-Aware Ranking in Search & Recommendation Systems"
    },
    {
      "text": "Causal Inference for Fairness in Recommender Systems",
      "number": null,
      "title": "Causal Inference for Fairness in Recommender Systems"
    },
    {
      "text": "Causal Inference for Recommendation: Foundations, Methods and Applications",
      "number": null,
      "title": "Causal Inference for Recommendation: Foundations, Methods and Applications"
    },
    {
      "text": "New Research Exposes Deepening Exploitation of Uber Drivers by Algorithmic Pay",
      "number": null,
      "title": "New Research Exposes Deepening Exploitation of Uber Drivers by Algorithmic Pay"
    },
    {
      "text": "Uber's Inequality Machine: Data on How AI-Driven Pay is Harming Workers",
      "number": null,
      "title": "Uber's Inequality Machine: Data on How AI-Driven Pay is Harming Workers"
    },
    {
      "text": "Ride-share matching algorithms generate income inequality",
      "number": null,
      "title": "Ride-share matching algorithms generate income inequality"
    },
    {
      "text": "Intersectional Two-sided Fairness in Recommendation",
      "number": null,
      "title": "Intersectional Two-sided Fairness in Recommendation"
    },
    {
      "text": "A Survey on Intersectional Fairness in Machine Learning",
      "number": null,
      "title": "A Survey on Intersectional Fairness in Machine Learning"
    }
  ],
  "metadata": {
    "source_file": "results\\original\\Gemini\\Computer Science\\Fairness_in_recommender_systems-_research_landscape_and_futu_20251226_212425_split.json",
    "processed_date": "2025-12-30T22:58:37.739202",
    "config": {
      "normalize_outline": true,
      "normalize_content": true,
      "normalize_references": true
    }
  }
}