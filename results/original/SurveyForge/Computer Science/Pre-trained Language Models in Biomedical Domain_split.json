{
  "outline": [
    [
      1,
      "A Comprehensive Survey on Pre-Trained Language Models in the Biomedical Domain"
    ],
    [
      2,
      "1 Introduction"
    ],
    [
      2,
      "2 Historical Evolution and Key Milestones"
    ],
    [
      3,
      "2.1 Early Foundations and Biomedical Text Processing"
    ],
    [
      3,
      "2.2 Transition to Domain-Specific Pre-Training"
    ],
    [
      3,
      "2.3 Breakthroughs in Domain-Specific Pre-Training and Adaptation"
    ],
    [
      3,
      "2.4 The Influence of Biomedical Datasets on Model Development"
    ],
    [
      2,
      "3 Core Technologies and Architectures"
    ],
    [
      3,
      "3.1 Transformer-Based Architectures in Biomedical NLP"
    ],
    [
      3,
      "3.2 Domain-Specific Pre-Training Strategies"
    ],
    [
      3,
      "3.3 Knowledge-Enhanced Modeling with Ontologies and Graphs"
    ],
    [
      3,
      "3.4 Model Efficiency and Adaptation Techniques"
    ],
    [
      3,
      "3.5 Multimodal and Hybrid Architectures for Biomedical Applications"
    ],
    [
      2,
      "4 Applications in Biomedical Tasks"
    ],
    [
      3,
      "4.1 Clinical Text Analysis and Information Extraction"
    ],
    [
      3,
      "4.2 Medical Question Answering and Diagnostic Support"
    ],
    [
      3,
      "4.3 Biomedical Literature Summarization and Information Retrieval"
    ],
    [
      3,
      "4.4 Drug Discovery and Genomic Analysis"
    ],
    [
      3,
      "4.5 Multimodal and Cross-Domain Applications"
    ],
    [
      2,
      "5 Evaluation and Benchmarking"
    ],
    [
      3,
      "5.1 Key Biomedical Datasets and Benchmarks"
    ],
    [
      3,
      "5.2 Evaluation Metrics and Performance Criteria"
    ],
    [
      3,
      "5.3 Challenges in Biomedical Model Benchmarking"
    ],
    [
      3,
      "5.4 Methodological Approaches to Benchmarking"
    ],
    [
      3,
      "5.5 Emerging Trends in Biomedical Benchmarking"
    ],
    [
      2,
      "6 Challenges and Limitations"
    ],
    [
      3,
      "6.1 Data Scarcity and Annotation Challenges"
    ],
    [
      3,
      "6.2 Model Interpretability and Ethical Concerns"
    ],
    [
      3,
      "6.3 Generalizability and Domain Adaptation Issues"
    ],
    [
      3,
      "6.4 Computational and Resource Constraints"
    ],
    [
      3,
      "6.5 Privacy and Security Risks"
    ],
    [
      2,
      "7 Emerging Trends and Future Directions"
    ],
    [
      3,
      "7.1 Multimodal Integration for Comprehensive Biomedical Analysis"
    ],
    [
      3,
      "7.2 Efficient and Scalable Model Design for Resource-Constrained Environments"
    ],
    [
      3,
      "7.3 Domain Adaptation and Transfer Learning for Generalization"
    ],
    [
      3,
      "7.4 Enhancing Model Interpretability and Explainability in Clinical Settings"
    ],
    [
      3,
      "7.5 Knowledge-Enhanced Language Models for Biomedical Reasoning"
    ],
    [
      3,
      "7.6 Emerging Applications of Pre-trained Language Models in Biomedical Research"
    ],
    [
      2,
      "8 Conclusion"
    ],
    [
      2,
      "References"
    ]
  ],
  "content": [
    {
      "heading": "A Comprehensive Survey on Pre-Trained Language Models in the Biomedical Domain",
      "level": 1,
      "content": ""
    },
    {
      "heading": "1 Introduction",
      "level": 2,
      "content": "The biomedical domain has witnessed a paradigm shift in recent years, driven by the increasing availability of textual data and the transformative potential of pre-trained language models (PLMs). These models have emerged as a cornerstone in biomedical natural language processing (NLP), offering unprecedented capabilities in understanding and generating human-like text. Their relevance in the biomedical domain is underscored by the complexity and volume of clinical and scientific texts, which require sophisticated linguistic and contextual analysis [1; 2; 3]. As the volume of biomedical literature continues to grow exponentially, the need for efficient and accurate processing of such texts has never been more pressing [4]. This subsection provides an overview of the significance of PLMs in this domain, highlighting their transformative potential and the motivation for this survey. It introduces the context of biomedical NLP, the challenges these models aim to address, and their broader implications for healthcare and research.\n\nThe evolution of PLMs has been marked by significant milestones, from the initial successes of models like BERT and GPT to the emergence of domain-specific variants such as BioBERT, BioALBERT, and BioGPT [1; 2; 3]. These models have demonstrated superior performance on a range of biomedical tasks, including named entity recognition (NER), relation extraction, and question answering [1; 2]. The shift from general-domain models to domain-specific architectures reflects the growing recognition of the unique linguistic and structural characteristics of biomedical texts, which often include specialized terminology, complex sentence structures, and domain-specific idioms [1; 2]. However, adapting general-purpose models to biomedical data presents several challenges, including the need for large-scale, high-quality annotated corpora and the development of domain-specific pre-training strategies [1; 2; 3].\n\nThe motivation for this survey lies in the urgent need to address these challenges and to provide a comprehensive understanding of the current state of PLMs in the biomedical domain. By reviewing the latest advancements, evaluating the strengths and limitations of different approaches, and identifying emerging trends, this survey aims to guide future research and development in this critical area. The survey also explores the practical implications of these models, including their applications in clinical decision support, drug discovery, and biomedical literature summarization [3; 5]. As the field continues to evolve, the integration of PLMs with other modalities, such as multimodal data and knowledge graphs, is expected to further enhance their capabilities and expand their applications [5; 6].\n\nIn conclusion, the adoption of PLMs in the biomedical domain represents a significant step forward in the automation and enhancement of biomedical text processing. By addressing the unique challenges of this domain and leveraging the latest advancements in NLP, these models hold great promise for improving healthcare outcomes and advancing biomedical research. This survey aims to provide a comprehensive and critical analysis of the current landscape, offering valuable insights for researchers, practitioners, and policymakers in the field."
    },
    {
      "heading": "2.1 Early Foundations and Biomedical Text Processing",
      "level": 3,
      "content": "The early efforts to apply language models to biomedical text processing laid the groundwork for the development of domain-specific pre-trained models. Prior to the advent of deep learning-based approaches, biomedical text mining relied heavily on rule-based and statistical methods, which were limited in their ability to handle the complexity and variability of medical language [4]. These early approaches often struggled with the unique challenges posed by biomedical texts, such as specialized terminology, ambiguous abbreviations, and the need for precise semantic understanding. The transition from general-purpose models to domain-specific ones was driven by the recognition that the language of biomedical literature is distinct from that of general-domain texts, requiring tailored solutions to achieve effective information extraction and semantic analysis.\n\nThe first attempts to adapt general-domain language models to biomedical tasks revealed significant limitations. Early models, such as those trained on large corpora like Wikipedia or news articles, were ill-suited for biomedical tasks due to their lack of exposure to domain-specific vocabulary and syntactic patterns. This gap was evident in tasks like named entity recognition (NER), where the ability to identify and classify biomedical entities such as genes, proteins, and diseases was severely hampered by the absence of relevant training data [7]. As a result, researchers began exploring domain adaptation techniques to bridge this gap, aiming to improve the performance of general-domain models on biomedical tasks.\n\nOne of the early strategies involved the use of transfer learning, where pre-trained models were fine-tuned on biomedical corpora to enhance their domain-specific capabilities. This approach was exemplified by the development of BioBERT, which was pre-trained on biomedical texts and demonstrated superior performance on tasks such as NER and relation extraction compared to general-domain models [7]. Additionally, researchers experimented with vocabulary adaptation, such as the development of specialized vocabularies like MEDVOC, to better capture the domain-specific terminology present in biomedical texts [8]. These efforts highlighted the importance of tailoring both the model architecture and training data to the specific characteristics of biomedical texts.\n\nDespite these early successes, the limitations of general-domain models in handling biomedical tasks remained evident. The complexity of biomedical language, coupled with the scarcity of high-quality annotated data, posed significant challenges for model development and evaluation. These challenges underscored the need for more systematic approaches to domain adaptation and the development of specialized pre-training strategies. As the field progressed, the recognition of these limitations spurred further innovation, leading to the emergence of more advanced domain-specific models and the refinement of pre-training techniques that would become central to the evolution of biomedical language models. This early foundation set the stage for the transformative advancements that would follow in the domain of biomedical text processing."
    },
    {
      "heading": "2.2 Transition to Domain-Specific Pre-Training",
      "level": 3,
      "content": "The transition from general-purpose pre-trained language models to domain-specific biomedical architectures marks a pivotal shift in the evolution of natural language processing (NLP) for biomedical applications. While early models such as BERT [9] and GPT demonstrated impressive performance on general-domain tasks, their effectiveness in specialized biomedical contexts was limited due to the unique linguistic structure, terminology, and complexity of biomedical texts. This gap spurred the development of domain-specific pre-training strategies, leading to the emergence of models like BioBERT [9], which were explicitly designed to capture the nuances of biomedical language and improve performance on tasks such as named entity recognition (NER) and relation extraction.\n\nBioBERT, a domain-adapted version of BERT, was one of the first models to demonstrate the potential of pre-training on large-scale biomedical corpora. By leveraging the BERT architecture and fine-tuning it on biomedical texts, BioBERT achieved significant improvements over general-domain models on biomedical NLP tasks. This approach highlighted the importance of domain-specific pre-training in enhancing model understanding of biomedical concepts, as evidenced by its superior performance on tasks such as NER (0.62% F1 score improvement) and biomedical question answering (12.24% MRR improvement) [9]. The success of BioBERT underscored the need for models that are not only pre-trained on large text corpora but also adapted to the specific linguistic and semantic patterns of biomedical literature.\n\nSubsequent research further expanded on this concept, introducing models such as SciFive [10], which demonstrated the effectiveness of domain-specific T5 models in biomedical tasks. These models showed that pre-training on large biomedical corpora significantly improves performance on tasks requiring complex semantic understanding, such as relation extraction and natural language inference. The rise of domain-specific pre-training also led to the development of models like BioALBERT [11], which combined parameter reduction techniques with domain-specific pre-training to improve efficiency while maintaining high performance on biomedical NER tasks.\n\nThe transition to domain-specific pre-training was not without challenges. The scarcity of high-quality annotated biomedical datasets posed a significant obstacle, necessitating the development of techniques such as transfer learning and domain adaptation [11]. Additionally, the complexity of biomedical language, including the prevalence of abbreviations, acronyms, and specialized terminology, required innovative approaches to vocabulary adaptation and contextual modeling. The emergence of models like BADREX [11] demonstrated the importance of handling abbreviation expansion and coreference resolution, further highlighting the need for domain-specific adaptations.\n\nOverall, the shift toward domain-specific pre-training has fundamentally transformed the landscape of biomedical NLP. By focusing on the unique characteristics of biomedical text, these models have enabled more accurate and reliable processing of clinical and scientific literature, paving the way for advanced applications in drug discovery, clinical decision support, and biomedical information retrieval. Future research is expected to continue refining these models, with a focus on improving interpretability, efficiency, and adaptability across diverse biomedical subdomains."
    },
    {
      "heading": "2.3 Breakthroughs in Domain-Specific Pre-Training and Adaptation",
      "level": 3,
      "content": "The evolution of domain-specific pre-training and adaptation in biomedical language modeling has witnessed significant breakthroughs, driven by the need to address the unique challenges of biomedical texts. These advancements have been pivotal in enhancing model performance on domain-specific tasks such as named entity recognition, relation extraction, and question answering. One of the key breakthroughs in this area is the development of curriculum learning strategies, which aim to gradually expose models to increasingly complex biomedical data, thereby improving their ability to capture nuanced domain-specific patterns. This approach has been shown to enhance model robustness and generalization, particularly in tasks requiring deep understanding of biomedical terminology and context [11]. Another critical innovation is the refinement of vocabulary adaptation techniques, which address the limitations of general-domain word embeddings in capturing the specialized lexicon of biomedical texts. For example, the MEDVOC project [11] has demonstrated the effectiveness of custom vocabularies in improving model performance on tasks such as entity recognition and relation extraction by incorporating domain-specific terminology and syntactic structures. Contrastive learning has also emerged as a powerful technique in biomedical pre-training, where the model is trained to distinguish between semantically related and unrelated biomedical entities. This approach has been successfully applied in models like SapBERT [11], which leverages contrastive learning to enhance the representation of biomedical entities and improve performance on tasks such as entity linking and relation extraction. Additionally, the integration of knowledge graphs and ontologies into pre-training pipelines has significantly advanced the field. By incorporating structured biomedical knowledge, models can better understand complex relationships and improve their semantic representations. For example, UmlsBERT [11] integrates the Unified Medical Language System (UMLS) Metathesaurus to augment contextual embeddings with domain-specific knowledge, leading to improved performance on clinical NLP tasks. Another notable advancement is the use of domain-specific pre-training corpora, which have been shown to outperform general-domain models in biomedical tasks. Models such as BioBERT [11] and BioGPT [10] have demonstrated that pre-training on large-scale biomedical corpora significantly improves performance on tasks such as named entity recognition and text generation. These models not only capture the semantic nuances of biomedical texts but also reduce the need for extensive fine-tuning on domain-specific tasks. Furthermore, the development of domain adaptation strategies has enabled models to generalize across different biomedical subdomains, such as genomics, clinical notes, and drug discovery. Techniques such as transfer learning and meta-learning have been instrumental in this regard, allowing models to adapt to new domains with limited labeled data [12]. These advancements have collectively contributed to the emergence of more robust, efficient, and interpretable biomedical language models. As the field continues to evolve, future research is likely to focus on the integration of multimodal data, the development of more efficient pre-training methods, and the enhancement of model interpretability to support clinical decision-making. The ongoing refinement of these techniques will be crucial in addressing the unique challenges of the biomedical domain and unlocking the full potential of pre-trained language models in healthcare applications."
    },
    {
      "heading": "2.4 The Influence of Biomedical Datasets on Model Development",
      "level": 3,
      "content": "The development and evaluation of pre-trained language models (PLMs) in the biomedical domain have been profoundly influenced by the availability and characteristics of large-scale biomedical datasets. These datasets serve as the foundation for pre-training, fine-tuning, and evaluation, enabling models to learn the specialized language, terminology, and structures prevalent in biomedical texts. Over time, the growth of domain-specific corpora has not only enhanced model accuracy but also expanded their generalization capabilities across a wide range of biomedical tasks, from named entity recognition (NER) to clinical decision support and drug discovery [1; 3; 13].\n\nOne of the most influential datasets in this space is PubMed, a vast repository of biomedical literature that has been extensively used for pre-training and benchmarking. Its large-scale, high-quality text has enabled models like BioBERT [1] and PubMedBERT [14] to achieve state-of-the-art performance in tasks such as NER and relation extraction. Similarly, the MIMIC-III dataset, which contains de-identified clinical notes, has been critical in developing models tailored for clinical NLP tasks [1; 3]. These datasets have provided the necessary diversity and depth for models to learn the nuances of biomedical language, including specialized terminologies, abbreviations, and complex syntactic structures.\n\nThe emergence of domain-specific datasets such as BLURB [15], the Clinical NLP datasets [16], and the recent DrBenchmark [17] has further advanced the field by enabling more targeted evaluations and facilitating the development of models that are optimized for specific subdomains. These datasets not only support the training of models but also serve as benchmarks for assessing their performance. For instance, the BLURB benchmark has been instrumental in evaluating relation extraction models, while DrBenchmark has provided a comprehensive evaluation framework for French biomedical models [17].\n\nThe impact of these datasets extends beyond model training; they also influence the design of pre-training objectives and fine-tuning strategies. For example, models like BioBART [18] and BioMedGPT [5] have leveraged domain-specific corpora to develop pre-training strategies that align with the unique challenges of biomedical texts, such as handling long-range dependencies and capturing semantic relationships. Furthermore, the integration of structured knowledge from ontologies like UMLS [19] and SNOMED-CT into pre-training pipelines has demonstrated the potential of combining textual and semantic information to enhance model performance.\n\nDespite these advances, challenges remain, particularly in ensuring data quality, handling domain variability, and addressing the computational demands of training on large-scale datasets. Emerging trends, such as the use of synthetic data generation and domain adaptation techniques [20], suggest that the future of biomedical PLMs will continue to be shaped by the evolution of these datasets. As the biomedical domain becomes increasingly data-driven, the role of high-quality, annotated, and diverse datasets will remain central to the development and evaluation of pre-trained language models."
    },
    {
      "heading": "3.1 Transformer-Based Architectures in Biomedical NLP",
      "level": 3,
      "content": "Transformer-based architectures have become the cornerstone of modern natural language processing (NLP) systems, particularly in the biomedical domain where the complexity of language, domain-specific terminology, and long-range dependencies pose significant challenges. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers leverage self-attention mechanisms to capture contextual relationships across long sequences, making them highly suitable for biomedical text mining tasks such as named entity recognition (NER), relation extraction, and question answering [1]. This subsection examines the adaptation and optimization of transformer-based models for biomedical NLP, focusing on their ability to handle domain-specific language, contextual embeddings, and the scalability required for large biomedical corpora.\n\nThe success of transformer models in biomedical NLP is largely attributed to their capacity to learn contextualized representations that capture the nuanced semantics of biomedical texts. For instance, models like BioBERT [1] and BioALBERT [2] are pre-trained on large-scale biomedical corpora, such as PubMed and clinical notes, allowing them to effectively model the specialized language and structure of biomedical literature. These models utilize the bidirectional transformer architecture to capture dependencies in both forward and backward directions, which is crucial for understanding complex biomedical contexts. Furthermore, the incorporation of domain-specific vocabularies, such as MEDVOC [21], enhances their ability to recognize and interpret biomedical terminology.\n\nDespite their effectiveness, transformer models face several challenges in biomedical NLP, including computational costs and model scalability. The size of biomedical datasets, often in the order of millions of documents, requires efficient training and inference strategies. To address these challenges, recent research has focused on optimizing transformer architectures through techniques such as parameter pruning, quantization, and low-rank adaptation [6]. For example, models like BioMamba [6] leverage the Mamba architecture, which provides a more efficient alternative to traditional transformers while maintaining performance on biomedical tasks.\n\nAdditionally, the integration of knowledge graphs and ontologies has emerged as a promising avenue for enhancing the interpretability and performance of transformer models in biomedical NLP. By incorporating structured biomedical knowledge, such as UMLS [21] and SNOMED-CT, these models can better contextualize their predictions and improve their robustness in clinical settings. Furthermore, multimodal approaches that combine textual and visual data, as seen in BioMedGPT [5], are beginning to show significant improvements in biomedical understanding and task performance.\n\nIn conclusion, the adaptation of transformer-based architectures to biomedical NLP represents a major advancement in the field, enabling more accurate and context-aware processing of biomedical texts. However, ongoing research is needed to address the challenges of scalability, efficiency, and interpretability. Future work should explore more efficient training strategies, better integration of domain knowledge, and the development of robust evaluation metrics to ensure the reliability and generalizability of these models in real-world biomedical applications."
    },
    {
      "heading": "3.2 Domain-Specific Pre-Training Strategies",
      "level": 3,
      "content": "Domain-specific pre-training strategies are crucial for equipping language models with the specialized knowledge and contextual understanding necessary to excel in biomedical tasks. Unlike general-domain models, which are pre-trained on a broad range of text, domain-specific pre-training leverages biomedical corpora to align model representations with the unique linguistic and conceptual patterns of the biomedical domain. This subsection examines the methodologies, objectives, and data sources that define these strategies, highlighting their impact on model performance and their evolving landscape.\n\nOne of the most prominent approaches in domain-specific pre-training is masked language modeling (MLM), which has been widely adopted in biomedical settings. By randomly masking tokens during pre-training and requiring the model to predict them based on context, MLM encourages the model to learn nuanced semantic and syntactic relationships within biomedical text. This technique has been successfully applied in models like BioBERT [1], which achieved significant improvements over general-domain models in tasks such as named entity recognition and relation extraction. The effectiveness of MLM is further enhanced by incorporating biomedical corpora, such as PubMed and clinical notes, into the pre-training pipeline [1; 2]. These corpora provide rich, domain-specific context that general-domain models lack, enabling better understanding of complex biomedical terminology and relationships.\n\nContrastive learning has emerged as another powerful paradigm for domain-specific pre-training, particularly in scenarios where labeled data is scarce. By learning to distinguish between positive and negative examples, contrastive models can capture more robust and generalizable representations. This approach has been successfully applied in models like SciFive [22], which demonstrated strong performance on biomedical relation extraction and question-answering tasks. Contrastive learning is especially beneficial in the biomedical domain, where the semantic richness of text necessitates models that can discern subtle differences in meaning and context.\n\nBeyond MLM and contrastive learning, other pre-training objectives, such as entity prediction and dialogue modeling, have also been explored to address specific challenges in biomedical text. For instance, models like BioBART [18] have integrated language generation tasks to enhance their ability to produce coherent and semantically accurate biomedical content. These approaches highlight the importance of tailoring pre-training objectives to the specific needs of the biomedical domain, where tasks range from information extraction to text generation.\n\nThe choice of data sources is equally critical in domain-specific pre-training. Large-scale biomedical corpora, including PubMed abstracts, clinical notes, and structured knowledge bases, provide the foundational data for these models. However, the quality, diversity, and coverage of these datasets vary, posing challenges for model generalization. Recent efforts, such as the BioRED dataset [23], aim to address these limitations by providing richly annotated, multi-type relational data that can better support complex biomedical tasks.\n\nWhile domain-specific pre-training has led to substantial performance gains, challenges remain. These include the need for more diverse and high-quality annotated data, as well as the balance between model size and efficiency. Future directions may involve hybrid pre-training strategies that combine multiple objectives and data sources, as well as advances in domain adaptation techniques to enhance model generalization across subdomains. As the biomedical NLP field continues to evolve, the development of more sophisticated and context-aware pre-training strategies will be essential to unlocking the full potential of language models in this critical domain."
    },
    {
      "heading": "3.3 Knowledge-Enhanced Modeling with Ontologies and Graphs",
      "level": 3,
      "content": "Biomedical language models have increasingly incorporated knowledge graphs, ontologies, and structured data to enhance their understanding of domain-specific concepts. This integration enables models to leverage structured knowledge for tasks such as entity linking, relationship extraction, and concept normalization, which are critical for biomedical text mining. The use of formalized knowledge representations allows models to capture semantic relationships and contextual nuances that may be challenging to infer solely from unstructured text. This subsection explores the methodologies, benefits, and challenges of integrating structured knowledge into biomedical language models.\n\nOne prominent approach involves the use of biomedical ontologies such as the Unified Medical Language System (UMLS) and SNOMED-CT to guide the pre-training and fine-tuning of language models. For example, UmlsBERT [19] integrates domain knowledge during the pre-training process by connecting words with the same underlying concept in UMLS, as well as leveraging semantic group knowledge to create clinically meaningful input embeddings. This approach enhances the model’s ability to encode clinical domain knowledge into word embeddings, leading to improved performance on named entity recognition (NER) and clinical natural language inference tasks. Similarly, the incorporation of knowledge graphs into pre-training pipelines, as seen in LinkBERT [24], leverages document links (e.g., hyperlinks or citation links) to improve multi-hop reasoning and few-shot question-answering tasks. By pretraining on graph-structured data, models can better capture relational patterns and dependencies that are essential in biomedical contexts.\n\nAnother method involves the alignment of language models with structured biomedical knowledge through techniques such as knowledge-based prompting and retrieval. For instance, SapBERT [25] uses a metric learning framework to self-align the representation space of biomedical entities, leveraging UMLS, a large collection of biomedical ontologies. This approach enables more accurate entity linking and synonym modeling, which is vital for tasks requiring precise semantic understanding. Additionally, knowledge graph embeddings have been employed to enhance semantic representation in biomedical models. For example, the integration of structured knowledge from biomedical ontologies into pre-trained models like BioBERT [1] has been shown to improve performance on tasks such as relation extraction and question answering by providing richer contextual information.\n\nHowever, integrating structured knowledge into language models presents several challenges. These include the need for high-quality, well-annotated knowledge bases, the complexity of aligning unstructured text with structured representations, and the computational cost of incorporating external knowledge. Furthermore, the dynamic nature of biomedical knowledge requires models to continuously adapt and update their understanding as new information emerges. Despite these challenges, the benefits of knowledge-enhanced modeling—such as improved interpretability, accuracy, and generalization—make it a promising direction for advancing biomedical NLP. Future research may focus on more efficient and scalable methods for integrating structured knowledge, as well as exploring hybrid approaches that combine textual and structured data for more robust modeling."
    },
    {
      "heading": "3.4 Model Efficiency and Adaptation Techniques",
      "level": 3,
      "content": "The efficiency and practical deployment of pre-trained biomedical language models have become critical concerns as these models grow in size and complexity. While large-scale models such as BioBERT, BioGPT, and BioMedGPT have demonstrated state-of-the-art performance on biomedical tasks, their computational and memory demands often limit their applicability in real-world settings, especially in resource-constrained environments [1; 3]. To address this challenge, researchers have explored various techniques to improve model efficiency, including quantization, knowledge distillation, and low-rank adaptation, each with distinct trade-offs in performance, model size, and inference speed.\n\nQuantization techniques aim to reduce the precision of model weights, typically from 32-bit floating-point to 8-bit integers or even lower, thereby significantly decreasing model size and accelerating inference without substantial loss of performance. For instance, the work of [26] demonstrates how quantization can enable the deployment of large models on edge devices while maintaining competitive performance on biomedical benchmarks. However, quantization can sometimes lead to precision loss, particularly for models with complex parameter distributions, necessitating careful calibration and post-training optimization [27].\n\nKnowledge distillation, another widely used approach, involves training a smaller model (the student) to mimic the behavior of a larger, more accurate model (the teacher). This method has been successfully applied in biomedical NLP tasks, where the student model can achieve performance close to that of the teacher while requiring fewer computational resources. The work in [28] showcases how distillation can be effectively used to adapt large models to clinical reasoning tasks, offering a scalable and efficient solution for domain-specific applications. However, the effectiveness of distillation depends heavily on the quality of the teacher model and the availability of diverse training data.\n\nLow-rank adaptation (LoRA), a parameter-efficient fine-tuning technique, has also gained traction in the biomedical domain. By introducing low-rank matrices to adapt pre-trained models, LoRA reduces the number of trainable parameters while maintaining model performance on specialized tasks. This approach is particularly beneficial for tasks with limited labeled data, such as rare disease identification, where full fine-tuning of large models is impractical [29]. Compared to traditional fine-tuning, LoRA offers a more efficient and cost-effective alternative, although it may not always match the performance of full parameter adaptation.\n\nThe discussion on model efficiency naturally leads into the development of multimodal and hybrid architectures, which aim to enhance the capabilities of biomedical language models by integrating diverse data types such as text, images, and structured biomedical data. These architectures are designed to address the limitations of unimodal systems, offering improved performance on complex tasks such as medical image-text alignment, clinical decision support, and comprehensive information retrieval. By leveraging the complementary strengths of different modalities, these models can better capture the nuanced relationships within biomedical data, thereby improving their performance and applicability in real-world clinical scenarios [30]. The integration of multimodal and hybrid approaches not only expands the scope of biomedical NLP but also opens new avenues for more robust and versatile language models."
    },
    {
      "heading": "3.5 Multimodal and Hybrid Architectures for Biomedical Applications",
      "level": 3,
      "content": "Multimodal and hybrid architectures represent a significant advancement in biomedical language modeling, addressing the limitations of unimodal systems by integrating diverse data types such as text, images, and structured biomedical data. These architectures are designed to enhance the capabilities of biomedical language models in complex tasks, including medical image-text alignment, clinical decision support, and comprehensive information retrieval. By leveraging the complementary strengths of different modalities, these models can better capture the nuanced relationships within biomedical data, thereby improving their performance and applicability in real-world clinical scenarios [30].\n\nThe design of hybrid architectures involves the fusion of textual and non-textual modalities, requiring sophisticated techniques to align and process heterogeneous data sources. For example, in the context of medical image-text alignment, models like BiomedCLIP [30] have been developed to pretrain on large-scale image-text pairs, enabling them to understand and generate captions for biomedical images. This approach not only improves the model's ability to describe images but also enhances its capacity to understand the context of biomedical literature through multimodal representations. Similarly, models like BioMedGPT [5] integrate text, images, and structured data, demonstrating the potential of hybrid architectures to support a wide range of biomedical tasks such as disease diagnosis, treatment planning, and knowledge discovery.\n\nA key challenge in the development of multimodal and hybrid architectures is the alignment of different modalities. This involves not only the technical challenge of integrating diverse data types but also the need to ensure that the model can effectively learn from and generalize across these modalities. For instance, the integration of structured data, such as electronic health records (EHRs) and clinical ontologies, requires careful consideration of how this data can be represented and utilized within the model. Techniques such as knowledge graphs and ontologies have been proposed to facilitate this integration, as seen in the work of [31], which provides a standardized schema for biomedical knowledge graphs.\n\nThe performance of multimodal and hybrid architectures is often evaluated using domain-specific benchmarks and datasets that reflect the complexity of real-world biomedical data. For example, the Biomedical Relation Extraction Dataset (BioRED) [23] and the PubMedQA dataset [32] provide rich, annotated data that enable the assessment of model capabilities in tasks such as relation extraction and question answering. These benchmarks are essential for evaluating the effectiveness of hybrid models and for guiding the development of more robust and generalizable architectures.\n\nEmerging trends in this field include the use of contrastive learning and domain adaptation techniques to improve the performance of multimodal models. Contrastive learning, as demonstrated in the work of [1], has shown promise in enhancing the model's ability to capture semantic relationships between different modalities. Additionally, domain adaptation strategies are being explored to enable models to generalize across different biomedical subdomains, as highlighted in the study by [33].\n\nIn conclusion, the development of multimodal and hybrid architectures for biomedical applications represents a critical direction in the evolution of pre-trained language models. These architectures offer significant potential to enhance the capabilities of biomedical language models, enabling them to address complex tasks with greater accuracy and efficiency. However, challenges such as data alignment, model generalization, and computational efficiency remain to be addressed, providing fertile ground for future research and innovation in this rapidly evolving field."
    },
    {
      "heading": "4.1 Clinical Text Analysis and Information Extraction",
      "level": 3,
      "content": "Clinical text analysis and information extraction play a pivotal role in transforming unstructured clinical data, such as electronic health records (EHRs) and clinical notes, into actionable insights. Pre-trained language models have significantly advanced the ability to extract structured information from these texts, enabling downstream clinical decision support systems. Among the key tasks in this domain are named entity recognition (NER), relation extraction, and semantic role labeling, each of which presents unique challenges and opportunities.\n\nNamed Entity Recognition (NER) is the foundational task in clinical text analysis, aiming to identify and classify medical entities such as diseases, medications, procedures, and anatomical terms. Traditional NER approaches often rely on rule-based systems or statistical models, but pre-trained language models have demonstrated superior performance due to their ability to capture contextual and domain-specific semantics. For instance, models like BioBERT [1] and scispaCy [34] have been fine-tuned on biomedical corpora to improve their accuracy in recognizing clinical entities. These models benefit from large-scale pre-training on biomedical texts, allowing them to understand the complex and often ambiguous language found in clinical narratives.\n\nRelation extraction further builds on NER by identifying semantic relationships between extracted entities, such as drug–adverse effect or gene–disease interactions. This task is crucial for knowledge discovery and clinical reasoning, as it enables the construction of structured biomedical knowledge graphs. Techniques such as graph-based models and dependency parsing have been applied to relation extraction, but pre-trained models like Bioformer [35] have shown improved performance by leveraging contextualized embeddings and domain-specific pre-training. These models are particularly effective in capturing long-range dependencies and nuanced relationships that are common in clinical texts.\n\nSemantic role labeling (SRL) complements NER and relation extraction by identifying the roles of entities in clinical events, such as the administration of a medication or the occurrence of a diagnostic procedure. SRL provides a deeper understanding of the clinical context, which is essential for applications like clinical decision support and patient safety monitoring. Recent advancements in SRL have leveraged pre-trained models like BioBART [18], which have demonstrated strong performance on clinical text due to their ability to generate contextually relevant outputs and capture syntactic and semantic structures effectively.\n\nThe integration of these tasks into clinical workflows has the potential to greatly enhance the efficiency and accuracy of healthcare delivery. However, challenges remain, including the need for high-quality annotated datasets, the complexity of clinical language, and the requirement for models to generalize across different clinical settings. Emerging trends, such as the use of multimodal models and domain adaptation strategies, offer promising solutions to these challenges [5; 13]. As the field continues to evolve, further research into model interpretability, scalability, and ethical considerations will be essential to ensure the safe and effective deployment of these technologies in real-world clinical environments."
    },
    {
      "heading": "4.2 Medical Question Answering and Diagnostic Support",
      "level": 3,
      "content": "Pre-trained language models have emerged as powerful tools for medical question answering (QA) and diagnostic support, addressing the critical need for accurate, context-aware, and reliable responses in clinical settings. These models, such as ClinicalGPT [36] and Med-PaLM 2 [37], are trained on extensive biomedical corpora and fine-tuned to handle domain-specific queries, providing support for both healthcare professionals and patients. The application of such models in medical QA systems is driven by their ability to process unstructured clinical texts, extract relevant information, and generate precise answers. This subsection explores the design, evaluation, and challenges associated with these models, as well as their potential to transform clinical decision-making.\n\nOne of the key strategies for enhancing medical QA systems is the integration of domain-specific knowledge and fine-tuning on specialized datasets. For instance, BioBERT [1] and BioALBERT [2] have demonstrated improved performance on biomedical NER and relation extraction tasks, which serve as foundational components for QA systems. Similarly, models like Med-PaLM 2 [37] and ClinicalGPT [36] leverage large-scale biomedical data and instruction tuning to enhance their ability to answer complex clinical questions. These models are often trained on curated datasets such as BioASQ [2] and PubMedQA [32], which provide structured annotations for factoid and list-based questions.\n\nThe performance of pre-trained models in medical QA is often evaluated using metrics such as F1 score, accuracy, and mean reciprocal rank (MRR). However, challenges such as data scarcity, domain shift, and model interpretability remain significant barriers. Studies have shown that while general-domain models like GPT-3 exhibit reasonable performance in few-shot settings, they often underperform compared to domain-specific models such as BioBERT or BioALBERT [38]. Furthermore, recent works have highlighted the importance of integrating external knowledge sources, such as medical ontologies and knowledge graphs, to enhance the factual accuracy and contextual understanding of QA systems [5].\n\nDiagnostic support systems, which aim to assist clinicians in interpreting symptoms, identifying potential conditions, and recommending treatments, also benefit from pre-trained language models. These systems often rely on complex reasoning and the ability to process multi-modal data, including clinical notes, imaging reports, and genetic information. Models like BioBART [18] and BioRED [23] have been employed for tasks such as relation extraction and clinical reasoning, demonstrating their effectiveness in supporting diagnostic workflows.\n\nDespite these advancements, several challenges persist, including the need for more robust evaluation benchmarks, improved model generalization, and better handling of rare or novel medical conditions. Future research should focus on enhancing model interpretability, leveraging multi-modal data, and developing more efficient adaptation strategies to ensure reliability and safety in clinical practice [37; 36]. As the field continues to evolve, pre-trained language models are poised to play a central role in advancing medical QA and diagnostic support, ultimately contributing to more accurate and efficient healthcare delivery."
    },
    {
      "heading": "4.3 Biomedical Literature Summarization and Information Retrieval",
      "level": 3,
      "content": "Pre-trained language models have emerged as powerful tools for addressing the growing challenge of information overload in biomedical literature. As the volume of biomedical research continues to expand exponentially, the need for efficient summarization and information retrieval systems has become critical. These models, particularly those tailored for the biomedical domain, have demonstrated significant potential in condensing complex scientific texts and enhancing the precision of literature search, thereby enabling researchers and clinicians to navigate the vast corpus of medical knowledge more effectively.\n\nBiomedical text summarization involves the extraction and condensation of key information from scientific articles, clinical reports, and other textual resources. Traditional methods often rely on heuristic rule-based systems or statistical models, which struggle to capture the nuanced relationships between biomedical entities and their contextual dependencies. Pre-trained models, on the other hand, leverage contextual embeddings to better understand the semantics of biomedical texts. For instance, BERT-based summarization models have shown superior performance compared to conventional approaches by capturing the syntactic and semantic structure of biomedical documents [7]. However, domain-specific adaptations, such as the use of BioBERT and SciBERT, have further enhanced the ability to handle domain-specific terminology and improve the relevance of generated summaries [39; 7].\n\nInformation retrieval in the biomedical domain presents additional challenges due to the complexity and variability of terminology. Pre-trained models like BERT and its variants have been adapted for this task through fine-tuning on biomedical corpora, leading to improved performance in ranking and retrieving relevant documents. For example, models trained on PubMed abstracts have outperformed general-domain models in tasks such as relevance ranking and query expansion [7; 40]. Furthermore, the integration of knowledge graphs and ontologies has enhanced the semantic understanding of biomedical terms, enabling more precise and context-aware retrieval [41].\n\nRecent advances have also explored the use of multimodal approaches, combining textual and structured data to enhance both summarization and retrieval tasks. For example, models like BioLinkBERT and LinkBERT leverage document-level links to improve the coherence and accuracy of summaries and the relevance of search results [24; 42]. These models demonstrate that incorporating relational and structural information from biomedical literature can significantly improve the quality of both summarization and retrieval.\n\nDespite these advances, several challenges remain, including the need for better domain adaptation strategies, the handling of rare or out-of-vocabulary terms, and the scalability of models to large biomedical corpora. Moreover, the dynamic nature of biomedical knowledge requires continuous updates and retraining of models to maintain their relevance and accuracy. Future research should focus on developing more efficient and robust methods for domain-specific pre-training, as well as on improving the interpretability and explainability of these models to ensure their reliability in clinical and research settings [43; 44]."
    },
    {
      "heading": "4.4 Drug Discovery and Genomic Analysis",
      "level": 3,
      "content": "Pre-trained language models (PLMs) have emerged as transformative tools in the fields of drug discovery and genomic analysis, offering novel approaches to understanding molecular relationships, predicting drug interactions, and enabling personalized medicine. By leveraging vast biomedical corpora and advanced representation learning techniques, these models have significantly advanced the ability to process and extract meaningful insights from complex biological data. In drug discovery, PLMs have been instrumental in identifying potential drug targets, predicting molecular interactions, and accelerating the development of novel therapeutics [3]. For instance, models such as Tx-LLM and BioGPT demonstrate the ability to generate and rank candidate molecules based on their predicted efficacy and safety profiles, reducing the need for extensive and costly experimental screening [3].\n\nIn genomic analysis, PLMs have facilitated the interpretation of DNA and RNA sequences by capturing long-range dependencies and contextual relationships within biological sequences. This has led to significant improvements in gene function prediction, variant interpretation, and the identification of regulatory elements [18]. For example, BioBART, a generative language model pre-trained on biomedical texts, has shown improved performance in tasks such as variant annotation and gene expression prediction compared to traditional methods [18]. Additionally, models like UmlsBERT have integrated domain-specific knowledge from the Unified Medical Language System (UMLS) to enhance the accuracy of biomedical entity recognition and relation extraction tasks, providing richer contextual information for genomic studies [19].\n\nPersonalized medicine benefits greatly from the application of PLMs in analyzing patient-specific data, including genetic profiles and clinical histories. These models can generate tailored treatment recommendations by synthesizing information from diverse sources, such as electronic health records, genomic data, and biomedical literature. For example, models like MedAdapter have been designed to adapt large language models (LLMs) to biomedical reasoning tasks, enabling more accurate and context-aware clinical decision support [45]. This has the potential to improve patient outcomes by facilitating precision medicine approaches that account for individual variability.\n\nDespite these advancements, challenges remain in ensuring the reliability and generalizability of PLMs across diverse biomedical tasks. Issues such as data scarcity, model interpretability, and the integration of structured biomedical knowledge continue to pose significant hurdles. Recent work has explored domain adaptation strategies, such as contrastive learning and curriculum learning, to enhance model performance on specialized tasks while maintaining computational efficiency [46; 47]. Additionally, the development of open-source models like MEDITRON and BioMegatron has underscored the importance of large-scale pre-training on domain-specific corpora for achieving state-of-the-art results in biomedical NLP tasks [26; 13].\n\nLooking ahead, the integration of multimodal data, including biomedical images and structured clinical data, is expected to further expand the capabilities of PLMs in drug discovery and genomic analysis. This, combined with advancements in domain adaptation and explainability, will be crucial in addressing the unique challenges of the biomedical domain and enabling the widespread adoption of these models in clinical and research settings."
    },
    {
      "heading": "4.5 Multimodal and Cross-Domain Applications",
      "level": 3,
      "content": "The integration of pre-trained language models (PLMs) with multimodal data and cross-domain adaptation strategies represents a transformative direction in biomedical natural language processing (NLP). This subsection explores how these approaches enhance the ability of models to address complex biomedical tasks, such as diagnostic support, knowledge discovery, and clinical decision-making, by leveraging diverse data modalities and transferring learned representations across domains.\n\nMultimodal models combine textual data with other forms of information, such as medical images, structured clinical data, and genomic sequences. These models are particularly valuable in scenarios where unstructured text alone is insufficient to capture the full context of biomedical information. For example, BiomedCLIP [11] demonstrates the potential of multimodal pre-training by leveraging 15 million biomedical image-text pairs to create a model capable of aligning textual descriptions with visual data. This approach enables more accurate interpretation of medical images, as evidenced by its state-of-the-art performance in radiology-specific tasks such as pneumonia detection. Similarly, models like Clinical Camel [11] integrate dialogue-based knowledge encoding to enhance their ability to synthesize clinical notes and provide contextually relevant insights. By fusing textual and clinical data, these models improve their ability to support clinical decision-making and patient care.\n\nCross-domain adaptation further expands the applicability of PLMs by enabling them to generalize across different biomedical subdomains. For instance, BioBART [10] is a generative language model that is pre-trained on PubMed abstracts and fine-tuned for biomedical language generation tasks. This model demonstrates that domain-specific pre-training can significantly enhance performance on tasks such as dialogue, summarization, and entity linking. Additionally, models like DrBERT [11] highlight the benefits of cross-lingual adaptation, showing that pre-trained models can be effectively fine-tuned on domain-specific data to achieve strong performance in French biomedical tasks. Such approaches are crucial in scenarios where annotated data is scarce, as they allow models to leverage knowledge from related domains or languages.\n\nThe integration of structured biomedical data, such as knowledge graphs and ontologies, further enriches the capabilities of PLMs. For example, KECI [12] employs external knowledge to enhance joint entity and relation extraction, demonstrating that incorporating structured information improves model accuracy and reliability. Similarly, the BioMegatron [11] model highlights the importance of large-scale pre-training on biomedical corpora, as it achieves significant improvements over previous state-of-the-art models in named entity recognition, relation extraction, and question answering tasks.\n\nDespite these advancements, several challenges remain. Multimodal and cross-domain models often require substantial computational resources and carefully curated datasets. Moreover, ensuring model interpretability and ethical compliance in clinical settings remains a critical concern. Future research should focus on developing more efficient training strategies, enhancing model generalization, and integrating multimodal and cross-domain approaches into real-world clinical workflows. By addressing these challenges, the next generation of PLMs will be better equipped to support the complex and evolving demands of biomedical research and healthcare."
    },
    {
      "heading": "5.1 Key Biomedical Datasets and Benchmarks",
      "level": 3,
      "content": "The evaluation of pre-trained language models in the biomedical domain hinges on the availability of high-quality, domain-specific datasets and benchmarks. These resources are essential for training, validating, and benchmarking models, particularly for tasks such as named entity recognition (NER), relation extraction (RE), and information retrieval (IR). The diversity and specificity of these datasets reflect the complex and evolving nature of biomedical text, which includes clinical notes, scientific publications, and structured medical ontologies. This subsection provides an overview of the foundational datasets and benchmarks that have shaped the evaluation of biomedical language models, highlighting their strengths, limitations, and the challenges they present.\n\nOne of the most widely used biomedical datasets is PubMed, a vast repository of biomedical literature that provides a rich source of scientific texts for training and evaluating models. PubMed has been instrumental in pre-training models such as BioBERT [1], which demonstrated significant improvements in biomedical NER and RE tasks. However, while PubMed offers a broad and diverse corpus, its primary limitation lies in its lack of structured annotations, making it less suitable for fine-grained NLP tasks that require detailed entity and relation labels [1].\n\nIn contrast, the MIMIC-III dataset provides a more structured and annotated source of clinical text, consisting of electronic health records (EHRs) with extensive metadata [48]. This dataset has been widely used for tasks such as clinical information extraction and patient outcome prediction. However, the complexity and sensitivity of EHR data pose significant challenges for model training and evaluation, as they require careful handling of privacy and data anonymization [48].\n\nThe BioNLP datasets, including the BioNLP-ST and BIONLP09, have been specifically designed for biomedical NER and RE tasks, providing annotated corpora that are essential for benchmarking domain-specific models [49]. These datasets, while highly valuable, are often limited in size and scope, which can hinder the generalizability of models trained on them. Recent studies have highlighted the need for larger, more diverse annotated datasets to better capture the variability of biomedical language [49].\n\nIn addition to traditional datasets, specialized benchmarks such as the BioASQ, BLURB, and MEDIC have been developed to evaluate the performance of models in specific biomedical tasks. BioASQ, for example, focuses on question-answering and information retrieval, while BLURB provides a standardized framework for relation extraction tasks [2; 15]. These benchmarks are crucial for assessing the robustness and adaptability of pre-trained models, but they often require domain-specific adaptations to account for the unique challenges of biomedical text [15].\n\nThe emergence of multilingual and cross-lingual benchmarks, such as KBioXLM and JMedBench, has further expanded the scope of biomedical evaluation, allowing models to be tested across different languages and regions [50; 51]. These datasets are particularly important for developing models that can generalize across diverse linguistic and cultural contexts, although they also introduce new challenges related to data scarcity and annotation quality.\n\nIn conclusion, the evaluation of biomedical language models relies on a combination of large-scale, domain-specific datasets and specialized benchmarks that reflect the complexity of biomedical text. While existing resources have enabled significant progress, there remains a need for more comprehensive, annotated, and diverse datasets to address the evolving challenges of biomedical NLP. Future research should focus on the development of scalable, multi-modal benchmarks that can better capture the nuances of biomedical language and support the continued advancement of pre-trained models in this domain."
    },
    {
      "heading": "5.2 Evaluation Metrics and Performance Criteria",
      "level": 3,
      "content": "The evaluation of pre-trained language models (PLMs) in the biomedical domain requires a nuanced understanding of both traditional and domain-specific metrics that reflect model accuracy, practical utility, and robustness. Traditional evaluation metrics such as accuracy, F1 score, and AUC-ROC are widely used for classification and information extraction tasks, providing a baseline for model performance. However, these metrics often fail to capture the intricacies of biomedical tasks, necessitating the development of domain-specific criteria that align with the unique challenges of biomedical text [1]. For instance, in tasks like named entity recognition (NER) and relation extraction, domain-specific metrics such as the BLURB score [2] and the MedS-Bench score [52] have been introduced to evaluate the model's ability to capture biomedical relationships and semantic nuances.\n\nIn addition to these metrics, performance indicators tailored for clinical applications, such as precision, recall, and coverage, are critical for assessing model reliability in high-stakes settings. These metrics are particularly relevant in tasks like medical question answering, where model outputs must be both accurate and actionable [52]. Furthermore, interpretability and reliability metrics, such as hallucination detection and consistency checks, are gaining prominence as researchers seek to ensure that models provide trustworthy and transparent results [52]. These metrics are essential for deploying models in clinical workflows, where errors can have significant consequences.\n\nEmerging trends in evaluation focus on the integration of multimodal data and the use of retrieval-augmented generation (RAG) to enhance model performance and reduce hallucinations [53]. For example, RAG techniques have been shown to improve the accuracy of biomedical question-answering systems by leveraging external knowledge sources [53]. Additionally, the use of domain-specific knowledge graphs and ontologies has been explored to enhance semantic representation and improve model performance in tasks like entity linking and relation extraction [18].\n\nThe development of cross-lingual and multilingual benchmarks is another emerging trend, aiming to evaluate model adaptability across different languages and regions [51]. These benchmarks are crucial for assessing the generalizability of models in diverse biomedical contexts. Furthermore, the integration of human-in-the-loop evaluation and expert validation is becoming increasingly important to ensure that models meet clinical relevance and reliability standards [52].\n\nDespite these advancements, challenges remain in benchmarking due to data scarcity, domain specificity, and variability in biomedical text [54]. Future research should focus on developing dynamic and continuously updated benchmarks that reflect the evolving nature of biomedical research [55]. Additionally, there is a need for more rigorous evaluation protocols that account for the ethical and privacy implications of deploying models in clinical settings [56]. By addressing these challenges, the field can move toward more robust, reliable, and clinically applicable pre-trained language models."
    },
    {
      "heading": "5.3 Challenges in Biomedical Model Benchmarking",
      "level": 3,
      "content": "The benchmarking of pre-trained language models (PLMs) in the biomedical domain presents a set of unique and complex challenges that distinguish it from general NLP benchmarking. These challenges stem from the specialized nature of biomedical data, the limited availability of high-quality annotated datasets, and the inherent difficulty in evaluating model reliability and generalizability. As a result, developing robust and effective evaluation frameworks for biomedical PLMs requires careful consideration of these factors.\n\nOne of the primary challenges in biomedical model benchmarking is the scarcity and variability of domain-specific datasets. Unlike general-domain corpora, which are often large and well-structured, biomedical datasets are frequently limited in size and annotated with inconsistent or incomplete labels. For example, while datasets like PubMed and MIMIC-III provide valuable resources, they often lack the comprehensive annotations necessary for evaluating complex tasks such as entity linking or relation extraction [57]. Furthermore, the domain-specific nature of biomedical language, characterized by specialized terminologies, abbreviations, and syntactic structures, complicates the development of universally applicable evaluation metrics [8].\n\nAnother significant challenge lies in the generalizability of models across different biomedical subdomains. While a model may perform well on a specific task, such as named entity recognition (NER), it may struggle when applied to a new domain, such as genomics or clinical decision support. This issue is exacerbated by the fact that biomedical texts vary widely in structure and content, making it difficult to design a single benchmark that captures the full scope of the domain. Recent studies have highlighted the need for domain adaptation strategies that can improve model performance on unseen domains without requiring extensive retraining [58].\n\nAdditionally, the evaluation of biomedical PLMs is complicated by the high stakes associated with their deployment in clinical settings. Errors in model predictions can have serious consequences, making it essential to ensure both accuracy and interpretability. However, many state-of-the art models lack transparency, making it difficult to assess their reliability. This issue is particularly relevant in tasks like medical question answering, where the ability to explain a model's reasoning is as important as its accuracy [59].\n\nTo address these challenges, researchers are increasingly focusing on the development of cross-domain benchmarks and the integration of knowledge graphs to enhance model performance. For instance, the use of biomedical ontologies such as UMLS has shown promise in improving the interpretability and reliability of models [60]. However, the effective integration of such knowledge remains a complex and ongoing research area.\n\nIn summary, the benchmarking of biomedical PLMs is a multifaceted challenge that requires addressing data quality, domain specificity, and model generalizability. Future research should focus on developing more robust evaluation frameworks, leveraging domain-specific knowledge, and ensuring the reliability of models in real-world applications."
    },
    {
      "heading": "5.4 Methodological Approaches to Benchmarking",
      "level": 3,
      "content": "Benchmarking pre-trained language models (PLMs) in the biomedical domain involves a systematic evaluation of model performance across diverse tasks, datasets, and domains. This subsection examines the methodological approaches employed to assess and compare these models, focusing on experimental strategies, evaluation protocols, and the integration of domain adaptation techniques. The goal is to establish robust and reliable benchmarks that reflect the unique challenges of biomedical NLP, such as domain-specific language, complex terminology, and data scarcity.\n\nOne of the key methodological approaches in benchmarking is the use of comparative analysis, which evaluates model performance across different training paradigms, including zero-shot, few-shot, and fine-tuned approaches. Zero-shot learning, for instance, assesses a model’s ability to perform tasks without explicit task-specific training, while few-shot learning evaluates its capacity to generalize from limited examples. Studies such as those by Li et al. [29] demonstrate that domain-adaptive pretraining significantly improves performance in low-resource settings, suggesting that domain-specific pre-training is essential for effective benchmarking. Similarly, the work by Liu et al. [40] highlights the importance of task-adaptive pretraining, where models are further fine-tuned on unlabeled data to improve performance on target tasks.\n\nAnother critical aspect of benchmarking is the integration of retrieval-augmented generation (RAG) and knowledge graphs to enhance model accuracy and reduce hallucinations. RAG combines the strengths of pre-trained language models with external knowledge sources, enabling models to generate more accurate and contextually relevant responses. The study by Zhang et al. [41] shows that incorporating knowledge graphs into the evaluation process can significantly improve performance in biomedical question-answering tasks, particularly in domains with high ambiguity and complexity. Additionally, the use of domain-specific benchmarks such as BLUE [40] and BLURB [44] provides standardized tasks and metrics, ensuring consistency and comparability across different models.\n\nDomain adaptation techniques also play a pivotal role in benchmarking, as they address the challenges of domain shift and data scarcity. Methods such as contrastive learning, curriculum learning, and adversarial training are employed to align source and target domain distributions, enabling models to generalize better across different biomedical subdomains. The work by Wang et al. [46] demonstrates that contrastive adaptation networks (CAN) can effectively reduce domain discrepancy and improve model performance. Furthermore, the paper by Chen et al. [61] introduces a curriculum learning-based approach for domain adaptation, which gradually increases the difficulty of training samples, thereby enhancing model robustness and generalization.\n\nFinally, the evaluation of model efficiency and resource consumption is an emerging trend in biomedical benchmarking. Techniques such as sparsity, distillation, and quantization are used to reduce computational costs while maintaining performance. The study by Zhao et al. [62] highlights the trade-offs between model size, accuracy, and efficiency, emphasizing the need for resource-aware evaluation protocols. As the biomedical domain continues to evolve, future benchmarking efforts must incorporate dynamic and continuously updated datasets, as well as cross-lingual and multi-modal evaluation frameworks, to ensure the relevance and scalability of pre-trained language models."
    },
    {
      "heading": "5.5 Emerging Trends in Biomedical Benchmarking",
      "level": 3,
      "content": "The evaluation and benchmarking of pre-trained language models (PLMs) in the biomedical domain have witnessed significant advancements, driven by the need for standardized, domain-specific assessment frameworks. Emerging trends in biomedical benchmarking reflect a shift towards more robust, adaptive, and comprehensive evaluation methodologies that address the unique challenges of biomedical data. One prominent direction is the development of cross-lingual and multilingual benchmarks, which aim to assess model adaptability across different languages and regions. For instance, the KBioXLM and JMedBench frameworks have demonstrated the importance of multilingual support in biomedical NLP, allowing models to generalize across diverse linguistic contexts [11]. These benchmarks not only enhance the portability of PLMs but also facilitate the deployment of models in global healthcare settings.\n\nAnother emerging trend is the integration of multimodal data into benchmarking frameworks. Traditional biomedical benchmarks have primarily focused on text-only data, but the increasing availability of image-text pairs in scientific literature has spurred the development of multimodal benchmarks. For example, the PMC-15M dataset, which comprises 15 million biomedical image-text pairs, has enabled the training of models like BiomedCLIP, which excel in tasks ranging from retrieval to visual question answering [11]. This shift underscores the importance of incorporating multimodal data to improve model performance in complex biomedical tasks that involve both textual and visual information.\n\nDomain adaptation and transfer learning techniques are also gaining traction in biomedical benchmarking. These approaches aim to enhance model generalization across different biomedical subdomains by leveraging pre-trained models and fine-tuning them on domain-specific data. The work on BioRED, a rich biomedical relation extraction dataset, highlights the potential of domain adaptation strategies to improve performance on tasks such as drug-disease and chemical-chemical relation extraction [11]. Additionally, studies like those on BioSyn demonstrate how model-based candidate selection and marginal likelihood maximization can improve entity normalization in biomedical NER tasks [11].\n\nDynamic and continuously updated benchmarks are another key trend in biomedical benchmarking. As biomedical research evolves rapidly, static benchmarks may become outdated, limiting the usefulness of PLMs. Projects like MedS-Bench and DrBenchmark emphasize the need for benchmarks that can track progress and remain relevant in the face of new discoveries and data [11]. These dynamic benchmarks not only provide a more accurate assessment of model performance but also encourage ongoing research and development in the field.\n\nFinally, the focus on model interpretability and reliability has become a critical component of biomedical benchmarking. With the increasing deployment of PLMs in clinical settings, it is essential to ensure that models are not only accurate but also transparent and trustworthy. Techniques such as attention visualization, saliency maps, and rule-based explanation frameworks are being explored to enhance model interpretability. Moreover, the integration of knowledge graphs and structured data into benchmarking frameworks, as seen in the work on Biolink Model, is expected to improve the reliability and factual accuracy of PLMs in biomedical tasks [10]. These developments signal a promising future for biomedical benchmarking, where models are not only evaluated for their performance but also for their robustness, interpretability, and clinical relevance."
    },
    {
      "heading": "6.1 Data Scarcity and Annotation Challenges",
      "level": 3,
      "content": "The biomedical domain presents unique challenges in the acquisition and curation of high-quality, annotated datasets, which are essential for the development and evaluation of pre-trained language models. Unlike general-domain corpora, which often benefit from extensive public resources and annotation efforts, biomedical data is inherently complex, specialized, and scarce. This scarcity stems from multiple factors, including the limited availability of domain-specific annotated texts, the high cost and expertise required for manual annotation, and the variability in annotation standards across different subdomains. These challenges significantly hinder the progress of biomedical NLP research and the robust deployment of language models in clinical and research settings [18; 6; 13].\n\nOne of the primary issues is the limited availability of annotated datasets that capture the intricacies of biomedical language. For instance, while large-scale datasets like PubMed and MIMIC-III have been widely used, they often lack the granularity required for fine-grained tasks such as named entity recognition (NER), relation extraction, and event extraction [1; 2; 6]. This data scarcity is compounded by the fact that many biomedical texts, particularly clinical notes, are unstructured and contain highly technical terminology, making automated annotation and standardization difficult. Moreover, the annotation process itself is resource-intensive and requires domain expertise, which is not always available or affordable [18; 63].\n\nThe lack of standardized annotation guidelines further exacerbates the problem. Different biomedical subdomains, such as genomics, pharmacology, and clinical diagnostics, often employ distinct terminologies and conventions, leading to inconsistencies in how entities and relationships are annotated. For example, a study on biomedical NLP highlighted the challenges of aligning annotations across tasks, as the same entity might be labeled differently depending on the context and the annotation schema used [18; 2; 6]. These inconsistencies not only reduce the reliability of model training but also complicate the evaluation of model performance across different benchmarks.\n\nTo address these challenges, researchers have explored various strategies, including semi-supervised learning, transfer learning, and the use of pre-trained models to mitigate the need for large annotated datasets. For instance, models like BioBERT and BioALBERT have demonstrated the potential of domain-specific pre-training on large biomedical corpora to improve performance on downstream tasks, even in the absence of extensive annotations [1; 2]. However, these approaches still rely on some level of annotated data, and the limitations of existing datasets remain a critical barrier to progress [6; 13].\n\nLooking ahead, future research must focus on developing more scalable and efficient methods for data annotation, such as active learning and crowdsourcing, as well as on improving the interoperability and standardization of biomedical annotations. Additionally, the integration of external knowledge sources, such as biomedical ontologies and knowledge graphs, could help enhance the quality and consistency of annotations, thereby supporting the development of more robust and generalizable language models in the biomedical domain [31; 18; 6]."
    },
    {
      "heading": "6.2 Model Interpretability and Ethical Concerns",
      "level": 3,
      "content": "Pre-trained language models (PLMs) have demonstrated remarkable performance in biomedical natural language processing (BioNLP), yet their deployment in clinical and research settings is hindered by significant challenges in interpretability and ethical concerns. The \"black box\" nature of these models makes it difficult to trace how they arrive at specific conclusions, especially in high-stakes biomedical contexts where transparency is critical. This lack of interpretability not only limits the trustworthiness of model outputs but also complicates the validation of their reliability in clinical decision-making. Furthermore, ethical concerns such as bias, privacy, and accountability arise when these models are applied to sensitive biomedical data, particularly in scenarios involving patient-level information. Addressing these issues is essential for the responsible and effective use of PLMs in biomedicine.\n\nInterpretability in biomedical language models is particularly challenging due to the complexity of the domain-specific language and the intricate interactions between entities and relationships in biomedical texts. Techniques such as attention visualization and saliency maps have been explored to shed light on model decision-making, but they often provide only partial insights into the underlying reasoning [1]. More rigorous approaches, such as rule-based explanation frameworks and human-in-the-loop validation, are required to ensure that model outputs align with clinical expertise [36]. Additionally, the integration of external knowledge sources, such as biomedical ontologies and knowledge graphs, can enhance model transparency by providing structured representations of domain-specific concepts [2].\n\nEthical concerns are equally pressing, as biases in training data can lead to discriminatory or unreliable model outputs. Biased datasets, often reflecting historical disparities in healthcare access and treatment, may result in models that perform poorly for underrepresented populations [38]. Ensuring fairness in model development requires careful curation of training data and the implementation of bias mitigation strategies during pre-training and fine-tuning [18]. Moreover, the use of sensitive patient data in model training raises privacy risks, necessitating the adoption of robust security measures such as differential privacy and secure federated learning [64]. These measures are critical for protecting patient confidentiality and ensuring compliance with regulatory standards.\n\nThe ethical implications of deploying PLMs in clinical environments extend beyond data privacy and bias. Transparency and accountability are paramount, especially when models are used to support diagnostic or therapeutic decisions. Clinicians must be able to understand and validate model outputs to ensure they align with established medical guidelines and clinical judgment. The development of explainable AI (XAI) techniques that provide clear, interpretable rationales for model predictions is therefore a key area of research [6]. Furthermore, the potential for model hallucinations—where models generate incorrect or fabricated information—poses a serious risk in biomedical applications, where misinformation can have life-threatening consequences [65].\n\nIn conclusion, the interpretability and ethical challenges of biomedical PLMs represent critical barriers to their widespread adoption. Addressing these challenges requires a multi-faceted approach that combines technical innovations in model explainability with ethical considerations in data curation, model development, and deployment. Future research should focus on advancing XAI methods, enhancing model fairness, and establishing robust frameworks for ethical AI in biomedicine. These efforts will be essential for realizing the full potential of PLMs while ensuring their responsible and equitable use in healthcare."
    },
    {
      "heading": "6.3 Generalizability and Domain Adaptation Issues",
      "level": 3,
      "content": "Generalizability and domain adaptation are critical challenges in the application of pre-trained language models (PLMs) to the biomedical domain. While these models have shown remarkable performance on specific tasks, their effectiveness often diminishes when applied to new or specialized domains, highlighting the need for robust adaptation strategies. The biomedical domain, characterized by diverse subdomains such as genomics, clinical notes, and drug discovery, presents unique challenges in terms of vocabulary, language structure, and task requirements, making domain adaptation a non-trivial endeavor.\n\nOne of the primary issues is the domain shift problem, where a model trained on one domain performs poorly on another due to differences in linguistic patterns and terminology [43]. This has prompted extensive research into domain adaptation techniques, including fine-tuning, transfer learning, and meta-learning. For instance, methods such as domain-specific pre-training [1] and knowledge augmentation [60] have been proposed to improve the model's understanding of biomedical terminology. However, these approaches often require additional fine-tuning and annotated data, which can be both time-consuming and resource-intensive.\n\nAnother challenge lies in the trade-off between model specialization and generalization. While domain-specific models can achieve superior performance on targeted tasks, they often struggle to generalize across different biomedical subdomains [43]. This has led to the development of domain-agnostic approaches that aim to learn features invariant across domains [58]. Techniques such as contrastive learning [66] and multi-task learning [33] have shown promise in improving model generalization, but their effectiveness remains dependent on the availability of high-quality, diverse training data.\n\nMoreover, the effectiveness of domain adaptation methods varies significantly across different tasks and domains. For example, while some studies [40] have demonstrated substantial improvements in named entity recognition and relation extraction, others highlight the limitations of these methods in tasks requiring fine-grained semantic understanding [57]. This variability underscores the importance of task-specific adaptation strategies and the need for domain-informed model design.\n\nLooking ahead, emerging trends in domain adaptation include the integration of external knowledge sources [67] and the use of meta-learning to improve model generalization [68]. These approaches aim to enhance the adaptability of PLMs while reducing the reliance on domain-specific data. Future research should focus on developing more efficient and scalable methods that can seamlessly adapt models to new biomedical domains without extensive retraining, thereby expanding their applicability in real-world scenarios."
    },
    {
      "heading": "6.4 Computational and Resource Constraints",
      "level": 3,
      "content": "The computational and resource demands of training and deploying large-scale pre-trained language models (PLMs) in biomedical applications represent a critical challenge that limits their practical applicability and widespread adoption. The complexity of biomedical data, combined with the high-dimensional nature of deep learning models, leads to significant computational costs, making it difficult to train and deploy these models in real-world clinical and research settings [1; 3; 13]. For instance, training models like BioMegatron [26] or MEDITRON-70B [13] requires massive amounts of GPU/TPU resources and energy, which is prohibitive for many institutions, especially in resource-constrained environments. This computational burden is further exacerbated by the need for extensive parameter tuning, model adaptation, and fine-tuning on specialized biomedical tasks, which often require additional training data and computational overhead [69; 27].\n\nThe deployment of large PLMs in clinical settings presents additional challenges, particularly in environments with limited computational infrastructure, such as small clinics or remote areas. These models often require substantial memory and processing power, which may not be available in such settings. To address these limitations, researchers have explored model compression techniques such as quantization and distillation, which aim to reduce model size and computational costs while maintaining performance [70]. For example, distillation methods have been successfully applied to compress large PLMs into smaller, more efficient variants that retain much of the original model’s performance, as demonstrated in studies on BioBERT and other domain-specific models [1]. Similarly, quantization techniques, which reduce the precision of model parameters, have been shown to significantly lower computational requirements without a substantial loss in accuracy [70].\n\nDespite these advances, there remains a critical trade-off between model size, performance, and deployment feasibility in biomedical applications. While larger models tend to achieve higher accuracy on complex tasks such as biomedical relation extraction and question answering, their computational demands make them impractical for many real-world applications. This has led to a growing interest in parameter-efficient fine-tuning (PEFT) approaches, which allow for model adaptation with minimal computational overhead. Methods such as LoRA [27] and adapter modules have shown promise in enabling efficient adaptation of large PLMs to specialized biomedical tasks without requiring full retraining of the model [27]. These techniques are particularly valuable in scenarios where data privacy or resource constraints limit the availability of labeled biomedical data for fine-tuning.\n\nEmerging trends in model efficiency, such as the development of lightweight models and the integration of knowledge graphs for semantic compression, offer further potential for reducing computational demands. However, these approaches also introduce new challenges, such as the need for careful design to maintain model robustness and generalization across biomedical subdomains. As the field continues to evolve, the development of scalable, efficient, and privacy-preserving PLMs remains a key research direction, with the ultimate goal of enabling broader accessibility and practical deployment of these models in healthcare and biomedical research."
    },
    {
      "heading": "6.5 Privacy and Security Risks",
      "level": 3,
      "content": "The use of pre-trained language models (PLMs) in biomedical applications, particularly those involving sensitive patient data, raises significant privacy and security concerns. These models, often trained on large-scale biomedical corpora, may inadvertently expose confidential information through data leakage, adversarial attacks, or model inversion. Ensuring the confidentiality, integrity, and availability of patient data is paramount, as breaches can have severe ethical, legal, and clinical consequences. This subsection addresses these challenges, highlighting the risks and exploring potential solutions.\n\nOne of the primary privacy risks is data leakage, where a model trained on sensitive biomedical data may inadvertently reveal information about the training data. For example, models trained on electronic health records (EHRs) might encode patient-specific patterns in their representations, leading to potential re-identification of individuals [11]. This risk is exacerbated when models are deployed in real-world scenarios without adequate safeguards. To mitigate this, differential privacy (DP) has emerged as a promising technique. By adding controlled noise to the training process, DP aims to protect individual data points while maintaining model utility [11]. However, the trade-off between privacy and model performance remains a challenge, as excessive noise can degrade accuracy, particularly in tasks requiring fine-grained understanding of biomedical texts [11].\n\nAnother critical risk is the vulnerability of PLMs to adversarial attacks, where malicious actors manipulate inputs to induce incorrect outputs or extract sensitive information. For instance, adversarial examples can be crafted to mislead models into providing incorrect diagnoses or treatment recommendations, with potentially life-threatening consequences [11]. Furthermore, model inversion attacks may allow attackers to reconstruct sensitive training data from model outputs, particularly when models are used for tasks like drug name extraction or entity recognition [11]. These vulnerabilities underscore the need for robust defense mechanisms, such as adversarial training, input sanitization, and model hardening. However, implementing such defenses in biomedical settings is complicated by the need for high accuracy and the complexity of medical language.\n\nBalancing model utility with patient privacy is a central challenge in biomedical PLM development. Federated learning (FL) offers a decentralized approach, enabling models to be trained across multiple data sources without centralizing sensitive data. This reduces the risk of data exposure but introduces new challenges, such as handling heterogeneous data distributions and ensuring model convergence [10]. Additionally, secure training protocols, such as homomorphic encryption and secure multi-party computation (MPC), provide additional layers of protection but come with significant computational overhead, making them less viable for real-time clinical applications [12].\n\nFinally, the ethical implications of deploying PLMs in healthcare settings must be carefully considered. Models must be transparent, auditable, and aligned with clinical guidelines to ensure patient trust and regulatory compliance. This requires not only technical safeguards but also interdisciplinary collaboration among AI researchers, clinicians, and policymakers. As the use of PLMs in biomedicine continues to expand, addressing these privacy and security risks will be essential to realizing their full potential while safeguarding patient interests."
    },
    {
      "heading": "7.1 Multimodal Integration for Comprehensive Biomedical Analysis",
      "level": 3,
      "content": "Multimodal integration represents a pivotal advancement in the application of pre-trained language models (PLMs) to the biomedical domain, enabling the synthesis of diverse data types—text, medical images, and structured clinical data—to support more comprehensive and accurate biomedical analysis. This approach addresses the limitations of unimodal models, which often fail to capture the multifaceted nature of biomedical information, by leveraging the complementary strengths of different modalities. Recent studies have demonstrated that integrating multimodal data into PLMs can significantly enhance performance across a range of biomedical tasks, including diagnosis, treatment recommendation, and biomedical research [5; 13].\n\nOne of the primary challenges in multimodal biomedical analysis is the alignment and fusion of heterogeneous data sources. Textual data, such as clinical notes and biomedical literature, provide rich semantic information, while medical images, such as MRI scans and X-rays, offer visual insights that are crucial for diagnostic tasks. Structured clinical data, such as electronic health records (EHRs), provide quantitative metrics that are essential for monitoring patient health. To effectively integrate these modalities, researchers have proposed various architectures that combine deep learning with domain-specific knowledge [5; 6].\n\nFor instance, the BioMedGPT model [5] introduces a unified framework that allows for seamless interaction between textual and visual modalities. By leveraging a large generative language model, BioMedGPT enables the interpretation of complex biomedical data through natural language, facilitating tasks such as medical image-text alignment and clinical decision support. Similarly, the MEDITRON-70B model [13] emphasizes the importance of domain-specific pre-training on medical corpora, demonstrating that models trained on large-scale biomedical data outperform general-purpose models in tasks such as named entity recognition and relation extraction.\n\nAnother critical aspect of multimodal integration is the development of robust feature extraction and representation techniques. Techniques such as vision-language pre-training (VLP) and contrastive learning have been successfully applied to biomedical data, enabling models to learn cross-modal representations that capture the relationships between text and images [5; 13]. These methods have been shown to improve the accuracy of tasks such as disease classification and treatment outcome prediction.\n\nDespite these advancements, several challenges remain. The computational complexity of multimodal models is significantly higher than that of unimodal models, requiring efficient optimization strategies to ensure scalability. Additionally, the lack of standardized multimodal datasets and evaluation metrics hinders the development and comparison of different approaches [5; 13]. Addressing these challenges will be crucial for the widespread adoption of multimodal PLMs in biomedical applications.\n\nLooking ahead, future research should focus on developing more efficient and scalable multimodal architectures, as well as creating standardized benchmarks to evaluate the performance of these models. By addressing these challenges, researchers can unlock the full potential of multimodal integration, enabling more accurate and holistic biomedical analysis."
    },
    {
      "heading": "7.2 Efficient and Scalable Model Design for Resource-Constrained Environments",
      "level": 3,
      "content": "The design of efficient and scalable pre-trained language models (PLMs) for resource-constrained environments represents a critical emerging trend in biomedical NLP. As the demand for deploying these models in clinical settings and low-bandwidth regions grows, the need to balance performance with computational efficiency becomes paramount. This subsection explores the strategies, techniques, and trade-offs involved in developing PLMs that are optimized for resource-limited scenarios while maintaining effectiveness on biomedical tasks.\n\nOne of the primary approaches to achieving efficiency is parameter-efficient fine-tuning. Techniques such as low-rank adaptation (LoRA) [2] and adapter modules [1] allow models to be adapted to specialized biomedical tasks with minimal additional parameters. These methods reduce the computational burden of fine-tuning, making them particularly suitable for scenarios where model size and training cost are constraints. For instance, LoRA has been shown to achieve comparable performance to full fine-tuning while significantly reducing memory and computational requirements [2].\n\nModel compression is another essential strategy, with techniques such as quantization and knowledge distillation playing a pivotal role. Quantization reduces the precision of model weights, often from 32-bit floating points to 8-bit integers, leading to substantial reductions in model size and inference latency [1]. Knowledge distillation, on the other hand, transfers the knowledge of a large, pre-trained model to a smaller, more efficient one, maintaining performance while minimizing computational costs [1]. These approaches have been successfully applied in biomedical settings, where resource limitations are often acute [71].\n\nLightweight models designed specifically for clinical deployment are also gaining traction. These models, often based on simplified architectures or reduced parameter counts, are optimized for inference speed and energy efficiency. For example, studies have demonstrated that smaller clinical language models can outperform larger models when trained on domain-specific data, particularly in tasks such as named entity recognition and relation extraction [72]. This suggests that model size alone is not the sole determinant of performance, and that domain-specific adaptation can lead to more efficient and effective solutions.\n\nScalability is another key consideration. Techniques such as dynamic model scaling and modular architectures enable models to adapt to varying resource availability. For instance, some studies have explored the use of modular PLMs that can be dynamically expanded or contracted based on the computational resources available [6]. This flexibility is particularly valuable in real-world clinical environments, where infrastructure may vary widely.\n\nDespite these advances, challenges remain. The trade-off between model efficiency and performance is a persistent issue, with many techniques requiring careful tuning to balance accuracy and resource usage. Additionally, ensuring that compressed models retain the necessary in-domain knowledge for complex biomedical tasks remains a challenge [6].\n\nIn summary, efficient and scalable model design for resource-constrained environments is a vital area of research in biomedical NLP. Through techniques such as parameter-efficient fine-tuning, model compression, and lightweight architectures, researchers are making significant strides in enabling the deployment of PLMs in real-world clinical and resource-limited settings. Future work will likely focus on improving these techniques further while addressing the ongoing challenges of performance and scalability."
    },
    {
      "heading": "7.3 Domain Adaptation and Transfer Learning for Generalization",
      "level": 3,
      "content": "Domain adaptation and transfer learning have emerged as critical enablers for enhancing the generalization of pre-trained language models (PLMs) across diverse biomedical subdomains and tasks. As the biomedical domain encompasses a wide array of specialized tasks—from clinical note analysis to drug discovery—models trained on general-domain corpora often struggle to generalize effectively due to domain-specific linguistic structures, terminologies, and data distributions. This subsection explores how domain adaptation and transfer learning strategies are being leveraged to bridge this gap, enabling PLMs to perform robustly across unseen biomedical domains. \n\nOne of the most widely studied approaches is domain-specific pre-training, where PLMs are fine-tuned on domain-specific corpora to align their learned representations with the target domain. For instance, BioBERT [7] demonstrates the effectiveness of pre-training on biomedical texts, significantly outperforming general-domain models in tasks like named entity recognition and relation extraction. Similarly, SciBERT [39] and CLIMATEBERT [73] extend this concept to scientific and climate-related domains, respectively, showcasing the importance of domain-specific corpora for model performance.\n\nHowever, pre-training on domain-specific corpora is not always feasible due to data scarcity or high costs. To address this, transfer learning techniques have gained prominence. These methods aim to transfer knowledge from a source domain to a target domain using limited labeled data. For example, the work of [74] proposes an unsupervised method to learn domain-specific word representations, effectively capturing domain-dependent semantics. Similarly, [58] introduces a model-agnostic approach that learns domain-invariant and domain-specific features, enhancing generalization across unseen domains. \n\nZero-shot and few-shot learning techniques are also playing a significant role in domain adaptation, particularly in scenarios where labeled data is scarce. [29] highlights the benefits of multi-phase adaptive pretraining, demonstrating that even with limited labeled data, models can achieve substantial performance gains. Additionally, [40] evaluates the effectiveness of pre-trained models like BERT and ELMo in biomedical tasks, showing that domain-adapted models outperform their general-domain counterparts.\n\nEmerging trends in this area include the use of multimodal data and knowledge integration. For instance, [24] leverages document-level links to enhance pre-training, while [41] integrates clinical domain knowledge into pre-training, improving performance on tasks like named entity recognition. \n\nLooking ahead, the integration of domain adaptation with efficient model compression and knowledge distillation remains a promising direction, especially for deploying models in resource-constrained environments. As the biomedical domain continues to evolve, the development of more robust and scalable domain adaptation methods will be essential for ensuring the broad applicability of PLMs in real-world settings."
    },
    {
      "heading": "7.4 Enhancing Model Interpretability and Explainability in Clinical Settings",
      "level": 3,
      "content": "The need for interpretable and explainable pre-trained language models in clinical settings has become increasingly pressing as these models are deployed in high-stakes healthcare environments. While pre-trained models like BioBERT [12] and BioGPT [11] have demonstrated superior performance in biomedical tasks, their \"black-box\" nature poses significant challenges for clinicians, regulatory bodies, and patients. Ensuring model transparency is not just a technical requirement but a critical ethical and practical necessity for building trust and enabling informed decision-making in clinical practice [10].\n\nModel interpretability in clinical NLP involves techniques that provide insights into how models arrive at specific conclusions, such as diagnosing conditions or recommending treatments. One widely studied approach is attention visualization, which highlights the parts of input text that the model focuses on during decision-making [10]. For example, attention mechanisms in models like BioBERT can be used to identify key biomedical entities or relationships that influence predictions. However, this method has limitations, as attention weights do not always correlate directly with the model's decision-making process [75]. To address this, researchers have explored saliency maps and gradient-based attribution methods, which provide more granular insights into model behavior [76].\n\nAnother promising direction is the integration of human-in-the-loop approaches, where clinicians provide feedback to refine model interpretations. These frameworks enable models to incorporate domain expertise and improve their explainability. For instance, UmlsBERT [10] leverages the Unified Medical Language System (UMLS) to enhance model reasoning, thereby improving its ability to provide contextually relevant explanations. Similarly, BioBART [11] has been fine-tuned to generate structured outputs that align with clinical documentation standards, thereby increasing interpretability for healthcare professionals.\n\nExplainability in diagnostic and decision-making tasks is particularly critical. Models must not only produce accurate predictions but also provide clear reasoning for clinical recommendations. Techniques such as rule-based explanation frameworks and knowledge-guided prompt engineering have been used to enhance model transparency. For example, models like MEDITRON-70B [11] have been designed to incorporate structured medical knowledge, allowing for more interpretable outputs that align with clinical workflows.\n\nDespite these advances, several challenges remain. The complexity of biomedical language, the presence of rare or ambiguous terms, and the need for context-specific explanations all contribute to the difficulty of achieving full interpretability. Moreover, there is a need for standardized evaluation metrics that assess not just accuracy, but also the clarity and relevance of model explanations.\n\nLooking ahead, the integration of explainability with domain adaptation and knowledge enhancement is likely to be a key area of research. As models become more sophisticated, ensuring they remain transparent and interpretable will be essential for their successful deployment in clinical practice. Future work should focus on developing robust, scalable, and clinically meaningful explainability frameworks that bridge the gap between model performance and human understanding [77]."
    },
    {
      "heading": "7.5 Knowledge-Enhanced Language Models for Biomedical Reasoning",
      "level": 3,
      "content": "Knowledge-enhanced language models have emerged as a critical avenue for advancing biomedical reasoning by integrating external knowledge sources such as biomedical ontologies, knowledge graphs, and structured databases. These models aim to address the limitations of purely data-driven approaches by incorporating structured, domain-specific knowledge that enhances their factual accuracy, reasoning capabilities, and interpretability. By leveraging such knowledge, these models can better understand complex biomedical relationships, improve entity normalization, and support more accurate and contextually relevant responses in tasks such as relation extraction, question answering, and clinical decision-making. This subsection examines the integration of external knowledge into pre-trained language models, the methodologies employed, and the implications for biomedical reasoning.\n\nOne prominent approach involves the use of biomedical ontologies and knowledge graphs to guide the pre-training and fine-tuning of language models. For instance, models like BioBERT [1] and BioBERT-v1.1 [1] have been pre-trained on biomedical corpora and further enhanced by incorporating knowledge from ontologies such as UMLS (Unified Medical Language System) and SNOMED-CT. This integration helps models better understand the semantic relationships between biomedical entities and improves their performance on tasks such as named entity recognition and relation extraction. Similarly, the KECI framework [78] leverages external knowledge to improve joint entity and relation extraction, demonstrating that the incorporation of structured knowledge significantly enhances model performance.\n\nAnother key strategy is the use of hybrid architectures that combine textual and structured data. For example, the BioRED dataset [23] provides rich annotations for biomedical relation extraction, enabling models to learn from both textual and relational information. Additionally, the integration of knowledge graphs into language models has shown promise in tasks such as biomedical question answering. The PubMedQA dataset [32], for instance, requires models to reason over biomedical research texts and extract answers based on the underlying relationships, highlighting the importance of structured knowledge in such tasks. Models like those in the BioBART framework [18] also incorporate generative capabilities to leverage knowledge from structured biomedical sources, further enhancing their reasoning abilities.\n\nThe integration of knowledge into language models is not without challenges. Issues such as data inconsistency, scalability, and the dynamic nature of biomedical knowledge must be addressed. Moreover, the alignment of unstructured text with structured knowledge remains a non-trivial task. Despite these challenges, the field is advancing rapidly, with emerging trends pointing towards more efficient and scalable methods for knowledge integration. Future research is likely to focus on improving model interpretability, enhancing the adaptability of knowledge-enhanced models to new domains, and developing more robust evaluation frameworks to assess the impact of knowledge integration on biomedical reasoning tasks. As the field continues to evolve, the fusion of language models with structured knowledge will play a pivotal role in advancing the accuracy and utility of biomedical AI systems."
    },
    {
      "heading": "7.6 Emerging Applications of Pre-trained Language Models in Biomedical Research",
      "level": 3,
      "content": "Pre-trained language models are increasingly being explored for their transformative potential in biomedical research, extending beyond traditional natural language processing (NLP) tasks to domains such as drug discovery, genomics, and personalized medicine. These models, including domain-specific variants like BioBERT, BioGPT, and BioMedGPT, have demonstrated the ability to process and extract meaningful insights from vast and complex biomedical text, offering new opportunities for scientific discovery and clinical applications. Emerging applications highlight the versatility of these models in addressing challenges that were previously intractable with conventional approaches.\n\nIn drug discovery, pre-trained language models are being used to identify novel drug candidates, predict molecular interactions, and accelerate the development of therapeutics. For instance, models like BioGPT and Tx-LLM leverage their understanding of biomedical text to generate hypotheses about drug-target interactions or predict the efficacy of compounds [12]. This is particularly valuable in scenarios where experimental validation is time-consuming and costly. Furthermore, models such as BERT-GT have been adapted for cross-sentence n-ary relation extraction, enabling the identification of complex relationships between drugs, genes, and diseases [10]. These advancements underscore the potential of language models to aid in the automation of knowledge discovery in drug development pipelines.\n\nGenomic and proteomic analysis is another frontier where pre-trained language models are making inroads. Models like BioMamba and BioBERT have been fine-tuned to handle biological sequences, treating them as \"text\" to leverage the power of contextualized embeddings. By doing so, these models can predict gene functions, interpret variant effects, and identify regulatory regions in DNA sequences [11]. This has opened up new possibilities for analyzing high-throughput genomic data, where traditional sequence alignment and annotation methods often fall short. Additionally, models like SciFive have demonstrated the utility of text-generation approaches in generating novel hypotheses or summarizing complex genomic findings [12].\n\nIn the realm of personalized medicine, pre-trained models are being used to integrate patient-specific data with biomedical knowledge to support tailored treatment recommendations. For example, BioBART has been adapted to generate summaries of clinical notes, which can be used to inform treatment decisions [11]. Similarly, models like GatorTronGPT have been trained on large clinical datasets to support tasks such as diagnostic reasoning and treatment planning [11]. These applications highlight the potential of pre-trained language models to bridge the gap between clinical data and personalized healthcare solutions.\n\nDespite these promising developments, several challenges remain, including the need for more robust model generalization, the integration of multimodal data, and the enhancement of interpretability in clinical settings. Future research is likely to focus on improving the efficiency of domain-specific pre-training, exploring hybrid architectures that combine text and structured biomedical data, and developing explainable AI frameworks to increase trust in model-driven decisions. As the biomedical landscape continues to evolve, the role of pre-trained language models is poised to expand, offering new tools to accelerate scientific progress and improve patient outcomes."
    },
    {
      "heading": "8 Conclusion",
      "level": 2,
      "content": "Pre-trained language models (PLMs) have revolutionized the biomedical domain by enabling advanced natural language processing (NLP) tasks, such as named entity recognition, information extraction, and clinical question answering. This survey has provided a comprehensive overview of the technological foundations, applications, challenges, and future directions of PLMs in biomedical contexts. From the early days of domain adaptation to the current era of specialized models like BioBERT, BioALBERT, and BioMamba, the field has seen remarkable progress. These models have demonstrated significant improvements in understanding domain-specific language, handling complex biomedical terminology, and achieving state-of-the-art performance on critical tasks [1; 2; 6]. However, the journey has not been without obstacles.\n\nA central challenge in biomedical NLP remains the scarcity of high-quality, annotated datasets. While large-scale corpora such as PubMed and MIMIC-III have facilitated model pre-training, the limited availability of diverse, well-annotated biomedical texts continues to hinder the development of robust, generalizable models. This issue is exacerbated by the variability in biomedical language, where domain-specific jargon, abbreviations, and syntactic structures complicate model training and evaluation [4]. Furthermore, the computational and resource demands of training and deploying large-scale models pose significant barriers to their widespread adoption, particularly in resource-constrained environments [79].\n\nDespite these challenges, the integration of domain-specific knowledge into PLMs has emerged as a promising avenue for improving performance. Techniques such as knowledge graph integration, domain-specific pre-training, and contrastive learning have shown considerable potential in enhancing model interpretability and generalization [80; 81]. Moreover, the rise of multimodal and hybrid architectures has opened new possibilities for leveraging not only text but also imaging and structured data to support complex biomedical tasks [82].\n\nLooking ahead, the future of biomedical language modeling lies in overcoming existing limitations through innovative research and interdisciplinary collaboration. Advances in efficient model design, such as parameter-efficient fine-tuning and model compression, are critical for making large-scale models more accessible [83]. Additionally, the development of robust evaluation frameworks and benchmarks is essential for assessing model reliability, fairness, and clinical utility [84; 54]. Finally, ethical considerations, including data privacy, model transparency, and responsible AI deployment, must remain at the forefront of future research to ensure that these models benefit both patients and clinicians effectively [85]. As the field continues to evolve, the potential for PLMs to transform biomedical research and clinical practice remains vast, provided that ongoing challenges are met with sustained innovation and collaboration."
    }
  ],
  "references": [
    "[1] Clinical BioBERT Hyperparameter Optimization using Genetic Algorithm",
    "[2] Results of the seventh edition of the BioASQ Challenge",
    "[3] How much can ChatGPT really help Computational Biologists in  Programming",
    "[4] Natural Language Processing in Biomedicine  A Unified System  Architecture Overview",
    "[5] Multimodal ChatGPT for Medical Applications  an Experimental Study of  GPT-4V",
    "[6] A Survey of Mamba",
    "[7] BioBERT  a pre-trained biomedical language representation model for  biomedical text mining",
    "[8] A Comparison of Word Embeddings for the Biomedical Natural Language  Processing",
    "[9] 6th International Symposium on Attention in Cognitive Systems 2013",
    "[10] A Speculative Study on 6G",
    "[11] Computer Science",
    "[12] Paperswithtopic  Topic Identification from Paper Title Only",
    "[13] Nemotron-4 340B Technical Report",
    "[14] Investigating drug translational research using PubMed articles",
    "[15] Quick Summary",
    "[16] Principles from Clinical Research for NLP Model Generalization",
    "[17] EgoBlur  Responsible Innovation in Aria",
    "[18] VBART  The Turkish LLM",
    "[19] Atrous Space Bender U-Net (ASBU-Net LogiNet)",
    "[20] Domain Adaptation for Medical Image Analysis  A Survey",
    "[21] MedMentions  A Large Biomedical Corpus Annotated with UMLS Concepts",
    "[22] Movie Description",
    "[23] Reactive fungal wearable",
    "[24] LinkBERT  Pretraining Language Models with Document Links",
    "[25] SimpleBERT  A Pre-trained Model That Learns to Generate Simple Words",
    "[26] BioMegatron  Larger Biomedical Domain Language Model",
    "[27] Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain",
    "[28] MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning",
    "[29] Don't Stop Pretraining  Adapt Language Models to Domains and Tasks",
    "[30] BiomedCLIP  a multimodal biomedical foundation model pretrained from  fifteen million scientific image-text pairs",
    "[31] Biolink Model  A Universal Schema for Knowledge Graphs in Clinical,  Biomedical, and Translational Science",
    "[32] PubMedQA  A Dataset for Biomedical Research Question Answering",
    "[33] Cross-type Biomedical Named Entity Recognition with Deep Multi-Task  Learning",
    "[34] The Anatomy of a Scientific Rumor",
    "[35] Forming Trees with Treeformers",
    "[36] ClinicalGPT  Large Language Models Finetuned with Diverse Medical Data  and Comprehensive Evaluation",
    "[37] PaLM 2 Technical Report",
    "[38] a survey on GPT-3",
    "[39] SciBERT  A Pretrained Language Model for Scientific Text",
    "[40] Transfer Learning in Biomedical Natural Language Processing  An  Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
    "[41] UmlsBERT  Clinical Domain Knowledge Augmentation of Contextual  Embeddings Using the Unified Medical Language System Metathesaurus",
    "[42] OpenBioLink  A benchmarking framework for large-scale biomedical link  prediction",
    "[43] Domain Generalization  A Survey",
    "[44] Pre-trained Language Models in Biomedical Domain  A Systematic Survey",
    "[45] GigaPevt  Multimodal Medical Assistant",
    "[46] Contrastive Adaptation Network for Unsupervised Domain Adaptation",
    "[47] On The Power of Curriculum Learning in Training Deep Networks",
    "[48] An Extensive Data Processing Pipeline for MIMIC-IV",
    "[49] An Approach to Reducing Annotation Costs for BioNLP",
    "[50] AC-KBO Revisited",
    "[51] JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models",
    "[52] Healthcare",
    "[53] BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine",
    "[54] BioPreDyn-bench  benchmark problems for kinetic modelling in systems  biology",
    "[55] A Continuous Benchmarking Infrastructure for High-Performance Computing  Applications",
    "[56] GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain",
    "[57] Advances in Big Data Bio Analytics",
    "[58] Domain Generalization via Model-Agnostic Learning of Semantic Features",
    "[59] Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
    "[60] Understanding URDF  A Dataset and Analysis",
    "[61] Curriculum Learning for Reinforcement Learning Domains  A Framework and  Survey",
    "[62] BioBART  Pretraining and Evaluation of A Biomedical Generative Language  Model",
    "[63] Natural Language Processing in Electronic Health Records in Relation to  Healthcare Decision-making  A Systematic Review",
    "[64] PM4Py.LLM  a Comprehensive Module for Implementing PM on LLMs",
    "[65] Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
    "[66] SpanBERT  Improving Pre-training by Representing and Predicting Spans",
    "[67] Social World Knowledge  Modeling and Applications",
    "[68] Learning to Generalize  Meta-Learning for Domain Generalization",
    "[69] Efficient Continual Pre-training by Mitigating the Stability Gap",
    "[70] TPAM  A Simulation-Based Model for Quantitatively Analyzing Parameter  Adaptation Methods",
    "[71] Cerberus  A Multi-headed Derenderer",
    "[72] Do We Still Need Clinical Language Models",
    "[73] ClimateBert  A Pretrained Language Model for Climate-Related Text",
    "[74] Unsupervised Cross-Domain Word Representation Learning",
    "[75] The 10 Research Topics in the Internet of Things",
    "[76] Proceedings 38th International Conference on Logic Programming",
    "[77] FORM version 4.0",
    "[78] Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced  Collective Inference",
    "[79] Constrained Fault-Tolerant Resource Allocation",
    "[80] Ontology, Ontologies, and Science",
    "[81] A Broad Study of Pre-training for Domain Generalization and Adaptation",
    "[82] Integrated multimodal artificial intelligence framework for healthcare  applications",
    "[83] Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning",
    "[84] BioImageLoader  Easy Handling of Bioimage Datasets for Machine Learning",
    "[85] The Mythos of Model Interpretability"
  ]
}