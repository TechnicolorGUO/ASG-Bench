{
  "outline": [
    [
      1,
      "Literature Review: A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction."
    ],
    [
      1,
      "A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction"
    ],
    [
      2,
      "1. Introduction"
    ],
    [
      2,
      "2. Key Concepts and Definitions"
    ],
    [
      3,
      "2.1 Taxonomy of Feature Selection"
    ],
    [
      3,
      "2.2 Stability vs. Accuracy"
    ],
    [
      3,
      "2.3 Causal vs. Associative Selection"
    ],
    [
      2,
      "3. Historical Development and Milestones"
    ],
    [
      2,
      "4. Current State-of-the-Art Methods and Techniques"
    ],
    [
      3,
      "4.1 Causal Feature Selection"
    ],
    [
      3,
      "4.2 Large Language Models (LLMs) for Feature Selection"
    ],
    [
      3,
      "4.3 Federated Feature Selection (FFS)"
    ],
    [
      3,
      "4.4 Swarm Intelligence and Evolutionary Algorithms"
    ],
    [
      3,
      "4.5 Attention-Based and Deep Feature Selection"
    ],
    [
      2,
      "5. Applications and Case Studies"
    ],
    [
      3,
      "5.1 Multi-Omics and Cancer Subtyping"
    ],
    [
      3,
      "5.2 Neurological Disorders"
    ],
    [
      3,
      "5.3 Cardiovascular and Metabolic Diseases"
    ],
    [
      2,
      "6. Challenges and Open Problems"
    ],
    [
      3,
      "6.1 The Stability-Accuracy Trade-off"
    ],
    [
      3,
      "6.2 Interpretability in Deep Learning"
    ],
    [
      3,
      "6.3 Heterogeneity in Federated Learning"
    ],
    [
      3,
      "6.4 Small Sample Size (HDLSS)"
    ],
    [
      2,
      "7. Future Research Directions"
    ],
    [
      2,
      "8. Conclusion"
    ],
    [
      2,
      "9. References"
    ]
  ],
  "content": [
    {
      "heading": "Literature Review: A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction.",
      "level": 1,
      "content": "*Generated on: 2025-12-26 05:53:33*\n*Progress: [15/25]*\n*Saved to: /Users/joanna/Desktop/Literature_Reviews/A_Review_of_Feature_Selection_Methods_for_Machine_Learning-B_20251226_055333.md*\n---"
    },
    {
      "heading": "A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction",
      "level": 1,
      "content": "**Key Points**\n*   **The Curse of Dimensionality:** High-dimensional healthcare data (genomics, EHRs) necessitates robust feature selection (FS) to prevent overfitting and improve model generalizability.\n*   **Evolution of Methods:** The field has progressed from traditional filter/wrapper methods to embedded deep learning layers, causal inference integration, and Large Language Model (LLM)-driven selection.\n*   **State-of-the-Art:** Current frontiers include **Causal Feature Selection** (e.g., CausalDRIFT) to identify true disease drivers rather than spurious correlations, and **Federated Feature Selection** to handle privacy-sensitive, non-IID data across institutions.\n*   **Interpretability & Stability:** There is a critical shift towards \"stable\" feature selection (measured by Nogueira’s index) and \"explainable\" AI (SHAP/LIME) to ensure clinical trust.\n*   **Future Directions:** Research is moving toward zero-shot feature selection using LLMs and hybrid swarm intelligence optimization for real-time clinical decision support.\n\n---"
    },
    {
      "heading": "1. Introduction",
      "level": 2,
      "content": "The advent of high-throughput sequencing, wearable technology, and digitized Electronic Health Records (EHRs) has ushered in an era of big data in healthcare. Machine learning (ML) has emerged as a pivotal tool for leveraging this data to predict disease risk, enabling the transition from reactive healthcare to precision medicine [cite: 1, 2]. However, the efficacy of ML models in clinical settings is frequently hampered by the \"curse of dimensionality,\" a phenomenon where the number of features (e.g., gene expression levels, clinical variables) vastly exceeds the number of samples [cite: 1].\n\nIn disease risk prediction, irrelevant or redundant features not only increase computational cost but also introduce noise that degrades predictive accuracy and obscures biological interpretability. Feature Selection (FS)—the process of identifying a subset of relevant features—is therefore not merely a preprocessing step but a fundamental requirement for building robust, generalizable, and clinically interpretable models [cite: 2, 3].\n\nThis review provides a comprehensive analysis of the landscape of feature selection methods for disease prediction. Unlike previous surveys that focus primarily on traditional statistical approaches, this paper critically examines state-of-the-art advancements emerging in 2024 and 2025, including **Causal Feature Selection**, **Federated Feature Selection**, and **LLM-driven selection**. We evaluate these methods based on their ability to handle high-dimensional data, ensure stability, and provide causal explanations, thereby bridging the gap between computational innovation and clinical utility."
    },
    {
      "heading": "2. Key Concepts and Definitions",
      "level": 2,
      "content": "To establish a rigorous framework, it is essential to define the core taxonomies and metrics governing feature selection in medical informatics."
    },
    {
      "heading": "2.1 Taxonomy of Feature Selection",
      "level": 3,
      "content": "Feature selection methods are traditionally categorized into three primary classes, though recent hybrid approaches have blurred these lines:\n\n1.  **Filter Methods:** These techniques evaluate features based on intrinsic statistical properties (e.g., correlation, mutual information, Chi-square) independent of any learning algorithm. They are computationally efficient but often ignore feature dependencies [cite: 2, 4].\n2.  **Wrapper Methods:** These methods treat feature selection as a search problem, evaluating subsets by training a specific predictive model (e.g., Recursive Feature Elimination or RFE). While they often yield higher accuracy, they are computationally expensive and prone to overfitting [cite: 4, 5].\n3.  **Embedded Methods:** Feature selection is integrated directly into the model training process. Examples include LASSO (L1 regularization) and tree-based importance (Random Forest, XGBoost). These offer a balance between computational efficiency and interaction modeling [cite: 2, 6]."
    },
    {
      "heading": "2.2 Stability vs. Accuracy",
      "level": 3,
      "content": "In clinical domains, **stability**—the robustness of the selected feature subset to perturbations in the training data—is as critical as predictive accuracy. A biomarker signature that changes entirely with the addition of a few patients is clinically useless. Stability is quantified using metrics such as the **Kuncheva Index** and **Nogueira’s Measure**, which assess the overlap between feature subsets selected across different data folds [cite: 7, 8]."
    },
    {
      "heading": "2.3 Causal vs. Associative Selection",
      "level": 3,
      "content": "Traditional FS identifies features correlated with the outcome. However, correlation does not imply causation. **Causal Feature Selection** aims to identify the Markov Blanket (MB) of the target variable—the minimal set of features that renders the target independent of all other variables. This distinguishes true risk factors from confounders and spurious correlates [cite: 9, 10, 11]."
    },
    {
      "heading": "3. Historical Development and Milestones",
      "level": 2,
      "content": "The evolution of feature selection for disease prediction can be traced through three distinct eras:\n\n*   **The Statistical Era (Pre-2000s):** Early approaches relied heavily on univariate statistical tests (t-tests, ANOVA) to filter variables. While interpretable, these methods failed to capture complex, non-linear interactions between risk factors [cite: 1].\n*   **The Machine Learning Era (2000s–2015):** The introduction of Support Vector Machines (SVM) and Random Forests popularized multivariate selection. Techniques like SVM-RFE (Recursive Feature Elimination) became gold standards for gene expression analysis [cite: 12, 13]. The concept of \"stability\" in FS was formalized during this period by Kuncheva (2007), addressing the reproducibility crisis in biomarker discovery [cite: 14].\n*   **The Deep Learning and Causal Era (2016–Present):** The rise of Deep Neural Networks (DNNs) led to \"black-box\" models, necessitating explainable AI (XAI) methods like SHAP and LIME for feature attribution [cite: 15, 16]. Concurrently, the integration of causal inference (e.g., CausalDRIFT) and privacy-preserving Federated Learning has redefined the field, addressing modern challenges of data silos and spurious correlations [cite: 11, 17]."
    },
    {
      "heading": "4. Current State-of-the-Art Methods and Techniques",
      "level": 2,
      "content": "This section details the advanced methodologies dominating the research landscape in 2024 and 2025."
    },
    {
      "heading": "4.1 Causal Feature Selection",
      "level": 3,
      "content": "One of the most significant recent shifts is the move from correlation-based to causality-based selection. High-dimensional medical datasets often contain \"spurious correlations\"—variables that correlate with the disease due to confounding rather than mechanism.\n\n*   **CausalDRIFT:** Introduced in 2025, CausalDRIFT leverages the Frisch-Waugh-Lovell theorem and Double Machine Learning to estimate the Average Treatment Effect (ATE) of features. It excels in high-dimensional, low-sample-size (HDLSS) settings (e.g., breast cancer datasets), achieving competitive accuracy while prioritizing features with causal influence rather than mere association [cite: 11].\n*   **Markov Blanket Discovery (SF-DRMB):** The Simultaneous-Fusion-of-Double-Rules Markov Blanket (SF-DRMB) algorithm addresses errors in conditional independence tests. By refining the identification of parents, children, and spouses of the target node, it improves the robustness of risk factor mining, outperforming traditional Bayesian network methods [cite: 10]."
    },
    {
      "heading": "4.2 Large Language Models (LLMs) for Feature Selection",
      "level": 3,
      "content": "The emergence of LLMs has introduced a paradigm shift known as **Zero-Shot Feature Selection**.\n\n*   **LLM-Select:** Recent studies (2024-2025) demonstrate that LLMs (e.g., GPT-4) can select predictive features given only feature names and a task description, without accessing the training data. This method, termed *LLM-Select*, has shown performance rivaling data-driven methods like LASSO, particularly in domains where semantic knowledge is valuable. It is especially useful for determining which features to collect *before* data acquisition begins [cite: 18, 19, 20].\n*   **Clinical Prediction with LLMs (CPLLM):** This approach fine-tunes pre-trained LLMs on historical medical records to predict disease diagnosis and readmission. It outperforms state-of-the-art structured data models (like Med-BERT) by leveraging the semantic richness of clinical narratives alongside structured features [cite: 21]."
    },
    {
      "heading": "4.3 Federated Feature Selection (FFS)",
      "level": 3,
      "content": "With the increasing enforcement of data privacy regulations (GDPR, HIPAA), centralized data pooling is often infeasible. Federated Learning (FL) allows model training across decentralized edge devices.\n\n*   **Synthetic Data and Zero Trust:** A 2025 breakthrough in FFS involves using synthetic data to initialize model parameters and a zero-trust security model. This method allows healthcare institutions to collaboratively identify globally relevant features without sharing raw patient data. It utilizes backward elimination and threshold variation to improve communication efficiency by 4 to 14 times compared to standard FL [cite: 17, 22].\n*   **Handling Non-IID Data:** Heterogeneity in medical data (Non-IID) across hospitals poses a major challenge. Recent FFS algorithms employ multi-objective optimization and mutual information to select features that are robust across diverse client distributions, mitigating the performance degradation typically seen in horizontal FL [cite: 23, 24]."
    },
    {
      "heading": "4.4 Swarm Intelligence and Evolutionary Algorithms",
      "level": 3,
      "content": "Meta-heuristic algorithms continue to evolve for optimizing feature subsets in complex search spaces.\n\n*   **Hybrid Swarm Methods:** Recent work combines Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) with classifiers like SVM. For instance, the **Leader-Follower PSO with Levy Flight (LFPSOLF)** algorithm prevents premature convergence to local optima, achieving up to 8% accuracy improvement in disease detection tasks while reducing feature count by over 40% [cite: 25, 26].\n*   **Genetic Algorithms with Ensemble Learning:** Integrating Genetic Algorithms (GA) with ensemble deep learning (optimized via Tunicate Swarm Algorithm) has shown superior performance in heart disease prediction, effectively navigating the trade-off between dimensionality reduction and information loss [cite: 27]."
    },
    {
      "heading": "4.5 Attention-Based and Deep Feature Selection",
      "level": 3,
      "content": "Deep learning models are increasingly incorporating intrinsic feature selection mechanisms.\n\n*   **Attention Mechanisms:** In multi-omics and image analysis, attention layers (e.g., Graph Attention Networks, BiFormer) dynamically weight features. For example, **DyGAF** (Dynamic Gene Attention Feature selection) identifies critical genetic biomarkers for COVID-19 by prioritizing genes with persistent importance across disease progression [cite: 28, 29].\n*   **Feature Selection Layers:** New architectures embed specific \"FS layers\" trained jointly with the network. This addresses the accuracy/interpretability trade-off by forcing the network to learn sparse weights for input features, making the deep model's decisions traceable to specific inputs [cite: 4]."
    },
    {
      "heading": "5.1 Multi-Omics and Cancer Subtyping",
      "level": 3,
      "content": "Multi-omics data (genomics, transcriptomics, proteomics) presents extreme dimensionality.\n*   **Case Study (TCGA):** A benchmark of 15 cancer datasets from The Cancer Genome Atlas (TCGA) evaluated stability across omics layers. It found that miRNA features consistently exhibited higher stability than mRNA or mutation data. Regularized classifiers (SVM with L1) provided the best balance of stability and accuracy for TP53 mutation prediction [cite: 8, 30].\n*   **Biomarker Discovery:** In adrenocortical carcinoma, factor analysis and network models were used to select features that are biologically coherent, contrasting with supervised methods that often select highly correlated but biologically distinct markers [cite: 31]."
    },
    {
      "heading": "5.2 Neurological Disorders",
      "level": 3,
      "content": "*   **Autonomic Dysreflexia (AD):** A study on spinal cord injury patients demonstrated that rigorous feature selection could reduce 36 physiological features down to just 5 relevant ones (related to skin nerve activity and blood pressure). A 5-layer neural network trained on this reduced set achieved **93.4% accuracy**, significantly outperforming models using the full feature set [cite: 3, 32].\n*   **Alzheimer’s and Parkinson’s:** Federated feature selection has been successfully applied to detect Parkinson’s disease using distributed medical databases, ensuring privacy while aggregating diagnostic patterns from neuroimaging data [cite: 33]."
    },
    {
      "heading": "5.3 Cardiovascular and Metabolic Diseases",
      "level": 3,
      "content": "*   **Heart Disease:** Ensemble techniques combining Random Forest and Gradient Boosting with Boruta feature selection have achieved near-perfect accuracy (approx. 98%) in specific datasets by isolating key risk factors like chest pain type and thallium stress test results [cite: 34, 35].\n*   **Diabetes:** Causal feature selection methods (like CausalDRIFT) have been applied to diabetes datasets. Interestingly, results suggest that in correlation-dominated datasets (like standard diabetes tables), traditional methods may still outperform causal ones, highlighting that causal FS is most beneficial when hidden confounders are present [cite: 11]."
    },
    {
      "heading": "6. Challenges and Open Problems",
      "level": 2,
      "content": "Despite significant progress, several critical challenges remain:"
    },
    {
      "heading": "6.1 The Stability-Accuracy Trade-off",
      "level": 3,
      "content": "While accuracy is the primary metric for ML competitions, **stability** is paramount for clinical adoption. A feature selection method that yields different biomarkers for the same disease across two similar patient cohorts is untrustworthy. Research indicates that many popular methods (e.g., standard RFE) have low stability. The **Nogueira measure** has been identified as a superior metric for quantifying this, yet it is not universally adopted in reporting [cite: 7, 8]."
    },
    {
      "heading": "6.2 Interpretability in Deep Learning",
      "level": 3,
      "content": "Deep learning models with embedded feature selection often suffer from a lack of transparency. While tools like **SHAP** (Shapley Additive exPlanations) and **LIME** (Local Interpretable Model-agnostic Explanations) provide post-hoc explanations, they can be computationally expensive and occasionally unstable themselves. There is a tension between the non-linear power of DL and the linear simplifications required for explanation [cite: 15, 36]."
    },
    {
      "heading": "6.3 Heterogeneity in Federated Learning",
      "level": 3,
      "content": "In Federated Learning, data is rarely Independent and Identically Distributed (IID). Feature selection algorithms that assume IID data often fail in federated settings. \"Client drift\"—where local models diverge due to different feature distributions—remains a significant hurdle for Federated Feature Selection [cite: 23, 24]."
    },
    {
      "heading": "6.4 Small Sample Size (HDLSS)",
      "level": 3,
      "content": "The \"High-Dimension, Low-Sample-Size\" problem persists, particularly in rare disease research. While CausalDRIFT shows promise here, most deep learning and LLM-based methods require substantial data or pre-training that may not be available for niche conditions [cite: 11]."
    },
    {
      "heading": "7. Future Research Directions",
      "level": 2,
      "content": "1.  **Integration of Causal Inference and Deep Learning:** Future frameworks should natively integrate causal discovery (e.g., structural equation modeling) into deep learning architectures to ensure that selected features represent actionable intervention targets rather than passive predictors [cite: 11, 37].\n2.  **LLM-Driven Data Collection:** Moving beyond prediction, LLMs could be used to design clinical trials by identifying which features are *worth* collecting, potentially reducing the cost and burden of data acquisition [cite: 18, 20].\n3.  **Robust Federated Protocols:** Developing FFS algorithms that are invariant to non-IID data distributions is critical. Techniques involving synthetic data generation at the edge (to normalize distributions without sharing real data) represent a promising avenue [cite: 17, 22].\n4.  **Dynamic/Real-Time Feature Selection:** For conditions like sepsis or Autonomic Dysreflexia, risk factors change dynamically. \"Online\" feature selection methods that can adapt to streaming physiological data in real-time are needed [cite: 3].\n5.  **Standardization of Stability Metrics:** The academic community must standardize the reporting of feature stability (using Nogueira’s measure) alongside accuracy to facilitate valid comparisons between novel algorithms [cite: 7, 8]."
    },
    {
      "heading": "8. Conclusion",
      "level": 2,
      "content": "Feature selection has evolved from a statistical necessity to a sophisticated domain intersecting causal inference, deep learning, and privacy-preserving computing. The current state-of-the-art in disease risk prediction is defined by a shift towards methods that are not only accurate but also **causal**, **stable**, and **privacy-compliant**.\n\nWhile deep learning and LLMs offer unprecedented power in extracting patterns from unstructured data, the integration of **Causal Feature Selection** (e.g., CausalDRIFT) and **Federated Feature Selection** represents the frontier of robust medical AI. These methods address the fundamental ethical and practical requirements of healthcare: understanding *why* a prediction is made and ensuring patient data remains secure. As the field matures, the focus must remain on validating these computational advances through rigorous clinical trials, ensuring that the \"informative\" features selected in silico translate to tangible patient benefits in vivo."
    }
  ],
  "references": [
    "[cite: 1] Pudjihartono, N., Fadason, T., Kempa-Liehr, A. W., & O'Sullivan, J. M. (2022). A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction. *Frontiers in Bioinformatics*, 2, 927312. [cite: 1, 2, 6, 38, 39]",
    "[cite: 4] Deep Learning-based Feature Selection. *PMC*, 8433983. [cite: 4]",
    "[cite: 34] Ensemble Learning for Disease Prediction. *MDPI*, 2023. [cite: 34]",
    "[cite: 40] Review of Feature Selection Techniques for Predicting Diseases. *IEEE Conference Publication*, 2020. [cite: 40]",
    "[cite: 17] Madathil, N. T., Alrabaee, S., & Belkacem, A. N. (2025). Enhancing Federated Feature Selection Through Synthetic Data and Zero Trust Integration. *IEEE Journal on Selected Areas in Communications*. [cite: 17, 22, 33, 41, 42]",
    "[cite: 43] Federated Learning in Healthcare. *PMC*, 12213103. [cite: 43]",
    "[cite: 44] Federated Learning in Healthcare: Balancing Privacy and Accuracy. *ResearchGate*, 2025. [cite: 44]",
    "[cite: 45] Teo, D. S. W., et al. (2024). Federated Learning in Health: A Systematic Review. *PMC*, 10897620. [cite: 45]",
    "[cite: 46] Federated Learning for Medical Data Analysis. *MDPI*, 2024. [cite: 46]",
    "[cite: 9] Causal Feature Selection Overview. *Fiveable*, 2024. [cite: 9]",
    "[cite: 37] Feature Selection and Causal Graphs. *PMC*, 11554952. [cite: 37]",
    "[cite: 12] Causal Feature Selection for Health State Identification. *Nonlinear Dynamics*, 2025. [cite: 12]",
    "[cite: 10] SF-DRMB: Simultaneous-Fusion-of-Double-Rules MB Learning. *IEEE Xplore*, 2023. [cite: 10, 47, 48]",
    "[cite: 11] CausalDRIFT: Causal Feature Selection Algorithm. *MedRxiv*, 2025. [cite: 11, 49, 50, 51, 52]",
    "[cite: 2] Pudjihartono, N., et al. (2022). Feature Selection for Disease Risk Prediction. *Frontiers in Bioinformatics*. [cite: 2]",
    "[cite: 3] Feature Selection for Autonomic Dysreflexia Detection. *PMC*, 9416695. [cite: 3, 32, 35, 53, 54, 55]",
    "[cite: 56] Common Feature Selection Techniques in ML. *ResearchGate*, 2024. [cite: 56]",
    "[cite: 57] Kuncheva Index Implementation. *GitHub*, 2024. [cite: 57]",
    "[cite: 14] Kuncheva, L. I. (2007). A Stability Index for Feature Selection. *AIAP*. [cite: 14]",
    "[cite: 7] Nogueira's Measure and Stability Analysis. *MDPI*, 2019. [cite: 7, 13, 58, 59]",
    "[cite: 15] Interpretable Feature Selection with SHAP and LIME. *PMC*, 10074303. [cite: 15]",
    "[cite: 16] SHAP and LIME for Chronic Kidney Disease. *IJARST*, 2024. [cite: 16]",
    "[cite: 36] Comparing MDA, LIME, and SHAP Stability. *SciSpace*, 2020. [cite: 36]",
    "[cite: 60] SHAP and LIME for Osteoarthritis Classification. *MDPI*, 2024. [cite: 60]",
    "[cite: 8] Stability of Feature Selection in Multi-Omics. *MDPI*, 2024. [cite: 8, 30]",
    "[cite: 18] LLM-Select: Feature Selection with Large Language Models. *arXiv*, 2024. [cite: 18, 19, 20, 61, 62, 63]",
    "[cite: 21] Clinical Prediction with Large Language Models (CPLLM). *PMC*, 11623460. [cite: 21]",
    "[cite: 30] Multi-Omics Feature Selection Stability. *ResearchGate*, 2024. [cite: 30]",
    "[cite: 5] Benchmark of Feature Selection for Multi-Omics. *ResearchGate*, 2022. [cite: 5]",
    "[cite: 31] Feature Selection for Multi-Omics Data. *Leiden University*, 2024. [cite: 31]",
    "[cite: 17] Federated Feature Selection with Synthetic Data. *IEEE Xplore*, 2025. [cite: 17]",
    "[cite: 22] Enhancing Federated Feature Selection. *ResearchGate*, 2025. [cite: 22]",
    "[cite: 33] Federated Learning for Parkinson's Disease. *Semantic Scholar*, 2022. [cite: 33]",
    "[cite: 6] Review of Feature Selection for Cancer Diagnosis. *MDPI*, 2025. [cite: 6]",
    "[cite: 11] CausalDRIFT Performance. *MedRxiv*, 2025. [cite: 11]",
    "[cite: 10] SF-DRMB Algorithm. *IEEE Xplore*, 2023. [cite: 10]",
    "[cite: 19] LLM-Select Overview. *The Moonlight*, 2024. [cite: 19]",
    "[cite: 20] LLM-Select Paper. *arXiv*, 2024. [cite: 20]",
    "[cite: 23] Federated Feature Selection and Heterogeneity. *arXiv*, 2024. [cite: 23]",
    "[cite: 24] Federated Feature Selection Thesis. *DIVA*, 2024. [cite: 24]",
    "[cite: 32] Autonomic Dysreflexia Detection. *Frontiers in Neuroinformatics*, 2022. [cite: 32]",
    "[cite: 25] Swarm Intelligence in Feature Selection. *ResearchGate*, 2025. [cite: 25]",
    "[cite: 26] Leader-Follower PSO for Disease Detection. *IEEE Xplore*, 2024. [cite: 26]",
    "[cite: 27] Tunicate Swarm Algorithm for Heart Disease. *TechScience*, 2025. [cite: 27]",
    "[cite: 64] Hybrid PSO-ACO for Feature Selection. *Cureus*, 2025. [cite: 64]",
    "[cite: 28] Attention-Based Feature Selection (DyGAF). *PMC*, 2025. [cite: 28]",
    "[cite: 29] Attention-Based Feature Selection Thesis. *City University of London*, 2025. [cite: 29]",
    "[cite: 7] Analysis of Stability Measures. *MDPI*, 2019. [cite: 7]"
  ]
}