{
  "outline": [
    [
      1,
      "Literature Review: Preferred reporting items for systematic review and meta-analysis of diagnostic test accuracy studies (PRISMA-DTA)- explanation, elaboration, and checklist."
    ],
    [
      1,
      "Preferred Reporting Items for Systematic Review and Meta-Analysis of Diagnostic Test Accuracy Studies (PRISMA-DTA): A Comprehensive Systematic Literature Review"
    ],
    [
      2,
      "1. Introduction and Background"
    ],
    [
      3,
      "1.1. The Imperative of Diagnostic Test Accuracy Reviews"
    ],
    [
      3,
      "1.2. Research Motivation and Objectives"
    ],
    [
      2,
      "2. Key Concepts and Definitions"
    ],
    [
      3,
      "2.1. Diagnostic Accuracy Metrics"
    ],
    [
      3,
      "2.2. Structural Differences in Systematic Reviews"
    ],
    [
      2,
      "3. Historical Development and Milestones"
    ],
    [
      3,
      "3.1. From QUOROM to PRISMA"
    ],
    [
      3,
      "3.2. The Need for a DTA Extension"
    ],
    [
      3,
      "3.3. Development of PRISMA-DTA (2018)"
    ],
    [
      3,
      "3.4. Explanation and Elaboration (2020)"
    ],
    [
      3,
      "3.5. PRISMA-DTA for Abstracts (2021)"
    ],
    [
      2,
      "4. Current State-of-the-Art Methods: The PRISMA-DTA Checklist"
    ],
    [
      3,
      "4.1. Modifications to the Original PRISMA"
    ],
    [
      3,
      "4.2. The Added Items"
    ],
    [
      3,
      "4.3. The Omitted Items"
    ],
    [
      3,
      "4.4. Key Modified Items"
    ],
    [
      2,
      "5. Applications and Case Studies: Adherence and Impact"
    ],
    [
      3,
      "5.1. Baseline Adherence (Pre-2019)"
    ],
    [
      3,
      "5.2. Five-Year Follow-Up (2025)"
    ],
    [
      3,
      "5.3. Factors Associated with Better Reporting"
    ],
    [
      3,
      "5.4. Application in COVID-19 Research"
    ],
    [
      2,
      "6. Challenges and Open Problems"
    ],
    [
      3,
      "6.1. \"PRISMA Signaling\" and Superficial Compliance"
    ],
    [
      3,
      "6.2. The Abstract Reporting Gap"
    ],
    [
      3,
      "6.3. Complexity of DTA Methodology"
    ],
    [
      2,
      "7. Future Research Directions"
    ],
    [
      3,
      "7.1. PRISMA-AI: Artificial Intelligence Extension"
    ],
    [
      3,
      "7.2. PRISMA-LSR: Living Systematic Reviews"
    ],
    [
      3,
      "7.3. Individual Participant Data (IPD)"
    ],
    [
      3,
      "7.4. Automation and Tool Development"
    ],
    [
      2,
      "8. Conclusion"
    ],
    [
      2,
      "9. References"
    ]
  ],
  "content": [
    {
      "heading": "Literature Review: Preferred reporting items for systematic review and meta-analysis of diagnostic test accuracy studies (PRISMA-DTA)- explanation, elaboration, and checklist.",
      "level": 1,
      "content": "*Generated on: 2025-12-26 05:11:49*\n*Progress: [10/25]*\n*Saved to: /Users/joanna/Desktop/Literature_Reviews/Preferred_reporting_items_for_systematic_review_and_meta-ana_20251226_051149.md*\n---"
    },
    {
      "heading": "Preferred Reporting Items for Systematic Review and Meta-Analysis of Diagnostic Test Accuracy Studies (PRISMA-DTA): A Comprehensive Systematic Literature Review",
      "level": 1,
      "content": "**Abstract**\n\nSystematic reviews and meta-analyses of diagnostic test accuracy (DTA) studies are pivotal for evidence-based medicine, guiding clinical decision-making by synthesizing data on the performance of medical tests. However, the quality of reporting in these reviews has historically been suboptimal, compromising their utility and reproducibility. The \"Preferred Reporting Items for Systematic Reviews and Meta-Analyses of Diagnostic Test Accuracy Studies\" (PRISMA-DTA) statement was developed to address the unique methodological challenges of DTA research, which differ significantly from therapeutic intervention reviews. This comprehensive literature review examines the development, structure, and impact of the PRISMA-DTA guideline, including its Explanation and Elaboration (E&E) document and the extension for abstracts. We analyze the historical context leading to its creation, detail the specific modifications made to the original PRISMA checklist—including the addition of items on the clinical role of the index test and statistical synthesis methods—and evaluate the current state of adherence based on recent bibliometric studies. Furthermore, we explore the intersection of PRISMA-DTA with emerging fields such as artificial intelligence (PRISMA-AI) and living systematic reviews (PRISMA-LSR). While adherence to PRISMA-DTA in full-text reports has shown modest improvement over time, reporting in abstracts remains deficient. This review identifies critical gaps in implementation and proposes future research directions to enhance the transparency and rigorous reporting of diagnostic evidence.\n\n---"
    },
    {
      "heading": "1.1. The Imperative of Diagnostic Test Accuracy Reviews",
      "level": 3,
      "content": "Diagnostic tests are the gateway to appropriate clinical management. The accuracy of a test—defined by its ability to correctly identify the presence or absence of a target condition—determines the subsequent clinical pathway, influencing treatment decisions, patient anxiety, and healthcare resource allocation. Systematic reviews of diagnostic test accuracy (DTA) studies synthesize evidence from multiple primary studies to provide summary estimates of sensitivity, specificity, and other accuracy metrics. These reviews are considered the highest level of evidence for assessing diagnostic technologies [cite: 1, 2].\n\nHowever, the value of a systematic review depends heavily on the clarity and completeness of its reporting. Incomplete reporting prevents readers from assessing the validity of the review's methods, the risk of bias in included studies, and the applicability of the findings to their specific clinical setting. Historically, DTA reviews have suffered from poor reporting quality, often failing to describe search strategies, eligibility criteria, or methods for data synthesis adequately [cite: 1, 3]."
    },
    {
      "heading": "1.2. Research Motivation and Objectives",
      "level": 3,
      "content": "The motivation for this review stems from the critical role that reporting guidelines play in reducing research waste. The original PRISMA statement, published in 2009 and updated in 2020, was designed primarily for systematic reviews of interventions (randomized controlled trials). It failed to address the specific nuances of diagnostic research, such as the lack of a \"control group\" in the traditional sense, the complexity of bivariate meta-analysis, and the unique sources of bias (e.g., spectrum bias, verification bias) inherent to DTA studies [cite: 4, 5].\n\nConsequently, the PRISMA-DTA extension was developed and published in 2018 [cite: 4], followed by an Explanation and Elaboration document in 2020 [cite: 2] and a checklist for abstracts in 2021 [cite: 6]. This paper aims to provide a comprehensive systematic review of the PRISMA-DTA framework. The objectives are to:\n1.  Elucidate the key concepts and definitions underpinning DTA reporting.\n2.  Trace the historical development of the guideline.\n3.  Analyze the specific components of the PRISMA-DTA checklist and the rationale for modifications from the generic PRISMA.\n4.  Evaluate the impact of the guideline on reporting quality through an analysis of adherence studies.\n5.  Identify challenges, such as \"PRISMA signaling,\" and future directions involving AI and living evidence synthesis.\n\n---"
    },
    {
      "heading": "2. Key Concepts and Definitions",
      "level": 2,
      "content": "To understand the necessity of PRISMA-DTA, one must first define the core components of diagnostic research that distinguish it from intervention research."
    },
    {
      "heading": "2.1. Diagnostic Accuracy Metrics",
      "level": 3,
      "content": "Unlike intervention studies that typically measure a treatment effect (e.g., odds ratio, hazard ratio), DTA studies measure the agreement between an **index test** (the test being evaluated) and a **reference standard** (the best available method for establishing the presence or absence of the condition). Key metrics include:\n*   **Sensitivity**: The probability that the index test is positive given that the target condition is present.\n*   **Specificity**: The probability that the index test is negative given that the target condition is absent.\n*   **Likelihood Ratios**: The ratio of the probability of a test result in patients with the disease to the probability in those without."
    },
    {
      "heading": "2.2. Structural Differences in Systematic Reviews",
      "level": 3,
      "content": "*   **Participants**: In DTA reviews, the \"participants\" are individuals suspected of having the target condition, rather than patients with a confirmed diagnosis randomized to treatment.\n*   **Target Condition**: The disease or clinical state that the index test is intended to detect.\n*   **Reference Standard**: The \"gold standard\" used to verify the index test results. Variability or imperfection in the reference standard is a major source of bias in DTA studies.\n*   **Unit of Analysis**: DTA reviews may analyze data per patient, per lesion, or per site, adding complexity to the statistical synthesis [cite: 7].\n\n---"
    },
    {
      "heading": "3.1. From QUOROM to PRISMA",
      "level": 3,
      "content": "The movement toward standardized reporting began with the QUOROM (Quality of Reporting of Meta-analyses) statement in 1999, which focused on meta-analyses of randomized controlled trials (RCTs). This evolved into the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement in 2009. While PRISMA became the global standard for reporting systematic reviews, its intervention-centric focus left gaps for other study designs [cite: 4, 8]."
    },
    {
      "heading": "3.2. The Need for a DTA Extension",
      "level": 3,
      "content": "Researchers identified that the original PRISMA items were often ill-suited for DTA reviews. For example, the concept of \"intervention\" does not apply to diagnostic tests, and \"outcome\" in DTA refers to accuracy measures rather than health endpoints like mortality. Furthermore, statistical methods for pooling sensitivity and specificity (e.g., hierarchical summary receiver operating characteristic models) differ fundamentally from the fixed- or random-effects models used for intervention effects [cite: 5, 9]."
    },
    {
      "heading": "3.3. Development of PRISMA-DTA (2018)",
      "level": 3,
      "content": "The PRISMA-DTA statement was developed by a multidisciplinary group of experts, including methodologists, statisticians, and journal editors. The process followed the EQUATOR Network's guidance for developing reporting guidelines:\n1.  **Systematic Review**: A review of existing reporting guidelines and methods for DTA reviews was conducted to generate potential items.\n2.  **Delphi Process**: A 3-round Delphi survey with 24 experts was used to refine the items.\n3.  **Consensus Meeting**: A face-to-face meeting was held to finalize the checklist.\n4.  **Pilot Testing**: The draft checklist was piloted to ensure usability [cite: 4, 8].\n\nThe final PRISMA-DTA statement was published in *JAMA* in 2018 [cite: 4]. It modified the original PRISMA checklist to reflect the specific requirements of DTA research."
    },
    {
      "heading": "3.4. Explanation and Elaboration (2020)",
      "level": 3,
      "content": "To facilitate the implementation of the guideline, an Explanation and Elaboration (E&E) paper was published in *The BMJ* in 2020 [cite: 2]. This document provided detailed rationales for each item, along with examples of good reporting from the published literature. It aimed to serve as a pedagogical resource for authors and peer reviewers [cite: 1, 10]."
    },
    {
      "heading": "3.5. PRISMA-DTA for Abstracts (2021)",
      "level": 3,
      "content": "Recognizing that abstracts are often the only part of a paper read by clinicians, the PRISMA-DTA group published a specific extension for abstracts in 2021 [cite: 6]. This 12-item checklist addressed the severe limitations found in DTA review abstracts, such as the failure to report confidence intervals or the reference standard used [cite: 6, 11].\n\n---"
    },
    {
      "heading": "4. Current State-of-the-Art Methods: The PRISMA-DTA Checklist",
      "level": 2,
      "content": "The PRISMA-DTA checklist consists of 27 items. While the numbering is preserved from the original PRISMA statement to facilitate comparison, the content has been significantly adapted."
    },
    {
      "heading": "4.1. Modifications to the Original PRISMA",
      "level": 3,
      "content": "Of the 27 original items:\n*   **8 items were unchanged**: These generic items (e.g., Title, Introduction rationale) apply equally to all systematic reviews.\n*   **17 items were modified**: These were adapted to use DTA terminology (e.g., changing \"intervention\" to \"index test\" and \"outcomes\" to \"diagnostic accuracy measures\") [cite: 5, 8].\n*   **2 items were added**: New items specific to DTA methodology.\n*   **2 items were omitted**: Items deemed irrelevant or lacking evidence in the DTA context."
    },
    {
      "heading": "4.2. The Added Items",
      "level": 3,
      "content": "Two new items were introduced to address critical aspects of diagnostic research:\n1.  **Item D1: Clinical Role of Index Test**: Authors must state the scientific and clinical background, including the intended use (e.g., triage, replacement, add-on) and clinical role of the index test. This context is vital because the accuracy of a test can vary significantly depending on its placement in the clinical pathway [cite: 7].\n2.  **Item D2: Statistical Methods for Meta-analysis**: Authors must report the statistical methods used for meta-analysis. This item emphasizes the need for specialized hierarchical models (e.g., bivariate model, HSROC) that account for the correlation between sensitivity and specificity, rather than simple univariate pooling which is often inappropriate for DTA data [cite: 7]."
    },
    {
      "heading": "4.3. The Omitted Items",
      "level": 3,
      "content": "The items related to **publication bias** (Original PRISMA Items 15 and 22) were removed in the DTA extension.\n*   **Rationale**: Unlike intervention trials where \"positive\" results (statistically significant differences) are more likely to be published, the drivers of publication bias in DTA studies are less clear. There is no simple p-value threshold that determines publication. Furthermore, traditional tools for detecting publication bias (e.g., funnel plots, Egger's test) are mathematically inappropriate for DTA data due to the interdependence of sensitivity and specificity. The PRISMA-DTA group concluded that statistical investigation of publication bias should not be routinely recommended until better methods and evidence exist [cite: 5, 12]."
    },
    {
      "heading": "4.4. Key Modified Items",
      "level": 3,
      "content": "*   **Item 6 (Eligibility Criteria)**: Must specify characteristics of the index test, reference standard, and target condition. It also requires a rationale for the reference standard chosen, as this is the benchmark for accuracy [cite: 13].\n*   **Item 13 (Diagnostic Accuracy Measures)**: Requires stating the principal metrics (sensitivity, specificity) and the unit of assessment (per-patient vs. per-lesion) [cite: 7].\n*   **Item 14 (Synthesis of Results)**: Describes methods for handling indeterminate results and variability between studies (heterogeneity), which is often high in diagnostic reviews [cite: 7].\n\n---"
    },
    {
      "heading": "5. Applications and Case Studies: Adherence and Impact",
      "level": 2,
      "content": "The ultimate measure of a reporting guideline's success is its adoption by the research community and the subsequent improvement in reporting quality."
    },
    {
      "heading": "5.1. Baseline Adherence (Pre-2019)",
      "level": 3,
      "content": "Before and immediately following the release of PRISMA-DTA, adherence was assessed to establish a baseline. A study by Salameh et al. (2019) evaluating reviews published between 2017 and 2018 found that DTA reviews reported, on average, only **71%** of the PRISMA-DTA items (18.6 out of 26 applicable items). Reporting was particularly poor for protocol registration, definitions of data extraction, and characteristics of included studies [cite: 14, 15].\n\nFor abstracts, the situation was worse. A baseline evaluation of DTA abstracts showed that only **50%** of the checklist items were reported (5.5 out of 11 items). Crucial elements like the reference standard and risk of bias assessments were frequently missing [cite: 14, 16]."
    },
    {
      "heading": "5.2. Five-Year Follow-Up (2025)",
      "level": 3,
      "content": "A recent follow-up study by Salameh et al., published in *The Journal of Applied Laboratory Medicine* in 2025, evaluated adherence five years post-publication.\n*   **Full-Text Adherence**: There was a statistically significant, albeit modest, improvement. The mean adherence rose to **78%** (20.3 out of 26 items). Improvements were noted in the reporting of statistical methods and study selection [cite: 17, 18].\n*   **Abstract Adherence**: Disappointingly, there was **no significant improvement** in abstract reporting, with adherence remaining stagnant at **52%** (5.7 out of 11 items). Items such as funding, strengths/limitations, and characteristics of included studies remain infrequently reported in abstracts [cite: 17, 18]."
    },
    {
      "heading": "5.3. Factors Associated with Better Reporting",
      "level": 3,
      "content": "The 2025 analysis identified several factors associated with higher reporting quality:\n*   **Journal Impact Factor (IF)**: Reviews published in higher IF journals tended to have better adherence [cite: 18].\n*   **Citation of PRISMA**: Authors who explicitly cited the PRISMA guideline or its extension reported more items [cite: 18].\n*   **Supplemental Material**: The use of appendices allowed authors to report more details, correlating with higher adherence scores [cite: 18].\n*   **Journal Endorsement**: While journal endorsement of guidelines is generally thought to improve quality, the association was not always consistent across all sub-analyses, suggesting that \"endorsement\" often does not translate to strict enforcement by editors [cite: 3, 15]."
    },
    {
      "heading": "5.4. Application in COVID-19 Research",
      "level": 3,
      "content": "The COVID-19 pandemic generated a surge in DTA reviews evaluating rapid antigen tests, PCR, and chest imaging. A systematic review of these COVID-19 DTA reviews found that while many claimed adherence to PRISMA, the actual quality was variable. Rapid reviews often bypassed critical steps like protocol registration (Item 5) or detailed risk of bias assessment (Item 19), highlighting the tension between speed and rigorous reporting during a health crisis [cite: 19, 20].\n\n---"
    },
    {
      "heading": "6.1. \"PRISMA Signaling\" and Superficial Compliance",
      "level": 3,
      "content": "A critical challenge identified in the literature is \"PRISMA signaling\"—the phenomenon where authors cite the PRISMA guideline to signal quality or satisfy journal requirements without genuinely adhering to the checklist items. A study by Cacciamani et al. and commentary by others suggest that peer reviewers and editors often fail to verify the actual presence of checklist items, treating the citation as a proxy for quality [cite: 21]. This leads to a discrepancy between claimed and actual adherence."
    },
    {
      "heading": "6.2. The Abstract Reporting Gap",
      "level": 3,
      "content": "The persistent lack of improvement in abstract reporting is a major concern. Abstracts are the primary means by which clinicians consume research. The omission of key details like the reference standard or the study design (case-control vs. cohort) in the abstract can lead to misinterpretation of the test's accuracy. The 2025 Salameh study suggests that strict word count limits in journals may discourage complete reporting, although their statistical analysis found no direct association between word count and adherence, pointing instead to a lack of awareness or enforcement [cite: 18]."
    },
    {
      "heading": "6.3. Complexity of DTA Methodology",
      "level": 3,
      "content": "The PRISMA-DTA guideline requires reporting of complex statistical methods (Item D2). Many authors, particularly those without strong statistical support, may struggle to implement and report hierarchical meta-analysis models correctly. This leads to \"spin\" or the use of inappropriate metrics (e.g., pooling sensitivity without specificity), which the guideline aims to prevent but cannot entirely eliminate without broader educational efforts [cite: 22].\n\n---"
    },
    {
      "heading": "7. Future Research Directions",
      "level": 2,
      "content": "The landscape of evidence synthesis is evolving, and the PRISMA-DTA framework is expanding to accommodate new technologies and methodologies."
    },
    {
      "heading": "7.1. PRISMA-AI: Artificial Intelligence Extension",
      "level": 3,
      "content": "With the explosion of AI and machine learning in diagnostic imaging and pathology, a new extension, **PRISMA-AI**, is under development and consultation [cite: 23, 24]. AI diagnostic studies have unique reporting needs, such as details on training vs. validation sets, algorithm architecture, and data preprocessing.\n*   **Intersection with DTA**: PRISMA-AI will likely function as a further elaboration of PRISMA-DTA for reviews of AI-based diagnostic tests. It will address issues like \"algorithmic bias\" and the reproducibility of code, which are not fully covered in the current DTA extension [cite: 23, 24]."
    },
    {
      "heading": "7.2. PRISMA-LSR: Living Systematic Reviews",
      "level": 3,
      "content": "Living Systematic Reviews (LSRs) are reviews that are continually updated as new evidence becomes available. The **PRISMA-LSR** extension was published in late 2024 [cite: 25, 26].\n*   **Relevance to DTA**: Diagnostic technologies evolve rapidly (e.g., new COVID-19 variants affecting test sensitivity). DTA reviews are prime candidates for the \"living\" model. Future research must integrate the PRISMA-LSR reporting items (e.g., reporting the status of the review and search dates) with the PRISMA-DTA checklist [cite: 26]."
    },
    {
      "heading": "7.3. Individual Participant Data (IPD)",
      "level": 3,
      "content": "Meta-analyses of Individual Participant Data (IPD) are the gold standard for DTA reviews, allowing for more granular analysis of threshold effects and covariates. While a **PRISMA-IPD** statement exists [cite: 13, 27], there is a need to harmonize it explicitly with DTA requirements, as IPD-DTA reviews face unique ethical and data-sharing challenges."
    },
    {
      "heading": "7.4. Automation and Tool Development",
      "level": 3,
      "content": "To combat \"PRISMA signaling,\" future research is exploring the use of automated tools to check manuscripts for adherence before submission. Tools that can scan a DTA review and flag missing items (e.g., \"No reference standard defined in Abstract\") could significantly reduce the burden on peer reviewers and improve quality [cite: 20].\n\n---"
    },
    {
      "heading": "8. Conclusion",
      "level": 2,
      "content": "The PRISMA-DTA statement represents a landmark development in the field of diagnostic research. By acknowledging the distinct methodological architecture of diagnostic test accuracy studies, it has provided a necessary framework for transparency and rigor. The 2018 statement, supported by the 2020 Explanation and Elaboration document, has successfully defined the standards for the field, introducing critical items regarding clinical context (Item D1) and statistical synthesis (Item D2) while removing inappropriate requirements like publication bias assessment.\n\nHowever, the journey toward complete reporting is ongoing. While full-text adherence has risen to nearly 80% as of 2025, the stagnation of abstract reporting at roughly 50% is a call to action for journal editors and authors. The \"PRISMA signaling\" phenomenon highlights that guidelines alone are insufficient; they must be enforced through rigorous peer review and potentially automated checks.\n\nAs diagnostic medicine enters the era of Artificial Intelligence and Living Evidence, the PRISMA-DTA framework must continue to evolve. The integration of PRISMA-AI and PRISMA-LSR will be essential to ensure that systematic reviews remain a trustworthy compass for clinical practice in an increasingly complex data landscape.\n\n---"
    }
  ],
  "references": [
    "1.  **Salameh JP, Moher D, McGrath TA, et al.** (2025). Assessing Adherence to the PRISMA-DTA Guideline in Diagnostic Test Accuracy Systematic Reviews: A Five-Year Follow-up Analysis. *The Journal of Applied Laboratory Medicine*, 10(2), 416-431. [cite: 17, 18]",
    "2.  **Salameh JP, Bossuyt PM, McGrath TA, et al.** (2020). Preferred reporting items for systematic review and meta-analysis of diagnostic test accuracy studies (PRISMA-DTA): explanation, elaboration, and checklist. *BMJ*, 370, m2632. [cite: 1, 2]",
    "3.  **McInnes MDF, Moher D, Thombs BD, et al.** (2018). Preferred Reporting Items for a Systematic Review and Meta-analysis of Diagnostic Test Accuracy Studies: The PRISMA-DTA Statement. *JAMA*, 319(4), 388-396. [cite: 4, 8, 28]",
    "4.  **Cohen JF, Deeks JJ, Hooft L, et al.** (2021). Preferred reporting items for journal and conference abstracts of systematic reviews and meta-analyses of diagnostic test accuracy studies (PRISMA-DTA for Abstracts): checklist, explanation, and elaboration. *BMJ*, 372, n265. [cite: 6]",
    "5.  **Salameh JP, McInnes MDF, et al.** (2019). Completeness of Reporting of Systematic Reviews of Diagnostic Test Accuracy Based on the PRISMA-DTA Reporting Guideline. *Clinical Chemistry*, 65(2). [cite: 14, 15]",
    "6.  **Akl EA, Khabsa J, Iannizzi C, et al.** (2024). Extension of the PRISMA 2020 statement for living systematic reviews (PRISMA-LSR): checklist and explanation. *BMJ*, 387, e079183. [cite: 26, 29]",
    "7.  **Cacciamani GE, Chu KY, et al.** (2023). PRISMA-AI reporting guidelines for systematic reviews and meta-analyses on AI in healthcare. *Nature Medicine*. [cite: 23, 24]",
    "8.  **Page MJ, McKenzie JE, Bossuyt PM, et al.** (2021). The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. *BMJ*, 372, n71. [cite: 30]",
    "9.  **Whiting P, Rutjes AWS, Westwood ME, et al.** (2011). QUADAS-2: a revised tool for the quality assessment of diagnostic accuracy studies. *Annals of Internal Medicine*, 155(8), 529-536. [cite: 19]",
    "10. **Rethlefsen ML, Kirtley S, Waffenschmidt S, et al.** (2021). PRISMA-S: an extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews. *Systematic Reviews*, 10(1), 39. [cite: 31]",
    "11. **McGrath TA, McInnes MDF, et al.** (2017). Overinterpretation of research findings: evidence of “spin” in systematic reviews of diagnostic accuracy studies. *Clinical Chemistry*, 63(8), 1353–1362. [cite: 22]",
    "12. **Hutton B, Salanti G, Caldwell DM, et al.** (2015). The PRISMA extension statement for reporting of systematic reviews incorporating network meta-analysis: PRISMA-NMA. *Annals of Internal Medicine*, 162(11), 777-784. [cite: 26]",
    "13. **Stewart LA, Clarke M, Rovers M, et al.** (2015). Preferred Reporting Items for Systematic Review and Meta-Analyses of individual participant data: the PRISMA-IPD Statement. *JAMA*, 313(16), 1657-1665. [cite: 13]",
    "14. **Tricco AC, Lillie E, Zarin W, et al.** (2018). PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation. *Annals of Internal Medicine*, 169(7), 467-473. [cite: 32]",
    "15. **Korevaar DA, Cohen JF, et al.** (2019). PRISMA-DTA for Abstracts: a new addition to the toolbox for test accuracy research. *Diagnostic and Prognostic Research*, 3. [cite: 16]"
  ]
}