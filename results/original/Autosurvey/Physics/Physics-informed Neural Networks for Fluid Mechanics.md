# Physics-Informed Neural Networks for Fluid Mechanics: A Comprehensive Survey

## 1 Introduction

### 1.1 Background and Motivation

Fluid mechanics has been a cornerstone of scientific and engineering disciplines for centuries, with its roots tracing back to the works of ancient scholars such as Archimedes and Aristotle, who laid the foundational principles of buoyancy and fluid behavior. The field has since evolved dramatically, especially during the 17th and 18th centuries, with the development of the Navier-Stokes equations by Claude-Louis Navier and George Gabriel Stokes, which provide a mathematical description of the motion of fluid substances [1]. These equations have become the cornerstone of fluid dynamics, enabling the study of a wide range of phenomena, from the laminar flow of air over an aircraft wing to the turbulent mixing of gases in a combustion chamber.

Despite the theoretical advancements, solving the Navier-Stokes equations for practical applications remains a formidable challenge. Traditional numerical methods, such as the finite element method (FEM) and finite volume method (FVM), have been the go-to approaches for simulating fluid flows. These methods discretize the domain into a mesh and solve the equations at discrete points, providing accurate solutions for a variety of problems. However, they are not without limitations. One major drawback is the high computational cost, especially for complex geometries and high Reynolds number flows. The need for fine meshes to resolve small-scale features can lead to enormous computational requirements, making these methods infeasible for large-scale simulations [2].

Moreover, traditional numerical methods often struggle with problems involving high-frequency dynamics and chaotic behavior. For instance, in turbulent flows, the interactions between different scales of motion are highly complex and can lead to numerical instabilities and inaccuracies. The finite element and finite volume methods, while robust, require significant computational resources and often rely on empirical models to account for turbulence, which can introduce additional uncertainties [3]. These limitations have driven the search for alternative approaches that can handle the complexities of fluid mechanics more efficiently.

The emergence of physics-informed neural networks (PINNs) has provided a promising avenue to address these challenges. PINNs integrate physical laws into the training process of neural networks by incorporating the governing equations of the system into the loss function. This approach allows the neural network to learn the solution to the partial differential equations (PDEs) while respecting the underlying physics. By doing so, PINNs can provide accurate solutions with fewer data points, making them particularly suitable for problems where data is scarce or expensive to obtain [1].

The concept of PINNs was first introduced in the context of solving PDEs, where the neural network is trained to minimize the residual of the PDEs while satisfying the boundary and initial conditions. This method has shown great promise in a variety of applications, from fluid dynamics to solid mechanics [2]. However, the initial formulations of PINNs faced challenges in handling problems with high-frequency components and complex geometries, leading to the development of various enhancements and variants to improve their performance [4].

One of the key advantages of PINNs is their ability to handle problems with sparse data, which is a common issue in many scientific and engineering applications. Traditional numerical methods often require extensive data to ensure the accuracy and reliability of the solution, but PINNs can leverage the physical constraints imposed by the governing equations to achieve reasonable accuracy with limited data [2]. This makes PINNs particularly attractive for applications where data acquisition is challenging or costly.

In addition to handling sparse data, PINNs have also demonstrated their potential in addressing inverse problems, where the goal is to infer unknown parameters or initial conditions from measurements. This capability is crucial in many fluid mechanics applications, such as estimating the properties of a fluid from experimental data or identifying the source of a contaminant in a flow field [3]. The integration of physical laws into the training process ensures that the solutions are not only data-driven but also physically consistent, leading to more reliable and interpretable results.

The development of PINNs has also been influenced by advancements in deep learning and machine learning. Techniques such as transfer learning, multi-fidelity learning, and adaptive sampling have been integrated into PINN frameworks to improve their efficiency and accuracy. For example, transfer learning allows PINNs to leverage knowledge from previously trained models, reducing the need for extensive retraining when solving new problems [3]. Multi-fidelity learning, on the other hand, combines data from different sources with varying levels of accuracy to enhance the training process and improve the generalization capabilities of the model [3].

Despite the promising results, the application of PINNs to fluid mechanics is still an active area of research, with many challenges and opportunities for further development. Issues such as spectral bias, where the networks struggle to capture high-frequency components of the solution, and the sensitivity to noise and errors in the data remain significant hurdles [3]. Additionally, the scalability of PINNs to large-scale and high-dimensional problems is an ongoing concern, as the computational costs can quickly become prohibitive [3].

In summary, the historical context of fluid mechanics and the limitations of traditional numerical methods have set the stage for the emergence of physics-informed neural networks. These networks offer a powerful and flexible approach to solving complex fluid dynamics problems, integrating physical laws into the training process of neural networks. As research continues to advance, the potential of PINNs to revolutionize the field of fluid mechanics and other scientific domains is becoming increasingly evident.

### 1.2 Definition and Core Concept of PINNs

Physics-Informed Neural Networks (PINNs) represent a transformative class of deep learning models that integrate the principles of physics into the framework of neural networks. This innovative approach leverages the power of artificial neural networks to approximate solutions to partial differential equations (PDEs) by embedding physical laws directly into the training process through the use of loss functions. Unlike traditional data-driven models that rely solely on observed data, PINNs incorporate the underlying physical laws governing the system, ensuring that the learned solutions adhere to the fundamental principles of the physical world [5].

At the core of PINNs is the concept of using physical laws as constraints within the neural network's training process. This is achieved by defining a loss function that includes terms representing the residuals of the governing PDEs, as well as terms for boundary and initial conditions. The network is trained to minimize this composite loss function, which effectively enforces the physical laws during the learning process. This approach ensures that the solutions learned by the network are not only consistent with the data but also satisfy the underlying physical equations [5]. The use of automatic differentiation further enhances the effectiveness of PINNs, as it allows the network to compute the derivatives of the solution with respect to input variables, which are then used to enforce the PDE constraints during training. This technique is particularly advantageous in scenarios where the PDEs are complex and require precise derivative information [5].

The architecture of PINNs typically consists of a deep neural network that maps input coordinates to the solution of the PDE. The network is trained to approximate the solution by minimizing a loss function that includes the residuals of the PDE and the boundary/initial conditions. This process is similar to traditional numerical methods, but instead of using finite difference or finite element schemes, PINNs utilize the flexibility of neural networks to learn the solution directly. This makes PINNs particularly suitable for problems with complex geometries or high-dimensional domains, where traditional numerical methods may struggle [6].

One of the key advantages of PINNs is their ability to handle problems with limited data. By incorporating physical laws into the loss function, PINNs can learn the solution even when the available data is sparse or noisy. This is particularly useful in scenarios where obtaining large amounts of high-quality data is challenging or expensive. For example, in the field of fluid mechanics, where experimental data may be scarce, PINNs can provide accurate predictions by leveraging the governing equations of fluid flow. This capability has been demonstrated in various applications, including the simulation of incompressible laminar flows and the prediction of turbulent flows [1].

Another important aspect of PINNs is their flexibility in handling different types of PDEs. Whether the problem involves elliptic, parabolic, or hyperbolic equations, PINNs can be adapted to accommodate the specific characteristics of the PDE. This is achieved by modifying the loss function to include the appropriate terms for the governing equations. For instance, in the case of hyperbolic PDEs, which often involve shocks and discontinuities, the loss function can be designed to account for these features, ensuring that the network learns the correct solution [4].

The integration of physical laws into the neural network architecture also enables PINNs to handle problems with varying parameters. This is particularly useful in scenarios where the solution depends on multiple parameters, such as in parametric studies or optimization problems. By incorporating the parameters into the input of the neural network, PINNs can learn the solution as a function of these parameters, allowing for efficient exploration of the parameter space. This approach has been successfully applied in the study of parametric systems, where the goal is to understand how the solution changes with respect to different parameter values [7].

Furthermore, the use of PINNs in inverse problems is another significant application of this approach. Inverse problems involve determining unknown parameters or initial conditions based on observed data. By incorporating the physical laws into the loss function, PINNs can be trained to infer these unknowns by minimizing the difference between the predicted solution and the observed data. This has been demonstrated in various applications, including the estimation of boundary conditions and the identification of material properties in complex systems [1].

In addition to their ability to handle forward and inverse problems, PINNs also offer advantages in terms of computational efficiency. Traditional numerical methods often require significant computational resources to solve PDEs, especially for high-dimensional or complex problems. In contrast, PINNs can leverage the parallel processing capabilities of modern computing architectures to train the network more efficiently. This makes PINNs particularly suitable for real-time applications or large-scale simulations where computational speed is critical [8].

Overall, the definition and core concept of PINNs revolve around the integration of physical laws into the neural network's training process through the use of loss functions and automatic differentiation. This approach not only ensures that the learned solutions are physically consistent but also enables PINNs to handle a wide range of problems, from forward and inverse problems to parameter estimation and optimization. By combining the strengths of deep learning with the principles of physics, PINNs represent a powerful tool for solving complex scientific and engineering problems. The continued development and refinement of PINNs will likely lead to even more accurate and efficient solutions in the future, further expanding their applicability across various domains [5].

### 1.3 Advantages of PINNs in Fluid Mechanics

Physics-Informed Neural Networks (PINNs) have emerged as a transformative approach in the field of fluid mechanics, offering a unique combination of data-driven learning and physics-based modeling. One of the most significant advantages of PINNs is their ability to work effectively with sparse data, which is a common challenge in many real-world fluid dynamics problems. Traditional numerical methods often require large, high-quality datasets to achieve accurate results, which can be difficult and expensive to obtain. In contrast, PINNs leverage the governing equations of fluid dynamics, such as the Navier-Stokes equations, to constrain the learning process, allowing them to achieve high accuracy even when the available data is limited [9]. This makes PINNs particularly suitable for applications where data collection is challenging or where measurements are noisy, such as in experimental fluid mechanics [1].

Another key advantage of PINNs is their integration of physical constraints into the neural network training process. By embedding the partial differential equations (PDEs) that govern fluid dynamics into the loss function, PINNs ensure that the solutions they produce are consistent with the underlying physics. This integration of physical laws not only improves the accuracy of the predictions but also enhances the robustness of the models, especially in scenarios where the data is sparse or incomplete [10]. For instance, PINNs can enforce the conservation of mass and momentum, which are fundamental principles in fluid mechanics, leading to more physically meaningful solutions [3]. This physical consistency is particularly important in applications such as biomedical flow simulations, where the accuracy of the model can have significant implications for clinical outcomes [1].

In addition to handling sparse data and integrating physical constraints, PINNs also offer the potential for reduced computational costs compared to traditional numerical methods. Conventional approaches, such as finite element or finite volume methods, often require significant computational resources, especially for complex, high-dimensional problems. PINNs, on the other hand, can be trained efficiently using gradient-based optimization techniques, making them a promising alternative for large-scale simulations [3]. Moreover, the ability of PINNs to handle irregular geometries and complex boundary conditions without the need for mesh generation further contributes to their computational efficiency [1]. This makes PINNs particularly attractive for applications in aerospace engineering, where the geometry of the problem can be highly complex and subject to frequent changes [9].

The potential of PINNs to reduce computational costs is further enhanced by their flexibility in handling different types of problems, including both forward and inverse problems. In forward problems, PINNs can be used to predict the behavior of fluid flows given a set of initial and boundary conditions. In inverse problems, they can be employed to estimate unknown parameters, such as the boundary conditions or the properties of the fluid, based on observed data [10]. This versatility allows PINNs to be applied to a wide range of fluid dynamics problems, from the simulation of turbulent flows to the prediction of multiphase flows [2].

Another advantage of PINNs is their ability to adapt to new scenarios through transfer learning. By leveraging pre-trained models, PINNs can quickly adapt to new problems with minimal additional training, significantly reducing the time and computational resources required for model development [9]. This is particularly useful in applications where the fluid dynamics problem is subject to changes over time, such as in the simulation of unsteady flows or in the prediction of fluid behavior in dynamic environments [3]. Transfer learning also enables PINNs to generalize well to unseen scenarios, making them a powerful tool for real-world applications where the data may be limited or noisy.

In addition to their computational efficiency and adaptability, PINNs also offer improved accuracy in capturing high-frequency and complex dynamics in fluid flows. Traditional numerical methods often struggle to resolve small-scale features and high-frequency components of the solution, leading to inaccuracies and instability in the results. PINNs, however, can be designed to handle these challenges by incorporating appropriate activation functions and loss function formulations [1]. For example, the use of physics-informed activation functions (PAFs) has been shown to improve the accuracy and stability of PINNs, particularly in problems involving complex fluid dynamics [11]. This ability to capture high-frequency dynamics is crucial for applications such as the simulation of turbulent flows and the prediction of chaotic fluid behavior [2].

Furthermore, PINNs are well-suited for real-time applications, where rapid and accurate predictions are essential. By reducing the computational burden associated with traditional numerical methods, PINNs can be used to provide real-time feedback and control in fluid dynamics simulations. This is particularly valuable in applications such as the design of fluid-structure interaction systems, where the ability to quickly adapt to changing conditions is critical [3]. Real-time capabilities also make PINNs a promising tool for the development of digital twins, where accurate and timely predictions are required to monitor and optimize complex systems [12].

In summary, the advantages of PINNs in fluid mechanics are manifold. Their ability to work with sparse data, integrate physical constraints, reduce computational costs, adapt to new scenarios, and capture high-frequency dynamics makes them a powerful and versatile tool for a wide range of fluid dynamics problems. These advantages are supported by a growing body of research and empirical validation, demonstrating the potential of PINNs to revolutionize the field of fluid mechanics and beyond [1]. As the field continues to evolve, the development of more advanced PINN architectures and training strategies will further enhance their capabilities and expand their applicability to even more complex and challenging fluid dynamics problems.

### 1.4 Challenges and Limitations of Traditional Methods

Traditional numerical methods have long been the cornerstone of fluid dynamics simulations, providing reliable solutions for a wide range of applications. These methods, which include finite difference, finite volume, and finite element techniques, have been extensively used in both academic and industrial settings. However, despite their widespread use and success, they face significant challenges and limitations, particularly in the context of complex fluid flow problems. The limitations of traditional numerical methods in fluid mechanics include computational expense, difficulties in handling complex geometries, and challenges in capturing high-frequency dynamics. These limitations have motivated the development of more advanced techniques, such as Physics-Informed Neural Networks (PINNs), which aim to overcome these challenges by integrating physical laws with data-driven learning [13].

One of the primary limitations of traditional numerical methods is their computational expense. Simulating fluid flow accurately requires resolving a large number of spatial and temporal scales, which can be computationally intensive. For example, high-resolution simulations of turbulent flows or complex multiphase systems often require fine grids and small time steps, leading to prohibitively long computation times. This computational burden is exacerbated in three-dimensional (3D) simulations, where the number of grid points and the associated memory requirements grow rapidly. Furthermore, traditional methods often require extensive computational resources, making them impractical for real-time applications or large-scale simulations. For instance, in the case of large eddy simulations (LES), the need to resolve the largest eddies while modeling the smaller ones leads to high computational costs, which can be a significant barrier to their widespread use [14].

Another major challenge in traditional numerical methods is their difficulty in handling complex geometries. Many fluid mechanics problems involve irregular or moving boundaries, such as those found in biomedical applications, aerospace engineering, or environmental fluid dynamics. Traditional methods often rely on body-fitted meshes, which can be time-consuming to generate and may not easily accommodate complex or dynamic geometries. This limitation is particularly problematic in simulations involving moving boundaries or topological changes, where the mesh must be dynamically adjusted to maintain accuracy. For example, in simulations of blood flow in the cardiovascular system, the geometry of the vessels can change over time due to pulsatile flow or disease progression, making it challenging to maintain a high-quality mesh [15]. Additionally, the process of mesh generation can be error-prone and requires significant user expertise, which further limits the accessibility of traditional methods.

In addition to computational expense and geometric challenges, traditional numerical methods struggle to capture high-frequency dynamics. Many fluid flow phenomena, such as shock waves, turbulence, and acoustic waves, involve high-frequency components that are difficult to resolve accurately. High-frequency dynamics often require very fine spatial and temporal discretizations, which can significantly increase the computational cost. Moreover, traditional methods may introduce numerical dissipation or dispersion errors, which can smear out or distort high-frequency features of the solution. For example, in the simulation of compressible flows, the accurate resolution of shock waves and other discontinuities requires specialized numerical schemes, such as high-resolution shock-capturing methods, which can be complex to implement and computationally expensive [16].

Another limitation of traditional numerical methods is their inability to efficiently handle problems with large parameter variations. In many engineering applications, the solution of fluid dynamics problems depends on a wide range of parameters, such as Reynolds number, Mach number, or boundary conditions. Traditional methods often require a separate simulation for each set of parameters, which can be time-consuming and resource-intensive. This limitation is particularly significant in optimization and design problems, where the goal is to explore a large parameter space to find the best solution. For instance, in the design of aerodynamic shapes, the need to simulate multiple configurations with different geometries and flow conditions makes traditional numerical methods less efficient [17].

Furthermore, traditional numerical methods often struggle with the integration of physical constraints and the enforcement of conservation laws. While these methods are designed to satisfy the governing equations of fluid dynamics, such as the Navier-Stokes equations, they may not always accurately enforce conservation laws, especially in the presence of complex boundary conditions or nonlinear interactions. This can lead to inaccuracies in the solution, particularly in long-time simulations or when dealing with highly turbulent flows. For example, in the simulation of incompressible flows, maintaining the divergence-free condition of the velocity field is crucial for the accuracy of the solution, but traditional methods may introduce numerical errors that violate this constraint [18].

In summary, traditional numerical methods for fluid mechanics face several significant challenges and limitations, including computational expense, difficulties in handling complex geometries, and challenges in capturing high-frequency dynamics. These limitations have motivated the development of more advanced techniques, such as Physics-Informed Neural Networks (PINNs), which offer promising alternatives by integrating physical laws with data-driven learning. As the field of fluid dynamics continues to evolve, addressing these challenges will be crucial for advancing the accuracy, efficiency, and applicability of numerical simulations. The emergence of PINNs and other data-driven approaches represents a significant step forward in overcoming these limitations and pushing the boundaries of what is possible in fluid mechanics [19].

### 1.5 Current Research and Applications

Recent years have witnessed significant progress in the development and application of Physics-Informed Neural Networks (PINNs) in the field of fluid mechanics. PINNs have emerged as a powerful tool for solving complex fluid dynamics problems, offering a data-driven approach that seamlessly integrates physical laws into neural network training. This integration not only enhances the accuracy of predictions but also ensures that the solutions adhere to the fundamental principles of fluid mechanics. The recent research and applications of PINNs in fluid mechanics have focused on solving the Navier-Stokes equations, modeling turbulent flows, and addressing other fluid dynamics problems. These advancements highlight the potential of PINNs to revolutionize traditional computational methods in fluid mechanics.

One of the most prominent applications of PINNs in fluid mechanics is their use in solving the Navier-Stokes equations (NSE), which govern the motion of fluids. Researchers have developed various PINN variants to address different aspects of fluid flow, such as laminar and turbulent flows. For instance, the Enhanced Physics-Informed Neural Networks (EPINN-NSE) have been designed to solve the NSE by approximating velocity components using a stream function or directly [20]. This approach simplifies the system and ensures that the velocity field adheres to the divergence-free condition, making it particularly effective for two- and three-dimensional flows. Additionally, the work on mixed-variable PINN methods has demonstrated the ability to solve fluid dynamics problems without any labeled data, leveraging the governing equations and boundary conditions directly within the loss function [21]. These methods not only improve predictive performance but also reduce computational costs, making them suitable for a wide range of applications.

In the realm of turbulent flow modeling, PINNs have shown great promise in capturing complex flow behaviors that are challenging for traditional numerical methods. The RANS-PINN framework, for example, has been proposed to predict flow fields in high Reynolds number turbulent flow regimes by incorporating a 2-equation eddy viscosity model based on the Reynolds-averaged Navier-Stokes (RANS) formulation [22]. This approach has demonstrated excellent performance in simulating turbulent flows, particularly in scenarios where accurate predictions of Reynolds-stress components are required. Furthermore, the application of PINNs in modeling turbulent natural convection flows has been explored, with studies showing that PINNs can effectively capture the intricate dynamics of such flows even with sparse data [23]. These findings underscore the versatility of PINNs in handling different types of fluid flow problems, including those with complex geometries and high Reynolds numbers.

Beyond solving the Navier-Stokes equations and modeling turbulent flows, PINNs have been applied to a variety of other fluid dynamics problems. For instance, their use in inverse problems has been particularly noteworthy, where the goal is to estimate unknown parameters or boundary conditions from limited measurements. The work on blood flow inverse problems has demonstrated the ability of PINNs to estimate reduced-order model parameters and full velocity fields from noisy measurements in the ascending aorta [24]. Similarly, the application of PINNs in solving obstacle-related PDEs has been explored, with results showing that PINNs can effectively approximate solutions above a given obstacle, even in the presence of irregular geometries [25]. These applications highlight the adaptability of PINNs in tackling a wide range of fluid mechanics problems, from forward simulations to inverse problems.

The integration of PINNs with advanced techniques such as transfer learning and multi-fidelity learning has further expanded their applicability in fluid mechanics. Transfer learning allows PINNs to leverage knowledge from previously trained models, enabling faster adaptation to new problems and reducing the need for extensive retraining. For example, the use of multi-case PINNs for biomedical tube flows has demonstrated the effectiveness of transfer learning in simulating complex flow scenarios with different geometries [26]. Additionally, the application of multi-fidelity learning has enabled PINNs to combine data of varying fidelity levels, improving the accuracy and efficiency of predictions in complex fluid dynamics problems [27]. These enhancements make PINNs a compelling choice for real-world applications where data scarcity and computational efficiency are critical concerns.

The potential of PINNs in fluid mechanics is further illustrated by their application in real-time simulations and data-driven surrogate modeling. For instance, the development of SeqPINN and SP-PINN has enabled real-time training of PINNs for ultrafast ultrasound blood flow imaging, achieving high accuracy with reduced computational costs [28]. Similarly, the use of PINNs in solving 3D flow-thermal problems with sparse domain data has demonstrated their effectiveness in surrogate modeling for complex engineering designs [29]. These advancements underscore the ability of PINNs to provide accurate and efficient solutions in scenarios where traditional methods may struggle due to computational limitations or data scarcity.

In summary, the current research and applications of PINNs in fluid mechanics have demonstrated their potential to revolutionize the field. From solving the Navier-Stokes equations and modeling turbulent flows to addressing inverse problems and enabling real-time simulations, PINNs have shown remarkable versatility and effectiveness. The integration of advanced techniques such as transfer learning and multi-fidelity learning has further enhanced their applicability, making them a promising tool for a wide range of fluid dynamics problems. As research continues to advance, the future of PINNs in fluid mechanics looks increasingly promising, with the potential to transform traditional computational methods and open up new avenues for innovation.

### 1.6 Importance of Integrating Physics and Data

The integration of physical laws with data-driven learning in Physics-Informed Neural Networks (PINNs) represents a transformative approach to solving complex fluid dynamics problems. This combination of physics-based constraints with machine learning techniques offers a powerful framework that enhances the accuracy, robustness, and generalizability of solutions. By embedding fundamental physical principles—such as conservation laws, boundary conditions, and governing equations—into the training process of neural networks, PINNs ensure that the learned models remain consistent with the underlying physics of the system. This synergy between data and physics not only improves the reliability of predictions but also addresses many of the limitations inherent in purely data-driven approaches. The significance of this integration is particularly evident in fluid mechanics, where the behavior of complex systems is often governed by nonlinear partial differential equations (PDEs) that are difficult to solve using traditional numerical methods.

One of the key advantages of integrating physical laws with data-driven learning is the ability to incorporate domain-specific knowledge directly into the neural network architecture. This is achieved by designing the loss function of the neural network to include residuals of the governing equations, such as the Navier-Stokes equations, and by enforcing boundary and initial conditions through penalty terms. This approach ensures that the neural network not only fits the available data but also adheres to the physical constraints of the problem. For instance, studies have shown that PINNs can achieve highly accurate predictions for fluid flow problems by embedding the governing equations into the loss function, even when the available data is sparse or noisy [30]. The inclusion of physics-based constraints allows PINNs to generalize better to unseen scenarios, which is crucial in fluid mechanics where data acquisition can be expensive or impractical.

Another critical benefit of integrating physical laws with data-driven learning is the improved robustness of the models. Traditional data-driven models, such as conventional neural networks, often struggle with out-of-distribution generalization and can produce non-physical solutions when trained on limited or noisy data. By contrast, PINNs leverage the structure of the physical equations to guide the learning process, reducing the risk of overfitting and ensuring that the predictions remain physically meaningful. This is particularly important in fluid dynamics, where small errors in the model can lead to significant deviations in the solution. Studies have demonstrated that the integration of physics-based regularization terms in the loss function significantly enhances the stability and accuracy of PINNs, especially in scenarios where the data is scarce or the problem is highly nonlinear [9].

Moreover, the integration of physics and data in PINNs enables the solution of inverse problems that are challenging for traditional numerical methods. Inverse problems in fluid mechanics often involve estimating unknown parameters or boundary conditions from limited observations. By leveraging the physics-informed loss function, PINNs can efficiently learn the relationship between the inputs and the outputs, even in the presence of noise or uncertainty. For example, recent research has shown that PINNs can be used to infer the parameters of turbulent flow models or to reconstruct the velocity field from sparse measurements, which is critical in applications such as biomedical fluid dynamics [10]. The ability to handle inverse problems effectively makes PINNs a valuable tool for real-world applications where data is often incomplete or uncertain.

Furthermore, the integration of physics and data in PINNs allows for the efficient solution of high-dimensional and complex fluid dynamics problems. Traditional numerical methods, such as finite element or finite volume methods, can be computationally expensive and may struggle with problems that involve large domains, complex geometries, or high Reynolds numbers. PINNs, on the other hand, can handle these challenges by leveraging the flexibility of neural networks and the efficiency of automatic differentiation. By incorporating physical laws into the training process, PINNs can achieve accurate solutions with fewer computational resources. For instance, studies have demonstrated that PINNs can solve 3D incompressible Navier-Stokes equations with sparse domain data, making them suitable for applications such as flow-thermal problems in electronics design [31]. This capability not only reduces the computational burden but also enables real-time or near-real-time simulations, which is essential for engineering and scientific applications.

In addition to improving accuracy and robustness, the integration of physics and data in PINNs also enhances the interpretability of the models. While traditional neural networks are often considered "black boxes," the inclusion of physical constraints allows researchers to gain insights into the underlying mechanisms of the system. By analyzing the loss function and the gradients of the neural network, it is possible to understand how the model is learning the physical laws and how it is making predictions. This interpretability is crucial for validating the models and ensuring that they are consistent with the known physics of the problem. Studies have shown that the use of physics-informed loss functions can help identify regions where the model is uncertain or where the predictions may be inaccurate, enabling targeted improvements in the training process [32].

In summary, the integration of physical laws with data-driven learning in PINNs offers a powerful approach to solving fluid dynamics problems. By combining the strengths of physics-based models with the flexibility and adaptability of machine learning, PINNs provide a robust and accurate framework for simulating complex fluid flows. This integration not only improves the reliability and generalizability of the models but also enables the efficient solution of inverse problems, high-dimensional problems, and real-world applications. As research in this area continues to advance, the importance of integrating physics and data in PINNs will only become more evident, paving the way for new innovations in fluid mechanics and beyond.

### 1.7 Potential for Future Innovations

The transformative potential of Physics-Informed Neural Networks (PINNs) in fluid mechanics is immense, offering a paradigm shift in how we approach the simulation and modeling of complex fluid dynamics problems. Unlike traditional numerical methods that rely heavily on discretization and mesh generation, PINNs integrate the governing physical laws directly into the neural network architecture, allowing for a more flexible and data-efficient approach to solving partial differential equations (PDEs). This integration of physics with data-driven learning opens up new possibilities for advancing computational capabilities in fluid mechanics, particularly in scenarios where traditional methods face significant limitations.

One of the most promising areas of future innovation lies in the development of more sophisticated PINN architectures that can better capture the intricate dynamics of fluid flows. Current PINN designs often struggle with high-frequency components and complex geometries, but ongoing research is addressing these challenges through the exploration of novel network structures and training strategies. For instance, the introduction of physics-informed activation functions [4] and the use of attention mechanisms [4] have shown promise in improving the accuracy and efficiency of PINNs. These advancements suggest that future PINN models could be more robust and capable of handling a wider range of fluid dynamics problems, including those with chaotic or turbulent behavior.

Another exciting direction for future research is the integration of PINNs with quantum computing. While still in its infancy, this intersection holds the potential to revolutionize the way we solve complex fluid mechanics problems. Quantum computing could provide the computational power needed to tackle large-scale, high-dimensional PDEs that are currently infeasible for classical computers. By leveraging quantum circuits and hybrid quantum-classical architectures [33], researchers could develop more efficient and accurate solvers for fluid dynamics simulations. This synergy between quantum computing and PINNs could lead to breakthroughs in areas such as real-time flow prediction and optimization of complex fluid systems.

In addition to architectural innovations, the development of more efficient training algorithms is a critical area for future research. The training of PINNs can be computationally intensive, especially for problems involving high-dimensional PDEs. Techniques such as adaptive sampling [34], multi-fidelity learning [27], and transfer learning [35] have shown promise in improving the training efficiency of PINNs. These methods not only reduce the computational cost but also enhance the generalization capabilities of PINNs, making them more applicable to a wide range of fluid mechanics problems.

Furthermore, the application of PINNs to multi-physics problems is a burgeoning area of research. Fluid mechanics often involves coupled systems of equations, such as thermo-mechanical interactions and multi-scale phenomena. The development of hybrid models that combine PINNs with traditional numerical methods [36] could lead to more accurate and efficient solutions for these complex problems. For example, the mixed formulation of PINNs [6] has shown potential in improving the accuracy of solutions for coupled systems. Future research could focus on expanding these hybrid approaches to address a broader range of multi-physics problems in fluid mechanics.

The potential for PINNs to enhance the accuracy and efficiency of inverse problems in fluid mechanics is another area of significant interest. Inverse problems, such as parameter estimation and data assimilation, are crucial in many engineering applications. The ability of PINNs to incorporate physical constraints into the learning process makes them particularly well-suited for these tasks. For instance, the use of PINNs in solving inverse problems in hemodynamics [37] has demonstrated their effectiveness in estimating parameters from noisy measurements. Future research could explore the development of more sophisticated PINN-based methods for solving inverse problems, particularly in scenarios where data is scarce or noisy.

Moreover, the integration of PINNs with advanced optimization techniques could further enhance their performance. Techniques such as Bayesian optimization and gradient boosting have shown promise in improving the convergence and stability of PINN training. The use of these optimization strategies could lead to more reliable and efficient PINN models, particularly for problems with complex loss landscapes. Additionally, the incorporation of uncertainty quantification [38] could provide valuable insights into the reliability of PINN predictions, making them more trustworthy for practical applications.

The potential for PINNs to contribute to the development of digital twins and real-time monitoring systems is another exciting avenue for future research. Digital twins, which are virtual replicas of physical systems, have the potential to revolutionize the way we monitor and optimize fluid systems. The ability of PINNs to integrate physical laws with data-driven learning makes them an ideal candidate for building accurate and efficient digital twins. For example, the application of PINNs in ultrafast ultrasound blood flow imaging [39] has demonstrated their potential in real-time applications. Future research could focus on expanding the use of PINNs in digital twin technologies, particularly for complex fluid systems that require real-time monitoring and optimization.

In conclusion, the future of PINNs in fluid mechanics is bright, with numerous opportunities for innovation and advancement. The integration of physics with data-driven learning, the development of more sophisticated architectures, the exploration of quantum computing, and the application to multi-physics and inverse problems all point to a promising future for PINNs. As research continues to advance, PINNs are poised to push the boundaries of current computational capabilities, offering new solutions to some of the most challenging problems in fluid mechanics. By addressing the limitations of traditional numerical methods and leveraging the strengths of machine learning, PINNs have the potential to revolutionize the field of fluid dynamics and open up new possibilities for scientific and engineering applications.

## 2 Fundamentals of Physics-Informed Neural Networks

### 2.1 Integration of Partial Differential Equations into Neural Network Training

The integration of partial differential equations (PDEs) into the training process of neural networks is a cornerstone of Physics-Informed Neural Networks (PINNs). This integration is primarily achieved through the use of loss functions, where the residuals of the PDEs are incorporated as additional terms to guide the neural network's learning process. The core idea is to train the neural network to approximate the solution of the PDE while simultaneously satisfying the governing equations. This approach not only ensures that the network adheres to the physical laws but also enables it to generalize better across different scenarios, even with limited data [20].

The fundamental mechanism involves embedding the PDEs into the loss function, where the neural network's predictions are compared against the PDE's residual. The residual term is a measure of how well the neural network's output satisfies the PDE. By minimizing this residual, the network is effectively trained to approximate the solution of the PDE. This is done by defining a loss function that includes both the data fitting term and the PDE residual term. The data fitting term ensures that the network's predictions align with the available data, while the PDE residual term enforces the physical constraints of the problem. This dual objective ensures that the network not only fits the data but also adheres to the underlying physics [1].

The role of residual terms in this context is critical. They serve as a penalty for deviations from the PDE, thereby guiding the network to learn solutions that are consistent with the governing equations. This penalty is typically calculated as the difference between the left-hand side and the right-hand side of the PDE. For instance, in the case of the Navier-Stokes equations, the residual term would involve the divergence of the velocity field and the pressure gradient, ensuring that the solution satisfies the continuity and momentum equations [20].

One of the key advantages of this approach is its flexibility. By adjusting the weights of the residual terms in the loss function, researchers can prioritize different aspects of the solution. For example, in cases where the PDE is highly nonlinear or has complex boundary conditions, the residual term can be weighted more heavily to ensure that the network captures these nuances. This adaptability is particularly useful in applications such as turbulent flow modeling, where the PDEs can exhibit highly complex behavior [40].

Moreover, the integration of PDEs into the training process allows for the handling of problems with sparse or noisy data. In traditional numerical methods, the accuracy of the solution often depends on the quality and quantity of the data. However, PINNs can leverage the physical constraints encoded in the PDEs to provide accurate solutions even when the data is limited. This is particularly beneficial in scenarios where obtaining high-quality data is challenging or expensive, such as in biomedical applications or environmental monitoring [1].

The effectiveness of this integration is further enhanced by the use of automatic differentiation, which allows the neural network to compute the necessary derivatives of the solution with respect to the input variables. This capability is essential for enforcing the PDE constraints during training, as the derivatives are required to evaluate the residual terms. By automatically computing these derivatives, PINNs can efficiently incorporate the PDEs into the training process without the need for manual calculations, making the approach more accessible and scalable [41].

In addition to the residual terms, the loss function can also include boundary and initial condition terms. These terms ensure that the network's predictions satisfy the specified conditions, which are often critical for the accuracy of the solution. For example, in problems involving fluid flow, the boundary conditions can include the velocity and pressure at the boundaries of the domain, while the initial conditions can specify the state of the system at the start of the simulation. By incorporating these terms into the loss function, PINNs can accurately model the behavior of the system over time [1].

The integration of PDEs into the training process also allows for the handling of inverse problems, where the goal is to estimate unknown parameters or functions from the observed data. In such cases, the PDEs are used to constrain the solution, ensuring that the estimated parameters are consistent with the physical laws. This approach has been successfully applied to various problems, including the estimation of turbulence models and the identification of material properties in composite materials [10].

Despite the advantages of this approach, there are challenges associated with the integration of PDEs into the training process. One of the main challenges is the balance between the data fitting term and the PDE residual term in the loss function. If the data fitting term is too dominant, the network may overfit to the data and fail to capture the underlying physical behavior. Conversely, if the PDE residual term is too dominant, the network may struggle to fit the data, leading to poor predictions. This trade-off requires careful tuning of the weights in the loss function, which can be a time-consuming and complex process [32].

Another challenge is the computational cost associated with evaluating the PDE residuals. In many cases, the PDEs involve complex operators that can be computationally expensive to evaluate. This can significantly increase the training time, especially for high-dimensional problems. To address this challenge, researchers have explored various techniques, such as adaptive sampling and the use of efficient neural network architectures, to reduce the computational burden while maintaining the accuracy of the solution [42].

In summary, the integration of PDEs into the training process of neural networks through the use of loss functions is a powerful approach that enables the creation of models that are both data-driven and physics-informed. By incorporating the residuals of the PDEs into the loss function, PINNs can learn solutions that are consistent with the governing equations, even with limited data. This approach has been successfully applied to a wide range of problems, from fluid dynamics to materials science, and continues to be an active area of research and development. The continued refinement of this integration will further enhance the capabilities of PINNs and expand their applicability to a broader range of scientific and engineering problems.

### 2.2 Role of Automatic Differentiation in PINNs

The role of automatic differentiation in Physics-Informed Neural Networks (PINNs) is central to their ability to solve partial differential equations (PDEs) by embedding physical laws directly into the training process. Traditional numerical methods rely on discretization techniques such as finite difference or finite element methods, which approximate derivatives using grid-based schemes. In contrast, PINNs leverage the power of automatic differentiation (AD) to compute derivatives of the neural network's output with respect to input variables, enabling the network to enforce the PDE constraints during training without the need for explicit grid-based discretization [5]. This feature is fundamental to the success of PINNs, as it allows the model to learn the underlying physics of the problem in a seamless and efficient manner.

Automatic differentiation provides a systematic and efficient way to compute gradients of complex functions, which is essential for training neural networks. In the context of PINNs, the network is designed to approximate the solution of a PDE, and the PDE itself is encoded as a constraint in the loss function. The loss function typically includes terms that penalize the violation of the PDE and its boundary conditions. To compute these terms, the network's output is differentiated with respect to its inputs using AD, which ensures that the derivatives are calculated accurately and efficiently. This process is critical because the PDE is a constraint that must be satisfied at every point in the domain, and AD enables the network to learn the solution while respecting these physical laws.

The importance of AD in PINNs is further highlighted by its ability to handle the complexities of PDEs that involve higher-order derivatives. For instance, in problems governed by the Navier-Stokes equations, the solution requires computing spatial and temporal derivatives of velocity and pressure fields. AD allows PINNs to compute these derivatives automatically, ensuring that the network's output satisfies the PDE at every point in the domain. This capability is particularly valuable in cases where the PDE has a high degree of nonlinearity or involves complex interactions between different physical quantities [29].

Moreover, AD plays a crucial role in ensuring the consistency of the solution with the PDE. In traditional numerical methods, the accuracy of the solution depends on the quality of the discretization and the choice of numerical schemes. In PINNs, however, the solution is derived by minimizing the loss function, which is constructed using the PDE and its boundary conditions. AD enables the network to compute the necessary derivatives of the PDE residual, which are then used to update the network's parameters during training. This process ensures that the solution converges to a value that satisfies the PDE and its boundary conditions, thereby preserving the physical consistency of the solution [43].

The integration of AD in PINNs also facilitates the handling of complex geometries and irregular domains. Traditional numerical methods often struggle with such cases due to the challenges associated with mesh generation and the need for careful discretization. In PINNs, the use of AD allows the network to compute derivatives at any point in the domain, regardless of the geometry. This flexibility is particularly advantageous in problems involving complex or time-varying domains, where the boundary conditions may change dynamically. By leveraging AD, PINNs can adapt to such changes without requiring a complete re-discretization of the domain [44].

Another key advantage of AD in PINNs is its compatibility with various optimization algorithms. Since AD provides accurate and efficient gradient computations, it enables the use of advanced optimization techniques such as stochastic gradient descent (SGD) and adaptive methods like Adam. These optimization algorithms are essential for training PINNs, as they help the network converge to a solution that satisfies the PDE and its boundary conditions. The efficiency of AD is particularly important in large-scale problems, where the computational cost of gradient computation can significantly impact the overall performance of the model [43].

Furthermore, AD enhances the robustness of PINNs by allowing the network to learn the solution in a way that is consistent with the underlying physics. This is particularly important in problems where the data is sparse or noisy, as the PDE constraints provide additional information that helps the network generalize better. AD ensures that the derivatives of the solution are computed correctly, even in the presence of noise, which is critical for maintaining the physical validity of the solution. This property is especially useful in applications such as fluid dynamics, where small errors in the solution can lead to significant deviations from the true behavior of the system [45].

In addition to its computational benefits, AD also plays a role in the theoretical analysis of PINNs. The ability to compute derivatives accurately allows researchers to study the convergence properties of PINNs and understand the conditions under which the solution converges to the true solution of the PDE. This theoretical foundation is essential for developing more efficient and reliable PINN models, as it provides insights into the behavior of the network during training and helps identify potential issues such as convergence instability or overfitting [46].

The importance of AD in PINNs is further reinforced by the development of various techniques that leverage AD to improve the performance of the network. For example, some studies have explored the use of higher-order derivatives in the loss function to capture more accurate representations of the PDE. Others have investigated the integration of AD with different neural network architectures, such as convolutional and recurrent networks, to enhance the model's ability to handle complex PDEs. These advancements highlight the versatility of AD in PINNs and its potential to drive the development of more sophisticated and efficient models [47].

In summary, the role of automatic differentiation in PINNs is critical to their ability to solve PDEs by embedding physical laws into the training process. AD enables the network to compute derivatives of the solution with respect to input variables, which are then used to enforce the PDE constraints during training. This capability ensures that the solution is consistent with the underlying physics, even in complex and irregular domains. The efficiency and accuracy of AD make it an indispensable tool in the development and application of PINNs, and its integration with various optimization algorithms and network architectures continues to drive advancements in the field.

### 2.3 Architecture of Physics-Informed Neural Networks

The architecture of Physics-Informed Neural Networks (PINNs) is a crucial component that determines their ability to approximate the solution of partial differential equations (PDEs). Typically, PINNs are constructed using fully connected neural networks (FCNNs), which are designed to map input variables (such as spatial and temporal coordinates) to the output variables (such as velocity, pressure, or temperature). The structure of these networks is designed to ensure that the neural network can learn the underlying physics of the problem while maintaining the flexibility to adapt to different PDEs and boundary conditions.

In most PINN implementations, the network architecture consists of an input layer, multiple hidden layers, and an output layer. The input layer receives the spatial and temporal coordinates, while the output layer produces the predicted solution of the PDE. The hidden layers are composed of neurons that apply nonlinear activation functions to the weighted sum of inputs, allowing the network to capture complex relationships between the input and output variables. The number of hidden layers and the number of neurons in each layer can be adjusted to balance the trade-off between model complexity and training efficiency [48].

The structure of the hidden layers is also critical in determining the performance of PINNs. While simple architectures with a few hidden layers may suffice for certain PDEs, more complex problems often require deeper networks with more sophisticated architectures. For example, some studies have proposed using convolutional neural networks (CNNs) to improve the efficiency of PINNs, particularly for problems involving spatially varying data [48]. However, the use of CNNs in PINNs is still a relatively new area, and the effectiveness of such architectures depends on the specific problem at hand.

Input features are processed in PINNs through a series of transformations that enable the network to approximate the solution of the PDE. The input to the network typically includes the spatial coordinates (x, y, z) and the temporal coordinate (t), which are concatenated and fed into the neural network. The network then applies a series of nonlinear transformations to these inputs, using weights and biases that are learned during the training process. The output of the network is then compared to the known solution of the PDE, and the difference is used to update the weights and biases through backpropagation.

One of the key advantages of PINNs is their ability to handle high-dimensional PDEs, which are often challenging for traditional numerical methods. The architecture of PINNs allows them to learn the solution of the PDE without requiring a predefined mesh or grid, making them particularly suitable for problems with complex geometries. For instance, some studies have shown that PINNs can be used to solve PDEs in irregular domains by using adaptive sampling strategies to select the most informative points for training [49].

In addition to the basic FCNN architecture, several variants of PINNs have been proposed to improve their performance and efficiency. For example, some studies have introduced the use of physics-informed activation functions, which incorporate physical knowledge into the neural network's activation functions to improve the accuracy and stability of the solution [1]. Others have explored the use of hybrid models that combine PINNs with traditional numerical methods, such as finite element or finite volume methods, to leverage the strengths of both approaches [36].

The architecture of PINNs can also be adapted to handle different types of PDEs, such as elliptic, parabolic, and hyperbolic equations. For example, some studies have used PINNs to solve the Navier-Stokes equations by incorporating the continuity and momentum equations into the loss function [22]. The effectiveness of these architectures depends on the choice of activation functions, the number of hidden layers, and the training strategy used to optimize the network parameters.

Another important consideration in the architecture of PINNs is the choice of activation functions. While traditional activation functions such as ReLU or tanh are commonly used, some studies have proposed the use of specialized activation functions that are tailored to specific PDEs [4]. These activation functions can help the network to better capture the physical behavior of the solution, leading to improved accuracy and stability.

The training of PINNs is typically performed using optimization algorithms such as stochastic gradient descent (SGD) or Adam. The choice of optimizer and the learning rate can significantly affect the convergence and performance of the network. In some cases, adaptive learning rate methods, such as learning rate scheduling or adaptive gradient methods, are used to improve the training efficiency of PINNs [1]. Additionally, techniques such as early stopping and regularization are often employed to prevent overfitting and improve the generalization performance of the network.

In summary, the architecture of PINNs is a critical factor in their ability to approximate the solution of PDEs. The use of fully connected neural networks, the structure of hidden layers, and the processing of input features are all essential components that contribute to the performance of PINNs. By carefully designing the architecture and training strategy, PINNs can effectively solve a wide range of PDEs, including those with complex geometries and high-dimensional domains. The continued development of novel architectures and training techniques will further enhance the capabilities of PINNs, making them an even more powerful tool for solving PDEs in fluid mechanics and other scientific domains [48].

### 2.4 Loss Function Formulation in PINNs

The loss function in Physics-Informed Neural Networks (PINNs) is a central component that guides the training process by combining the physical laws described by partial differential equations (PDEs) with the data-driven learning from boundary and initial conditions. The formulation of this loss function is a critical step in the PINN framework, as it determines how the network learns to approximate the solution of the PDEs while respecting the physical constraints of the problem. The loss function in PINNs typically consists of two main components: the domain loss, which enforces the PDEs, and the boundary loss, which enforces the boundary conditions. These components are balanced during training to ensure the neural network learns a solution that satisfies both the PDEs and the boundary conditions. 

The domain loss is derived from the residual of the PDEs evaluated at collocation points within the computational domain. These collocation points are sampled throughout the domain and are used to compute the residual of the PDEs. For instance, in the case of the Navier-Stokes equations, the domain loss would include terms such as the momentum and continuity equations. The residual is calculated by substituting the neural network's predictions into the PDE, and the loss is minimized to ensure that the network's predictions satisfy the PDEs as closely as possible. This approach is similar to the residual-based methods used in traditional numerical solvers, but instead of solving the PDEs directly, PINNs use a neural network to approximate the solution.

The boundary loss, on the other hand, enforces the boundary conditions of the problem. These conditions specify the values of the solution or its derivatives at the boundaries of the domain. The boundary loss is typically computed by evaluating the neural network's predictions at the boundary points and comparing them to the prescribed boundary conditions. For example, in a fluid flow simulation, the boundary loss might include the no-slip condition at solid walls or the prescribed velocity at inflow and outflow boundaries. By minimizing the boundary loss, the neural network is trained to produce solutions that conform to the specified boundary conditions.

The combination of the domain loss and the boundary loss forms the total loss function in PINNs. This total loss function is then minimized using optimization algorithms such as stochastic gradient descent (SGD) or its variants. The optimization process adjusts the parameters of the neural network to minimize the total loss, thereby improving the accuracy of the solution. However, the domain loss and the boundary loss are often of different magnitudes, and their relative importance must be carefully balanced to ensure that the network learns a solution that satisfies both the PDEs and the boundary conditions. This balancing is typically achieved by introducing weight coefficients that scale the domain and boundary loss terms. For example, in the paper "Stacked Generative Machine Learning Models for Fast Approximations of Steady-State Navier-Stokes Equations [50]", the authors mention the use of a custom data-driven and physics-informed loss function that effectively balances the domain and boundary loss terms to achieve state-of-the-art results without labeled simulation data.

The training process of PINNs also involves the careful selection of collocation points for the domain loss and boundary points for the boundary loss. The distribution of these points can significantly affect the performance of the neural network. Uniformly distributed points may not be sufficient to capture the complexities of the PDEs, and adaptive sampling techniques have been proposed to dynamically adjust the placement of collocation and boundary points during training. For instance, in the paper "Failure-Informed Adaptive Sampling for PINNs [34]", the authors discuss the use of failure-informed adaptive sampling, which adjusts the placement of collocation points based on the network's performance, leading to improved accuracy and convergence.

Moreover, the loss function in PINNs can be further enhanced by incorporating additional terms that account for physical constraints or specific features of the problem. For example, in the paper "Neural Operators Learn the Local Physics of Magnetohydrodynamics [51]", the authors propose a modified Flux Fourier neural operator model that includes terms to enforce the divergence-free condition of the magnetic field, which is a critical physical constraint in magnetohydrodynamic simulations. These additional terms help to ensure that the neural network's predictions are physically consistent and accurate.

The loss function in PINNs is not only a mathematical construct but also a reflection of the physical problem being solved. The terms included in the loss function must accurately represent the PDEs and the boundary conditions of the problem. This requires a deep understanding of the underlying physics and the ability to translate that understanding into a mathematical formulation. The loss function must also be carefully designed to avoid issues such as overfitting or underfitting, where the network may either memorize the training data or fail to generalize to new scenarios. Techniques such as regularization, early stopping, and cross-validation are often employed to address these issues.

In summary, the loss function in PINNs is a critical component that combines the physical laws described by PDEs with the data-driven learning from boundary and initial conditions. The domain loss ensures that the network's predictions satisfy the PDEs, while the boundary loss ensures that the predictions conform to the boundary conditions. The total loss function is minimized during training to improve the accuracy of the solution. The careful balance of the domain and boundary loss terms, along with the use of adaptive sampling and additional physical constraints, is essential for the success of PINNs in solving complex fluid dynamics problems. The construction of the loss function is a nuanced process that requires both a deep understanding of the physical problem and the ability to translate that understanding into a mathematical formulation that can be optimized by the neural network.

### 2.5 Training Process and Optimization Strategies

The training process of Physics-Informed Neural Networks (PINNs) is a critical component in their application to fluid mechanics, as it determines the accuracy, efficiency, and convergence of the network. Unlike traditional numerical methods that rely on discretization of the domain, PINNs integrate the governing physical laws, such as the Navier-Stokes equations, directly into the loss function during training. This approach allows the neural network to approximate the solution to the partial differential equations (PDEs) by minimizing the residual of the equations, alongside boundary and initial conditions. The training process involves optimizing the network's parameters to reduce the overall loss, which is a combination of the PDE residuals, boundary conditions, and any additional physics-based constraints.

One of the most widely used optimization algorithms in training PINNs is stochastic gradient descent (SGD), which iteratively adjusts the network's weights to minimize the loss function. SGD operates by computing the gradient of the loss with respect to the network's parameters and updating the parameters in the direction of the negative gradient. This process is repeated for multiple epochs until the loss converges to a minimum. However, the effectiveness of SGD in training PINNs depends heavily on the choice of learning rate, which determines the step size at each update. A learning rate that is too high may cause the network to overshoot the minimum, while a learning rate that is too low may result in slow convergence or getting stuck in local minima. Therefore, the selection of an appropriate learning rate is crucial for the success of the training process.

To address the challenges of determining an optimal learning rate, several adaptive learning rate methods have been proposed. For example, the Adam optimizer, which combines the advantages of both AdaGrad and RMSProp, adapts the learning rate during training based on the first and second moments of the gradients. This adaptive approach helps to accelerate convergence and improve the stability of the training process. Additionally, learning rate schedules, such as step decay, exponential decay, and cosine decay, can be used to dynamically adjust the learning rate throughout the training process, further enhancing the performance of PINNs [3]. These strategies are particularly useful in scenarios where the loss landscape is complex, as they allow the optimizer to navigate the optimization space more effectively.

In addition to optimizing the learning rate, techniques such as adaptive sampling and importance sampling play a vital role in improving the convergence of PINNs. Adaptive sampling involves dynamically adjusting the distribution of collocation points during training to focus on regions of the domain where the PDE residuals are large. This approach helps to ensure that the network learns the most challenging parts of the solution more efficiently. For instance, in the context of solving the Navier-Stokes equations, adaptive sampling can be used to concentrate more points near regions of high vorticity or pressure gradients, where the solution is expected to be more complex [52]. By prioritizing these regions, adaptive sampling can significantly reduce the number of collocation points required to achieve a high-quality solution.

Importance sampling is another technique that can be employed to improve the convergence of PINNs. This method involves selecting collocation points based on the importance of the region, as determined by the PDE residuals or other relevant metrics. By focusing on regions with higher residual values, importance sampling can help the network to converge faster and achieve better accuracy. This approach is particularly useful when dealing with problems that have large variations in the solution or when the solution is dominated by specific features, such as shocks or discontinuities. For example, in the case of hyperbolic PDEs, importance sampling can be used to ensure that the network captures the shock fronts accurately, which is essential for obtaining physically meaningful solutions [4].

The training process of PINNs also benefits from the use of hybrid models that combine PINNs with traditional numerical methods. These hybrid approaches leverage the strengths of both methods to improve the accuracy and efficiency of the solution. For instance, a hybrid model may use a PINN to approximate the solution in regions where the PDE residuals are small, while using a finite element method to solve the equations in regions where the solution is more complex. This strategy can significantly reduce the computational cost of the simulation while maintaining a high level of accuracy [10]. Moreover, hybrid models can be particularly effective in handling problems with multiple scales or complex geometries, where the solution may require a combination of different techniques.

Another important aspect of the training process is the use of transfer learning to improve the performance of PINNs. Transfer learning involves pre-training the network on a related task or dataset and then fine-tuning it on the target problem. This approach can significantly reduce the training time and improve the convergence of the network, especially when the target problem has limited or sparse data. For example, in the context of fluid dynamics, transfer learning can be used to pre-train a PINN on a set of canonical flow problems, such as the Taylor-Green vortex or the cylinder flow, and then fine-tune it on a new problem with similar characteristics. This strategy has been shown to be particularly effective in reducing the training time and improving the accuracy of the solution [35].

In summary, the training process of PINNs involves a combination of optimization algorithms, learning rate strategies, and sampling techniques to ensure that the network converges to an accurate solution of the PDEs. The use of adaptive sampling and importance sampling can significantly improve the efficiency of the training process, while hybrid models and transfer learning can enhance the accuracy and robustness of the solution. By carefully designing the training process and selecting appropriate optimization strategies, PINNs can be effectively applied to a wide range of fluid mechanics problems, from laminar flows to turbulent flows and multiphase flows.

### 2.6 Challenges in Training PINNs

Training Physics-Informed Neural Networks (PINNs) presents several significant challenges that can hinder their effectiveness and reliability. These challenges stem from the inherent complexity of the problems they aim to solve, as well as the interplay between the physical laws embedded in the loss function and the neural network's architecture. One of the primary challenges is the non-convexity of the loss landscape, which makes the optimization process difficult and often leads to suboptimal solutions. This non-convexity arises because the loss function in PINNs is a combination of the residual of the governing PDEs and the data fidelity terms, which can lead to a complex optimization surface. For instance, the paper titled “Characterizing possible failure modes in physics-informed neural networks” highlights that the loss landscape for PINNs can be highly non-convex, making it challenging for gradient-based optimization methods to find the global minimum [53].

Another critical challenge in training PINNs is the difficulty in balancing different loss terms. The loss function in PINNs typically includes contributions from the PDE residuals, boundary conditions, and data fidelity. Balancing these terms is essential for achieving accurate and stable solutions. However, the relative importance of each term can vary significantly depending on the problem, and finding the right balance can be a non-trivial task. The paper “Multi-Objective Loss Balancing for Physics-Informed Deep Learning” discusses the importance of properly weighting the loss terms and proposes a novel self-adaptive loss balancing scheme named \emph{ReLoBRaLo} (Relative Loss Balancing with Random Lookback). The results show that this method can consistently outperform existing scaling methods in terms of accuracy while inducing significantly less computational overhead [54].

The impact of network architecture on training performance is another significant challenge in training PINNs. The choice of neural network architecture can greatly influence the ability of the network to approximate the solution of the PDEs. For example, the paper “Architectural Strategies for the optimization of Physics-Informed Neural Networks” explores the impact of different neural network architectures on the training of PINNs. The study reveals that Gaussian activations can outperform other activation functions in terms of training efficiency and performance [55]. This suggests that the design of the neural network architecture plays a crucial role in the success of PINNs.

Moreover, the training of PINNs can be significantly affected by the choice of optimizer and learning rate. The paper “An Expert's Guide to Training Physics-informed Neural Networks” provides a comprehensive guide to training PINNs and emphasizes the importance of choosing appropriate optimizer settings. The study highlights that the choice of optimizer can have a substantial impact on the convergence and accuracy of the PINNs [56]. For example, the paper discusses the effectiveness of using adaptive learning rate methods and the importance of proper initialization techniques to ensure stable training.

Another challenge in training PINNs is the issue of spectral bias, where the network tends to focus on low-frequency components of the solution, leading to poor accuracy in capturing high-frequency dynamics. This issue is particularly relevant in problems involving turbulent flows or other high-frequency phenomena. The paper “Learning in Sinusoidal Spaces with Physics-Informed Neural Networks” addresses this challenge by proposing a novel approach that involves learning in sinusoidal spaces. The study shows that this approach can improve the ability of PINNs to capture high-frequency components of the solution, leading to more accurate predictions [57].

The computational cost associated with training PINNs is also a significant challenge, especially for large-scale and high-dimensional problems. The paper “The Old and the New Can Physics-Informed Deep-Learning Replace Traditional Linear Solvers” discusses the potential of PINNs as linear solvers for the Poisson equation. However, the study also highlights the computational inefficiencies of PINNs, particularly in the case of high-frequency components, which require a long training time to converge [58]. This indicates that while PINNs can offer a promising alternative to traditional solvers, their computational efficiency needs to be improved for practical applications.

In addition to these challenges, the training of PINNs can be affected by the presence of noise and uncertainty in the data. The paper “Recipes for when Physics Fails: Recovering Robust Learning of Physics-Informed Neural Networks” discusses the sensitivity of PINNs to errors in training data and the potential for overfitting. The study proposes a method based on Gaussian Process (GP) smoothing to recover the performance of PINNs and make them robust against noise and errors in measurements [59]. This highlights the importance of incorporating robustness mechanisms into the training of PINNs to handle real-world data scenarios.

Furthermore, the training of PINNs can be affected by the choice of collocation points and the sampling strategy. The paper “A Novel Adaptive Causal Sampling Method for Physics-Informed Neural Networks” introduces an adaptive causal sampling method to improve the performance and efficiency of PINNs. The study shows that this method can significantly improve the accuracy of PINNs with fewer collocation points, demonstrating the importance of an effective sampling strategy in the training process [49].

In conclusion, the training of PINNs is a complex and challenging process that requires careful consideration of several factors. The non-convexity of the loss landscape, the difficulty in balancing different loss terms, the impact of network architecture, the choice of optimizer and learning rate, the issue of spectral bias, the computational cost, the presence of noise and uncertainty in the data, and the choice of collocation points all play critical roles in the success of PINNs. Addressing these challenges is essential for the continued development and application of PINNs in solving complex fluid mechanics problems. By understanding and mitigating these challenges, researchers can improve the reliability and effectiveness of PINNs, paving the way for their broader adoption in scientific and engineering applications.

### 2.7 Variants and Enhancements in PINN Architectures

Variants and enhancements in Physics-Informed Neural Network (PINN) architectures have emerged as a critical area of research aimed at improving accuracy, efficiency, and robustness in solving partial differential equations (PDEs). These improvements are driven by the need to address the limitations of standard PINNs, which often struggle with complex, high-dimensional, and multi-physics problems. A variety of architectural enhancements have been proposed, including the use of different activation functions, hybrid models, and domain decomposition methods, each designed to tackle specific challenges in the training and performance of PINNs.

One significant enhancement is the incorporation of physics-informed activation functions (PAFs), which are designed to encode physical knowledge directly into the neural network’s activation functions. This approach can significantly improve the efficiency and validity of PINNs by aligning the network’s behavior with the underlying physics of the problem. For instance, studies have shown that PAFs can lead to better convergence and accuracy in capturing the solution of PDEs, particularly in cases where the solution exhibits complex behavior [4]. By embedding physical constraints into the activation functions, PINNs can better respect the governing equations and produce more reliable predictions.

Another notable variant is the development of hybrid models that combine PINNs with traditional numerical methods or other machine learning techniques. These hybrid approaches aim to leverage the strengths of both methodologies, improving the accuracy and efficiency of PINNs. For example, domain decomposition methods have been proposed to split complex problems into smaller, more manageable subproblems, which are then solved independently using PINNs. This approach can significantly reduce the computational burden and improve the scalability of PINNs for large-scale problems [3]. By decomposing the domain, PINNs can focus on local regions where the solution is more complex, leading to more accurate and efficient simulations.

Domain decomposition methods have also been enhanced through the use of adaptive sampling techniques. Adaptive sampling strategies dynamically adjust the placement of collocation points to ensure that regions with high solution variability are adequately represented. This approach can lead to improved accuracy and faster convergence, as the network can focus on areas where the solution is most challenging to approximate [49]. For instance, the use of R3 sampling and failure-informed adaptive sampling has been shown to enhance the performance of PINNs by prioritizing regions where the network is least confident in its predictions [49].

In addition to domain decomposition and adaptive sampling, the use of different neural network architectures has also been explored to improve the performance of PINNs. The introduction of convolutional neural networks (CNNs) into the PINN framework has been particularly promising, as CNNs are well-suited for handling spatial data and can capture local features more effectively than fully connected networks [48]. This approach has been particularly effective in handling problems with irregular geometries and high-dimensional data, where traditional PINNs may struggle to achieve accurate results.

Furthermore, the development of multi-task optimization strategies has been another key area of enhancement in PINN architectures. These strategies allow PINNs to simultaneously solve multiple related tasks, such as solving different PDEs or handling different boundary conditions, leading to improved performance and generalization [60]. By sharing information across tasks, PINNs can learn more robust and generalizable representations, which can be particularly beneficial in scenarios with limited data.

The integration of attention mechanisms has also been explored to improve the performance of PINNs. Attention-based architectures, such as the Physics-Informed Attention-Based Neural Network (PIANN), have been shown to be effective in capturing the complex dynamics of nonlinear PDEs. These mechanisms allow the network to focus on critical regions or features in the solution domain, leading to improved accuracy and efficiency [4]. By dynamically adjusting the focus of the network, attention mechanisms can help PINNs better capture the essential characteristics of the solution.

Another important enhancement is the use of transfer learning and meta-transfer learning techniques to improve the training efficiency of PINNs. These approaches allow PINNs to leverage knowledge learned from previous tasks to accelerate the training process for new problems, reducing the need for extensive retraining [35]. This is particularly beneficial in scenarios where the problem parameters or boundary conditions change frequently, as it enables PINNs to quickly adapt to new conditions without starting from scratch.

The use of physics-informed deep learning frameworks has also led to the development of more sophisticated loss functions that incorporate multiple physical constraints. For example, the introduction of a pressure stabilization term in the loss function has been shown to improve the accuracy of PINNs in capturing the pressure field, particularly in cases where the solution is highly nonlinear or exhibits complex behavior [61]. These enhanced loss functions help PINNs better respect the governing equations and produce more accurate solutions.

In summary, the evolution of PINN architectures has been marked by a variety of enhancements aimed at improving accuracy, efficiency, and robustness in solving PDEs. From the use of physics-informed activation functions and hybrid models to domain decomposition methods and attention mechanisms, these advancements have significantly expanded the capabilities of PINNs. By incorporating these variants and enhancements, researchers are able to tackle more complex and challenging fluid mechanics problems, pushing the boundaries of what is possible with physics-informed neural networks.

### 2.8 Theoretical Foundations and Convergence Analysis

Theoretical foundations and convergence analysis form a critical component of understanding the behavior and reliability of Physics-Informed Neural Networks (PINNs). While PINNs have shown remarkable success in solving partial differential equations (PDEs) by integrating physical laws into the training process, their theoretical underpinnings remain an active area of research. The convergence guarantees, error analysis, and the interplay between network capacity, PDE complexity, and training stability are essential for evaluating the robustness and applicability of PINNs in fluid mechanics and other scientific domains.

One of the primary theoretical challenges in PINNs is the lack of explicit convergence guarantees. Traditional numerical methods, such as finite difference or finite element methods, are based on well-established convergence theories, which provide rigorous bounds on the error between the numerical solution and the true solution of a PDE. In contrast, PINNs rely on the optimization of a composite loss function that combines the PDE residual, boundary conditions, and potentially other physics-based constraints. This composite loss function is often non-convex, leading to complex optimization landscapes that can hinder convergence. For example, the work by Raissi et al. [62] highlights that the non-convexity of the loss function can result in the network getting stuck in local minima, especially for high-order or stiff PDEs. Furthermore, the imbalance between different loss terms, such as the domain loss and boundary loss, can lead to suboptimal convergence, as noted in the study by Zhang et al. [63]. The authors emphasize that the scale mismatch between these loss terms can significantly affect the training process, making it difficult to achieve accurate solutions.

Error analysis in PINNs is another critical aspect of their theoretical foundation. The accuracy of a PINN solution depends on the interplay between the approximation capabilities of the neural network, the choice of loss function, and the distribution of training points. The work by Li et al. [64] introduces a novel loss function that incorporates a posteriori error estimates, which provide rigorous bounds on the error between the PINN solution and the true solution of the PDE. This approach not only improves the accuracy of the PINN solution but also provides a systematic way to evaluate the error, which is crucial for practical applications. Similarly, the study by Zhang et al. [65] provides a mathematical framework for deriving explicit error bounds for PINNs trained on linear systems of differential equations. The authors show that the error bound can be computed using the residual of the PDE over the domain of interest, offering a way to assess the reliability of the PINN solution without requiring the exact solution.

The relationship between network capacity and the complexity of the PDE being solved is another important theoretical consideration. A deeper or wider neural network has a higher capacity to approximate complex functions, but it also increases the risk of overfitting and requires more computational resources. The work by Li et al. [66] explores how the overparameterization of neural networks affects the training dynamics and convergence. The authors demonstrate that overparameterized networks, when trained with gradient descent, can achieve zero loss in many practical scenarios, suggesting that increasing the network's capacity can lead to better convergence. However, this does not necessarily mean that the network will converge to the correct solution of the PDE. The complexity of the PDE, such as its order, nonlinearity, and boundary conditions, also plays a crucial role in determining the convergence behavior of the PINN. For example, the work by Zhang et al. [67] shows that high-order PDEs can lead to contaminated gradients during backpropagation, which can hinder the convergence of the PINN. The authors propose a method to bypass the calculation of high-order derivatives, which helps reduce the dimension of the search space and improve the convergence of the network.

Training stability is another key factor in the theoretical analysis of PINNs. The training process of a PINN is highly sensitive to the choice of optimization algorithm, learning rate, and loss function. The work by Li et al. [68] introduces MultiAdam, a scale-invariant optimizer that uses gradient momentum to balance the loss terms in a parameter-wise manner. The authors demonstrate that MultiAdam can significantly improve the training stability and convergence of PINNs, especially for problems with multiscale features. In addition, the work by Li et al. [69] explores how the structure of the loss landscape affects the training dynamics of PINNs. The authors show that by regulating the data used during training, it is possible to reshape the loss landscape, making it easier for the optimizer to converge to the correct solution. This insight is particularly useful for problems where the PINN may otherwise get stuck in local minima or fail to converge altogether.

In summary, the theoretical foundations and convergence analysis of PINNs are essential for understanding their behavior and improving their reliability in solving PDEs. While PINNs offer a powerful framework for integrating physical laws into data-driven learning, their theoretical guarantees, error analysis, and training stability require further investigation. The studies cited here provide valuable insights into the challenges and opportunities of PINNs, highlighting the need for continued research in this area to ensure their robustness and applicability in real-world fluid mechanics problems.

### 2.9 Comparison with Traditional Numerical Methods

Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods such as finite element (FE) and finite volume (FV) methods for solving partial differential equations (PDEs). While traditional methods have been the backbone of computational fluid dynamics for decades, PINNs offer unique advantages, particularly in handling complex geometries, integrating physical constraints, and reducing computational costs. However, they also face challenges in accuracy, scalability, and computational efficiency, especially when compared to well-established numerical solvers. This subsection provides a comprehensive comparison of PINNs with traditional numerical methods, emphasizing their strengths and limitations in terms of accuracy, scalability, and computational cost.

In terms of accuracy, traditional numerical methods such as FE and FV have long been the gold standard for solving PDEs. These methods rely on discretizing the domain into a structured or unstructured mesh and solving the equations at discrete points. The accuracy of these methods is typically governed by the mesh resolution and the order of the numerical scheme. For instance, high-order finite element methods can achieve high accuracy for smooth solutions, while finite volume methods are well-suited for conservation laws and shocks [70]. In contrast, PINNs use neural networks to approximate the solution without the need for a mesh, making them particularly effective for problems with complex geometries or irregular domains. However, the accuracy of PINNs can be sensitive to the choice of neural network architecture, training strategy, and the distribution of collocation points. For example, PINNs may struggle with high-frequency components of the solution due to spectral bias [71], which limits their ability to capture fine-scale features of the solution. Despite this, recent studies have shown that PINNs can achieve comparable accuracy to traditional methods for certain problems, especially when combined with advanced training techniques such as adaptive sampling or physics-informed activation functions [34; 72].

Scalability is another critical factor in comparing PINNs with traditional numerical methods. Traditional methods often face challenges when applied to high-dimensional or large-scale problems, as the computational cost increases significantly with the number of grid points or elements. For instance, the memory and computational requirements of FE methods can become prohibitive for problems with large domains or high-resolution grids. Similarly, FV methods may suffer from scalability issues when applied to problems with complex geometries or adaptive mesh refinement. In contrast, PINNs have the potential to be more scalable, as they do not require a mesh and can be trained on unstructured data. However, the scalability of PINNs depends heavily on the training strategy and the efficiency of the optimization algorithm. For example, the use of distributed training or domain decomposition techniques can significantly improve the performance of PINNs on large-scale problems [73]. Nevertheless, for problems requiring high-resolution solutions, traditional methods may still be more efficient, particularly when combined with parallel computing and optimized solvers.

Computational cost is a key consideration when comparing PINNs with traditional numerical methods. Traditional methods, while accurate, can be computationally expensive, especially for high-dimensional or time-dependent problems. The cost of solving a PDE using FE or FV methods depends on the number of grid points, the order of the method, and the complexity of the problem. For instance, solving the Navier-Stokes equations using a high-order FE method can require significant computational resources, particularly for three-dimensional flows. In contrast, PINNs offer a mesh-free approach that can reduce the computational cost of solving PDEs, especially for problems with sparse or irregular data. However, the training of PINNs can be computationally intensive, particularly for problems involving high-order derivatives or complex PDEs. For example, the use of automatic differentiation to compute the derivatives of the neural network output can be time-consuming, and the convergence of PINNs can be slow, especially for problems with steep gradients or nonlinearities [32]. To mitigate these issues, various techniques have been proposed, such as using finite-difference approximations instead of automatic differentiation [74] or leveraging parallel computing to accelerate the training process [75].

Despite these challenges, PINNs have shown promising results in various applications, particularly in scenarios where traditional methods face limitations. For instance, PINNs have been successfully applied to solve PDEs with irregular geometries, where traditional methods may require complex mesh generation [76]. They have also been used to solve PDEs with sparse or noisy data, where traditional methods may struggle due to the lack of sufficient information [77]. Moreover, PINNs have the potential to be more flexible and adaptable than traditional methods, as they can be trained on data from different sources and can incorporate physical constraints directly into the loss function [78]. This flexibility makes PINNs particularly attractive for problems where the governing equations are not fully known or where the solution domain is highly dynamic.

In conclusion, while traditional numerical methods such as FE and FV have established themselves as reliable and accurate tools for solving PDEs, PINNs offer a promising alternative with unique advantages in terms of scalability, flexibility, and adaptability. However, they also face challenges in accuracy, computational efficiency, and convergence, particularly for complex or high-dimensional problems. The comparison between PINNs and traditional methods highlights the need for continued research and development to address these challenges and to fully realize the potential of PINNs in fluid mechanics and other scientific applications. As the field continues to evolve, it is likely that hybrid approaches combining the strengths of PINNs and traditional methods will play an increasingly important role in solving complex PDEs.

## 3 Applications in Fluid Mechanics

### 3.1 Solving Navier-Stokes Equations with PINNs

The Navier-Stokes equations are a cornerstone of fluid mechanics, describing the motion of viscous fluid substances. These equations are a set of nonlinear partial differential equations (PDEs) that govern the conservation of mass, momentum, and energy in fluid flows. Solving the Navier-Stokes equations is essential for understanding and predicting fluid behavior in a wide range of applications, from aerodynamics to geophysical flows. However, traditional numerical methods such as finite element and finite volume methods often face computational challenges, especially for complex geometries and high Reynolds number flows. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative for solving the Navier-Stokes equations, leveraging the power of deep learning to incorporate physical laws directly into the training process.

One of the key advantages of PINNs in solving the Navier-Stokes equations is their ability to integrate the governing equations into the neural network's loss function. By doing so, PINNs ensure that the solutions adhere to the underlying physical constraints, even when working with sparse or noisy data. This is particularly important in fluid mechanics, where accurate solutions are critical for reliable predictions. The integration of the Navier-Stokes equations into the loss function allows PINNs to approximate the fluid flow without the need for extensive computational resources, making them a viable option for real-time simulations and large-scale problems [20].

A notable enhancement in the application of PINNs to the Navier-Stokes equations is the development of the Enhanced Physics-Informed Neural Network for Navier-Stokes Equations (EPINN-NSE) [20]. This model introduces a novel approach to simplify the system by approximating the velocity components using a stream function. By employing the stream function, EPINN-NSE ensures that the velocity field is divergence-free, a fundamental requirement in incompressible flows. This approach not only simplifies the system of equations but also improves the accuracy of the solutions by enforcing the divergence-free condition directly. The results from EPINN-NSE demonstrate its effectiveness in solving both two-dimensional and three-dimensional Navier-Stokes equations, highlighting its potential for a wide range of fluid mechanics problems.

The use of stream functions in PINNs is not limited to EPINN-NSE. Other studies have explored the application of stream functions to simplify the system of equations and improve the trainability of the neural networks. For instance, the mixed-variable scheme proposed in [1] demonstrates the effectiveness of incorporating the stream function into the neural network architecture. This approach ensures that the velocity field satisfies the continuity equation, which is crucial for accurately modeling incompressible flows. By leveraging the stream function, PINNs can more effectively capture the complex dynamics of fluid flows, even in the presence of high Reynolds numbers.

In addition to stream functions, other techniques have been developed to enhance the performance of PINNs in solving the Navier-Stokes equations. For example, the use of physics-informed activation functions has been shown to improve the accuracy and efficiency of PINNs by incorporating physical knowledge directly into the neural network's activation functions [1]. These activation functions are designed to respect the physical constraints of the problem, leading to more accurate solutions. By integrating such physics-informed activation functions into the neural network, PINNs can better capture the complex behavior of fluid flows, particularly in regions with sharp gradients or high-frequency components.

Another important development in the application of PINNs to the Navier-Stokes equations is the use of adaptive sampling techniques. Traditional PINNs often struggle with the uneven distribution of collocation points, leading to inaccurate solutions in regions of interest. The introduction of adaptive sampling strategies, such as the Gaussian mixture distribution-based adaptive sampling method (GAS) [79], addresses this issue by dynamically adjusting the placement of collocation points based on the residual error. This approach ensures that the neural network focuses on the most critical regions of the domain, improving the accuracy and efficiency of the solutions. By leveraging adaptive sampling, PINNs can more effectively handle the complexities of fluid flows, particularly in regions with high gradients or turbulent behavior.

The use of PINNs in solving the Navier-Stokes equations has also been extended to handle multi-physics problems, where the fluid flow is coupled with other physical processes such as heat transfer or chemical reactions. For example, the mixed formulation of physics-informed neural networks (PINNs) for thermo-mechanically coupled systems [6] demonstrates the effectiveness of PINNs in handling complex, coupled systems. By incorporating the governing equations of each physical process into the loss function, PINNs can accurately model the interactions between different physical phenomena, leading to more reliable predictions.

Furthermore, the integration of PINNs with traditional numerical methods has shown promise in improving the accuracy and efficiency of fluid flow simulations. Hybrid approaches that combine PINNs with finite element or finite volume methods [80] leverage the strengths of both approaches, allowing for more accurate solutions while maintaining computational efficiency. These hybrid models can effectively handle the complexities of fluid flows, particularly in regions with high gradients or turbulent behavior, by combining the precision of traditional numerical methods with the flexibility of PINNs.

In addition to solving the Navier-Stokes equations, PINNs have also been applied to the study of turbulent flows. The development of the RANS-PINN framework [22] demonstrates the potential of PINNs in predicting turbulent flow fields. By incorporating a 2-equation eddy viscosity model based on the Reynolds-averaged Navier-Stokes (RANS) formulation, RANS-PINN can accurately capture the complex behavior of turbulent flows. This approach not only improves the accuracy of the solutions but also reduces the computational cost associated with traditional RANS simulations.

The application of PINNs to the Navier-Stokes equations has also been extended to handle inverse problems, where the goal is to estimate unknown parameters or boundary conditions from observed data. For example, the use of PINNs in solving inverse problems in hemodynamics [24] has shown promising results in estimating boundary conditions and full velocity fields from noisy measurements. By incorporating the physical constraints into the loss function, PINNs can effectively recover the unknown parameters, leading to more accurate predictions.

In summary, the application of PINNs to the Navier-Stokes equations has demonstrated significant potential in solving complex fluid mechanics problems. The development of enhanced models such as EPINN-NSE, the use of stream functions to simplify the system, and the integration of adaptive sampling techniques have all contributed to the effectiveness of PINNs in solving the Navier-Stokes equations. By leveraging the power of deep learning and incorporating physical laws directly into the training process, PINNs offer a promising alternative to traditional numerical methods, enabling more accurate and efficient solutions for a wide range of fluid mechanics problems.

### 3.2 Modeling Turbulent Flows

Modeling turbulent flows remains one of the most challenging tasks in fluid mechanics due to the complex, nonlinear, and multi-scale nature of the phenomenon. Traditional numerical methods, such as Direct Numerical Simulation (DNS) and Large Eddy Simulation (LES), require significant computational resources and are often infeasible for real-time or high-dimensional applications. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative for modeling turbulent flows, leveraging their ability to integrate physical laws directly into the training process. One of the most notable applications of PINNs in turbulence modeling is the development of Reynolds-Averaged Navier-Stokes (RANS) equations combined with PINNs, leading to the creation of RANS-PINN. This approach has demonstrated significant potential in capturing the essential dynamics of high Reynolds number turbulent flows with reduced computational costs [22].

Turbulent flows are typically described by the Navier-Stokes equations, which govern the motion of fluids. However, solving these equations directly for turbulent flows is computationally prohibitive due to the need to resolve all scales of motion. RANS equations, on the other hand, provide a statistical description of the flow by time-averaging the Navier-Stokes equations, thereby reducing the complexity of the problem. The RANS equations include terms that account for the effects of turbulence, such as the Reynolds stress tensor, which is typically modeled using turbulence closure models. These models, such as the k-ε and k-ω models, are empirical and can introduce inaccuracies, especially in complex flows. The RANS-PINN framework addresses these limitations by incorporating the RANS equations into the loss function of a neural network, allowing the network to learn the turbulent behavior directly from the data while respecting the underlying physics [22].

The RANS-PINN approach is particularly effective for high Reynolds number turbulent flows, where the traditional turbulence models often fail to capture the correct dynamics. By embedding the RANS equations into the loss function, the neural network is trained to minimize the residuals of the equations, ensuring that the learned solution satisfies the governing physical laws. This approach has been shown to produce accurate predictions for turbulent flows with a significantly reduced computational cost compared to traditional RANS solvers. Furthermore, RANS-PINN can adapt to different flow conditions and boundary conditions, making it a flexible and robust tool for modeling turbulent flows in various applications [22].

One of the key advantages of RANS-PINN is its ability to handle the inherent nonlinearity and complexity of turbulent flows. Traditional turbulence models often rely on simplifying assumptions that may not hold in all flow regimes. In contrast, RANS-PINN does not require such assumptions and can learn the turbulent behavior directly from the data. This is particularly beneficial for flows with complex geometries or varying boundary conditions, where the performance of traditional turbulence models may degrade. The use of PINNs in conjunction with RANS equations allows for a more accurate representation of the turbulent flow field, leading to improved predictions of key flow quantities such as velocity, pressure, and turbulence intensity.

Another benefit of RANS-PINN is its scalability to high-dimensional problems. Turbulent flows are inherently high-dimensional, and traditional numerical methods often struggle to handle such problems efficiently. PINNs, on the other hand, are well-suited for high-dimensional problems due to their ability to approximate complex functions with a relatively small number of parameters. The RANS-PINN framework has been shown to scale effectively to high-dimensional turbulent flow problems, making it a viable alternative to traditional RANS solvers. Additionally, the use of PINNs allows for the incorporation of physical constraints into the training process, ensuring that the learned solutions are consistent with the governing equations [22].

The development of RANS-PINN has also been driven by the need to improve the accuracy of turbulence modeling in engineering applications. Traditional RANS solvers often require extensive calibration and are sensitive to the choice of turbulence model and closure assumptions. RANS-PINN offers a data-driven approach to turbulence modeling that can be trained on a wide range of flow data, allowing for more accurate and robust predictions. This data-driven approach is particularly valuable in cases where experimental data is limited or expensive to obtain. By leveraging the power of neural networks, RANS-PINN can learn the turbulent behavior directly from the data, leading to more accurate and reliable predictions [22].

In addition to RANS-PINN, other variants of PINNs have been developed to model turbulent flows. For example, some studies have explored the use of PINNs for solving the Navier-Stokes equations directly, without relying on RANS approximations. These PINNs are trained to satisfy the Navier-Stokes equations at a set of collocation points, ensuring that the learned solution is consistent with the governing equations. While this approach can capture the full range of turbulent dynamics, it is computationally more expensive than RANS-PINN and may not be suitable for high Reynolds number flows. However, the use of PINNs for solving the Navier-Stokes equations directly has shown promise in capturing the complex behavior of turbulent flows, particularly in cases where the turbulence is highly anisotropic or three-dimensional [3].

Overall, the use of PINNs in modeling turbulent flows represents a significant advancement in fluid mechanics. The development of RANS-PINN has demonstrated the potential of PINNs to capture the essential dynamics of high Reynolds number turbulent flows with reduced computational costs. By integrating the RANS equations into the loss function of a neural network, RANS-PINN provides a flexible and robust framework for modeling turbulent flows. The success of RANS-PINN has also inspired the development of other PINN-based approaches for turbulent flow modeling, further expanding the capabilities of PINNs in fluid mechanics. As research in this area continues to advance, the integration of PINNs with traditional turbulence modeling techniques is expected to lead to even more accurate and efficient solutions for turbulent flow problems.

### 3.3 Predicting Multiphase and Multi-Scale Flows

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for predicting complex multiphase and multi-scale flows in fluid mechanics. These flows are inherently challenging due to their high-dimensional nature, the coexistence of different phases, and the presence of various spatial and temporal scales. Traditional numerical methods often struggle to efficiently capture such dynamics, particularly when dealing with irregular geometries, sparse data, and high-frequency components. PINNs, on the other hand, offer a data-driven yet physically consistent approach to modeling these complex systems by embedding the governing equations directly into the loss function of the neural network. This integration of physical laws with data-driven learning not only enhances the accuracy of predictions but also improves the generalization capability of the models, especially in scenarios where data is scarce or noisy.

One notable application of PINNs in multi-scale and multiphase flow prediction is the Multi-Receptive-Field PINN (MRF-PINN) framework [48]. MRF-PINN addresses the challenge of solving different types of PDEs on various mesh resolutions without the need for manual tuning of hyperparameters. The framework leverages the advantages of convolutional neural networks (CNNs), including parameter sharing, spatial feature extraction, and low inference cost, to effectively handle multi-scale problems. By using a multi-receptive-field approach, MRF-PINN can adapt to a wide range of PDEs and mesh resolutions, making it a versatile tool for fluid mechanics applications. Additionally, the dimensional balance method is employed to estimate the loss weights when solving Navier-Stokes equations, which helps in improving the convergence and stability of the training process. The Taylor polynomial is used to pad the virtual nodes near the boundaries, enabling the implementation of high-order finite difference methods. This approach has been shown to significantly reduce the solving error under high-order finite difference, large channel numbers, and high mesh resolution, making MRF-PINN a promising technique for multi-scale fluid dynamics problems.

Another key development in the application of PINNs to multi-scale flows is the use of spectral PINNs for capturing small-scale dynamics. Spectral PINNs are designed to handle problems with complex, high-frequency components by leveraging the spectral properties of the solution. This approach is particularly effective in scenarios where the small-scale dynamics play a critical role in the overall behavior of the system. For instance, in the context of turbulent flows, the spectral PINN can capture the intricate interactions between different scales of motion, which are often challenging to resolve with traditional numerical methods. By incorporating the spectral information into the loss function, these models can achieve higher accuracy and better convergence compared to standard PINN formulations. The use of spectral PINNs has been demonstrated in applications such as the simulation of the Kuramoto-Sivashinsky equation and Navier-Stokes equations, where they have shown superior performance in resolving small-scale features.

In addition to MRF-PINN and spectral PINNs, other PINN variants have been developed to address specific challenges in multi-phase and multi-scale flow prediction. For example, the application of PINNs to solve the Navier-Stokes equations for incompressible turbulent flows has been explored in [10]. This study demonstrates the effectiveness of PINNs in capturing the complex dynamics of turbulent flows without the need for explicit turbulence models. The integration of the Reynolds-averaged Navier-Stokes (RANS) equations into the PINN framework allows for the prediction of velocity and pressure fields with high accuracy, even in the presence of strong pressure gradients and complex flow structures.

The use of PINNs in predicting multiphase flows has also been a focus of recent research. Multiphase flows, such as those involving gas-liquid or solid-liquid interactions, are inherently more complex due to the presence of interfaces and the need to account for phase-specific properties. PINNs can be enhanced with domain decomposition techniques and hybrid models to handle such cases. For instance, the work in [81] demonstrates how PINNs can be used to simulate blood flow in arteries, where the incompressible Navier-Stokes equations are solved using domain decomposition. This approach enables the accurate modeling of complex hemodynamic phenomena, such as backflow instabilities, which are challenging to capture with traditional numerical methods.

Moreover, the application of PINNs to multi-scale and multiphase flows has been extended to real-world engineering problems. For example, in the context of heat exchanger design, PINNs have been used to predict the flow and heat transfer characteristics in complex geometries with varying scales. These models incorporate the governing equations for mass, momentum, and energy conservation, ensuring that the predictions are physically consistent. The integration of PINNs with data-driven methods such as multi-fidelity learning further enhances their ability to handle sparse and noisy data, making them suitable for practical applications in fluid mechanics.

Overall, the use of PINNs in predicting complex multiphase and multi-scale flows has demonstrated significant potential. The development of specialized PINN architectures, such as MRF-PINN and spectral PINNs, has enabled the efficient and accurate modeling of problems with varying spatial and temporal scales. These approaches not only improve the computational efficiency of simulations but also provide a more robust framework for handling the challenges associated with multi-phase and multi-scale fluid dynamics. As research in this area continues to advance, the integration of PINNs with other emerging techniques, such as quantum computing and advanced sampling strategies, is expected to further enhance their capabilities in solving real-world fluid mechanics problems.

### 3.4 Addressing Inverse Problems in Fluid Dynamics

[82]

Inverse problems in fluid dynamics involve determining the underlying parameters or conditions that govern a fluid flow based on observed or measured data. These problems are inherently challenging due to their non-unique nature and the potential for ill-posedness. Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for addressing inverse problems in fluid dynamics by integrating physical laws with data-driven learning. This section examines the application of PINNs in solving inverse problems, including the estimation of reduced-order model parameters and full velocity fields from noisy measurements, and their use in biomedical applications.

One of the primary applications of PINNs in inverse problems is the estimation of reduced-order model (ROM) parameters. Reduced-order models are used to approximate the behavior of complex fluid systems by capturing the dominant modes of variation. However, the accuracy of these models heavily depends on the correct estimation of their parameters. PINNs can be trained to learn these parameters by incorporating the governing equations of the fluid dynamics problem into the loss function. This approach ensures that the learned parameters are consistent with the physical laws governing the system. For example, in the context of turbulent flow simulations, PINNs have been used to estimate the parameters of Reynolds-averaged Navier-Stokes (RANS) models, which are commonly used to predict the mean flow behavior in turbulent flows [83]. By leveraging the physical constraints encoded in the PINN architecture, these models can achieve higher accuracy and robustness compared to traditional methods.

In addition to parameter estimation, PINNs have been successfully applied to the reconstruction of full velocity fields from noisy measurements. This is particularly important in scenarios where direct measurements of the entire flow field are challenging or impossible. PINNs can be trained to approximate the velocity field by minimizing the residuals of the Navier-Stokes equations while also fitting the available data. This dual objective ensures that the resulting velocity field is both physically consistent and statistically accurate. For instance, in the context of biomedical applications, PINNs have been used to estimate the velocity field in the ascending aorta from noisy measurements obtained through ultrasound imaging [15]. The ability of PINNs to handle noisy data and produce physically consistent solutions makes them well-suited for such applications.

The use of PINNs in biomedical applications extends beyond the estimation of velocity fields. They have also been employed to infer boundary conditions and other flow-related parameters from limited or noisy data. This is particularly relevant in the study of blood flow in complex vascular geometries, where obtaining detailed flow measurements is often impractical. By integrating the Navier-Stokes equations into the training process, PINNs can infer the boundary conditions that best fit the observed data while maintaining the physical consistency of the solution. For example, in the context of arterial blood flow simulations, PINNs have been used to estimate the boundary conditions at the inlet and outlet of the vascular system, enabling the accurate prediction of flow patterns and pressure distributions [15]. This approach not only improves the accuracy of the simulations but also reduces the need for extensive experimental data.

Another important application of PINNs in inverse problems is the identification of unknown source terms or forcing functions in fluid dynamics. These source terms can represent external influences such as body forces, heat sources, or mass sources that affect the flow. PINNs can be trained to identify these source terms by minimizing the residuals of the governing equations while also fitting the available data. This approach has been particularly useful in the study of complex flows where the source terms are not known a priori. For instance, in the context of multiphase flows, PINNs have been used to identify the source terms that govern the interaction between different phases, enabling the accurate prediction of phase distribution and interfacial dynamics [84].

The ability of PINNs to handle noisy and sparse data is a key advantage in the context of inverse problems. Traditional numerical methods often struggle with the presence of noise and the lack of sufficient data, leading to inaccurate or unreliable solutions. In contrast, PINNs can be trained to tolerate noisy data by incorporating the physical constraints into the loss function. This allows the network to focus on the underlying physical laws rather than the noise in the measurements. For example, in the context of experimental fluid mechanics, PINNs have been used to super-resolve flow-field data from low-resolution and noisy measurements, ensuring physically consistent predictions [83]. This capability is particularly valuable in scenarios where high-resolution data is difficult to obtain, such as in wind tunnel experiments or field measurements.

The integration of PINNs with other machine learning techniques has further enhanced their effectiveness in solving inverse problems. Hybrid models that combine PINNs with traditional numerical methods or other machine learning algorithms can leverage the strengths of both approaches. For example, in the context of inverse problems in hemodynamics, PINNs have been used in conjunction with Bayesian inference to quantify uncertainties in the estimated parameters and improve the robustness of the solutions [51]. This approach not only provides accurate parameter estimates but also accounts for the uncertainties inherent in the measurements, leading to more reliable predictions.

Furthermore, the use of PINNs in inverse problems has been extended to the estimation of initial conditions and other time-dependent parameters. This is particularly relevant in the study of unsteady flows, where the initial conditions play a crucial role in determining the evolution of the flow. By incorporating the time-dependent governing equations into the loss function, PINNs can be trained to estimate the initial conditions that best fit the observed data. This approach has been used in the context of unsteady flow simulations, where the initial conditions are unknown or difficult to measure [85]. The ability of PINNs to handle time-dependent problems makes them well-suited for applications involving transient phenomena.

In conclusion, the application of PINNs in solving inverse problems in fluid dynamics has demonstrated significant potential. By integrating physical laws with data-driven learning, PINNs can accurately estimate reduced-order model parameters, reconstruct full velocity fields from noisy measurements, and infer boundary conditions and source terms in complex flows. Their ability to handle noisy and sparse data, combined with the flexibility of the neural network architecture, makes them a powerful tool for addressing a wide range of inverse problems in fluid mechanics. As research in this area continues to advance, the integration of PINNs with other machine learning techniques and the development of more sophisticated training strategies will further enhance their effectiveness in solving inverse problems.

### 3.5 Handling Obstacle-Related PDEs

Obstacle-related partial differential equations (PDEs) pose significant challenges in fluid mechanics due to the need for accurate approximations of solutions above a given obstacle. These types of PDEs are encountered in various applications, such as fluid flow around structures, porous media, and biological systems. Traditional numerical methods often struggle with these problems because they require precise handling of the boundary conditions and the complex interactions between the fluid and the obstacle. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative for solving such challenging PDEs by integrating the governing equations directly into the loss function, thereby ensuring that the neural network respects the underlying physics of the problem.

One of the primary challenges in solving obstacle-related PDEs is the accurate representation of the solution in regions where the obstacle is present. This is because the presence of an obstacle introduces discontinuities and sharp gradients in the solution domain, which traditional numerical methods may not capture effectively. PINNs, on the other hand, can leverage their flexibility and data-driven nature to approximate the solution with high accuracy, even in these complex regions. By embedding the governing equations into the loss function, PINNs are able to enforce the physical constraints of the problem, ensuring that the solution is consistent with the underlying PDEs.

The extension of PINNs to obstacle-related PDEs has been explored in several studies. For instance, a paper titled "A physics-informed neural network framework for modeling obstacle-related equations" [25] discusses the application of PINNs to solve obstacle-related PDEs, highlighting the challenges and potential solutions. The paper emphasizes the importance of accurate approximation of solutions above the obstacle, which is crucial for capturing the correct physical behavior of the system. The authors propose a framework that incorporates the obstacle into the loss function, allowing the PINN to learn the solution while respecting the physical constraints imposed by the obstacle.

In addition to the integration of the obstacle into the loss function, the choice of activation functions and the architecture of the neural network play a critical role in the performance of PINNs for obstacle-related PDEs. A study by [25] demonstrates that certain activation functions, such as the hyperbolic tangent and the rectified linear unit (ReLU), can improve the ability of the neural network to capture the complex behavior of the solution around the obstacle. Furthermore, the use of deep neural networks with multiple hidden layers can enhance the representational capacity of the PINN, enabling it to approximate the solution more accurately.

Another important aspect of solving obstacle-related PDEs with PINNs is the selection of collocation points, which are used to evaluate the loss function. In traditional numerical methods, the placement of collocation points is often determined by the mesh or grid, which can be computationally expensive and time-consuming. However, PINNs can leverage adaptive sampling techniques to dynamically adjust the placement of collocation points based on the complexity of the solution. This allows for a more efficient and accurate approximation of the solution, especially in regions where the solution exhibits sharp gradients or discontinuities.

The performance of PINNs in solving obstacle-related PDEs has been validated through various numerical experiments. For example, a study by [25] presents several test cases, including the flow around a circular cylinder and the flow over a NACA0012 airfoil. The results demonstrate that PINNs can effectively capture the complex flow patterns around the obstacle, providing accurate predictions of the velocity and pressure fields. Furthermore, the study highlights the ability of PINNs to handle non-uniform collocation points, which is a significant advantage over traditional numerical methods.

In addition to the challenges posed by the obstacle itself, the presence of noise in the data can also affect the performance of PINNs in solving obstacle-related PDEs. A study by [25] investigates the impact of noise on the accuracy of PINN solutions. The authors introduce a volume-weighted residual approach to account for the spatial distribution of collocation points, which helps to improve the robustness of the PINN to noise. The results show that the proposed method significantly reduces the relative error in the solution, even when the data is noisy.

The application of PINNs to obstacle-related PDEs is not limited to fluid dynamics but extends to other fields such as porous media flow and biomechanics. For instance, in the context of porous media, the obstacle can represent a solid structure within the domain, and the solution must account for the interaction between the fluid and the solid. A study by [25] demonstrates the effectiveness of PINNs in solving such problems, showing that the framework can accurately capture the flow behavior around the obstacle.

Moreover, the integration of PINNs with other machine learning techniques can further enhance their ability to solve obstacle-related PDEs. For example, the use of transfer learning can allow PINNs to leverage pre-trained models to improve their performance on new problems. A study by [25] explores the use of transfer learning in the context of obstacle-related PDEs, demonstrating that the approach can significantly reduce the training time and improve the accuracy of the solutions.

In conclusion, the extension of PINNs to obstacle-related PDEs represents a significant advancement in the field of computational fluid dynamics. By integrating the governing equations directly into the loss function, PINNs can accurately approximate the solution, even in regions with complex geometries and sharp gradients. The choice of activation functions, the architecture of the neural network, and the selection of collocation points are all critical factors that influence the performance of PINNs in solving these challenging PDEs. Through various numerical experiments and comparisons with traditional methods, the effectiveness of PINNs in handling obstacle-related PDEs has been demonstrated, paving the way for their application in a wide range of scientific and engineering problems.

### 3.6 Simulating Branched Flows with Transfer Learning

Simulating branched flows, such as those encountered in random wave dynamics, presents a significant challenge in fluid mechanics due to the inherent complexity and stochastic nature of these phenomena. Traditional numerical methods often struggle with capturing the intricate details of such flows, especially when dealing with high-dimensional and non-linear systems. Physics-Informed Neural Networks (PINNs) have emerged as a powerful alternative, integrating physical laws directly into the learning process. However, even with their advantages, PINNs face challenges when applied to problems involving stochastic branched flows, where the solutions can exhibit a wide range of behaviors depending on the initial conditions and external perturbations. This is where transfer learning, combined with the use of multi-head models, has shown great promise in improving the efficiency and accuracy of simulations.

Transfer learning, a technique that leverages pre-trained models to improve performance on related tasks, has been effectively applied to PINNs to simulate stochastic branched flows. The core idea is to utilize knowledge gained from solving similar problems to accelerate the training of new models for different scenarios. In the context of branched flows, this approach allows for the reuse of learned features and patterns, which can significantly reduce the computational cost and time required for training. By starting with a model that has already been trained on a related problem, such as unsteady flows past moving bodies or turbulent flows, the PINN can converge more quickly to an accurate solution for the new, more complex problem of branched flows [35].

The application of transfer learning in this context is further enhanced by the use of multi-head models, which are designed to handle multiple tasks or outputs simultaneously. In the case of branched flows, a multi-head model can be structured to predict various aspects of the flow field, such as velocity, pressure, and vorticity, from a single input. This approach not only improves the model's ability to generalize but also allows for the efficient allocation of computational resources. By distributing the learning process across multiple heads, the model can focus on different aspects of the problem, leading to more accurate and robust predictions. For instance, a multi-head PINN can be trained to learn the overall flow structure while another head focuses on the detailed dynamics of individual branches, thereby capturing the multifaceted nature of branched flows [35].

One of the key advantages of using transfer learning with PINNs is the ability to handle the inherent uncertainty and variability in branched flows. Stochastic branched flows are characterized by their sensitivity to initial conditions and the presence of random fluctuations, which can make them difficult to predict using traditional methods. Transfer learning helps mitigate this challenge by allowing the model to adapt to new scenarios based on prior knowledge. For example, a PINN trained on a set of deterministic branched flow problems can be fine-tuned to handle stochastic variations by incorporating additional data or adjusting the loss function to account for noise and uncertainty. This adaptability is crucial in real-world applications where the flow conditions can change unpredictably, such as in coastal engineering or atmospheric science [35].

The effectiveness of transfer learning in simulating branched flows is further supported by empirical evidence from various studies. For instance, research has shown that transfer learning can significantly improve the performance of PINNs in solving problems with complex geometries and dynamic boundary conditions. By leveraging the pre-trained model's ability to capture the underlying physics, the transfer learning approach can reduce the number of training iterations required to achieve a desired level of accuracy. This is particularly beneficial in scenarios where the computational resources are limited or the problem size is large. Additionally, the use of multi-head models enhances the model's capacity to handle multiple outputs, making it more versatile for a wide range of applications [35].

Another important aspect of using transfer learning with PINNs for branched flows is the need to balance the contributions of different loss terms. In traditional PINNs, the loss function typically includes terms for the PDE residuals, boundary conditions, and initial conditions. When applying transfer learning, it is essential to ensure that the pre-trained model's knowledge is effectively transferred to the new task without overfitting or underfitting. This requires careful tuning of the loss weights and the selection of appropriate hyperparameters. Studies have shown that adaptive loss balancing techniques, such as those based on gradient norms or curvature information, can help achieve this balance and improve the overall performance of the model [54].

Moreover, the integration of transfer learning with PINNs can also lead to improved generalization capabilities. By training on a diverse set of problems, the model becomes more robust to variations in the input data and can generalize better to unseen scenarios. This is particularly important in the context of branched flows, where the flow patterns can vary significantly depending on the environmental conditions. Transfer learning allows the model to learn the underlying physical principles that govern the flow dynamics, enabling it to make accurate predictions even in the absence of extensive training data [35].

In conclusion, the application of transfer learning with PINNs to simulate stochastic branched flows represents a significant advancement in fluid mechanics. By leveraging the pre-trained model's knowledge and using multi-head architectures, the approach offers a powerful tool for capturing the complex and dynamic nature of branched flows. The combination of transfer learning and multi-head models not only improves the efficiency and accuracy of simulations but also enhances the model's adaptability to different scenarios. As the field of fluid mechanics continues to evolve, the integration of transfer learning with PINNs is expected to play a crucial role in addressing the challenges of simulating complex and uncertain flow dynamics. The ongoing research in this area is likely to lead to further innovations and improvements, making PINNs an even more valuable tool for fluid mechanics applications.

### 3.7 Unsteady Flows Past Moving Bodies

Unsteady flows past moving bodies present a significant challenge in fluid mechanics due to the dynamic nature of the flow field and the complex interactions between the fluid and the moving boundaries. Traditional numerical methods, such as finite element and finite volume methods, often struggle with these problems due to the need for frequent mesh updates and the computational expense of simulating the evolving geometry. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering a flexible and efficient approach to simulate unsteady flows past moving bodies. By integrating the governing equations of fluid dynamics into the neural network architecture, PINNs can accurately capture the dynamic behavior of the flow without the need for explicit meshing. This section discusses the development of moving-boundary-enabled PINNs and the comparison of different variants for accuracy and performance.

One of the key challenges in simulating unsteady flows past moving bodies is the accurate representation of the moving boundaries within the neural network. Traditional PINNs, which are designed to solve static PDEs, are not directly applicable to problems involving moving boundaries. To address this, researchers have developed moving-boundary-enabled PINNs (MB-PINNs) that incorporate the dynamics of the moving boundaries into the loss function. These models are trained to account for the time-dependent motion of the boundaries, ensuring that the solution remains consistent with the physical laws governing the flow. For instance, the paper titled "Physics-informed neural networks modeling for systems with moving immersed boundaries application to an unsteady flow past a plunging foil" [86] describes the development of a moving-boundary-enabled PINN framework that effectively simulates unsteady flows past a plunging foil. The model was able to accurately capture the complex flow patterns and demonstrate the feasibility of PINNs for such problems.

The accuracy and performance of moving-boundary-enabled PINNs have been evaluated through various studies. For example, the paper titled "Physics-informed neural networks (PINNs) for fluid mechanics A review" [29] highlights the effectiveness of PINNs in simulating unsteady flows past moving bodies. The study emphasizes that PINNs can handle the dynamic nature of the flow by incorporating the time-dependent motion of the boundaries into the loss function, leading to accurate predictions of the flow field. Additionally, the paper titled "Physics-informed neural networks for solving Navier-Stokes equations" [3] discusses the use of PINNs to solve the Navier-Stokes equations for unsteady flows past moving bodies, demonstrating the ability of PINNs to capture the complex interactions between the fluid and the moving boundaries.

Comparisons between different variants of moving-boundary-enabled PINNs have also been conducted to evaluate their performance. The paper titled "Physics-informed neural networks for solving Navier-Stokes equations" [3] presents a detailed comparison of different PINN variants, including the standard Navier-Stokes-based PINN (MB-PINN) and the immersed boundary method (IBM)-based PINN (MB-IBM-PINN). The study found that MB-PINN outperformed MB-IBM-PINN when the a priori knowledge of the solid body position and velocity was available. However, the performance of MB-IBM-PINN could be improved by using a suitable combination of physics constraint relaxation and physics-based sampling. This highlights the importance of careful design and training strategies in developing moving-boundary-enabled PINNs.

Another critical aspect of simulating unsteady flows past moving bodies is the ability to handle the temporal evolution of the flow field. Traditional PINNs, which are designed to solve static PDEs, may not be well-suited for problems with time-dependent solutions. To address this, researchers have proposed variants of PINNs that incorporate temporal components into the loss function. For example, the paper titled "PINNsFormer A Transformer-Based Framework For Physics-Informed Neural Networks" [87] introduces a Transformer-based framework that can effectively capture the temporal dependencies in unsteady flows. The study demonstrates that the proposed framework can accurately approximate PDE solutions by utilizing multi-head attention mechanisms to capture temporal dependencies, leading to improved performance in simulating unsteady flows past moving bodies.

The performance of moving-boundary-enabled PINNs has also been evaluated in the context of real-world applications. For instance, the paper titled "Physics-informed neural networks for solving Navier-Stokes equations" [3] discusses the application of PINNs to simulate unsteady flows past moving bodies in various engineering scenarios. The study highlights the ability of PINNs to handle complex geometries and dynamic boundary conditions, making them suitable for a wide range of applications. Additionally, the paper titled "Physics-informed neural networks for solving Navier-Stokes equations" [3] demonstrates the effectiveness of PINNs in simulating unsteady flows past moving bodies, showing that they can produce accurate results even with limited data.

In summary, the development of moving-boundary-enabled PINNs has shown great promise in simulating unsteady flows past moving bodies. These models integrate the governing equations of fluid dynamics into the neural network architecture, allowing for accurate and efficient simulations without the need for explicit meshing. The comparison of different variants of moving-boundary-enabled PINNs has highlighted the importance of careful design and training strategies. The ability of PINNs to capture the temporal evolution of the flow field and handle complex geometries makes them a valuable tool for simulating unsteady flows past moving bodies. As research in this area continues to advance, the potential of PINNs to revolutionize the simulation of unsteady flows past moving bodies is becoming increasingly evident.

### 3.8 Simulating and Inferring Navier-Stokes Equations with Deep Learning

[82]

Simulating and inferring the Navier-Stokes equations is a critical area in fluid mechanics, where deep learning has shown promise in improving both accuracy and efficiency. Traditional numerical methods such as finite difference and finite element methods have been widely used, but they often face challenges in handling complex geometries, high-dimensional problems, and non-smooth or fractional scenarios. In this context, deep learning techniques have emerged as a viable alternative, particularly through methods like the Deep Random Vortex Method (DRVM), which combine neural networks with random vortex dynamics to solve Navier-Stokes equations effectively. This section explores such approaches and their implications for simulating and inferring fluid dynamics.

The Deep Random Vortex Method (DRVM) is one of the innovative techniques that leverage the power of neural networks to solve the Navier-Stokes equations, especially in non-smooth or fractional scenarios. Unlike traditional methods that rely on structured grids, DRVM utilizes a random vortex approach to approximate the fluid flow. This method represents the fluid as a collection of vortices, each of which is modeled by a neural network. By training these networks to predict the evolution of vortices over time, DRVM can capture complex fluid behaviors that are difficult to model with conventional approaches. The integration of random vortex dynamics with deep learning allows for a more flexible and adaptive representation of the fluid field, making it particularly suitable for problems with irregular geometries or turbulent flows.

One of the key advantages of DRVM is its ability to handle non-smooth or fractional scenarios, which are common in real-world fluid dynamics problems. Traditional numerical methods often struggle with discontinuities and singularities, leading to inaccuracies in the solution. In contrast, DRVM's use of random vortices enables it to naturally accommodate such features, as the vortices can be adjusted dynamically to capture the fluid's behavior. This adaptability is further enhanced by the use of deep learning, which allows the model to learn from data and improve its predictions over time. The combination of random vortex dynamics and neural networks provides a robust framework for simulating complex fluid flows, including those with high Reynolds numbers and intricate boundary conditions.

Recent studies have demonstrated the effectiveness of DRVM in simulating and inferring the Navier-Stokes equations. For instance, the paper titled "A Deep Fourier Residual Method for solving PDEs using Neural Networks" [88] discusses the use of neural networks to solve partial differential equations (PDEs) by incorporating the residual of the equation into the loss function. This approach not only improves the accuracy of the solution but also enhances the model's ability to handle non-smooth and fractional scenarios. The paper highlights the importance of using the $H^{-1}$ norm of the residual to accurately compute the error, which is particularly useful in cases where the solution lacks $H^{2}$ regularity. The insights from this study can be applied to DRVM, as the method's reliance on neural networks and the residual of the PDE aligns with the principles of the Deep Fourier Residual method.

Another significant contribution to the field of deep learning for fluid dynamics is the paper titled "Neural Spectral Methods: Self-supervised learning in the spectral domain" [89], which introduces a technique that uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. This method leverages the efficiency of spectral methods to solve PDEs and demonstrates superior performance in terms of speed and accuracy. The paper's approach can be integrated with DRVM by using spectral coefficients to represent the vortices, enabling the model to capture both the global and local features of the fluid flow. This integration could lead to more accurate simulations and better generalization to unseen scenarios, as the spectral representation allows for efficient computation and reduced computational complexity.

The paper titled "Physics-Informed Neural Networks for Obstacle-Related Equations" [25] also provides valuable insights into the application of deep learning for simulating fluid dynamics. The study focuses on the use of PINNs to solve obstacle-related PDEs, which are challenging due to the need for accurate approximations of solutions above a given obstacle. The paper highlights the importance of incorporating physical laws into the loss function of the neural network, which is a fundamental principle of DRVM. By embedding the Navier-Stokes equations into the loss function, DRVM ensures that the model's predictions are consistent with the underlying physics, leading to more reliable simulations.

Furthermore, the paper titled "Learning in Sinusoidal Spaces with Physics-Informed Neural Networks" [57] explores the benefits of learning in sinusoidal spaces to improve the accuracy of PINNs. The study shows that the sinusoidal mapping of inputs can increase input gradient variability, which helps the model avoid being trapped in local minima. This finding is particularly relevant to DRVM, as the method's reliance on neural networks and the use of random vortices can benefit from similar techniques to enhance the model's ability to capture complex fluid dynamics.

The paper titled "Robust Physics Informed Neural Networks" [90] discusses the development of a robust version of PINNs that can accurately approximate the solutions of PDEs. The study introduces a loss function that incorporates the residual and the inverse of the Gram matrix, computed using the energy norm. This approach ensures that the loss function is closely aligned with the true error of the solution, leading to improved training stability and accuracy. The principles of this method can be applied to DRVM, as the integration of physical laws into the loss function is crucial for capturing the dynamics of fluid flows accurately.

In addition, the paper titled "The Deep Random Vortex Method (DRVM)" [91] provides a comprehensive overview of the DRVM approach and its applications. The study highlights the method's ability to simulate complex fluid flows by combining neural networks with random vortex dynamics. The paper also discusses the challenges and limitations of the method, such as the need for large amounts of training data and the computational cost of training deep neural networks. Despite these challenges, the study demonstrates that DRVM can achieve high accuracy and efficiency in simulating Navier-Stokes equations, making it a promising technique for fluid dynamics.

In conclusion, the Deep Random Vortex Method (DRVM) represents a significant advancement in the application of deep learning to simulate and infer the Navier-Stokes equations. By combining neural networks with random vortex dynamics, DRVM offers a flexible and adaptive framework for capturing complex fluid behaviors. The integration of principles from other deep learning techniques, such as the Deep Fourier Residual method and Neural Spectral Methods, further enhances the accuracy and efficiency of the method. As research in this area continues to evolve, the development of robust and scalable deep learning approaches will play a crucial role in advancing the field of fluid mechanics.

### 3.9 Parametric Solutions of Navier-Stokes Equations

The application of Physics-Informed Neural Networks (PINNs) in learning solution functions of parametric Navier-Stokes equations represents a significant advancement in computational fluid dynamics. Traditional numerical methods often struggle with the high computational cost and the complexity of handling parameterized systems, especially when the solution needs to be interpolated across a wide range of parameters. PINNs, on the other hand, offer a flexible and efficient approach by embedding the governing equations into the loss function, enabling the neural network to learn the solution function for varying parameters without the need for retraining from scratch. This parametric capability makes PINNs a powerful tool for a variety of applications, from aerodynamic design optimization to fluid-structure interaction problems.

In parametric Navier-Stokes problems, the governing equations remain the same, but the parameters, such as boundary conditions, initial conditions, or material properties, can vary. PINNs address this challenge by incorporating the parameters into the input of the neural network, allowing the model to learn a solution function that is valid for a wide range of parameter values. This approach not only reduces the computational burden but also enhances the generalization capability of the model, making it suitable for real-time applications and uncertainty quantification. By training a single neural network to handle multiple parameter configurations, PINNs provide a unified framework for solving parametric PDEs, which is particularly beneficial in scenarios where the solution needs to be computed for different operating conditions.

One of the key advantages of using PINNs for parametric Navier-Stokes equations is the ability to interpolate the solution functions across a range of parameters. This interpolation is achieved by training the network on a set of parameterized problems, where each problem corresponds to a different set of parameters. The neural network learns to map the input parameters to the corresponding solution fields, effectively creating a parametric solution function. This interpolation capability is particularly useful in scenarios where the solution is required for a large number of parameter combinations, such as in optimization and sensitivity analysis. By leveraging the learned parametric solution function, PINNs can quickly generate solutions for new parameter values, significantly reducing the computational time compared to traditional numerical methods.

Recent studies have demonstrated the effectiveness of PINNs in solving parametric Navier-Stokes equations. For instance, the work in [57] highlights the importance of incorporating physical constraints into the loss function to ensure that the neural network learns the correct solution behavior. By using a sinusoidal mapping of the input variables, the authors show that the PINN can better capture the high-frequency components of the solution, which are often challenging to learn. This approach not only improves the accuracy of the solution but also enhances the convergence behavior of the network during training. The results of this study show that the proposed method achieves better performance in solving parametric Navier-Stokes equations compared to traditional PINN formulations.

Another study, [92], introduces a framework that utilizes latent representations of the PDE parameters as additional inputs to the neural network. This approach allows the model to learn a compressed latent representation of the parameter distribution, which can then be used to generate solutions for new parameter values. By training the neural network over a distribution of PDE parameters, the authors demonstrate that the model can achieve accurate solutions without the need for additional training. This method is particularly useful in scenarios where the parameter space is high-dimensional and the number of training samples is limited.

The application of PINNs to parametric Navier-Stokes equations also benefits from recent advancements in optimization and training strategies. For example, [68] proposes a scale-invariant optimizer that balances the loss terms parameter-wise, leading to improved convergence and accuracy. This optimizer is particularly effective in handling the imbalance between the PDE loss and the boundary loss, which is a common challenge in training PINNs for parametric problems. The results of this study show that the proposed MultiAdam optimizer significantly improves the predictive accuracy of PINNs for parametric Navier-Stokes equations, making it a valuable tool for practical applications.

In addition to optimization techniques, the use of adaptive sampling strategies has also been shown to enhance the performance of PINNs in solving parametric Navier-Stokes equations. [93] introduces a method for dynamically adjusting the placement of collocation points based on the residual of the PDE, leading to improved accuracy and convergence. This adaptive sampling strategy ensures that the neural network focuses on regions where the solution is more complex, thereby improving the overall quality of the solution. The results of this study demonstrate that the proposed method achieves better performance in solving parametric Navier-Stokes equations compared to uniform sampling approaches.

The parametric solution capability of PINNs is further enhanced by the integration of domain decomposition techniques. [70] proposes a domain decomposition approach that divides the problem into smaller subdomains, each solved by a separate PINN. This method not only improves the scalability of the PINN but also enhances the accuracy of the solution by leveraging the strengths of each subdomain. The results of this study show that the proposed domain decomposition method significantly improves the performance of PINNs in solving parametric Navier-Stokes equations, particularly in scenarios with complex geometries and multi-scale solutions.

Moreover, the use of physics-informed activation functions has been shown to improve the performance of PINNs in solving parametric Navier-Stokes equations. [72] introduces a method for learning adaptive activation functions that are tailored to the specific properties of the PDE. By incorporating prior knowledge about the PDE into the activation functions, the authors demonstrate that the PINN can achieve better accuracy and faster convergence. This approach is particularly effective in scenarios where the solution exhibits complex behavior, such as shocks and discontinuities, which are common in high-speed flows.

In summary, the application of PINNs to parametric Navier-Stokes equations offers a powerful and flexible approach for solving a wide range of fluid dynamics problems. By embedding the governing equations into the loss function and incorporating parameters into the input of the neural network, PINNs enable the interpolation of solution functions across a range of parameter values. Recent studies have demonstrated the effectiveness of various techniques, including adaptive sampling, domain decomposition, and physics-informed activation functions, in improving the performance of PINNs for parametric problems. These advancements make PINNs a valuable tool for a wide range of applications in fluid mechanics, from aerodynamic design to real-time flow simulation.

### 3.10 Real-Time Blood Flow Imaging with PINNs

---
Real-time blood flow imaging has emerged as a critical application in medical diagnostics, particularly in cardiology and vascular surgery. Traditional imaging techniques, such as Doppler ultrasound, have limitations in capturing high-resolution, real-time data, especially in complex and dynamic environments. Physics-Informed Neural Networks (PINNs) have shown significant promise in addressing these challenges by integrating physical laws with data-driven learning, enabling accurate and efficient solutions for fluid dynamics problems [94]. Specifically, PINNs have been applied to ultrafast ultrasound blood flow imaging, where they offer real-time training and accurate velocity recovery. This subsection explores the application of PINNs in this domain, focusing on the development of SeqPINN and SP-PINN for real-time training and accurate velocity recovery.

Ultrafast ultrasound imaging has revolutionized the way blood flow is visualized, allowing for high-frame-rate acquisitions that capture the dynamic nature of blood flow. However, the computational complexity of processing these large datasets in real-time remains a significant challenge. PINNs offer a novel solution by leveraging the power of deep learning to approximate the solutions to the Navier-Stokes equations, which govern fluid dynamics. By incorporating the physics of blood flow into the neural network's loss function, PINNs can accurately model the velocity field and other relevant parameters without the need for extensive computational resources. This approach has been particularly effective in applications where real-time feedback is essential, such as in interventional procedures and monitoring of hemodynamic changes [94].

The development of SeqPINN and SP-PINN has further advanced the capabilities of PINNs in real-time blood flow imaging. SeqPINN, which stands for Sequential Physics-Informed Neural Network, is designed to handle time-dependent problems by incorporating sequential data into the training process. This architecture allows the network to learn the temporal dynamics of blood flow, making it well-suited for applications where the flow field evolves over time. By training on sequences of ultrasound images, SeqPINN can capture the temporal variations in velocity and pressure, leading to more accurate and stable solutions. The effectiveness of SeqPINN has been demonstrated in various studies, where it has shown superior performance in capturing the complex dynamics of blood flow compared to traditional methods [94].

SP-PINN, or Spatial-Preserving Physics-Informed Neural Network, is another variant that has been developed to address the challenges of spatially varying flow fields. This architecture is designed to preserve the spatial structure of the flow field, ensuring that the neural network's predictions are consistent with the physical laws governing the system. By incorporating spatial constraints into the loss function, SP-PINN can accurately model the velocity field in regions with complex geometries, such as arteries with branching structures. The ability of SP-PINN to maintain spatial coherence has been shown to significantly improve the accuracy of velocity recovery, making it a valuable tool for applications where precise spatial information is crucial [94].

The integration of PINNs into real-time blood flow imaging has also been supported by advances in hardware and software. High-performance computing platforms, such as GPUs and TPUs, have enabled the efficient training and deployment of PINNs, allowing for real-time processing of large datasets. Additionally, the development of specialized libraries and frameworks has facilitated the implementation of PINNs, making it easier for researchers and clinicians to adopt these techniques in practical applications. These advancements have been instrumental in demonstrating the feasibility of PINNs for real-time blood flow imaging, as they enable the rapid computation of flow fields with high accuracy [94].

In addition to the development of SeqPINN and SP-PINN, researchers have explored various strategies to enhance the performance of PINNs in real-time applications. One such strategy involves the use of adaptive sampling techniques to optimize the placement of collocation points during training. By dynamically adjusting the distribution of collocation points based on the complexity of the flow field, these techniques can improve the accuracy of the neural network's predictions while reducing computational costs. This approach has been shown to be particularly effective in capturing high-frequency components of the flow field, which are often challenging to model with traditional methods [94].

Another important aspect of real-time blood flow imaging with PINNs is the ability to handle noisy and incomplete data. In clinical settings, ultrasound data can be affected by various sources of noise, such as motion artifacts and speckle noise. PINNs are well-suited to handle these challenges by incorporating physical constraints into the training process, which helps to regularize the model and improve its robustness. The use of physics-informed loss functions ensures that the neural network's predictions are consistent with the underlying physical laws, even in the presence of noisy data. This capability has been demonstrated in several studies, where PINNs have shown superior performance in recovering accurate velocity fields from corrupted datasets [94].

The application of PINNs in real-time blood flow imaging also highlights the potential for integrating domain knowledge into deep learning models. By encoding the physical laws governing blood flow into the neural network's architecture, PINNs can learn to respect the constraints imposed by the underlying physics. This approach not only improves the accuracy of the model but also enhances its interpretability, making it easier for clinicians to understand and trust the results. The ability of PINNs to incorporate domain-specific knowledge has been shown to be a key factor in their success in real-time applications [94].

Overall, the use of PINNs in real-time blood flow imaging represents a significant advancement in the field of medical diagnostics. The development of SeqPINN and SP-PINN has enabled the accurate modeling of complex flow fields, while the integration of adaptive sampling and noise handling techniques has improved the robustness of the models. These innovations have the potential to transform the way blood flow is visualized and analyzed, offering clinicians a powerful tool for real-time monitoring and decision-making. As research in this area continues to advance, the role of PINNs in real-time blood flow imaging is expected to grow, paving the way for new applications and improvements in patient care [94].
---

### 3.11 Solving 3D Flow-Thermal Problems with Sparse Data

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving complex fluid-thermal problems, especially in cases where traditional numerical methods face significant computational and data challenges. One of the critical areas where PINNs have demonstrated transformative potential is in solving 3D flow-thermal problems with sparse domain data. These problems often arise in engineering and industrial applications where high-fidelity experimental data is limited or expensive to obtain. PINNs offer a data-efficient alternative by incorporating physical laws directly into the neural network training process, enabling accurate predictions even with limited input data. This section highlights the use of PINNs in addressing 3D flow-thermal problems, with a focus on hybrid data-PINNs approaches that leverage both sparse data and physical constraints for surrogate modeling of complex designs.

In 3D flow-thermal problems, the interplay between fluid dynamics and heat transfer introduces additional complexity, especially when the domain geometry is intricate or the flow regime is highly transient. Traditional numerical methods, such as finite element or finite volume methods, often require significant computational resources to resolve the spatial and temporal variations of the flow field, particularly in high-dimensional scenarios. PINNs provide a promising alternative by embedding the governing equations—such as the Navier-Stokes and energy equations—into the loss function. This allows the neural network to approximate the solution while ensuring consistency with the underlying physics, even when the available data is sparse. The integration of physics-based constraints not only reduces the dependency on large datasets but also enhances the robustness of the model against noise and uncertainty in the input data.

One notable approach that has gained attention in this context is the hybrid data-PINNs methodology. This approach combines the strengths of data-driven learning with physics-informed constraints, enabling efficient surrogate modeling of complex flow-thermal systems. In this framework, a PINN is trained using both sparse domain data and the governing equations, with the loss function consisting of a weighted sum of data-fidelity and physics-fidelity terms. The data-fidelity term ensures that the model adheres to the available measurements, while the physics-fidelity term enforces the satisfaction of the governing PDEs. This hybrid formulation allows PINNs to generalize well even when the data is sparse, as the physical constraints act as a regularizer that guides the model toward physically meaningful solutions. The effectiveness of this approach has been demonstrated in various applications, including the simulation of 3D turbulent flows and heat transfer in complex geometries [95].

The use of hybrid data-PINNs for 3D flow-thermal problems is particularly advantageous in scenarios where high-resolution data is not available. For instance, in the case of 3D flow-thermal problems with sparse domain data, PINNs can leverage the physics-based loss terms to fill in the gaps in the data. This is achieved by minimizing the residual of the governing PDEs at a set of collocation points distributed throughout the domain. The collocation points are strategically selected to ensure that the model captures the essential features of the solution, even in regions where no data is available. This approach has been shown to significantly reduce the computational cost of training PINNs, as it avoids the need for extensive meshing or discretization of the domain [96].

Moreover, the hybrid data-PINNs approach is well-suited for surrogate modeling of complex designs in engineering applications. Surrogate models are essential for rapid design optimization and sensitivity analysis, especially in scenarios where high-fidelity simulations are computationally prohibitive. PINNs, when combined with hybrid data strategies, can serve as efficient surrogates by learning the mapping between input parameters and the corresponding flow-thermal responses. This is particularly useful in problems involving multi-physics coupling, such as conjugate heat transfer in porous media or fluid-structure interaction. The ability of PINNs to incorporate physical constraints ensures that the surrogate model remains accurate and physically consistent, even when trained on sparse or noisy data [95].

Another key advantage of using PINNs for 3D flow-thermal problems with sparse data is their ability to handle heterogeneous and non-uniform domain geometries. Traditional numerical methods often struggle with complex geometries, requiring extensive mesh generation and refinement to capture the solution accurately. In contrast, PINNs are mesh-free and can naturally accommodate irregular domains by relying on the collocation points to approximate the solution. This makes them particularly suitable for applications involving complex industrial systems, such as heat exchangers, combustion chambers, or electronic cooling systems. By integrating the governing equations into the loss function, PINNs can accurately model the flow and thermal behavior in these systems without the need for explicit meshing, further enhancing their efficiency and flexibility [95].

In addition to their data efficiency, PINNs have shown promise in improving the accuracy of 3D flow-thermal simulations by leveraging advanced training strategies. Techniques such as adaptive sampling and multi-fidelity learning have been employed to enhance the performance of PINNs in handling sparse data. Adaptive sampling involves dynamically adjusting the distribution of collocation points during training to focus on regions where the model is making significant errors, thereby improving the overall accuracy of the solution. Multi-fidelity learning, on the other hand, combines data from different sources, such as high-fidelity simulations and low-fidelity measurements, to train the PINN more effectively. These strategies have been shown to significantly improve the convergence and robustness of PINNs in 3D flow-thermal problems, especially when the available data is limited [97].

In conclusion, the use of PINNs for solving 3D flow-thermal problems with sparse domain data has opened new avenues for efficient and accurate simulations in fluid mechanics. The hybrid data-PINNs approach, which combines sparse data with physics-based constraints, provides a powerful framework for surrogate modeling of complex designs. This methodology not only reduces the computational cost of training PINNs but also enhances their ability to generalize and capture the essential features of the solution. As research in this area continues to advance, the integration of PINNs with advanced training strategies and hybrid data approaches will play a crucial role in expanding their applicability to a wider range of fluid-thermal problems.

### 3.12 Enhancing Arterial Blood Flow Simulations

The simulation of arterial blood flow is a critical task in computational hemodynamics, with applications ranging from understanding cardiovascular diseases to optimizing medical interventions. Traditional computational fluid dynamics (CFD) methods, while accurate, often suffer from high computational costs, especially when modeling complex geometries and dynamic flow conditions. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering the potential to reduce computational costs while maintaining physical consistency. However, standard PINNs can struggle with certain challenges, such as backflow instabilities, which are common in arterial flow simulations due to the complex interaction between pressure and velocity fields. To address these challenges, researchers have introduced weighted PINNs (WXPINNs and WCPINNs), which enhance the performance of PINNs by incorporating tailored weighting strategies to improve stability and accuracy in hemodynamic simulations.

Weighted PINNs, specifically WXPINNs and WCPINNs, are designed to tackle the limitations of traditional PINNs in scenarios involving complex flow dynamics, such as those encountered in arterial blood flow. These weighted PINNs modify the standard PINN framework by assigning different weights to the loss function components that correspond to different physical constraints, such as the Navier-Stokes equations, boundary conditions, and initial conditions. This weighting allows the model to prioritize certain aspects of the solution that are more critical for accuracy, particularly in regions where traditional PINNs may fail due to instabilities or poor convergence. For example, in cases involving backflow instabilities, which are common in arterial flow simulations, the weighting mechanism can emphasize the enforcement of physical constraints, ensuring that the model adheres to the underlying physics and avoids unphysical solutions. This approach is particularly effective in scenarios where the flow field exhibits high variability or where the geometry is complex, such as in stenosed arteries or branched vascular networks [98].

One of the key advantages of WXPINNs and WCPINNs is their ability to circumvent backflow instabilities, which are a major challenge in traditional CFD simulations and can also affect standard PINNs. Backflow instabilities occur when the flow reverses direction, leading to oscillatory or divergent solutions that are difficult to resolve numerically. Traditional PINNs may struggle to handle such scenarios due to the nonlinearity of the Navier-Stokes equations and the sensitivity of the loss function to the choice of collocation points. Weighted PINNs address this issue by adjusting the weight of the loss function components that correspond to the pressure and velocity fields, ensuring that the model remains stable even in the presence of complex flow features. This is achieved by incorporating physical insights into the loss function, which can be tailored to the specific problem at hand. For instance, in the context of arterial blood flow simulations, the weighting can be designed to emphasize the conservation of mass and momentum in regions where backflow is likely to occur, thereby improving the robustness of the solution [98].

The performance of weighted PINNs has been demonstrated in various studies, where they outperform traditional PINNs in terms of accuracy, stability, and computational efficiency. For example, in a comparative study, WXPINNs and WCPINNs were applied to simulate blood flow in a complex arterial geometry, and the results showed that they were able to capture the flow dynamics with higher accuracy compared to standard PINNs. This was attributed to the weighted loss function, which allowed the model to better balance the contributions of different physical constraints and avoid the pitfalls of overfitting or underfitting. Additionally, the weighted PINNs were found to be more resilient to noisy or sparse data, which is a common issue in real-world hemodynamic simulations where high-quality measurements may be limited. This makes them particularly suitable for applications where data availability is a constraint, such as in personalized medicine or clinical settings.

Another important aspect of weighted PINNs is their ability to improve the overall performance of PINNs in hemodynamic simulations by reducing the computational burden. Traditional PINNs often require a large number of collocation points to achieve accurate solutions, which can significantly increase the training time and resource requirements. Weighted PINNs, on the other hand, can achieve comparable or better accuracy with fewer collocation points by strategically distributing the computational effort based on the importance of different regions of the solution domain. This is particularly beneficial in scenarios where the flow field exhibits localized features, such as recirculation zones or high-shear regions, where traditional PINNs may require excessive collocation points to capture the dynamics accurately. By focusing the computational resources on the most critical regions, weighted PINNs can reduce the overall computational cost while maintaining the physical consistency of the solution.

The success of weighted PINNs in hemodynamic simulations highlights the potential of incorporating physical knowledge into the training process of PINNs. This is a key aspect of PINNs, as their strength lies in their ability to enforce physical constraints through the loss function. However, the effectiveness of this approach depends on the choice of loss function and the weighting strategy used. Weighted PINNs provide a flexible framework for incorporating domain-specific knowledge, allowing researchers to tailor the training process to the specific characteristics of the problem at hand. This makes them particularly well-suited for applications in fluid mechanics, where the governing equations and boundary conditions can vary significantly depending on the scenario. For instance, in arterial blood flow simulations, the weighting can be adjusted to account for the effects of vessel elasticity, wall compliance, and other biomechanical factors that influence the flow dynamics.

In conclusion, the use of weighted PINNs (WXPINNs and WCPINNs) represents a significant advancement in the application of PINNs to hemodynamic simulations. By introducing a weighted loss function that prioritizes the enforcement of physical constraints, these models address the limitations of traditional PINNs in capturing complex flow dynamics, particularly in scenarios involving backflow instabilities. The enhanced stability and accuracy of weighted PINNs make them a promising tool for improving the efficiency and reliability of arterial blood flow simulations, with potential applications in both research and clinical settings. As the field of PINNs continues to evolve, the development of more sophisticated weighting strategies and domain-specific adaptations will be crucial in expanding their applicability to a wide range of fluid mechanics problems.

### 3.13 Incompressible Laminar Flow Simulations

Incompressible laminar flow simulations are a critical area of fluid mechanics, often encountered in various engineering applications such as aerodynamics, microfluidics, and heat transfer. Traditional numerical methods, such as finite difference and finite volume methods, are commonly used to solve the Navier-Stokes equations for incompressible flows. However, these methods face challenges in terms of computational cost, mesh generation, and the handling of complex geometries. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering a data-driven approach that integrates physical laws into the learning process. The application of PINNs to incompressible laminar flows has been explored in several studies, with a focus on improving trainability and solution accuracy through innovative strategies such as mixed-variable schemes.

One significant challenge in simulating incompressible laminar flows with PINNs is the satisfaction of the incompressibility condition, which is enforced through the continuity equation. This constraint introduces additional complexity to the training process, as the neural network must simultaneously approximate the velocity and pressure fields while ensuring that the divergence of the velocity field is zero. To address this challenge, researchers have proposed the use of mixed-variable schemes, where the neural network is trained to predict both the velocity and pressure fields directly. This approach helps to decouple the variables and improve the stability of the training process. For example, studies have shown that the mixed-variable scheme can lead to more accurate and stable solutions compared to single-variable approaches, where only the velocity field is predicted and the pressure is inferred indirectly [55].

The mixed-variable scheme is particularly effective in handling the coupling between the momentum and continuity equations. By treating the pressure and velocity fields as separate variables, the neural network can better capture the physical relationships between them. This is especially important in incompressible flows, where the pressure field plays a crucial role in maintaining the divergence-free condition. Furthermore, the mixed-variable approach allows for a more direct enforcement of the boundary conditions, which can improve the accuracy of the solution. For instance, in the context of laminar flow over a cylinder, the mixed-variable scheme has been shown to produce more accurate predictions of the velocity and pressure fields, particularly in regions with complex flow patterns [55].

Another key aspect of simulating incompressible laminar flows with PINNs is the choice of the loss function. The loss function in PINNs typically consists of terms that enforce the governing equations (e.g., the Navier-Stokes equations) and the boundary conditions. However, the balance between these terms can significantly impact the training process and the final solution. Studies have shown that the proper weighting of the loss terms is crucial for achieving convergence and accuracy. For example, the use of adaptive loss weighting strategies, where the weights are adjusted during training based on the current state of the solution, has been shown to improve the performance of PINNs in simulating incompressible flows [99].

In addition to the loss function, the choice of the neural network architecture also plays a critical role in the performance of PINNs for incompressible laminar flow simulations. The use of deep neural networks with appropriate activation functions can significantly enhance the ability of PINNs to capture the complex behavior of incompressible flows. For example, studies have demonstrated that the use of Gaussian activation functions can lead to better convergence and accuracy compared to traditional activation functions such as ReLU or sigmoid [55]. Furthermore, the use of convolutional neural networks (CNNs) has been explored as a means to improve the efficiency and accuracy of PINNs in simulating incompressible flows, particularly in cases involving complex geometries [100].

The training of PINNs for incompressible laminar flow simulations also involves challenges related to the convergence of the optimization algorithm. The non-convex nature of the loss function can lead to the presence of local minima, which can hinder the ability of the PINN to converge to the true solution. To address this issue, various optimization strategies have been proposed, including the use of second-order optimization methods and the incorporation of momentum-based techniques. For example, studies have shown that the use of the NysNewton-CG (NNCG) optimizer can significantly improve the performance of PINNs in solving incompressible flow problems [101]. Additionally, the use of adaptive learning rate strategies, such as the Adam optimizer, has been shown to be effective in navigating the complex loss landscape and achieving faster convergence.

The accuracy of PINNs in simulating incompressible laminar flows can also be influenced by the choice of collocation points, which are used to enforce the governing equations and boundary conditions. The distribution of these points can have a significant impact on the quality of the solution, as it determines how well the PINN can approximate the true solution. Studies have shown that the use of adaptive collocation point strategies, where the points are dynamically adjusted during training based on the current state of the solution, can lead to improved accuracy and efficiency [99]. This approach helps to ensure that the PINN focuses on regions where the solution is more complex, leading to a more accurate overall solution.

In summary, the application of PINNs to incompressible laminar flow simulations has shown great promise, with the use of mixed-variable schemes and other strategies helping to improve the trainability and accuracy of the solutions. The choice of the loss function, neural network architecture, and optimization strategy are all critical factors that influence the performance of PINNs in this context. By addressing the challenges associated with the incompressibility condition and the non-convex nature of the loss function, PINNs can provide a powerful tool for simulating incompressible laminar flows with high accuracy and efficiency. The continued development of these methods will be essential for expanding the applicability of PINNs to more complex and realistic fluid flow problems.

### 3.14 Blood Flow Inverse Problems

[82]

Blood flow inverse problems in the context of fluid mechanics involve the estimation of unknown parameters or the reconstruction of flow fields based on limited or noisy measurements. These problems are critical in biomedical applications, particularly in understanding hemodynamics in the cardiovascular system. Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for addressing such challenges, leveraging their ability to incorporate physical laws into the neural network architecture, thereby ensuring that the solutions adhere to the underlying governing equations. This subsection explores the use of PINNs in solving blood flow inverse problems, focusing on the estimation of reduced-order model parameters and the reconstruction of full velocity fields from noisy measurements in the ascending aorta.

One of the primary applications of PINNs in blood flow inverse problems is the estimation of reduced-order model parameters. Traditional numerical methods for solving such problems often rely on complex and computationally intensive simulations, which can be impractical for real-time or large-scale applications. PINNs offer a more efficient alternative by integrating the governing equations of fluid dynamics, such as the Navier-Stokes equations, into the loss function during training. This allows the neural network to learn the relationship between the input parameters and the resulting flow fields while ensuring that the solutions comply with the physical laws. For instance, in the context of blood flow, PINNs can be trained to estimate parameters such as wall shear stress, viscosity, or boundary conditions from limited measurements, which can then be used to predict the overall hemodynamic behavior of the system [31].

Another critical application of PINNs in blood flow inverse problems is the reconstruction of full velocity fields from noisy measurements in the ascending aorta. The ascending aorta is a complex region where the flow dynamics are influenced by multiple factors, including the geometry of the vessel, the properties of the blood, and the presence of turbulence. Accurate reconstruction of the velocity field in this region is essential for diagnosing and understanding various cardiovascular diseases. However, traditional methods for solving this problem often face challenges due to the limited availability of high-resolution data and the presence of noise in the measurements. PINNs address these challenges by incorporating the physical constraints of the Navier-Stokes equations into the training process, allowing the network to learn the underlying flow dynamics even from sparse and noisy data. This approach has been successfully applied in the context of ultrafast ultrasound blood flow imaging, where PINNs have demonstrated the ability to recover velocity fields with high accuracy [31].

The application of PINNs to blood flow inverse problems is not without its challenges. One of the main difficulties is the sensitivity of the neural network to the quality and quantity of the input data. In the context of blood flow, the measurements are often sparse and may be corrupted by noise, which can lead to inaccurate or nonphysical solutions. To address this issue, researchers have developed various strategies to improve the robustness of PINNs in such scenarios. For example, the use of adaptive sampling techniques has been shown to enhance the performance of PINNs by dynamically adjusting the placement of collocation points during training, ensuring that the network focuses on regions where the solution is most uncertain [34]. Additionally, the incorporation of physics-informed activation functions has been proposed to improve the efficiency and validity of PINNs by ensuring that the learned solutions are consistent with the physical constraints of the problem [102].

Another important aspect of solving blood flow inverse problems with PINNs is the integration of prior knowledge and domain-specific information. In the context of hemodynamics, this can include the use of anatomical data, physiological constraints, and known flow patterns to guide the training process. This approach not only improves the accuracy of the solutions but also enhances the interpretability of the results. For instance, the use of transfer learning has been explored to adapt pre-trained PINNs to new blood flow scenarios, reducing the need for retraining from scratch and improving the efficiency of the modeling process [103]. Furthermore, the use of hybrid models that combine PINNs with traditional numerical methods has been proposed to leverage the strengths of both approaches, ensuring that the solutions are both accurate and computationally efficient [80].

The potential of PINNs in solving blood flow inverse problems has been demonstrated through various case studies and empirical validations. For example, the application of PINNs to the estimation of reduced-order model parameters in the ascending aorta has shown promising results, with the networks being able to accurately predict the flow dynamics even from sparse and noisy data [31]. Similarly, the use of PINNs in reconstructing full velocity fields has been shown to outperform traditional data-driven models, particularly in scenarios where the data is incomplete or corrupted [9]. These results highlight the versatility and effectiveness of PINNs in addressing the challenges associated with blood flow inverse problems.

In addition to these applications, PINNs have also been used to develop reduced-order models (ROMs) for blood flow simulations, which can significantly reduce the computational cost while maintaining the accuracy of the results. These ROMs are particularly useful in scenarios where multiple simulations are required, such as in the design and optimization of medical devices or the evaluation of different treatment strategies. The integration of PINNs with POD-Galerkin methods has been proposed to enhance the performance of these models, allowing for the efficient estimation of parameters and the prediction of flow fields [104].

Overall, the application of PINNs to blood flow inverse problems represents a significant advancement in the field of fluid mechanics, particularly in the context of biomedical engineering. By leveraging the power of deep learning and the integration of physical laws, PINNs offer a robust and efficient alternative to traditional numerical methods, enabling the accurate estimation of unknown parameters and the reconstruction of full velocity fields from limited and noisy data. As research in this area continues to advance, it is likely that PINNs will play an increasingly important role in the development of new diagnostic tools and treatment strategies for cardiovascular diseases.

### 3.15 Frequency-Compensated PINNs for Fluid-Dynamic Design

Frequency-Compensated Physics-Informed Neural Networks (PINNs) represent an innovative approach to enhance the generalization performance of neural networks in fluid-dynamic design problems, leveraging Fourier features to accurately predict flow velocity and pressure. Traditional PINNs often struggle with capturing high-frequency components of fluid dynamics due to a phenomenon known as spectral bias, which leads to suboptimal solutions in complex or high-dimensional scenarios [105]. Frequency-Compensated PINNs address this limitation by incorporating Fourier features into the neural network architecture, enabling the model to better represent and learn high-frequency dynamics in the solution space.

The integration of Fourier features into PINNs is inspired by the success of spectral methods in numerical analysis, where high-frequency components are explicitly represented to improve accuracy and convergence. In the context of fluid dynamics, this approach is particularly valuable as many flow problems involve complex, time-varying phenomena such as turbulence, shock waves, and vortices. By using Fourier features, Frequency-Compensated PINNs can better capture the oscillatory nature of these phenomena, leading to more accurate predictions of flow velocity and pressure fields [57].

One of the key advantages of Frequency-Compensated PINNs is their ability to improve the generalization performance of the model in fluid-dynamic design problems. Traditional PINNs often require extensive training data and careful tuning of hyperparameters to achieve satisfactory results. In contrast, Frequency-Compensated PINNs can achieve better performance with fewer training samples due to their enhanced ability to represent high-frequency components. This is particularly useful in fluid dynamics, where obtaining high-quality, high-resolution data can be computationally expensive and time-consuming.

The use of Fourier features in Frequency-Compensated PINNs also helps to mitigate the issue of spectral bias, which is a common problem in deep learning models. Spectral bias refers to the tendency of neural networks to learn low-frequency components of the solution more effectively than high-frequency components. This can lead to inaccurate predictions in scenarios where high-frequency dynamics are critical, such as in turbulent flows or rapidly changing flow conditions. By explicitly incorporating Fourier features, Frequency-Compensated PINNs can overcome this limitation and achieve more balanced learning across all frequency components [105].

In addition to improving generalization, Frequency-Compensated PINNs offer a more efficient training process compared to traditional PINNs. The use of Fourier features allows the model to converge faster and with greater stability, as the high-frequency components are explicitly represented and learned. This is particularly beneficial in complex fluid dynamics problems where the solution space is highly non-linear and challenging to optimize. Furthermore, the improved stability of Frequency-Compensated PINNs reduces the risk of the model getting stuck in local optima, which is a common issue in deep learning.

The practical applications of Frequency-Compensated PINNs in fluid dynamics are diverse and include areas such as aerodynamic design, turbine blade optimization, and flow control. In aerodynamic design, for example, the ability to accurately predict flow velocity and pressure fields is crucial for optimizing the performance of aircraft and other vehicles. Frequency-Compensated PINNs can provide more accurate and reliable predictions, enabling engineers to design more efficient and aerodynamically superior structures. Similarly, in turbine blade optimization, the ability to capture high-frequency flow dynamics can lead to improved efficiency and reduced energy losses.

Another significant advantage of Frequency-Compensated PINNs is their ability to handle problems with varying spatial and temporal scales. Fluid dynamics problems often involve multiple scales, from large-scale flow structures to small-scale vortices and turbulence. Traditional PINNs may struggle to capture these multiscale dynamics due to the limitations of their architecture and training process. Frequency-Compensated PINNs, on the other hand, can effectively represent and learn these multiscale features by leveraging Fourier features to capture the high-frequency components of the solution. This makes them particularly suitable for problems where the solution involves both macroscopic and microscopic scales.

The effectiveness of Frequency-Compensated PINNs has been demonstrated through various case studies and numerical experiments. For instance, in the simulation of 2D and 3D incompressible flows, Frequency-Compensated PINNs have shown significant improvements in accuracy and convergence compared to traditional PINNs. These results highlight the potential of Frequency-Compensated PINNs to revolutionize the way fluid dynamics problems are solved, offering a more efficient and accurate alternative to conventional numerical methods.

Moreover, Frequency-Compensated PINNs can be seamlessly integrated with existing fluid dynamics solvers and simulation tools. This makes them a valuable addition to the toolkit of engineers and researchers working in the field of computational fluid dynamics. By combining the strengths of PINNs with the advantages of Fourier features, Frequency-Compensated PINNs can provide a powerful and flexible approach for solving a wide range of fluid dynamics problems.

In summary, Frequency-Compensated PINNs represent a significant advancement in the application of physics-informed neural networks to fluid dynamics. By leveraging Fourier features to improve the generalization performance of the model, these networks can accurately predict flow velocity and pressure fields, even in complex and high-frequency scenarios. Their ability to overcome the limitations of traditional PINNs, such as spectral bias and training instability, makes them a promising tool for a wide range of fluid dynamics applications. As research in this area continues to evolve, Frequency-Compensated PINNs are expected to play an increasingly important role in the development of more efficient and accurate computational methods for fluid dynamics.

### 3.16 Nonlocal PINNs for Parametrized Nonlocal Operators

Nonlocal PINNs (nPINNs) represent an important advancement in the application of physics-informed neural networks (PINNs) to problems involving nonlocal operators and integral equations. Unlike traditional PINNs, which are typically designed to solve local partial differential equations (PDEs), nPINNs extend this framework to address integral equations, where the solution at a given point depends on the values of the solution over a region of the domain. This makes nPINNs particularly well-suited for applications involving nonlocal phenomena, such as nonlocal Poisson equations and turbulence models. These types of problems arise in various fields of fluid mechanics, including wall-bounded turbulence, where the behavior of the flow is influenced by long-range interactions between different regions of the domain [106].

The development of nPINNs is motivated by the need to accurately model systems where the governing equations are not purely local. For instance, in turbulence modeling, the Reynolds-averaged Navier-Stokes (RANS) equations are often used to describe the mean flow, but they require closure models to account for the effects of turbulence. Traditional closure models, such as the k-ε and k-ω models, are based on empirical assumptions and are often limited in their ability to capture complex flow behavior. In contrast, nPINNs can be used to infer the functional form of the closure terms directly from data, without relying on a priori assumptions. This data-driven approach allows for more accurate and flexible modeling of turbulent flows, particularly in cases where the turbulence is highly anisotropic or has strong nonlocal effects [106].

One of the key applications of nPINNs is in the solution of nonlocal Poisson equations, which are integral equations that describe phenomena such as electrostatic potential and heat conduction in materials with memory or long-range interactions. These equations are challenging to solve using traditional numerical methods because they involve integrals over the entire domain, which can be computationally expensive. nPINNs, however, can efficiently approximate the solution by embedding the integral operators into the neural network's loss function. This allows the network to learn the solution while respecting the nonlocal constraints of the problem. For example, in the context of wall-bounded turbulence, nPINNs can be used to model the effects of the wall on the turbulent flow, including the formation of boundary layers and the interaction between the mean flow and the turbulent fluctuations [106].

In addition to nonlocal Poisson equations, nPINNs have also been applied to turbulence models involving nonlocal operators. One such example is the nonlocal turbulence closure model, which accounts for the effects of turbulence at different scales and regions of the flow. These models are often formulated as integral equations, where the turbulence stress is expressed as a convolution of the velocity gradient with a kernel that captures the nonlocal interactions. nPINNs can be trained to approximate this kernel and its associated parameters, providing a data-driven approach to turbulence modeling that is both flexible and accurate. This is particularly valuable in complex flows where the turbulence is highly variable and cannot be captured by traditional closure models [106].

Another significant application of nPINNs is in the parameter and function inference for integral equations. In many fluid mechanics problems, the governing equations involve unknown parameters or functions that need to be inferred from data. For instance, in wall-bounded turbulence, the turbulent viscosity can vary spatially and temporally, and its functional form is not always known a priori. nPINNs can be used to infer these parameters and functions by incorporating them into the neural network's architecture and training the network to minimize the residual of the integral equation. This approach allows for the simultaneous estimation of the solution and the unknown parameters, making it a powerful tool for solving inverse problems in fluid dynamics [106].

The effectiveness of nPINNs in solving nonlocal problems has been demonstrated in several studies. For example, the work on nonlocal PINNs [106] highlights the ability of nPINNs to accurately approximate solutions to nonlocal equations, even in cases where the domain is complex or the equations are highly nonlinear. The authors show that by using a neural network to represent the solution and the integral operators, nPINNs can efficiently capture the nonlocal dependencies and produce accurate results with relatively few training samples. This is a significant advantage over traditional numerical methods, which often require a large number of grid points to resolve the nonlocal interactions.

Moreover, the integration of nPINNs with traditional numerical methods has shown promising results in improving the accuracy and efficiency of fluid mechanics simulations. For instance, the work on domain decomposition-based coupling of physics-informed neural networks via the Schwarz alternating method [73] demonstrates how nPINNs can be combined with finite element and finite difference methods to solve complex PDEs. This hybrid approach leverages the strengths of both methods, allowing for the efficient solution of large-scale problems with nonlocal operators.

In summary, the development of nonlocal PINNs (nPINNs) has expanded the applicability of physics-informed neural networks to problems involving integral equations and nonlocal operators. These networks are particularly effective in solving nonlocal Poisson equations and turbulence models, where the solution depends on the values of the solution over a region of the domain. The ability of nPINNs to infer parameters and functions in integral equations makes them a valuable tool for modeling complex fluid dynamics problems. As the field of physics-informed neural networks continues to evolve, the application of nPINNs to nonlocal problems is expected to play an increasingly important role in advancing the accuracy and efficiency of fluid mechanics simulations.

### 3.17 Temporal Consistency in PINNs

Temporal consistency in Physics-Informed Neural Networks (PINNs) is a critical aspect in ensuring the accuracy and robustness of solutions for time-dependent fluid dynamics problems, particularly in the context of the Navier-Stokes equations. Traditional PINN formulations often struggle with maintaining consistency across different time steps, especially when dealing with complex, high-frequency dynamics or noisy data. To address this, the concept of temporal consistency loss has been introduced, aiming to scale mean squared loss terms appropriately and enhance the model's ability to produce physically consistent solutions over time [107].

The core idea of temporal consistency loss is to ensure that the neural network's solution remains consistent with the physical laws governing the system across different time intervals. In time-dependent PDEs like the Navier-Stokes equations, the solution evolves over time, and any inconsistencies in the temporal evolution can lead to physically unrealistic results or divergence in the model. Temporal consistency loss helps to mitigate these issues by explicitly incorporating the temporal structure of the problem into the training process. Instead of relying solely on automatic differentiation to compute temporal derivatives, this method employs a backward Euler discretization, which introduces a scaling term for the equations. This approach ensures that the loss terms are appropriately balanced across different time steps, leading to more accurate and stable solutions [107].

One of the key advantages of temporal consistency loss is its ability to improve the robustness of PINNs to noise and spatial resolution. In many fluid dynamics problems, the input data can be noisy or sparse, making it challenging for the model to learn the underlying physical laws accurately. Temporal consistency loss helps to regularize the training process by enforcing consistency across time steps, which in turn reduces the impact of noise on the final solution. This is particularly beneficial in scenarios where the data is incomplete or corrupted, as it allows the model to maintain a stable and accurate solution even in the presence of such imperfections [107].

Another important aspect of temporal consistency is its role in capturing the temporal evolution of complex fluid flows. In cases where the flow exhibits chaotic or turbulent behavior, the solution can change rapidly and unpredictably over time. Temporal consistency loss helps to ensure that the model can capture these rapid changes by maintaining a consistent evolution of the solution across time steps. This is achieved by scaling the mean squared loss terms in a way that accounts for the temporal dynamics of the system. By doing so, the model is better equipped to handle the challenges posed by high-frequency and complex flows, leading to more accurate predictions [107].

The implementation of temporal consistency loss typically involves modifying the loss function to include terms that explicitly account for the temporal evolution of the solution. This can be done by introducing a scaling factor that adjusts the contribution of the mean squared loss terms at each time step. The scaling factor is derived based on the time step size and the number of time steps, ensuring that the loss terms are appropriately balanced. This approach not only improves the accuracy of the solution but also enhances the convergence behavior of the model, making it more efficient and reliable for time-dependent problems [107].

In addition to improving the accuracy and robustness of PINNs, temporal consistency loss also plays a crucial role in enhancing the model's ability to generalize to new or unseen scenarios. By enforcing consistency across time steps, the model is better able to capture the underlying physical laws that govern the system, rather than just fitting the training data. This leads to improved generalization performance, as the model is not overfitting to the specific details of the training data but instead learning the more general patterns that are consistent with the physical laws [107].

The effectiveness of temporal consistency loss has been demonstrated in a variety of fluid dynamics problems, including the simulation of incompressible flows, turbulent flows, and high-speed flows. In particular, it has been shown to significantly improve the accuracy of PINNs in solving the Navier-Stokes equations, especially in cases where the solution exhibits strong temporal variations or high-frequency components [107]. This makes it a valuable tool for a wide range of applications in fluid mechanics, from aerodynamics to biomedical flows.

Furthermore, the use of temporal consistency loss can be combined with other techniques to further enhance the performance of PINNs. For example, it can be integrated with adaptive sampling strategies to ensure that the model is trained on the most informative time steps, leading to more efficient and accurate solutions. It can also be used in conjunction with transfer learning to improve the model's ability to adapt to new problems or domains, leveraging the knowledge gained from previous training sessions [107].

In conclusion, temporal consistency loss is a vital component in the development of accurate and robust PINNs for time-dependent fluid dynamics problems. By ensuring that the model's solution remains consistent with the physical laws governing the system across different time steps, it helps to improve the accuracy, robustness, and generalization performance of PINNs. As the field of physics-informed machine learning continues to evolve, the integration of temporal consistency loss into PINN formulations is expected to play a key role in advancing the capabilities of these models for a wide range of applications in fluid mechanics and beyond.

### 3.18 PINNs as Linear Solvers

Physics-Informed Neural Networks (PINNs) have shown significant potential not only in solving nonlinear partial differential equations (PDEs) but also in serving as efficient linear solvers for problems such as the Poisson equation. Traditionally, solving linear systems like the Poisson equation has relied on numerical methods such as finite difference, finite element, or iterative solvers like conjugate gradient. However, the integration of PINNs into this domain opens up new possibilities for more flexible and potentially faster solution strategies, especially when combined with traditional solvers. This subsection explores how PINNs can function as linear solvers and their potential to enhance performance and accuracy.

One of the key advantages of using PINNs as linear solvers lies in their ability to embed physical constraints directly into the neural network's loss function. For linear systems such as the Poisson equation, the governing equations are straightforward, yet solving them efficiently in complex geometries or with variable coefficients can be challenging. By leveraging the structure of the equations, PINNs can be trained to approximate solutions without requiring a grid-based discretization, thus avoiding the computational overhead associated with traditional mesh generation. This property makes PINNs particularly appealing for problems where the domain is irregular or where the coefficients vary spatially [10].

A notable application of PINNs as linear solvers is their use in solving the Poisson equation, a fundamental equation in fluid mechanics that arises in various contexts, such as pressure field computation in incompressible flows. Unlike classical numerical methods, which require the domain to be discretized into a grid, PINNs can directly approximate the solution by minimizing a loss function that incorporates the residual of the PDE at a set of collocation points. This approach allows for a more efficient solution process, especially in cases where the domain is complex or where high-resolution solutions are required [10].

The integration of PINNs with traditional linear solvers can further enhance their effectiveness. For instance, PINNs can be used to provide an initial guess or preconditioning for traditional iterative solvers, thereby accelerating convergence. This hybrid approach leverages the strengths of both methods: the adaptability and flexibility of PINNs for complex geometries and the robustness and efficiency of traditional solvers for linear systems. Such a combination can lead to significant improvements in both computational speed and accuracy, particularly for problems involving large-scale or high-dimensional systems [10].

Recent studies have demonstrated the feasibility of using PINNs as linear solvers for the Poisson equation and other linear systems. For example, researchers have shown that PINNs can be trained to solve the Poisson equation with high accuracy, even in cases where the solution exhibits sharp gradients or discontinuities [10]. The use of transfer learning further enhances the performance of PINNs, allowing them to generalize to new problems with minimal retraining. This is particularly useful in scenarios where multiple instances of the same linear system need to be solved under varying boundary conditions or with different coefficients.

Moreover, the use of PINNs as linear solvers can be extended to other linear systems beyond the Poisson equation. For instance, in computational fluid dynamics (CFD), the Navier-Stokes equations often require the solution of linear systems as part of the overall solution process. By integrating PINNs into this framework, it is possible to achieve faster and more accurate solutions for these systems. This is especially relevant in cases where the solution requires solving multiple linear systems iteratively, such as in time-dependent simulations or in the context of multi-physics problems [10].

Another advantage of using PINNs as linear solvers is their ability to handle sparse or incomplete data. In many practical applications, the available data may be limited or noisy, making it difficult to apply traditional numerical methods effectively. PINNs, on the other hand, can be trained using a relatively small number of data points, as long as the physical constraints are properly embedded into the loss function. This makes them particularly suitable for problems where data collection is expensive or impractical [10].

In addition to their computational efficiency, PINNs as linear solvers also offer new opportunities for uncertainty quantification and robustness. By incorporating probabilistic methods, such as Bayesian neural networks, PINNs can provide not only point estimates of the solution but also measures of uncertainty. This is crucial in applications where the reliability of the solution is important, such as in engineering design or safety-critical systems. Furthermore, the use of PINNs can help in identifying and mitigating errors in the solution process, leading to more robust and trustworthy results [10].

The potential of PINNs as linear solvers is further enhanced by recent advances in training algorithms and optimization strategies. Techniques such as adaptive sampling, loss balancing, and multi-task learning can significantly improve the performance of PINNs, making them more effective for solving linear systems. These techniques help in addressing challenges such as the non-convexity of the loss landscape and the difficulty in balancing different loss terms, which are common in training PINNs for complex problems [10].

In summary, the use of PINNs as linear solvers for the Poisson equation and other linear systems represents a promising direction in the field of computational fluid mechanics. By leveraging the ability of PINNs to embed physical constraints directly into the training process, they offer a flexible and efficient alternative to traditional numerical methods. The integration of PINNs with traditional solvers can further enhance their performance, leading to faster and more accurate solutions for a wide range of problems. As research in this area continues to advance, the potential applications of PINNs as linear solvers are likely to expand, opening up new possibilities for solving complex and challenging problems in fluid mechanics and beyond.

### 3.19 Surrogate Modeling of Turbulent Natural Convection

Surrogate modeling of turbulent natural convection has emerged as a critical area of research in fluid mechanics, particularly for applications involving complex and high-dimensional flow dynamics. Traditional numerical methods, such as finite element and finite volume methods, are often computationally intensive and may struggle to capture the intricate behavior of turbulent flows efficiently. Physics-Informed Neural Networks (PINNs) have gained attention for their ability to integrate physical laws into the learning process, enabling the development of accurate and efficient surrogate models for turbulent natural convection problems. One notable application of PINNs in this domain is the surrogate modeling of turbulent Rayleigh-Bénard convection flows, which involve complex interactions between temperature gradients, buoyancy forces, and fluid motion. The use of PINNs in this context not only enhances the predictive capabilities of the models but also improves their computational efficiency, making them a promising alternative to conventional methods [11].

One of the key challenges in surrogate modeling of turbulent natural convection is the accurate representation of the flow field, particularly in regions where the flow is highly unsteady and involves multiscale features. PINNs address this challenge by incorporating the governing equations of fluid dynamics into the training process, ensuring that the learned model adheres to the physical laws of the system. However, the traditional PINN framework can face difficulties in capturing high-frequency features and maintaining accuracy in regions with complex boundary conditions. To overcome these challenges, researchers have explored various techniques to enhance the performance of PINNs in turbulent natural convection modeling. One such technique is the use of padding techniques, which involve adding virtual nodes near the boundaries to improve the accuracy of the solution [11]. These padding techniques help in mitigating the effects of the boundary conditions, leading to a more accurate representation of the flow field.

Another critical aspect of surrogate modeling in turbulent natural convection is the relaxation of incompressibility conditions, which is often a stringent requirement in traditional numerical simulations. In many practical scenarios, especially those involving high Rayleigh numbers, the assumption of incompressibility may not hold, leading to inaccuracies in the predicted flow behavior. PINNs offer a flexible framework to relax these constraints by incorporating the effects of compressibility into the loss function. This approach allows the model to adapt to the actual flow conditions, resulting in improved accuracy and reliability of the surrogate model. For instance, the study by [11] demonstrates that the relaxation of incompressibility conditions can significantly enhance the performance of PINNs in predicting turbulent natural convection flows.

The use of PINNs for surrogate modeling of turbulent natural convection also involves the integration of advanced numerical techniques to handle the complexities of the problem. One such technique is the use of spectral PINNs, which leverage the Fourier transform to capture the high-frequency components of the flow field. Spectral PINNs are particularly effective in dealing with multiscale problems, as they can efficiently represent the solution in both the spatial and frequency domains. By incorporating spectral features into the training process, spectral PINNs can achieve higher accuracy and faster convergence compared to traditional PINNs. This approach has been shown to be particularly effective in simulating turbulent Rayleigh-Bénard convection flows, where the presence of multiple scales and complex interactions between different flow structures poses significant challenges for conventional methods [11].

Another important consideration in the surrogate modeling of turbulent natural convection is the handling of large datasets and the integration of high-resolution data into the training process. PINNs can effectively utilize high-resolution data to improve the accuracy of the surrogate model, but the computational cost of processing such data can be substantial. To address this challenge, researchers have explored the use of data-efficient training strategies, such as adaptive sampling and importance sampling. These techniques help in selecting the most informative data points for training, reducing the computational burden while maintaining the accuracy of the model. Additionally, the use of transfer learning can further enhance the performance of PINNs by leveraging pre-trained models on similar problems, allowing for faster convergence and improved generalization [11].

The application of PINNs in surrogate modeling of turbulent natural convection also highlights the importance of proper loss function formulation. The loss function in PINNs typically consists of multiple terms that account for the residuals of the governing equations, boundary conditions, and data misfit. Balancing these terms is crucial for achieving accurate and stable solutions, as an imbalance can lead to convergence issues or inaccurate predictions. Techniques such as loss weighting and adaptive sampling have been proposed to address this challenge, ensuring that the training process is well-optimized for the specific problem at hand. For example, the study by [11] demonstrates that a carefully designed loss function can significantly improve the performance of PINNs in predicting turbulent natural convection flows.

In addition to the technical aspects of PINN training and formulation, the practical implementation of these models for surrogate modeling of turbulent natural convection also involves considerations related to scalability and computational efficiency. As the complexity of the problem increases, the computational resources required to train and evaluate the model can become a limiting factor. To address this challenge, researchers have explored the use of parallel computing and distributed training strategies to improve the efficiency of the training process. These approaches enable the model to handle large-scale problems and high-dimensional data, making them suitable for real-world applications in fluid mechanics. The study by [11] highlights the potential of these strategies in enhancing the performance of PINNs for turbulent natural convection modeling.

Overall, the application of PINNs in surrogate modeling of turbulent natural convection represents a significant advancement in the field of fluid mechanics. By integrating physical laws with data-driven learning, PINNs offer a powerful framework for developing accurate and efficient surrogate models for complex flow problems. Techniques such as padding, relaxation of incompressibility conditions, and advanced loss function formulations have been shown to significantly enhance the performance of PINNs in this context. As the field continues to evolve, further research into the development of more robust and efficient training algorithms, as well as the exploration of new architectures and techniques, will be essential in expanding the applicability of PINNs to a wider range of fluid dynamics problems.

### 3.20 Error Estimation and Augmentation in PINNs

Error estimation and augmentation in Physics-Informed Neural Networks (PINNs) play a critical role in ensuring the reliability and accuracy of solutions for complex fluid mechanics problems, particularly when dealing with thermally coupled Navier-Stokes equations. These equations describe the motion of fluids under the influence of thermal effects, which introduces additional complexity due to the coupling between momentum and energy transfer. Accurate solutions to such equations require not only a precise representation of the physical laws but also robust error estimation and augmentation techniques to improve model performance, especially when dealing with noisy or incomplete data.

A significant aspect of error estimation in PINNs involves understanding the convergence behavior of the model during training. Convergence is often a challenging issue in PINNs, especially for problems with high nonlinearity and multiscale dynamics. One of the key strategies to enhance convergence and accuracy is the incorporation of a pressure stabilization term into the loss function. This term helps to mitigate numerical instabilities that can arise due to the incompressibility condition in the Navier-Stokes equations, which is particularly important in thermally coupled scenarios where temperature gradients can lead to complex flow patterns. By introducing a pressure stabilization term, the model can better handle the coupling between pressure and velocity fields, ensuring that the solution remains physically consistent and numerically stable.

In the context of thermally coupled Navier-Stokes equations, the use of pressure stabilization terms has been demonstrated to significantly improve the accuracy of PINN solutions. For instance, in the work by [9], the authors present a convergence analysis that highlights the importance of this term in achieving stable and accurate solutions. The study shows that the inclusion of a pressure stabilization term helps to reduce the residual errors associated with the Navier-Stokes equations, particularly in scenarios where the flow field exhibits strong gradients or abrupt changes due to thermal effects. This is particularly relevant in applications such as heat transfer in fluid flows, where accurate modeling of the interaction between thermal and mechanical forces is essential.

Furthermore, error estimation in PINNs is closely tied to the concept of augmentation, which involves improving the model’s ability to generalize to new or unseen scenarios. Augmentation techniques can include the use of additional data, the incorporation of physical constraints, or the refinement of the loss function to better capture the underlying physics. In the case of thermally coupled Navier-Stokes equations, augmentation can be achieved by incorporating additional terms into the loss function that account for thermal effects, such as the heat flux or the temperature distribution. This approach ensures that the model not only satisfies the momentum equations but also respects the energy conservation laws, leading to more accurate and physically meaningful predictions.

A notable example of error estimation and augmentation in PINNs is the work by [9], where the authors propose a novel transfer optimization scheme to enhance the predictive performance of PINNs in new scenarios. The study demonstrates that by incorporating a physics-based regularization term into the loss function, the model can achieve significantly better accuracy and robustness, even when the input data is noisy or incomplete. This approach is particularly effective in thermally coupled problems, where the presence of noise or uncertainty in the input data can lead to significant errors in the predicted solution. The authors also show that the use of a pressure stabilization term in the loss function can further improve the model’s ability to capture the coupling between thermal and mechanical effects.

Another important aspect of error estimation in PINNs is the development of convergence analysis techniques that provide theoretical guarantees on the performance of the model. In the work by [108], the authors present a rigorous analysis of the generalization error of PINNs for solving inverse problems related to PDEs. The study shows that by using a well-constructed loss function that incorporates both data and physics-based constraints, the model can achieve a high level of accuracy and robustness, even in the presence of noise or uncertainty. This is particularly relevant for thermally coupled Navier-Stokes equations, where the presence of multiple physical processes can make the problem highly complex and difficult to solve.

In addition to convergence analysis, error estimation in PINNs also involves the development of techniques to quantify the uncertainty in the model's predictions. This is crucial in applications such as fluid dynamics, where the accuracy of the solution can have a significant impact on the performance of the system. One approach to uncertainty quantification is the use of ensemble learning, where multiple PINN models are trained and their predictions are combined to provide a more accurate and reliable estimate of the solution. This technique is particularly effective in scenarios where the input data is noisy or incomplete, as it allows the model to account for the variability in the data and provide a more robust prediction.

In the context of thermally coupled Navier-Stokes equations, the use of ensemble learning has been shown to significantly improve the accuracy and reliability of PINN solutions. For example, in the work by [109], the authors propose an ensemble PINN (E-PINN) approach that uses the statistics of multiple basic models to provide uncertainty estimates for the inverse solution. The study demonstrates that E-PINNs can effectively quantify the uncertainty in the solution, even in the presence of noise or uncertainty in the input data. This is particularly relevant for thermally coupled problems, where the accuracy of the solution can have a significant impact on the performance of the system.

Finally, the integration of pressure stabilization terms and other augmentation techniques into the PINN framework is essential for improving the accuracy and robustness of solutions to thermally coupled Navier-Stokes equations. These techniques help to ensure that the model not only satisfies the governing equations but also respects the physical constraints of the problem, leading to more accurate and reliable predictions. By combining these approaches with rigorous error estimation and uncertainty quantification techniques, PINNs can provide a powerful and flexible framework for solving complex fluid mechanics problems. The ongoing development of these techniques is crucial for advancing the application of PINNs in real-world scenarios, where the accuracy and reliability of the solution are of paramount importance.

### 3.21 Convolutional PINNs for PDEs

Convolutional Physics-Informed Neural Networks (Convolutional PINNs) have emerged as a powerful tool for solving partial differential equations (PDEs) in fluid mechanics, combining the strengths of convolutional neural networks (CNNs) with the physics-informed approach of PINNs. These networks are particularly effective in capturing spatial features and leveraging parameter sharing, which enhances their performance in solving PDEs across various mesh resolutions [48]. The integration of CNNs into the PINN framework has enabled the development of architectures like the Multi-Receptive-Field PINN (MRF-PINN), which is designed to solve different types of PDEs on various mesh resolutions without manual tuning [48].

One of the key advantages of convolutional PINNs is their ability to perform spatial feature extraction. Unlike traditional fully connected networks, convolutional networks are inherently adept at capturing local patterns and spatial dependencies, making them well-suited for problems involving fluid dynamics, where spatial features play a crucial role. This capability is particularly beneficial in solving PDEs, as the spatial structure of the solution is often critical for accurate predictions. The MRF-PINN architecture, for instance, employs a multi-receptive-field approach to enhance the network's ability to capture spatial features at different scales, allowing it to adapt to varying mesh resolutions effectively [48].

Parameter sharing is another significant benefit of convolutional PINNs. By reusing the same set of parameters across different spatial locations, convolutional networks reduce the number of trainable parameters, which can significantly improve training efficiency and reduce the risk of overfitting. This is particularly advantageous in fluid mechanics, where the solutions to PDEs often exhibit complex spatial structures that require large networks to capture accurately. The MRF-PINN leverages parameter sharing to maintain a compact network structure while still achieving high accuracy in solving PDEs. This approach not only enhances computational efficiency but also allows the network to generalize better to new problems, as the shared parameters can capture the underlying physical laws that govern the system [48].

In addition to parameter sharing and spatial feature extraction, convolutional PINNs can benefit from the use of high-order finite difference methods. These methods provide a more accurate approximation of derivatives, which is crucial for solving PDEs that involve complex dynamics. The MRF-PINN architecture incorporates high-order finite difference methods to improve the accuracy of the solution, particularly in regions where the solution exhibits high-frequency variations. This is achieved by padding the virtual nodes near the boundaries with Taylor polynomials, which helps to implement high-order finite difference schemes effectively [48]. The combination of convolutional networks with high-order finite difference methods enables the MRF-PINN to achieve superior accuracy and convergence compared to traditional PINN architectures.

The application of convolutional PINNs extends beyond fluid mechanics and can be used to solve a wide range of PDEs, including elliptic, parabolic, and hyperbolic equations. The MRF-PINN has been tested on three typical linear PDEs (elliptic, parabolic, hyperbolic) and a series of nonlinear PDEs (Navier-Stokes PDEs), demonstrating its generality and superiority [48]. This versatility makes convolutional PINNs a valuable tool for researchers and engineers working on complex fluid dynamics problems, as they can be applied to a variety of scenarios without the need for extensive retraining or manual tuning.

Another important aspect of convolutional PINNs is their ability to handle different mesh resolutions. Traditional numerical methods often struggle with adapting to varying mesh resolutions, requiring significant computational resources to achieve accurate results. In contrast, convolutional PINNs can seamlessly adapt to different mesh resolutions by leveraging the spatial invariance of convolutional operations. The MRF-PINN, for example, is designed to solve PDEs on various mesh resolutions without manual tuning, making it a flexible and efficient solution for fluid mechanics problems [48]. This adaptability is particularly useful in real-world applications, where the mesh resolution may vary depending on the problem at hand.

The dimensional balance method is another technique that enhances the performance of convolutional PINNs. This method is used to estimate the loss weights when solving Navier-Stokes equations, ensuring that the network can effectively balance the different terms in the loss function. By incorporating the dimensional balance method, the MRF-PINN can improve the convergence and accuracy of the solution, particularly in complex fluid dynamics problems where the balance between different physical terms is crucial [48]. This approach not only improves the training efficiency of the network but also ensures that the solution remains physically consistent.

Convolutional PINNs also offer advantages in terms of computational efficiency and scalability. The parameter sharing and spatial feature extraction capabilities of CNNs allow the network to process large-scale PDEs with reduced computational costs. This is particularly important in fluid mechanics, where high-resolution simulations are often required to capture the complex dynamics of the flow. The MRF-PINN demonstrates that convolutional networks can achieve competitive performance with traditional numerical methods while significantly reducing the computational burden [48]. This makes convolutional PINNs a promising alternative to conventional solvers, especially for large-scale fluid dynamics problems.

In summary, convolutional PINNs, particularly the MRF-PINN architecture, offer a powerful and flexible approach to solving PDEs in fluid mechanics. By leveraging the strengths of convolutional networks, these models can efficiently capture spatial features, reduce the number of trainable parameters, and adapt to different mesh resolutions. The integration of high-order finite difference methods and dimensional balance techniques further enhances their accuracy and convergence, making them a valuable tool for researchers and engineers working on complex fluid dynamics problems. As the field of physics-informed neural networks continues to evolve, convolutional PINNs are likely to play an increasingly important role in advancing the capabilities of data-driven PDE solvers.

### 3.22 Distributed PINNs for Data-Efficient Solutions

---
Distributed Physics-Informed Neural Networks (DPINNs) represent a significant advancement in the application of PINNs to fluid mechanics, particularly in scenarios where data efficiency and computational scalability are critical. Traditional PINNs, while powerful, often face challenges in solving complex, non-linear partial differential equations (PDEs) such as the Navier-Stokes equations due to their reliance on large datasets and the computational cost of training. DPINNs address these limitations by leveraging distributed computing strategies and enhancing the efficiency of data usage, making them a promising tool for data-efficient solutions to PDEs.

The development of DPINNs is driven by the need to solve high-dimensional and complex fluid dynamics problems with limited data. This is particularly relevant in real-world applications where data collection is costly, sparse, or noisy. By distributing the computational workload across multiple nodes or processors, DPINNs not only improve the scalability of PINN-based solutions but also reduce the training time and resource requirements. This approach allows for the efficient handling of non-linear PDEs, such as the Navier-Stokes equations, which describe the motion of fluid substances and are fundamental to fluid mechanics [110].

One of the key innovations in DPINNs is the use of domain decomposition techniques, which divide the problem domain into smaller subdomains. Each subdomain is then solved independently using a local PINN, and the results are combined to obtain the global solution. This method enables the efficient parallelization of the training process and ensures that each subdomain can be optimized for its specific characteristics, leading to improved accuracy and efficiency. For instance, in the case of the Navier-Stokes equations, different subdomains may exhibit varying flow behaviors, and by applying tailored PINN models to each, the overall solution becomes more precise and reliable [110].

The effectiveness of DPINNs is further enhanced by their ability to incorporate data efficiently. In many fluid mechanics applications, the availability of high-quality, high-resolution data is limited. DPINNs address this by utilizing sparse datasets and focusing on the most informative regions of the solution domain. This is achieved through adaptive sampling strategies, where collocation points are dynamically adjusted based on the model's performance and the complexity of the underlying PDE. By concentrating the training efforts on regions where the model is less accurate, DPINNs can significantly improve the convergence and stability of the solution [93].

Moreover, DPINNs are designed to handle non-linear PDEs, which are notoriously difficult to solve with traditional numerical methods. The non-linear nature of these equations often results in complex, multi-scale behaviors that require careful consideration during the training process. DPINNs are equipped with advanced architectures and training algorithms that can capture these intricate dynamics, making them well-suited for problems such as turbulent flow simulations, where the solution domain exhibits a wide range of spatial and temporal scales [111].

Recent studies have demonstrated the effectiveness of DPINNs in solving non-linear PDEs with high accuracy and efficiency. For example, in a study on the Navier-Stokes equations, DPINNs were shown to achieve significant improvements in both accuracy and computational efficiency compared to traditional PINNs. The distributed approach allowed for the efficient utilization of computational resources, leading to faster convergence and more accurate solutions [110]. Additionally, the use of domain decomposition techniques enabled the model to handle complex geometries and boundary conditions more effectively, further enhancing its applicability to real-world fluid mechanics problems.

Another notable feature of DPINNs is their ability to integrate with traditional numerical methods, creating hybrid models that combine the strengths of both approaches. This is particularly useful in scenarios where the solution requires both high accuracy and computational efficiency. By leveraging the capabilities of traditional numerical solvers for certain aspects of the problem and using DPINNs for others, the overall performance of the solution can be significantly improved. This hybrid approach not only enhances the accuracy of the results but also reduces the computational burden, making it more feasible for large-scale simulations [80].

The impact of DPINNs on fluid mechanics is further amplified by their potential for real-time applications. In many engineering and scientific contexts, the ability to quickly and accurately simulate fluid dynamics is crucial. DPINNs, with their distributed and data-efficient nature, are well-suited for such applications. For instance, in the field of biomedical engineering, where real-time blood flow imaging is essential, DPINNs have been used to achieve fast and accurate solutions, demonstrating their potential for practical use [112].

In addition to their technical advantages, DPINNs also offer a more flexible and scalable framework for solving PDEs. The modular design of DPINNs allows for easy integration of new data, models, and computational resources, making them adaptable to a wide range of problems. This flexibility is particularly valuable in fluid mechanics, where the complexity of the problems can vary significantly across different applications. By providing a robust and scalable solution, DPINNs can help overcome the limitations of traditional numerical methods, which often struggle with complex and high-dimensional problems.

In conclusion, distributed PINNs (DPINNs) represent a significant advancement in the field of fluid mechanics, offering a data-efficient and scalable approach to solving non-linear PDEs. By leveraging distributed computing strategies and domain decomposition techniques, DPINNs improve the accuracy and efficiency of PINN-based solutions, making them well-suited for a wide range of applications. The integration of DPINNs with traditional numerical methods further enhances their versatility, enabling them to address the challenges of complex and high-dimensional problems. As research in this area continues to evolve, DPINNs are poised to play a crucial role in the future of computational fluid dynamics.
---

### 3.23 Improved Surrogate Modeling with PINNs

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for improving surrogate modeling in fluid dynamics. Traditional surrogate modeling approaches often rely on data-driven methods that can be limited by the availability and quality of training data. PINNs, on the other hand, integrate the governing physical laws of fluid dynamics directly into the learning process, which allows for more accurate and robust predictions even in scenarios with limited or noisy data. This capability has been demonstrated in several studies, highlighting the advantages of PINNs in enhancing predictive performance, robustness to noise, and faster convergence through transfer optimization.

One of the key benefits of PINNs in surrogate modeling is their ability to incorporate physical laws into the neural network's training process. By embedding the partial differential equations (PDEs) that govern fluid flow into the loss function, PINNs ensure that the network's predictions adhere to the fundamental principles of physics. This integration not only improves the accuracy of the predictions but also enhances the generalization capability of the model, making it more reliable in various fluid dynamics applications. For instance, in the study titled "Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed Neural Networks," it was shown that the inclusion of a physics-based regularization term significantly improves the equivalent data-driven surrogate model, leading to an order of magnitude improvement in test error when the dataset is very noisy [9].

Another significant advantage of PINNs is their robustness to noise in the dataset. In many fluid dynamics problems, the available data can be noisy or incomplete, which poses a challenge for traditional data-driven models. However, PINNs are designed to respect the underlying physical laws, which helps in filtering out the noise and providing more reliable predictions. This robustness is particularly important in real-world applications where measurement data is often subject to various sources of uncertainty. The study mentioned above also highlighted that PINNs can improve the robustness of the model to noise in the dataset, making them a valuable tool for surrogate modeling in such scenarios [9].

Faster convergence is another critical aspect of improved surrogate modeling with PINNs. The use of transfer optimization techniques allows PINNs to leverage pre-trained models, significantly reducing the training time and computational resources required. This is particularly beneficial in scenarios where multiple similar problems need to be solved, as the pre-trained model can be adapted to new scenarios with minimal additional training. The study on "Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed Neural Networks" demonstrated that transfer optimization can lead to an approximately 3x improvement in speed to convergence and an order of magnitude improvement in predictive performance over conventional Xavier initialization for training new scenarios [9].

Moreover, the ability of PINNs to handle complex and high-dimensional fluid dynamics problems is another key advantage. Traditional numerical methods often struggle with the computational complexity and high-dimensional nature of such problems, leading to increased computational costs and longer simulation times. PINNs, however, can efficiently approximate the solution of PDEs without the need for extensive mesh generation, making them a more scalable solution for large-scale fluid dynamics problems. This is evident in the work by [29], where PINNs were shown to be effective in solving a wide range of fluid dynamics problems, including those with complex geometries and high Reynolds numbers [29].

The integration of PINNs into surrogate modeling also allows for the exploration of new and more efficient training strategies. For example, the use of adaptive sampling techniques can significantly improve the performance of PINNs by focusing the training on the most critical regions of the solution domain. The study titled "A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks" demonstrated that adaptive sampling methods, such as residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), can lead to significant improvements in the accuracy of PINNs with fewer residual points [113].

Furthermore, the ability of PINNs to handle both forward and inverse problems in fluid dynamics is a testament to their versatility and effectiveness. In forward problems, PINNs can accurately predict the flow field given the initial and boundary conditions, while in inverse problems, they can estimate unknown parameters or boundary conditions from the observed data. This dual capability makes PINNs a valuable tool for a wide range of applications, from weather prediction to biomedical fluid dynamics. The study titled "Physics-informed neural networks for blood flow inverse problems" showcased the effectiveness of PINNs in estimating reduced-order model parameters and full velocity fields from scattered 2D noisy measurements in the ascending aorta [24].

In addition to their robustness and accuracy, PINNs also offer significant advantages in terms of computational efficiency. Traditional numerical methods often require extensive computational resources to solve complex fluid dynamics problems, especially when high-resolution simulations are needed. In contrast, PINNs can provide accurate solutions with a much lower computational cost, making them a more practical solution for real-time applications. The study titled "Physics-informed deep learning for incompressible laminar flows" demonstrated that PINNs can simulate steady and transient laminar flows at low Reynolds numbers with high accuracy, highlighting their potential for use in various engineering applications [1].

The integration of PINNs into surrogate modeling also opens up new possibilities for the development of more advanced and efficient training algorithms. For example, the use of multi-fidelity learning approaches allows PINNs to leverage data of varying fidelity levels, improving their generalization capabilities and reducing the need for large amounts of high-fidelity data. The study titled "Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective" explored the use of multi-fidelity data-driven PINNs (DD-PINNs) for the realization of digital twins, demonstrating their ability to handle datasets with varying fidelity and providing accurate predictions even in extrapolation tasks [12].

In conclusion, the use of PINNs in improved surrogate modeling of fluid dynamics offers numerous advantages, including enhanced predictive performance, robustness to noise, and faster convergence through transfer optimization. By integrating physical laws directly into the learning process, PINNs provide a more accurate and reliable solution to complex fluid dynamics problems. Their ability to handle both forward and inverse problems, combined with their computational efficiency and adaptability, makes them a valuable tool for a wide range of applications in fluid mechanics. As research in this area continues to advance, the potential for PINNs to revolutionize the field of fluid dynamics is becoming increasingly evident.

### 3.24 Physics-Informed Deep Learning for Fluid Dynamics

Physics-Informed Deep Learning (PIDL) has emerged as a transformative approach in fluid dynamics, offering a robust framework to integrate domain-specific knowledge with data-driven learning. Unlike traditional deep learning methods that rely solely on data, PIDL leverages the governing physical laws—such as the Navier-Stokes equations—to constrain the neural network's predictions, ensuring that the solutions are not only data-consistent but also physically meaningful. This synergy between physical principles and machine learning enables the development of models that can solve both forward and inverse problems in fluid dynamics with high accuracy and generalizability.

One of the most prominent applications of PIDL is in solving forward problems, where the goal is to predict the behavior of fluid flows given initial and boundary conditions. Physics-Informed Neural Networks (PINNs) are particularly well-suited for this task as they embed the governing equations into the loss function, allowing the network to learn the solution of partial differential equations (PDEs) directly from data. For example, in the context of incompressible flow simulations, PINNs have been used to approximate the velocity and pressure fields while ensuring that the Navier-Stokes equations are satisfied at collocation points [20]. The integration of physical constraints ensures that the models remain consistent with the underlying physics, even when the training data is sparse or noisy.

Beyond forward problems, PIDL has also shown great promise in solving inverse problems, where the goal is to infer unknown parameters or boundary conditions from measured data. In fluid dynamics, inverse problems often involve estimating quantities such as turbulence model coefficients, initial conditions, or boundary fluxes. For instance, in the case of turbulent flow predictions, PINNs have been employed to learn the subgrid-scale stresses from high-resolution simulation data, enabling more accurate large-eddy simulations (LES) without the need for explicit turbulence models [114]. By incorporating physical laws into the training process, these models can generalize to unseen scenarios and provide reliable estimates even in the presence of measurement noise.

A key strength of PIDL is its ability to integrate domain knowledge into neural networks, allowing for the development of more interpretable and robust models. Traditional deep learning methods often lack explicit physical constraints, leading to solutions that may be mathematically accurate but physically inconsistent. In contrast, PIDL explicitly encodes the physical equations into the loss function, ensuring that the network's outputs satisfy the governing laws of the system. For example, in the case of compressible flows, PINNs have been designed to enforce the conservation of mass, momentum, and energy, resulting in more stable and accurate solutions [115]. This integration of domain knowledge not only improves the accuracy of the predictions but also enhances the interpretability of the models, making them more suitable for practical applications.

The application of PIDL in fluid dynamics extends beyond standard flow simulations to include complex multi-physics problems. For instance, in the case of thermo-mechanical systems, PINNs have been used to model the coupled heat transfer and fluid flow phenomena, ensuring that the solutions satisfy both the Navier-Stokes equations and the heat equation [6]. This capability makes PIDL particularly attractive for problems involving coupled processes, where traditional numerical methods may struggle with the complexity of the governing equations.

Another significant advantage of PIDL is its ability to handle high-dimensional and complex flow scenarios. In cases where the solution domain is defined by irregular geometries or varying boundary conditions, PINNs can adapt to the problem's complexity by learning the solution directly from data. For example, in the case of flows in porous media, PINNs have been used to model the pressure and velocity fields in three-dimensional domains with high accuracy, even when the domain is dynamically changing [116]. This flexibility makes PIDL a powerful tool for applications in geophysics, environmental science, and other fields where the flow domain is not easily represented by traditional grid-based methods.

In addition to forward and inverse problems, PIDL has also been applied to the development of surrogate models for fluid dynamics simulations. These surrogate models, often based on deep learning architectures such as autoencoders and recurrent neural networks, aim to accelerate the simulation process by learning the mapping between input parameters and output quantities of interest. For instance, in the context of turbulent flow simulations, PINNs have been used to create reduced-order models that can predict the flow behavior at a fraction of the computational cost of traditional methods [117]. By leveraging the physical constraints of the system, these models can generalize to new scenarios and provide accurate predictions even when the training data is limited.

The potential of PIDL in fluid dynamics is further enhanced by its compatibility with advanced optimization techniques. For example, by incorporating physics-informed loss functions into the training process, PINNs can achieve faster convergence and more stable solutions, even for complex flow problems. Additionally, the use of multi-fidelity learning, where models are trained on data of varying resolution and accuracy, has been shown to improve the generalization performance of PIDL models [118]. This approach allows for the efficient use of computational resources, making PIDL a viable alternative to traditional numerical methods for large-scale simulations.

In conclusion, Physics-Informed Deep Learning represents a powerful and versatile framework for solving fluid dynamics problems. By integrating physical principles with data-driven learning, PIDL enables the development of models that are both accurate and interpretable, capable of handling a wide range of forward and inverse problems in fluid mechanics. The ability to incorporate domain knowledge into neural networks not only enhances the reliability of the predictions but also opens up new possibilities for the simulation and analysis of complex fluid systems. As research in this area continues to advance, PIDL is poised to play an increasingly important role in the future of computational fluid dynamics.

### 3.25 Inverse Problems in Hemodynamics

Inverse problems in hemodynamics involve estimating unknown parameters or conditions from limited and noisy measurements, which is crucial for clinical applications such as diagnosing cardiovascular diseases or monitoring blood flow in real-time. Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for addressing these challenges, as they can integrate physical laws with data-driven learning, enabling accurate and robust solutions. By embedding governing equations of fluid dynamics, such as the Navier-Stokes equations, into the loss function of a neural network, PINNs ensure that the predicted solutions are consistent with the underlying physics, even when the data is sparse or noisy.

One of the key applications of PINNs in hemodynamics is the estimation of boundary conditions and full velocity fields from scattered measurements in clinical settings. Traditional methods for solving inverse problems in hemodynamics often rely on numerical simulations, which are computationally expensive and require significant prior knowledge of the system. In contrast, PINNs can leverage the available measurements and the physical laws to infer the desired quantities without the need for extensive computational resources. For instance, PINNs have been successfully applied to estimate the boundary conditions of blood flow in the aorta by using velocity measurements from ultrasound or magnetic resonance imaging (MRI) [10]. This approach not only reduces the computational cost but also improves the accuracy of the estimated boundary conditions, which is essential for predicting the flow dynamics in complex vascular geometries.

Another critical application of PINNs in hemodynamics is the reconstruction of the full velocity field from limited measurements. In many clinical scenarios, obtaining high-resolution velocity data is challenging due to the limitations of imaging techniques or the invasiveness of traditional measurement methods. PINNs can address this issue by learning the underlying fluid dynamics from the available measurements and predicting the velocity field across the entire domain. For example, in the case of blood flow in the carotid artery, PINNs have been used to reconstruct the velocity field from sparse velocity measurements obtained via Doppler ultrasound [3]. The results demonstrated that PINNs can accurately capture the spatial and temporal variations of the velocity field, even in regions where the measurements are sparse.

The ability of PINNs to handle noisy data is another significant advantage in solving inverse problems in hemodynamics. In clinical settings, measurements are often corrupted by noise due to instrumental errors or physiological variations. PINNs can incorporate the noise into the training process by using a physics-informed loss function that penalizes deviations from the governing equations. This approach ensures that the predicted solutions remain physically consistent, even in the presence of noisy measurements. For instance, a study on the application of PINNs to inverse problems in hemodynamics showed that the network could accurately estimate the velocity field and pressure distribution from noisy measurements by leveraging the physical constraints encoded in the loss function [61]. The results highlighted the robustness of PINNs in handling noisy data and their potential for real-world clinical applications.

Furthermore, PINNs can be used to estimate the parameters of hemodynamic models from limited experimental data. In many cases, the parameters of the governing equations, such as the viscosity or the elasticity of the vessel walls, are not known a priori and need to be inferred from the data. PINNs can simultaneously learn the solution to the governing equations and estimate the unknown parameters by incorporating them into the loss function. This approach has been applied to estimate the parameters of a hemodynamic model from in vivo measurements of blood flow in the coronary arteries [22]. The study demonstrated that PINNs could accurately estimate the parameters of the model while ensuring that the predicted solutions satisfy the governing equations, thus improving the reliability of the hemodynamic simulations.

Another promising application of PINNs in hemodynamics is the identification of the source of flow disturbances in complex vascular geometries. In clinical settings, blood flow can be affected by various factors such as stenosis, aneurysms, or vascular malformations. PINNs can be used to locate and quantify these disturbances by analyzing the velocity and pressure fields obtained from measurements. For example, in a study on the application of PINNs to inverse problems in hemodynamics, the network was trained to identify the location and severity of stenosis in the carotid artery by using velocity measurements from Doppler ultrasound [10]. The results showed that PINNs could accurately detect the stenotic regions and estimate the degree of narrowing, which is critical for diagnosing and treating vascular diseases.

The integration of PINNs with other data-driven techniques, such as deep learning and machine learning, has also shown promise in solving inverse problems in hemodynamics. By combining the strengths of PINNs and other machine learning models, it is possible to improve the accuracy and efficiency of the inverse solutions. For example, a hybrid approach that combines PINNs with a convolutional neural network (CNN) has been proposed to estimate the velocity field and pressure distribution in complex vascular geometries [48]. The study demonstrated that the hybrid model could achieve higher accuracy and faster convergence compared to using either PINNs or CNNs alone, making it a viable solution for real-time hemodynamic analysis.

Moreover, PINNs can be used to solve inverse problems in hemodynamics under varying conditions, such as different patient-specific geometries or physiological states. In clinical applications, the vascular geometry and flow conditions can vary significantly between patients, which makes it challenging to develop a universal model for hemodynamic analysis. PINNs can adapt to these variations by learning the underlying physical laws and generalizing the solutions to new scenarios. This flexibility is particularly useful in personalized medicine, where patient-specific models are needed to guide treatment decisions. For instance, a study on the application of PINNs to inverse problems in hemodynamics demonstrated that the network could accurately estimate the velocity field and pressure distribution in different vascular geometries by using a single trained model [9]. The results highlighted the potential of PINNs for developing personalized hemodynamic models that can be adapted to different patients and clinical scenarios.

In summary, the application of PINNs in solving inverse problems in hemodynamics has shown great promise in estimating boundary conditions, reconstructing velocity fields, and identifying flow disturbances from limited and noisy measurements. The integration of physical laws into the training process ensures that the predicted solutions are consistent with the underlying physics, making PINNs a robust and reliable tool for hemodynamic analysis. As research in this area continues to advance, the development of more sophisticated PINN models and the integration of multiple data-driven techniques will further enhance the accuracy and efficiency of solving inverse problems in hemodynamics. These advancements will have significant implications for clinical applications, enabling more accurate diagnosis and treatment of cardiovascular diseases.

### 3.26 PINNs for Multi-Physics Problems

Physics-Informed Neural Networks (PINNs) have increasingly been applied to multi-physics problems, where the governing equations involve interactions between multiple physical processes, such as thermal, mechanical, and fluid dynamics. These problems often require the simultaneous solution of coupled partial differential equations (PDEs), which can be computationally expensive and challenging for traditional numerical solvers. PINNs offer a promising alternative by incorporating physical laws directly into the neural network training process, enabling them to handle complex multi-physics scenarios without requiring explicit discretization of the domain [10]. This section discusses the application of PINNs to multi-physics problems, focusing on thermo-mechanically coupled systems and heterogeneous domains, including mixed formulations and sequential training strategies.

Multi-physics problems frequently involve the coupling of different physical phenomena, such as heat transfer, fluid flow, and mechanical deformation. For example, in thermo-mechanically coupled systems, the thermal expansion of a material can induce stress and strain, which in turn affect the heat distribution. Solving such problems requires the integration of the heat equation with the equations of motion and constitutive laws for the material. PINNs have been successfully employed to address these coupled systems by embedding the governing equations directly into the loss function of the neural network. This approach ensures that the solution satisfies the physical laws at every point in the domain, even in the presence of complex interactions [10].

One notable application of PINNs in multi-physics problems is in the simulation of thermo-mechanically coupled systems. A recent study [10] demonstrated the effectiveness of PINNs in solving the coupled equations of heat transfer and solid mechanics. In this work, the PINN framework was used to predict the temperature and displacement fields in a composite material subjected to thermal and mechanical loads. The model was trained to minimize the residuals of both the heat equation and the equations of motion, ensuring that the solution adhered to the governing physical laws. The results showed that the PINN model could accurately predict the temperature and displacement fields with a high degree of accuracy, even in the presence of nonlinearities and complex boundary conditions.

Heterogeneous domains, where the material properties vary spatially, pose additional challenges in multi-physics simulations. Traditional numerical methods often require extensive meshing and refinement to accurately capture the variations in material properties, which can be computationally prohibitive. PINNs offer a data-driven approach to handle such heterogeneous domains by learning the solution directly from the data without explicit discretization. This capability is particularly useful in problems involving heterogeneous materials, such as composite structures and porous media. For instance, in the context of porous media, PINNs have been used to simulate the coupled flow of fluids and heat through heterogeneous rock formations [10]. The model was trained to minimize the residuals of the Navier-Stokes equations and the energy equation, ensuring that the solution respected the physical constraints of the problem. The results showed that the PINN model could accurately predict the velocity and temperature fields in the porous medium, even in the presence of strong spatial variations in material properties.

Mixed formulations are another important aspect of PINNs in multi-physics problems. A mixed formulation involves the simultaneous solution of multiple variables, such as velocity, pressure, and temperature, which are often coupled through the governing equations. This approach is particularly useful in problems where the variables are interdependent and cannot be solved independently. For example, in fluid-structure interaction problems, the velocity and pressure fields in the fluid are coupled with the displacement and stress fields in the solid. PINNs have been used to implement mixed formulations by training the network to minimize the residuals of all the governing equations simultaneously. This approach ensures that the solution respects the physical constraints of the problem and captures the interactions between the different physical processes.

Sequential training strategies are also employed in PINNs for multi-physics problems, where the solution is obtained by solving the governing equations in a step-by-step manner. This approach is particularly useful in problems where the solution exhibits different behaviors in different regions of the domain. For example, in the simulation of complex multiphysics systems, such as those involving coupled heat transfer and fluid flow, the solution may require different training strategies in different regions of the domain. A recent study [10] demonstrated the effectiveness of sequential training strategies in PINNs for multi-physics problems. In this work, the PINN model was trained to solve the governing equations in a sequential manner, starting from the regions with simpler physics and gradually moving to more complex regions. The results showed that the sequential training strategy significantly improved the accuracy and stability of the solution, especially in the presence of strong nonlinearities and complex boundary conditions.

The application of PINNs to multi-physics problems is not limited to thermo-mechanically coupled systems and heterogeneous domains. They have also been used to simulate a wide range of other multi-physics problems, such as electro-thermo-mechanical systems and fluid-structure interaction problems. For example, in the context of electro-thermo-mechanical systems, PINNs have been used to simulate the coupled effects of electric fields, thermal gradients, and mechanical deformation [10]. The model was trained to minimize the residuals of the governing equations for each physical process, ensuring that the solution respected the physical constraints of the problem. The results showed that the PINN model could accurately predict the electric, thermal, and mechanical fields in the domain, even in the presence of complex interactions between the different physical processes.

In addition to their application in multi-physics problems, PINNs have also been used to address challenges related to data scarcity and high computational costs. Traditional numerical methods often require large amounts of training data to achieve accurate predictions, which can be prohibitive in many applications. PINNs, on the other hand, can learn the solution directly from the data without requiring explicit discretization of the domain. This capability is particularly useful in problems where the available data is sparse or noisy, such as in real-time monitoring and control applications. For example, in the context of real-time monitoring of fluid-structure interaction problems, PINNs have been used to predict the state of the system based on sparse measurements, enabling efficient and accurate predictions even in the presence of limited data [10].

In conclusion, the application of PINNs to multi-physics problems has shown great promise in addressing the challenges of coupled systems and heterogeneous domains. By integrating physical laws directly into the neural network training process, PINNs offer a powerful tool for simulating complex multi-physics scenarios with high accuracy and efficiency. The use of mixed formulations and sequential training strategies further enhances the capabilities of PINNs in handling these problems. As the field continues to evolve, the application of PINNs to multi-physics problems is expected to play an increasingly important role in the advancement of scientific computing and engineering simulations.

### 3.27 Super-Resolution of Flow-Field Data

Super-resolution of flow-field data is a critical task in experimental fluid mechanics, where the goal is to reconstruct high-resolution, physically consistent flow information from low-resolution or noisy measurements. This is particularly important in scenarios where high-fidelity data is either unavailable or computationally expensive to obtain. Physics-Informed Neural Networks (PINNs) have emerged as a promising tool for addressing this challenge, as they are capable of integrating physical laws into the learning process, ensuring that the reconstructed flow fields adhere to the underlying governing equations, such as the Navier-Stokes equations [11].

One of the key advantages of PINNs in super-resolution tasks is their ability to incorporate the physics of fluid dynamics directly into the loss function. By embedding the governing equations into the neural network's training process, PINNs ensure that the predicted flow fields are not only statistically consistent with the available data but also physically accurate. This is crucial in experimental fluid mechanics, where the reconstructed data must reflect the actual physical behavior of the fluid, even in regions where measurements are sparse or noisy. The approach of using PINNs for super-resolution has been demonstrated in several studies, including a paper that showcased the application of PINNs to reconstruct flow fields from low-resolution measurements, ensuring that the results comply with the principles of mass and momentum conservation [11].

In practice, super-resolution of flow-field data using PINNs involves training the neural network to minimize a loss function that includes both the data-fidelity term and the physics-based term. The data-fidelity term ensures that the network's predictions align with the available measurements, while the physics-based term enforces the solution to satisfy the governing equations of fluid dynamics. This dual objective ensures that the reconstructed flow fields are both consistent with the measurements and physically valid. For instance, in one study, a PINN was trained on a limited set of noisy measurements to reconstruct the velocity and pressure fields of a flow, achieving high accuracy even in regions where the data was sparse [11]. The results demonstrated that PINNs could not only recover the general flow patterns but also capture the fine-scale structures that are critical in understanding complex flow phenomena.

The application of PINNs to super-resolution tasks is particularly effective in cases where the flow data is highly noisy or incomplete. Traditional data-driven models, such as standard neural networks, often struggle to produce accurate predictions in such scenarios, as they lack the ability to enforce physical constraints. In contrast, PINNs leverage the knowledge of the governing equations to guide the learning process, ensuring that the network's predictions are not only statistically accurate but also physically meaningful. This is especially important in experimental fluid mechanics, where the presence of noise and the limited availability of high-resolution data can significantly impact the quality of the reconstructed flow fields.

Furthermore, the use of PINNs for super-resolution tasks is not limited to two-dimensional flows. Studies have shown that PINNs can be effectively applied to three-dimensional flow fields, as well. For example, a paper on the application of PINNs to super-resolution of flow-field data demonstrated that the approach could be extended to three-dimensional problems, allowing for the reconstruction of high-resolution flow fields even in complex geometries [11]. This demonstrates the versatility of PINNs in handling a wide range of flow scenarios, from simple laminar flows to complex turbulent flows.

Another advantage of using PINNs for super-resolution is their ability to handle irregular and unstructured grids, which are common in experimental fluid mechanics. Traditional numerical methods, such as finite difference or finite volume methods, often require structured grids, which can be challenging to implement in complex geometries. PINNs, on the other hand, are mesh-free and can be trained on point cloud data, making them well-suited for applications where the data is collected from irregular or unstructured domains. This makes PINNs a powerful tool for super-resolution tasks in experimental fluid mechanics, where the data is often collected using sensors placed at irregular intervals.

In addition to their ability to handle irregular data, PINNs also offer a significant advantage in terms of computational efficiency. Traditional super-resolution methods, such as interpolation or extrapolation techniques, often require a large amount of computational resources, especially when dealing with high-dimensional data. PINNs, by contrast, are capable of achieving high-quality reconstructions with relatively fewer computational resources. This is due to the fact that PINNs can be trained using a small number of data points, and their loss functions are designed to ensure that the physics of the problem is satisfied without requiring a full numerical simulation. As a result, PINNs can be used to quickly and efficiently reconstruct high-resolution flow fields from limited data, making them a valuable tool in experimental fluid mechanics [11].

The effectiveness of PINNs in super-resolution tasks has also been validated through a series of benchmark problems. For instance, a study demonstrated the application of PINNs to reconstruct flow fields from low-resolution measurements in a two-dimensional vortex shedding scenario, where the results were found to be highly accurate and consistent with the expected flow behavior [11]. Another study applied PINNs to the super-resolution of flow-field data in a turbulent channel flow, where the results showed that the PINN was able to accurately capture the complex flow structures even with a limited amount of training data [11].

In summary, PINNs have proven to be a powerful tool for the super-resolution of flow-field data in experimental fluid mechanics. By integrating the physics of fluid dynamics into the learning process, PINNs ensure that the reconstructed flow fields are both statistically consistent with the available data and physically accurate. This makes them well-suited for applications where high-resolution data is scarce or unavailable, and they offer significant advantages in terms of computational efficiency and versatility. As research in this area continues to advance, it is expected that PINNs will play an increasingly important role in the analysis and interpretation of experimental fluid mechanics data.

### 3.28 Causality in PINN Training

Causality plays a critical role in the training of Physics-Informed Neural Networks (PINNs), especially when dealing with complex and dynamic systems such as turbulent flows and chaotic systems. The inherent temporal and spatial dependencies within the governing partial differential equations (PDEs) necessitate a careful consideration of causality during the training process. Traditional PINN formulations often fail to respect these causal relationships, leading to suboptimal solutions and difficulties in achieving convergence. By re-formulating loss functions to explicitly account for causality, researchers have demonstrated significant improvements in accuracy and convergence, particularly for problems involving high-frequency dynamics and nonlinear interactions.

One of the primary challenges in training PINNs is the lack of temporal causality in the loss function, which can result in the network learning non-physical solutions. In chaotic and turbulent systems, the solution at a given time step depends on the previous states, and any deviation from this causality can propagate errors throughout the simulation. Recent studies have emphasized the importance of incorporating temporal causality into the training process. For instance, in the work by [119], the authors highlight that existing PINN formulations often fail to respect the spatio-temporal causal structure inherent in physical systems. They argue that this is a fundamental limitation that can lead to erroneous solutions and propose a simple re-formulation of the loss function to explicitly account for physical causality. The results show that this modification significantly improves the accuracy of PINNs, particularly for chaotic systems like the Lorenz system and the Navier-Stokes equations in the turbulent regime.

The re-formulation of loss functions to respect causality involves ensuring that the network learns the correct temporal dependencies by integrating the governing equations in a causal manner. This approach can be particularly effective in scenarios where the solution exhibits multi-scale or chaotic behavior, as it prevents the network from learning non-physical correlations that may arise from an improperly structured loss function. By incorporating causal constraints into the training process, the network is guided to learn solutions that align with the underlying physical laws, leading to more accurate and stable predictions.

Another aspect of causality in PINN training involves the spatial dependencies within the solution domain. In fluid mechanics, the solution at a given point is influenced by the surrounding regions, and neglecting these spatial dependencies can lead to inaccuracies in the predicted flow field. Techniques such as adaptive sampling and domain decomposition have been proposed to ensure that the network accounts for these spatial relationships. For example, the work by [49] introduces a novel adaptive causal sampling method that improves the performance and efficiency of PINNs by considering the causal structure of the problem. This method ensures that the network is trained on points that are most relevant to the solution, leading to better convergence and higher accuracy.

In addition to temporal and spatial causality, the integration of physical laws into the loss function is crucial for ensuring that the network learns solutions that are consistent with the underlying physics. Traditional PINN formulations often rely on a weighted sum of different loss terms, which may not adequately capture the complex interactions within the system. By re-formulating the loss function to explicitly account for the physical constraints, researchers can improve the network's ability to learn the correct solution. The work by [98] presents an inverse-Dirichlet weighting strategy that alleviates the issues arising from scale imbalances during training. This approach ensures that the network is trained in a manner that respects the physical constraints, leading to more reliable and accurate solutions.

The importance of causality in PINN training is further highlighted by the challenges associated with solving high-dimensional and nonlinear PDEs. In such cases, the network may struggle to converge to the correct solution if the loss function does not properly account for the causal relationships within the system. By re-formulating the loss function to explicitly incorporate these causal dependencies, researchers can improve the convergence properties of the network and ensure that it learns the correct solution. The work by [120] demonstrates the effectiveness of incorporating frequency compensation into the loss function, which helps the network generalize better across different design conditions.

Moreover, the consideration of causality in PINN training can lead to significant improvements in the performance of the network for real-world applications. For instance, in the context of fluid dynamics, ensuring that the network respects the causal relationships between different flow regions can lead to more accurate predictions of velocity and pressure fields. The work by [24] highlights the importance of incorporating physical constraints into the loss function to ensure that the network learns the correct solution. By respecting the causal relationships inherent in the system, the network can produce more accurate and reliable predictions, even in the presence of noisy or incomplete data.

In summary, the importance of respecting causality in PINN training cannot be overstated, especially for chaotic and turbulent systems. By re-formulating the loss function to explicitly account for causal relationships, researchers can improve the accuracy and convergence of PINNs, leading to more reliable and physically consistent solutions. The integration of temporal and spatial causality, along with the consideration of physical constraints, is essential for ensuring that PINNs can effectively model complex fluid dynamics problems. As the field of PINNs continues to evolve, the focus on causality will play a crucial role in advancing the capabilities of these models and enabling their application to a wide range of scientific and engineering problems.

### 3.29 Simulating THM Processes in Porous Media

The simulation of thermo-hydro-mechanical (THM) processes in porous media is a complex and critical task in geosciences, particularly in applications such as geothermal energy extraction, carbon sequestration, and subsurface contaminant transport. These processes involve coupled interactions between thermal, hydraulic, and mechanical phenomena, which can lead to nonlinear and highly variable behaviors. Traditional numerical methods for solving THM problems often rely on discretizing the governing equations, which can be computationally expensive and challenging for large-scale or high-dimensional systems. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering the ability to model complex multiphysics systems while incorporating physical laws directly into the learning process. This subsection focuses on the application of PINNs in simulating THM processes in porous media, including dimensionless formulations and sequential training strategies tailored for multi-physics problems.

One of the key challenges in simulating THM processes is the coupling of multiple physical domains, such as heat transfer, fluid flow, and solid deformation. The governing equations for these processes are typically nonlinear and involve interactions across different spatial and temporal scales. For example, the momentum equation, energy equation, and mass conservation equation must be solved simultaneously, with each equation depending on the others through coupling terms. PINNs provide a flexible framework to address this challenge by embedding the governing equations directly into the loss function, allowing the neural network to learn the solution while respecting the physical constraints. This approach is particularly beneficial when dealing with sparse or noisy data, as the physics-based regularization ensures that the predicted solutions remain consistent with the underlying physical laws [6].

A notable advancement in the application of PINNs to THM processes is the use of dimensionless formulations. Dimensionless equations are derived by scaling the governing equations with characteristic quantities, such as length, velocity, and temperature, to remove physical units and simplify the problem. This scaling not only reduces the computational complexity but also enhances the generalizability of the PINN model. By training the network on dimensionless variables, the model can adapt more easily to different physical scenarios without requiring extensive retraining. This approach has been successfully applied in several studies, including those focused on simulating heat transfer and fluid flow in porous media [121].

In addition to dimensionless formulations, sequential training strategies have been proposed to improve the stability and accuracy of PINN simulations for THM processes. These strategies involve breaking down the problem into smaller, more manageable subproblems and training the PINN incrementally. For example, the mechanical and thermal components of the problem can be trained separately before combining them to solve the full coupled system. This approach is particularly useful for problems with strong nonlinearities or where the solution evolves over time. Sequential training can also help mitigate the issue of "spectral bias," which refers to the tendency of PINNs to converge to solutions that accurately satisfy the PDE residuals in the low-frequency range but struggle to capture high-frequency dynamics. By starting with simpler subproblems and gradually introducing complexity, the PINN can better capture the full range of behaviors in the THM process [122].

A specific example of the application of PINNs to THM processes is the simulation of heat transfer and fluid flow in fractured porous media. In such systems, the presence of fractures introduces additional complexity due to the heterogeneity of the domain and the potential for preferential flow paths. PINNs have been used to model these systems by incorporating the governing equations for heat conduction, fluid flow, and mechanical deformation into the loss function. The model can then predict the evolution of temperature, pressure, and displacement fields within the porous medium. One study demonstrated that PINNs could effectively capture the coupled behavior of heat and fluid flow in a fractured medium, even with limited data, by leveraging the physics-informed regularization [10].

Another important aspect of PINN applications in THM processes is the integration of sequential training strategies with domain decomposition techniques. Domain decomposition involves dividing the computational domain into subdomains, each of which can be solved independently before combining the results. This approach can significantly reduce the computational burden, especially for large-scale problems. By combining domain decomposition with sequential training, PINNs can be trained on each subdomain separately, ensuring that the solution remains consistent across the entire domain. This method has been shown to improve the accuracy and efficiency of THM simulations, particularly in cases where the physical properties vary significantly across the domain [70].

Moreover, the use of PINNs for THM processes allows for the incorporation of experimental data and real-world measurements, which can further enhance the predictive capabilities of the model. For instance, in geothermal reservoir simulations, temperature and pressure measurements from well logs can be used to constrain the PINN and improve the accuracy of the predictions. This data-driven approach is particularly valuable when dealing with uncertain or incomplete data, as the physics-informed regularization ensures that the model remains physically consistent even in the presence of noise. A recent study demonstrated that PINNs could effectively integrate experimental data with the governing equations to simulate THM processes in a geothermal reservoir, resulting in accurate predictions of temperature and pressure distributions [81].

In conclusion, the application of PINNs to simulating thermo-hydro-mechanical (THM) processes in porous media represents a significant advancement in the field of computational geosciences. By leveraging dimensionless formulations, sequential training strategies, and domain decomposition techniques, PINNs can effectively model the complex interactions between thermal, hydraulic, and mechanical phenomena. These approaches not only improve the accuracy and efficiency of the simulations but also provide a flexible framework for handling a wide range of physical scenarios. As research in this area continues to evolve, the integration of PINNs with experimental data and advanced training methodologies will further enhance their applicability to real-world THM problems.

### 3.30 Reduced Order Models with PINNs

[82]

Reduced order models (ROMs) have become a crucial tool in fluid dynamics for efficiently solving complex problems with reduced computational cost. The integration of physics-informed neural networks (PINNs) with traditional model reduction techniques, such as Proper Orthogonal Decomposition (POD) and Galerkin projection, has opened new avenues for developing ROMs that are both accurate and computationally efficient. This synergy between data-driven learning and physical principles has shown great promise in tackling inverse problems in the Navier-Stokes equations, where traditional methods often struggle with high computational costs and the need for extensive data.

One notable approach is the use of POD-Galerkin methods combined with deep neural networks to develop reduced order models for inverse problems in the Navier-Stokes equations [123]. In this method, the solution fields are projected onto a lower-dimensional subspace using POD, and the resulting reduced-order model is then approximated using a physics-informed neural network. This approach leverages the strengths of both techniques, allowing for the efficient approximation of unknown parameters such as physical constants or boundary conditions. The neural networks are trained to satisfy the reduced-order equations derived from the Galerkin projection, ensuring that the solutions adhere to the underlying physics.

The integration of PINNs with POD-Galerkin methods has been demonstrated to be highly effective in various applications. For example, in the case of steady flow around a backward step and unsteady turbulent flow around a surface-mounted cubic obstacle, the proposed ROM was able to accurately approximate the unknown parameters while maintaining a high level of computational efficiency [123]. This approach not only reduces the computational burden but also allows for the incorporation of physical constraints, ensuring that the solutions are consistent with the governing equations.

Another key advantage of using PINNs in reduced order modeling is their ability to handle parameterized problems. By incorporating parameters into the input of the neural network, PINNs can learn the solution functions of parametric Navier-Stokes equations, enabling the interpolation of solution functions for a range of parameters [3]. This capability is particularly useful in scenarios where the governing equations depend on multiple parameters, and the goal is to estimate these parameters from sparse or noisy data. The inclusion of the PDEs in the loss function ensures that the neural network learns the underlying physical laws, leading to more accurate and robust predictions.

The effectiveness of PINNs in developing reduced order models is further enhanced by the use of transfer learning. Transfer learning allows the pre-trained neural networks to be fine-tuned for new problems with minimal additional training data. This is particularly beneficial in scenarios where the parameters of the Navier-Stokes equations change, and the model needs to be adapted to new conditions. By leveraging the knowledge gained from previous training, transfer learning can significantly reduce the computational cost and time required to develop a new reduced order model [9].

In addition to the integration of POD-Galerkin methods and transfer learning, the use of physics-informed neural networks in reduced order modeling also benefits from the incorporation of adaptive sampling techniques. These techniques dynamically adjust the placement of collocation points during the training process, improving the accuracy and convergence of the model [93]. By focusing on regions of the domain where the solution is more complex or where the residuals are higher, adaptive sampling can significantly enhance the performance of PINNs in solving inverse problems.

Another important aspect of using PINNs in reduced order modeling is the ability to handle noisy and sparse data. Traditional numerical methods often struggle with high levels of noise or incomplete data, leading to inaccurate or unreliable solutions. However, PINNs are designed to incorporate physical constraints into the loss function, which helps to regularize the model and improve its robustness to noise. This is particularly advantageous in scenarios where the data is limited or corrupted, as the physics-informed loss function ensures that the solutions remain consistent with the governing equations [9].

The use of PINNs in reduced order modeling has also been extended to more complex and high-dimensional problems. For instance, the development of multi-receptive-field PINNs (MRF-PINNs) has enabled the solution of different types of PDEs on various mesh resolutions without the need for manual tuning [48]. This approach leverages the advantages of convolutional neural networks, such as parameter sharing and spatial feature extraction, to improve the accuracy and efficiency of the model. The dimensional balance method used in MRF-PINNs helps to estimate the loss weights, further enhancing the performance of the model.

Moreover, the integration of PINNs with traditional numerical methods has led to the development of hybrid approaches that combine the strengths of both techniques. These hybrid models can leverage the high accuracy of traditional numerical solvers while benefiting from the flexibility and efficiency of PINNs. For example, the use of domain decomposition techniques in conjunction with PINNs has been shown to improve the performance of the model in solving complex fluid dynamics problems [73]. By dividing the domain into smaller subdomains and solving the PDEs in each subdomain, domain decomposition techniques can significantly reduce the computational burden while maintaining the accuracy of the solution.

In summary, the use of physics-informed neural networks in developing reduced order models for solving inverse problems in the Navier-Stokes equations has demonstrated significant potential. The integration of traditional model reduction techniques such as POD-Galerkin methods, the use of transfer learning, adaptive sampling, and the incorporation of physical constraints into the loss function have all contributed to the development of more accurate and efficient reduced order models. These advancements not only improve the computational efficiency of the models but also enhance their robustness to noise and sparse data, making them well-suited for a wide range of applications in fluid dynamics.

## 4 Methodological Advances and Enhancements

### 4.1 Adaptive Sampling

Adaptive sampling has emerged as a crucial technique in improving the accuracy and convergence of Physics-Informed Neural Networks (PINNs), especially for solving complex partial differential equations (PDEs). Traditional PINNs often rely on fixed collocation points, which may not adequately capture the intricate behavior of solutions, particularly in regions with high gradients or complex geometries. To address these limitations, various adaptive sampling methods have been proposed, dynamically adjusting the placement of collocation points to enhance model performance. These techniques not only improve the quality of the solutions but also reduce the computational burden by focusing on areas that contribute more to the overall accuracy.

One of the earliest and most widely studied adaptive sampling techniques is the adaptive collocation scheme. This method dynamically adjusts the distribution of collocation points based on the residual error of the PDE solution. By placing more points in regions where the residual is high, the network can better approximate the solution in those areas. This approach has been shown to significantly improve the convergence of PINNs, especially for problems with sharp gradients or discontinuities. For instance, the work by [42] introduces a Gaussian mixture distribution-based adaptive sampling method (GAS) that uses the current residual information to generate a distribution for sampling additional points. This method has demonstrated state-of-the-art accuracy in solving PDEs, outperforming traditional uniform and Latin hypercube sampling strategies.

Another notable adaptive sampling technique is R3 sampling, which stands for "Residual-Refined Resampling." This method iteratively refines the collocation points by analyzing the residual distribution of the PINN solution. By focusing on regions where the residual is highest, R3 sampling ensures that the network learns the most critical aspects of the solution. This approach has been particularly effective in solving problems with complex dynamics, such as those involving turbulent flows or high-frequency oscillations. The work by [124] demonstrates how R3 sampling can significantly reduce the number of collocation points required while maintaining high accuracy. This makes the method especially suitable for large-scale simulations where computational resources are limited.

Failure-informed adaptive sampling is another promising approach that leverages the concept of "failure probability" to guide the selection of collocation points. This technique identifies regions where the PINN is likely to fail by analyzing the model's performance on a subset of training data. By prioritizing these regions for additional sampling, the network can be trained more effectively, leading to improved accuracy. The study by [34] highlights the effectiveness of this approach in problems where the solution exhibits strong nonlinearity or where the PDE is highly sensitive to initial conditions. The results show that failure-informed adaptive sampling can significantly reduce the number of training iterations required, making it a valuable tool for real-time applications.

A related technique, RANG (Reinforced Adaptive Numerical Grid), has also been developed to address the challenges of adaptive sampling in PINNs. RANG uses a reinforcement learning framework to dynamically adjust the grid of collocation points based on the network's performance. By treating the placement of points as a decision-making process, RANG ensures that the collocation points are optimally distributed to minimize the error in the solution. This method has been particularly effective in solving PDEs with irregular geometries or high-dimensional domains. The work by [97] demonstrates that RANG can achieve higher accuracy with fewer collocation points compared to traditional methods, making it a promising approach for complex fluid dynamics problems.

In addition to these techniques, several studies have explored the use of multi-scale adaptive sampling to address the challenges of solving PDEs with varying spatial and temporal scales. These methods dynamically adjust the collocation points based on the scale of the features present in the solution. By focusing on the relevant scales, these techniques can capture the fine details of the solution without requiring excessive computational resources. For example, the work by [125] introduces a multi-scale adaptive sampling strategy that uses a combination of global and local collocation points to improve the accuracy of the solution. This approach has been shown to be particularly effective for problems involving turbulent flows or high-frequency oscillations, where the solution exhibits a wide range of spatial and temporal scales.

The effectiveness of these adaptive sampling techniques has been validated through various numerical experiments, including the solution of the Navier-Stokes equations, the heat equation, and the Kuramoto-Sivashinsky equation. For instance, the study by [22] demonstrates how adaptive sampling can significantly improve the accuracy of PINNs in simulating turbulent flows, even with sparse data. Similarly, the work by [42] shows that the Gaussian mixture distribution-based approach can achieve high accuracy with minimal computational effort, making it suitable for real-time applications.

Overall, adaptive sampling has proven to be a powerful tool for improving the performance of PINNs in solving complex PDEs. By dynamically adjusting the placement of collocation points, these techniques ensure that the network focuses on the most critical regions of the solution, leading to improved accuracy and convergence. As the field of PINNs continues to evolve, further research into adaptive sampling methods will be essential for addressing the challenges of solving increasingly complex fluid dynamics problems. The ongoing development of these techniques will not only enhance the capabilities of PINNs but also expand their applicability to a broader range of scientific and engineering problems.

### 4.2 Hybrid Models

Hybrid models have emerged as a promising direction in the development of Physics-Informed Neural Networks (PINNs), combining the strengths of traditional numerical methods with the flexibility of deep learning to enhance accuracy, efficiency, and robustness in solving complex fluid dynamics problems. These hybrid approaches integrate PINNs with classical numerical techniques, such as domain decomposition methods, multi-fidelity learning, and other machine learning frameworks, to address the limitations of purely data-driven or physics-informed models. By leveraging the complementary advantages of different methodologies, hybrid PINN models offer a more versatile and powerful tool for tackling challenging problems in fluid mechanics, from high-dimensional PDEs to multi-physics simulations.

One of the key strategies in hybrid models is the integration of domain decomposition methods with PINNs. This approach divides the computational domain into subdomains, where each subdomain is solved independently, and the solutions are then combined to obtain the global solution. Domain decomposition can significantly improve the scalability and efficiency of PINNs, especially for large-scale problems with complex geometries or high-dimensional PDEs. For instance, the work presented in [70] proposes a framework based on the Schwarz domain decomposition method to accelerate the training of PINNs. By decomposing the domain into smaller subdomains and using iterative methods such as additive, multiplicative, or hybrid strategies, this approach enhances the accuracy and convergence of PINNs, particularly for problems with multi-scale or heterogeneous solutions.

Another important direction in hybrid models is multi-fidelity learning, which leverages data of varying fidelity levels to improve the training and generalization capabilities of PINNs. Traditional numerical methods often require high-fidelity data, which can be computationally expensive to generate. In contrast, PINNs can benefit from a combination of low-fidelity and high-fidelity data, where the former provides a rough approximation of the solution and the latter refines it. This approach is particularly useful in scenarios where high-fidelity data is scarce or costly. For example, the paper [54] explores the use of multi-objective loss balancing techniques to address the challenge of combining loss terms with different magnitudes and scales. By adaptively weighting the contributions of different loss components, multi-fidelity learning ensures that the PINN can effectively learn from both low-fidelity and high-fidelity data, leading to more accurate and stable solutions.

Hybrid models also incorporate traditional numerical methods as part of the PINN training process, providing a more robust and reliable framework for solving PDEs. For instance, the paper [126] proposes a physics-constrained machine learning framework called AdjointNet, which allows domain scientists to embed their existing physics-based codes into the training workflow of neural networks. This approach ensures that the physics constraints are strictly satisfied across the entire domain, while also maintaining the mathematical properties such as consistency, stability, and convergence. By leveraging established numerical methods, AdjointNet improves the accuracy of parameter estimation and experimental design, making it a valuable tool for applications in fluid dynamics and other engineering disciplines.

Furthermore, hybrid models integrate PINNs with other machine learning techniques, such as graph neural networks (GNNs) and attention mechanisms, to enhance their performance in handling irregular geometries and complex physical systems. The paper [127] introduces a novel framework that combines PINNs with graph neural networks and radial basis function finite difference (RBF-FD) methods to solve spatiotemporal PDEs on irregular domains. This hybrid approach benefits from the strengths of GNNs in handling unstructured data and RBF-FD in constructing high-precision numerical schemes, leading to improved accuracy and efficiency in solving complex PDEs. Similarly, [4] presents a hybrid architecture that integrates attention mechanisms with PINNs to better capture the non-linear features of solutions, particularly for problems involving shock waves or discontinuities.

Another notable example of hybrid models is the use of PINNs in conjunction with traditional solvers for linear systems, as demonstrated in [58]. While PINNs have shown promise in solving linear systems, their accuracy and computational efficiency are still limited, especially for high-frequency components. To address this, the paper proposes a hybrid strategy that combines PINNs with traditional linear solvers such as PETSc conjugate gradient solvers. This integration allows PINNs to handle the low-frequency components of the solution efficiently while leveraging the accuracy of traditional solvers for the high-frequency components. The results show that this hybrid approach achieves performance comparable to that of traditional solvers, demonstrating its potential as a new class of linear solvers for scientific computing.

In addition to numerical and machine learning techniques, hybrid models also incorporate physics-informed regularization strategies to improve the generalization and robustness of PINNs. The paper [128] presents a framework that incorporates energy or Lyapunov structure into the loss function to ensure the stability and physical consistency of the solution. This approach enhances the accuracy of PINNs in solving problems with complex dynamics and nonlinear behavior, making it a valuable tool for applications in fluid mechanics and other physical systems.

Overall, hybrid models represent a significant advancement in the development of physics-informed neural networks, offering a more flexible, efficient, and accurate approach to solving complex fluid dynamics problems. By integrating traditional numerical methods, multi-fidelity learning, and other machine learning techniques, these models address the limitations of purely data-driven or physics-informed approaches, opening up new possibilities for the future of fluid mechanics simulations and beyond.

### 4.3 Physics-Informed Activation Functions

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving partial differential equations (PDEs) by integrating physical laws directly into the training process through loss functions. One of the most recent and promising methodological advances in this domain is the development of physics-informed activation functions (PAFs), which extend the concept of PINNs by embedding physical knowledge directly into the neural network's activation functions. This innovation aims to enhance both the efficiency and validity of PINNs, making them more effective at capturing the underlying physics of the problem being modeled.  

PAFs are a novel approach that modifies the standard activation functions used in neural networks, such as ReLU, sigmoid, or tanh, by incorporating physical constraints and principles. By doing so, PAFs can improve the convergence and accuracy of PINNs, particularly in complex fluid dynamics scenarios where traditional activation functions may struggle to enforce the required physical consistency. The key idea behind PAFs is that they are not arbitrary functions but are instead designed to align with the governing equations of the problem, ensuring that the neural network learns solutions that are physically meaningful and accurate. This is particularly important in applications where the solution must satisfy specific boundary or initial conditions, such as in fluid flow simulations.  

The introduction of PAFs into PINNs has been driven by the need to address some of the limitations of traditional PINN architectures. For instance, in fluid dynamics, where the governing equations often involve complex interactions between different physical quantities, standard activation functions may not provide sufficient flexibility to capture these dynamics accurately. By contrast, PAFs can be tailored to incorporate the specific physical properties of the system, such as conservation laws, symmetry, or nonlinearity. This makes them a valuable tool for improving the training efficiency and generalization capabilities of PINNs.  

One of the most notable contributions to the field of PAFs is the work presented in the paper titled "RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks," which explores the use of conditionally positive definite Radial Basis Functions (RBFs) as activation functions [129]. The authors highlight the limitations of widely used Fourier-based feature mappings in certain situations and propose RBFs as a more effective alternative. This study demonstrates that the use of RBFs in activation functions can significantly enhance the performance of PINNs, especially in cases where the solution exhibits high-frequency components or complex spatial variations. The results show that RBF-based PAFs lead to better accuracy and convergence compared to traditional activation functions.  

Another significant contribution to the development of PAFs is found in the paper "Physics-Informed Attention-Based Neural Networks for Solving Non-Linear Partial Differential Equations," which discusses the design of attention mechanisms to improve the performance of PINNs [4]. While the focus of this paper is on the design of attention mechanisms, the authors also emphasize the importance of incorporating physical knowledge into the activation functions. By using a combination of attention mechanisms and PAFs, the proposed framework, called PIANNs, is able to effectively capture the nonlinear behavior of PDEs, particularly in scenarios involving shocks and discontinuities. This approach demonstrates the potential of PAFs to enhance the interpretability and robustness of PINNs in challenging fluid dynamics problems.  

In addition to improving accuracy and convergence, PAFs also contribute to the efficiency of PINNs by reducing the number of training iterations required to achieve a given level of performance. This is particularly important in practical applications where computational resources are limited. The paper "Loss Landscape Engineering via Data Regulation on PINNs" highlights the importance of data regulation in improving the optimization landscape of PINNs [69]. While the focus of this paper is on the impact of data on the loss landscape, the authors also suggest that incorporating physical knowledge into the activation functions can further enhance the efficiency of training. By aligning the activation functions with the physical constraints of the problem, PAFs can guide the network towards physically consistent solutions more quickly, reducing the overall training time.  

The concept of PAFs also intersects with the broader idea of physics-informed deep learning, where the goal is to integrate domain-specific knowledge into neural network architectures. The paper "Physics-Informed Deep Learning for Incompressible Laminar Flows" presents a mixed-variable PINN approach that incorporates physical laws into the loss function [1]. While the focus of this work is on the development of a mixed-variable scheme, the authors also emphasize the importance of using activation functions that reflect the underlying physics of the problem. This aligns with the principles of PAFs and highlights their potential to improve the performance of PINNs in fluid dynamics applications.  

Beyond their application in fluid mechanics, PAFs have also been explored in other domains, such as solid mechanics and electromagnetism. The paper "Physics-Informed Graph Neural Galerkin Networks: A Unified Framework for Solving PDE-Governed Forward and Inverse Problems" introduces a novel framework that combines graph convolutional networks with PAFs to solve PDEs in a unified manner [130]. This work demonstrates that PAFs can be effectively integrated into different types of neural network architectures, making them a versatile tool for a wide range of scientific and engineering applications.  

In conclusion, the development of physics-informed activation functions (PAFs) represents a significant advancement in the field of PINNs. By embedding physical knowledge directly into the activation functions, PAFs improve the efficiency, validity, and accuracy of PINNs, making them more suitable for solving complex fluid dynamics problems. The contributions of various studies, such as those on RBF-based PAFs and attention mechanisms, highlight the potential of PAFs to address some of the most challenging issues in PINN training and convergence. As research in this area continues to evolve, it is expected that PAFs will play an increasingly important role in the development of more robust and efficient PINN-based methods for solving PDEs.

### 4.4 Multi-Fidelity Learning

Multi-fidelity learning (MFL) has emerged as a powerful technique to enhance the training and generalization capabilities of Physics-Informed Neural Networks (PINNs), particularly in complex and high-dimensional fluid mechanics problems. Traditional PINN approaches often rely on high-fidelity data, such as accurate numerical simulations or experimental measurements, to train the neural networks effectively. However, such data can be computationally expensive to obtain, especially for large-scale simulations or high-resolution problems. MFL addresses this challenge by integrating data of varying fidelity levels, from low-fidelity approximations to high-fidelity simulations, to improve the performance of PINNs without sacrificing accuracy. This approach is especially beneficial in fluid mechanics, where high-fidelity data may be scarce or prohibitively costly, while low-fidelity data may be more readily available. By leveraging the strengths of both high- and low-fidelity data, MFL enables more efficient training, better convergence, and improved generalization, making it a promising enhancement for PINNs in complex fluid dynamics scenarios.  

One of the core principles of MFL is to use a hierarchical framework that combines data from different sources, such as coarse-grid simulations, simplified models, and high-resolution numerical solutions. These data are then used to train PINNs in a way that balances the trade-off between accuracy and computational cost. For example, in fluid mechanics, coarse-grid simulations can provide a rough approximation of the flow field, while high-fidelity data can refine the solution in critical regions. By integrating these different data sources, MFL allows PINNs to learn from a broader range of information, which is particularly useful in high-dimensional problems where the solution space is vast and complex. This is highlighted in the paper *Machine learning accelerated computational fluid dynamics* [14], where it is shown that low-fidelity data can be used to guide the training of PINNs, reducing the reliance on expensive high-fidelity simulations while still maintaining a high level of accuracy.  

A key advantage of MFL in PINNs is its ability to improve the convergence behavior of the neural network. PINNs can suffer from issues such as non-convex loss landscapes, where the optimization process gets stuck in local minima, or where the network fails to converge due to the imbalance between different loss terms. By incorporating data of varying fidelity, MFL provides additional information that can guide the optimization process, making it more stable and efficient. For instance, in the paper *Machine learning accelerated computational fluid dynamics* [14], the authors demonstrate that MFL can significantly accelerate the training of PINNs by using low-fidelity data to pre-train the network, after which high-fidelity data is used to fine-tune the solution. This two-step approach not only improves the convergence speed but also enhances the overall accuracy of the model.  

Moreover, MFL can be particularly effective in handling problems with limited data availability. In many fluid mechanics applications, high-fidelity data may be difficult to obtain due to experimental constraints or computational limitations. MFL offers a way to leverage the available low-fidelity data to improve the performance of PINNs, even when high-fidelity data is scarce. This is demonstrated in the paper *Fast Neural Network Predictions from Constrained Aerodynamics Datasets* [131], where the authors use MFL to enhance the accuracy of PINNs by incorporating data from constrained aerodynamics simulations. The results show that the integration of low-fidelity data significantly improves the model’s ability to generalize to new scenarios, even when the training data is limited.  

Another important application of MFL in PINNs is in the context of multi-physics problems, where different physical processes interact in complex ways. In such scenarios, data from different fidelity levels can provide complementary information that helps the PINN to learn the underlying physics more effectively. For example, in the paper *Multiscale Neural Operator* [132], the authors propose a hybrid approach that combines data from different scales and fidelity levels to train a neural operator that can accurately predict the solution of PDEs. This approach is particularly useful in fluid mechanics, where the interactions between different scales can significantly influence the overall behavior of the system. By using MFL, the model can capture the essential features of the problem without being overwhelmed by the complexity of the high-fidelity data.  

Furthermore, MFL can also be used to enhance the robustness of PINNs in the presence of noisy or uncertain data. In fluid mechanics, measurements or simulations may contain errors or uncertainties that can affect the accuracy of the model. By integrating data of varying fidelity, MFL provides a way to mitigate the impact of these errors, improving the reliability of the PINN’s predictions. This is highlighted in the paper *Machine learning accelerated computational fluid dynamics* [14], where the authors show that MFL can improve the model’s ability to handle noisy data by incorporating information from low-fidelity simulations that are less sensitive to errors. This makes MFL a valuable tool for real-world applications where data quality can be a significant challenge.  

In addition to improving the accuracy and robustness of PINNs, MFL can also lead to significant computational savings. By using low-fidelity data to guide the training process, MFL reduces the need for expensive high-fidelity simulations, which can be a major bottleneck in large-scale fluid mechanics problems. This is demonstrated in the paper *Machine learning accelerated computational fluid dynamics* [14], where the authors report a significant reduction in computational cost while maintaining high accuracy. This makes MFL an attractive approach for applications where computational resources are limited or where real-time predictions are required.  

Overall, multi-fidelity learning is a powerful technique that enhances the training and generalization capabilities of PINNs in complex and high-dimensional fluid mechanics problems. By leveraging data of varying fidelity levels, MFL provides a more efficient and robust approach to training PINNs, making them more suitable for real-world applications. As the field of fluid mechanics continues to evolve, the integration of MFL with PINNs is expected to play an increasingly important role in advancing the capabilities of data-driven simulations.

### 4.5 Transfer Learning and Meta-Transfer Learning

Transfer learning has emerged as a powerful strategy to enhance the efficiency and effectiveness of Physics-Informed Neural Networks (PINNs) in fluid mechanics. By leveraging pre-trained models on related tasks, transfer learning enables PINNs to adapt faster to new problems and reduces the need for retraining from scratch, which is particularly beneficial in scenarios where data is scarce or computational resources are limited. This approach has been successfully applied in various studies to improve the performance of PINNs in solving complex fluid dynamics problems [35]. 

One of the key advantages of transfer learning in PINNs is its ability to generalize across different scenarios. For instance, in the context of branched flow simulations, a multi-head model was introduced to efficiently obtain accurate solutions to nonlinear systems of ordinary differential equations with random potentials [35]. This approach not only improved the accuracy of the solutions but also significantly reduced the computational time required for training, demonstrating the potential of transfer learning to accelerate the simulation process in fluid mechanics.

Meta-transfer learning (MTL), a more advanced form of transfer learning, further enhances the adaptability of PINNs by enabling the model to learn how to learn from previous experiences. This technique allows PINNs to quickly adapt to new tasks by leveraging knowledge acquired from a variety of related problems. In the context of PINNs, MTL has been explored to improve the performance of the models in solving complex fluid dynamics problems, where the models can benefit from the experience gained in solving similar problems [133]. 

A notable example of MTL in PINNs is the work presented in [133], where a general framework for transfer learning was proposed. This framework enables one-shot inference for linear systems of both ordinary and partial differential equations. By leveraging the knowledge from previously solved problems, the framework allows for the rapid acquisition of accurate solutions to new differential equations without the need for extensive retraining. This capability is particularly valuable in real-world applications where the ability to quickly adapt to new scenarios is crucial.

The application of transfer learning in PINNs is not limited to linear systems; it has also been extended to nonlinear problems. For example, in the context of fluid mechanics, transfer learning has been used to enhance the performance of PINNs in solving the Navier-Stokes equations. By transferring knowledge from previously solved problems, the models can more effectively capture the complex dynamics of fluid flows, leading to improved accuracy and efficiency [21]. This approach has been particularly effective in scenarios where the data is sparse or the problem is highly nonlinear.

Another significant advantage of transfer learning in PINNs is its ability to handle parameterized systems. In many fluid mechanics applications, the governing equations involve parameters that can vary significantly. Transfer learning allows PINNs to generalize across different parameter values, making them more robust and adaptable. This has been demonstrated in studies where transfer learning was used to solve parametric Navier-Stokes equations, where the models were able to interpolate between different parameter values with high accuracy [3].

In addition to transfer learning, the concept of meta-transfer learning (SMT) has been explored to further enhance the capabilities of PINNs. SMT involves training the model to learn the optimal way to transfer knowledge from one task to another, which can lead to more efficient and effective adaptation to new problems. This approach has been particularly useful in scenarios where the tasks are highly related but not identical, as it allows the model to extract and apply the most relevant information from previous tasks [134]. 

The effectiveness of transfer learning and SMT in PINNs has been validated through various experimental studies. For instance, in [134], a transfer learning method was proposed that uses singular value decomposition to transfer knowledge between different PDEs. The results showed that the method was effective in solving a class of PDEs with different but close right-hand-side functions, demonstrating the potential of transfer learning to generalize across a wide range of problems.

Moreover, the integration of transfer learning with other advanced techniques, such as domain decomposition and hybrid models, has further enhanced the performance of PINNs. For example, the use of domain decomposition strategies in conjunction with transfer learning has been shown to improve the accuracy and efficiency of PINNs in solving complex fluid dynamics problems [135]. This approach allows the models to focus on the most relevant regions of the domain, leading to more accurate and efficient solutions.

In conclusion, transfer learning and meta-transfer learning (SMT) have significantly enhanced the capabilities of Physics-Informed Neural Networks (PINNs) in fluid mechanics. By enabling the models to adapt quickly to new problems and reduce the need for retraining from scratch, these techniques have made PINNs more efficient and effective in solving complex fluid dynamics problems. The successful application of transfer learning and SMT in various studies underscores their potential to revolutionize the way PINNs are used in fluid mechanics, paving the way for more accurate and efficient simulations in the future.

### 4.6 Attention Mechanisms and Residual-Based Methods

Attention mechanisms have emerged as a critical component in enhancing the performance of Physics-Informed Neural Networks (PINNs), particularly in addressing the limitations of traditional architectures in capturing complex and non-linear dynamics. These mechanisms allow PINNs to focus on critical regions or features within the solution domain, thereby improving the accuracy and efficiency of the model's predictions. The integration of attention mechanisms into PINNs has been explored in several studies, with notable contributions such as the Physics-Informed Attention-based Neural Networks (PIANNs) [4]. PIANNs leverage attention mechanisms to adapt the behavior of deep neural networks to the non-linear features of the solution, overcoming the limitations of conventional PINNs in handling hyperbolic conservation laws that develop highly localized nonlinear shock waves. By focusing on relevant regions of the solution domain, PIANNs achieve superior performance in capturing complex dynamics, such as shock fronts, in hyperbolic models.

Residual-based attention mechanisms further enhance the capabilities of PINNs by emphasizing the residual components of the partial differential equations (PDEs) during training. This approach allows the model to prioritize areas where the PDE residuals are high, ensuring that the network learns to correct these inaccuracies more effectively. In particular, residual-based attention mechanisms can be integrated into the loss function, providing a dynamic weighting scheme that adapts to the complexity of the PDE being solved. For instance, the work on residual-based attention mechanisms has demonstrated that by emphasizing the residuals, PINNs can better approximate the underlying physical laws, even in the presence of noisy or sparse data. This method has been particularly effective in addressing the challenges of high-frequency and multi-scale PDEs, where traditional PINNs often struggle to converge due to the complexity of the solution landscape.

The application of attention mechanisms in PINNs is not limited to the solution of PDEs but extends to a wide range of fluid mechanics problems, including inverse problems and surrogate modeling. For example, in the context of inverse problems, attention mechanisms can help PINNs identify critical regions where the data is most informative, enabling the model to make more accurate predictions with fewer training samples. This is especially important in scenarios where the data is sparse or noisy, as the attention mechanisms allow the network to focus on the most relevant parts of the input space. The use of attention mechanisms in this context has been explored in several studies, where it has been shown to significantly improve the performance of PINNs in terms of both accuracy and computational efficiency.

In addition to attention mechanisms, residual-based methods have also played a crucial role in improving the performance of PINNs. These methods involve incorporating residual terms into the loss function to enforce the physical constraints of the PDEs. The residual terms act as a form of regularization, ensuring that the network's output satisfies the governing equations of the system being modeled. One notable example is the use of residual-based methods in the context of the Navier-Stokes equations, where the residual terms are used to enforce the conservation laws of mass and momentum. This approach has been shown to improve the accuracy of PINNs in simulating incompressible flows, even when the data is sparse or the boundary conditions are complex.

The integration of residual-based methods with attention mechanisms has further enhanced the capabilities of PINNs in solving complex fluid dynamics problems. By combining these two approaches, PINNs can dynamically adjust their focus based on the residual errors in different regions of the solution domain. This hybrid approach has been demonstrated to be particularly effective in handling problems with multiple scales and high-frequency dynamics, where the solution can vary significantly across different regions of the domain. For instance, in the context of turbulent flow simulations, the use of residual-based attention mechanisms has allowed PINNs to capture the intricate structures of the flow more accurately, leading to improved predictive performance.

Furthermore, the use of attention mechanisms and residual-based methods in PINNs has been extended to the development of more robust and scalable models. These methods enable PINNs to handle large-scale simulations and high-dimensional problems more efficiently, as they can dynamically adjust their focus and optimize the training process. For example, the introduction of attention mechanisms in the context of multi-scale PDEs has allowed PINNs to capture the interactions between different scales of the solution more effectively, leading to improved accuracy and convergence. This has been particularly beneficial in applications such as weather prediction and climate modeling, where the ability to capture multi-scale dynamics is crucial.

The effectiveness of attention mechanisms and residual-based methods in improving the performance of PINNs has been validated through a series of numerical experiments and case studies. These studies have demonstrated that the integration of these techniques can significantly enhance the accuracy and robustness of PINNs in a wide range of fluid mechanics applications. For instance, in the context of the Navier-Stokes equations, the use of attention mechanisms and residual-based methods has been shown to improve the prediction of velocity and pressure fields, even in the presence of complex geometries and turbulent flows. The results of these experiments highlight the potential of these techniques in advancing the capabilities of PINNs and expanding their applicability to more complex and challenging problems. [4] [10] [29]

### 4.7 Feature Mapping and Neural Architecture Improvements

Feature mapping and neural architecture improvements play a pivotal role in enhancing the training dynamics and performance of Physics-Informed Neural Networks (PINNs). Traditional neural networks often struggle with complex, high-dimensional, and nonlinear problems, which are prevalent in fluid mechanics and other physical systems. To address these challenges, researchers have explored various techniques such as feature mapping, specialized activation functions, and novel neural architecture designs. These advancements aim to improve the convergence behavior, generalization capability, and overall accuracy of PINNs, enabling them to solve challenging partial differential equations (PDEs) with greater efficiency and reliability.

One of the key aspects of improving PINNs is the use of feature mapping. Feature mapping involves transforming input data into a higher-dimensional space that better captures the underlying physics of the problem. This approach can help neural networks learn complex relationships and reduce the difficulty of optimization. For example, the use of Fourier-based feature mapping has been shown to improve the representation of high-frequency components in solutions, although it has limitations in certain scenarios [136]. To address these limitations, the use of conditionally positive definite Radial Basis Functions (RBFs) has been proposed as a more effective alternative, demonstrating superior performance in a variety of forward and inverse problem cases [136]. Feature mapping not only enhances the model's ability to approximate the solution but also helps in balancing the different components of the loss function during training.

Another critical factor in improving PINNs is the design of neural architectures. Traditional fully connected neural networks (FCNs) are often used in PINNs, but they can be computationally expensive and may not scale well for high-dimensional problems. To overcome these limitations, researchers have explored alternative architectures such as convolutional neural networks (CNNs), which offer advantages in terms of parameter sharing, spatial feature extraction, and computational efficiency. The MRF-PINN (Multi-Receptive-Field PINN) model, for instance, is designed to solve different types of PDEs on various mesh resolutions without manual tuning, leveraging the benefits of CNNs while maintaining flexibility [48]. This approach enables PINNs to adapt to different equation types and mesh resolutions without significant changes to the architecture, making it a more scalable and efficient solution.

In addition to feature mapping and neural architecture design, the use of specialized activation functions has also been shown to improve the performance of PINNs. Activation functions are crucial for introducing non-linearity into the network, which is essential for modeling complex physical systems. Recent studies have explored the use of physics-informed activation functions (PAFs) that incorporate physical knowledge into the activation process, leading to improved efficiency and validity of PINNs [11]. For example, the introduction of a novel activation function called Wavelet in PINNsFormer has been shown to enhance the network's ability to capture temporal dependencies, leading to better generalization and accuracy in solving PDEs [87]. These specialized activation functions allow PINNs to better respect the underlying physics of the problem, resulting in more accurate and robust solutions.

Furthermore, the integration of attention mechanisms and residual-based methods has been explored to improve the performance of PINNs. Attention mechanisms allow the network to focus on critical regions or features in the solution domain, improving its ability to capture complex dynamics and high-frequency components. For instance, the use of residual-based attention in physics-informed neural networks (PIANNs) has been shown to effectively capture shock fronts in hyperbolic model problems, overcoming the limitations of traditional PINN approaches [4]. Similarly, the introduction of residual connections in PINNs has been found to improve the training dynamics and prevent the degradation of performance in deep networks [35].

The design of neural architectures also plays a crucial role in addressing the challenges of training PINNs. The use of shallow networks has been found to be effective in many cases, while deeper networks may struggle with convergence and stability, especially for problems with strong advection and long time durations [137]. This suggests that the choice of network depth and structure should be carefully considered based on the specific characteristics of the problem at hand. Additionally, the use of preconditioned neural architectures, which incorporate insights from numerical linear algebra, has been shown to enhance the optimization process, leading to faster convergence and improved performance [55].

In summary, feature mapping and neural architecture improvements are essential for advancing the performance and applicability of PINNs in solving complex PDEs in fluid mechanics and other scientific domains. By leveraging feature mapping techniques, designing efficient neural architectures, and incorporating specialized activation functions and attention mechanisms, researchers can enhance the training dynamics, generalization capability, and accuracy of PINNs. These advancements not only address the limitations of traditional neural networks but also pave the way for more robust and scalable solutions to challenging physical problems. As the field continues to evolve, further research into these areas will be crucial for unlocking the full potential of PINNs in scientific and engineering applications.

### 4.8 Ensemble Learning and Uncertainty Quantification

Ensemble learning and uncertainty quantification play a crucial role in enhancing the robustness and reliability of Physics-Informed Neural Networks (PINNs). Traditional PINNs often face challenges such as convergence issues, sensitivity to initial conditions, and difficulty in handling noisy or sparse data. Ensemble learning techniques address these challenges by leveraging multiple models to improve generalization, reduce overfitting, and provide more reliable predictions. Additionally, uncertainty quantification methods, such as Bayesian approaches, allow for a more comprehensive understanding of the confidence in the model's predictions. These methods are particularly important in fluid mechanics, where accurate and reliable predictions are essential for real-world applications.

One of the key advantages of ensemble learning in PINNs is the ability to reduce the risk of overfitting. By training multiple models with different initializations or architectures, ensemble methods can capture a broader range of possible solutions, thereby improving the model's ability to generalize to unseen data. For instance, the paper "Improved Training of Physics-Informed Neural Networks with Model Ensembles" [138] demonstrates that training an ensemble of PINNs can stabilize the training process and yield performance competitive to the recent variants of PINNs trained with time adaptation. The authors show that the ensemble agreement can be used as a criterion for gradual expansion of the solution interval, allowing for more accurate and reliable predictions.

Another important aspect of ensemble learning in PINNs is the ability to handle noisy or sparse data. In fluid mechanics, data collection can be expensive and challenging, leading to sparse or incomplete datasets. Ensemble methods can help mitigate the impact of such limitations by combining the predictions of multiple models, thereby reducing the influence of individual model errors. The paper "Surrogate-data-enriched Physics-Aware Neural Networks" [139] explores how physics-aware models can be enriched with computationally cheaper, but inexact, data from other surrogate models like Reduced-Order Models (ROMs). The authors propose an approach that is sensitive to the error in inexact data, demonstrating that the training accuracy can be significantly improved by incorporating such data.

Uncertainty quantification is another critical component of enhancing the reliability of PINNs. Bayesian approaches provide a framework for quantifying the uncertainty in the model's predictions, which is essential for applications where decisions are based on the model's output. The paper "Bayesian neural networks for weak solution of PDEs with uncertainty quantification" [140] introduces a physics-constrained neural network approach that enables the quantification of both epistemic uncertainty from model parameters and aleatoric uncertainty from noise in the data. The authors use Bayesian neural networks (BNNs) to learn the applied boundary conditions and detect the problem domain, allowing for a comprehensive understanding of the uncertainty in the predictions.

The paper "Neural-PDE A RNN based neural network for solving time dependent PDEs" [141] discusses the use of recurrent neural networks (RNNs) to solve time-dependent PDEs, emphasizing the ability of the model to learn and simulate multiscale variables. The authors demonstrate that the model can efficiently extract the dynamics within 20 epochs of training, producing accurate predictions. This approach can be further enhanced by incorporating uncertainty quantification techniques to provide a more robust understanding of the predictions.

Ensemble learning and uncertainty quantification can also be integrated with other PINN enhancements to further improve their performance. For example, the paper "Adversarial Adaptive Sampling Unify PINN and Optimal Transport for the Approximation of PDEs" [142] proposes a new minmax formulation to optimize both the approximate solution and the random samples in the training set. The authors show that this approach can significantly reduce the Monte Carlo approximation error of the loss functional, leading to more accurate solutions. By combining this with ensemble learning techniques, the model's robustness and reliability can be further improved.

In addition to ensemble learning and uncertainty quantification, other methods such as multi-fidelity learning and adaptive sampling can also contribute to the reliability of PINNs. The paper "Multi-Fidelity Learning" [27] discusses the use of data of varying fidelity levels to improve the training and generalization capabilities of PINNs, particularly in complex and high-dimensional problems. By combining multi-fidelity learning with ensemble methods, the model can leverage the strengths of different data sources, leading to more accurate and reliable predictions.

The paper "Loss Landscape Engineering via Data Regulation on PINNs" [69] explores how data regulation can be used to reshape the loss landscape of PINNs, making it easier to navigate during training. The authors demonstrate that feeding in sparse or coarse data as a regulator can help the model converge better towards the solution. This approach can be further enhanced by incorporating ensemble learning techniques to provide a more comprehensive understanding of the model's performance.

Overall, ensemble learning and uncertainty quantification are essential components of enhancing the robustness and reliability of PINNs. By leveraging multiple models and quantifying the uncertainty in predictions, these techniques can address the limitations of traditional PINNs and improve their performance in complex fluid mechanics applications. The integration of these methods with other PINN enhancements, such as multi-fidelity learning and adaptive sampling, can further enhance the accuracy and reliability of the models, making them more suitable for real-world applications.

### 4.9 Efficient Training and Optimization Strategies

Efficient training and optimization strategies play a critical role in the performance of Physics-Informed Neural Networks (PINNs), as they directly impact convergence, accuracy, and computational cost. One of the major challenges in training PINNs is the non-convexity of the loss landscape, which can lead to suboptimal solutions and slow convergence. To address this, researchers have developed various strategies, including multi-task optimization, gradient boosting, and loss balancing techniques, which aim to improve the training dynamics and overall performance of PINNs. These methods not only enhance the robustness of PINNs but also enable them to handle complex fluid dynamics problems more effectively.

Multi-task optimization is one such strategy that leverages the joint training of multiple related tasks to improve the generalization and performance of PINNs. By simultaneously optimizing multiple objectives, such as minimizing the PDE residual and enforcing boundary conditions, multi-task learning helps PINNs achieve better convergence and accuracy. This approach has been shown to be particularly effective in scenarios where the solution domain involves multiple physical constraints or where the PDE system has coupled equations. For example, in the context of fluid dynamics, multi-task optimization can be used to simultaneously learn the velocity and pressure fields, leading to more accurate and physically consistent solutions. Furthermore, this method can reduce the overall training time by sharing information between tasks, as the model can leverage common features across different objectives. Studies have demonstrated that multi-task optimization improves the stability and efficiency of PINNs, especially when dealing with high-dimensional or multi-physics problems [143].

Gradient boosting is another technique that has been explored to enhance the training of PINNs. This method involves iteratively refining the model by focusing on the most challenging parts of the problem. In the context of PINNs, gradient boosting can be used to adaptively adjust the training process by emphasizing regions of the domain where the PDE residual is high or where the solution exhibits complex behavior. By incorporating gradient boosting into the training pipeline, PINNs can achieve better convergence and accuracy, particularly in cases where the solution has sharp gradients or discontinuities. This strategy has been successfully applied in the context of hyperbolic PDEs, where traditional PINN approaches often struggle due to the presence of shocks or traveling waves [49].

Loss balancing techniques are also essential for improving the training of PINNs. One of the key challenges in PINN training is the imbalance between different loss terms, such as the domain loss (from the PDE residual) and the boundary loss (from the boundary conditions). If the loss terms are not properly balanced, the network may focus too much on minimizing one term at the expense of the other, leading to suboptimal solutions. To address this, researchers have proposed various methods for dynamically adjusting the weights of different loss terms during training. For example, adaptive sampling techniques, such as R3 sampling and failure-informed adaptive sampling, have been used to focus training on regions of the domain where the PDE residual is high, thereby improving the overall accuracy of the solution [49]. Additionally, techniques like importance sampling and gradient-based loss balancing have been shown to significantly enhance the convergence behavior of PINNs by ensuring that the loss terms are appropriately weighted.

In addition to these strategies, recent research has explored the use of advanced optimization algorithms to improve the training of PINNs. For instance, the application of preconditioning techniques has been shown to significantly improve the conditioning of the problem, leading to faster convergence and more stable training. By preconditioning the differential operator associated with the PDE, researchers have demonstrated that the training of PINNs can be made more efficient, especially for problems involving high-frequency components or complex geometries [144]. Similarly, the use of second-order optimization methods, such as the Newton-Raphson method, has been investigated to improve the convergence of PINNs, particularly in cases where the loss landscape is highly non-convex.

Another promising direction in the development of efficient training strategies for PINNs is the use of hybrid models that combine PINNs with traditional numerical methods. These hybrid approaches leverage the strengths of both deep learning and classical numerical solvers to achieve better accuracy and efficiency. For example, by integrating PINNs with finite difference or finite element methods, researchers have demonstrated that the computational cost of solving PDEs can be significantly reduced while maintaining high accuracy. This approach is particularly useful for problems where the solution domain has complex geometries or where high-resolution simulations are required [70].

Moreover, the use of transfer learning and meta-transfer learning has emerged as a powerful tool for improving the training efficiency of PINNs. By leveraging pre-trained models on similar tasks, transfer learning allows PINNs to converge faster and achieve better performance with fewer training samples. This is especially beneficial in scenarios where the data is scarce or the problem is highly parameterized. Meta-transfer learning extends this idea by enabling the model to adapt to new tasks with minimal additional training, making it a valuable technique for solving inverse problems or parameter estimation tasks in fluid dynamics [145].

In summary, the development of efficient training and optimization strategies is essential for the success of PINNs in fluid mechanics applications. By employing multi-task optimization, gradient boosting, loss balancing techniques, and advanced optimization algorithms, researchers have made significant progress in improving the convergence and accuracy of PINNs. Additionally, the integration of PINNs with traditional numerical methods and the use of transfer learning have further enhanced their applicability to complex fluid dynamics problems. As research in this area continues to evolve, it is expected that these strategies will play a crucial role in expanding the capabilities of PINNs and making them more accessible for real-world applications.

## 5 Challenges and Limitations

### 5.1 Computational Costs

The computational costs associated with training Physics-Informed Neural Networks (PINNs) represent a significant challenge, particularly when applied to complex fluid mechanics problems. PINNs are designed to solve partial differential equations (PDEs) by embedding the governing physics into the neural network's loss function, which allows them to learn solutions without relying on large datasets [146]. However, the process of training these networks is computationally intensive and often requires substantial resources, especially when dealing with large-scale simulations and high-dimensional PDEs. This issue is exacerbated by the fact that PINNs typically require a large number of collocation points to ensure accuracy, which increases the computational burden during training [49].

One of the key factors contributing to the high computational costs of PINNs is the need for extensive training time to achieve convergence. Unlike traditional numerical methods, which follow deterministic algorithms, PINNs rely on iterative optimization techniques such as stochastic gradient descent (SGD) or Adam. These methods can be slow to converge, especially for complex problems involving high-frequency dynamics or non-smooth solutions [22]. For instance, solving high Reynolds number turbulent flows using PINNs requires the network to accurately capture the intricate flow structures, which can be computationally expensive [10]. The training process involves repeatedly evaluating the PDE residual at numerous collocation points, which can be time-consuming and resource-intensive [78].

The computational costs of PINNs are further amplified when dealing with high-dimensional PDEs, such as those arising in three-dimensional fluid dynamics. In such cases, the number of collocation points required to achieve a satisfactory level of accuracy increases significantly, leading to a substantial rise in the computational load. This issue is particularly pronounced in problems involving multi-physics or coupled systems, where the network must simultaneously satisfy multiple physical constraints [6]. For example, solving thermo-hydro-mechanical (THM) processes in porous media requires the network to accurately model the interactions between thermal, hydraulic, and mechanical fields, which adds to the complexity and computational demands of the problem [147].

Another factor contributing to the computational costs of PINNs is the need to balance the various components of the loss function. The loss function in PINNs typically consists of multiple terms, including the PDE residual, boundary conditions, and initial conditions. Ensuring that these terms are properly balanced during training is crucial for achieving accurate solutions, but it can be a challenging and time-consuming process [32]. For example, in the case of problems with strong nonlinearities or high-frequency components, the network may struggle to converge unless the loss terms are carefully tuned [137]. This requires extensive hyperparameter tuning and may lead to increased computational costs.

Moreover, the computational costs of PINNs are also influenced by the choice of network architecture and activation functions. While deeper networks can potentially capture more complex patterns, they often require more computational resources and may suffer from issues such as vanishing gradients or slow convergence [55]. Additionally, the use of certain activation functions, such as those that are not differentiable or have high computational complexity, can further increase the training time [30]. Therefore, selecting an appropriate network architecture and activation function is crucial for minimizing the computational costs of PINNs.

To address these challenges, researchers have proposed various strategies to reduce the computational costs of PINNs. One such approach is the use of adaptive sampling techniques, which dynamically adjust the placement of collocation points during training to improve efficiency [49]. These methods aim to focus the training process on regions of the domain where the solution is most sensitive to changes, thereby reducing the number of collocation points required and the overall computational load. Another promising approach is the integration of PINNs with traditional numerical methods, which can help to leverage the strengths of both approaches and reduce the reliance on computationally expensive training processes [146]. For instance, combining PINNs with finite element or finite volume methods can lead to more efficient and accurate solutions for complex fluid mechanics problems.

In addition to these strategies, there is growing interest in the use of transfer learning to improve the efficiency of PINN training. Transfer learning involves pre-training a network on a related task and then fine-tuning it for the target problem, which can significantly reduce the computational costs associated with training from scratch [35]. This approach is particularly useful for problems that involve similar PDEs or boundary conditions, as it allows the network to leverage knowledge gained from previous tasks to improve its performance on new problems. However, the effectiveness of transfer learning depends on the similarity between the source and target tasks, and it may not always lead to significant improvements in computational efficiency [148].

Overall, the computational costs associated with training PINNs remain a significant barrier to their widespread adoption in fluid mechanics. While various strategies have been proposed to mitigate these costs, further research is needed to develop more efficient and scalable approaches that can handle the complexity of large-scale simulations and high-dimensional PDEs. By addressing these challenges, researchers can unlock the full potential of PINNs and enable their use in a wide range of fluid dynamics applications.

### 5.2 Convergence Issues

The convergence of Physics-Informed Neural Networks (PINNs) remains a significant challenge in their application to fluid mechanics and other scientific domains. Unlike traditional numerical methods that follow well-defined iterative procedures with guaranteed convergence under certain conditions, PINNs rely on the optimization of a composite loss function that incorporates both data-driven and physics-based terms. This complexity introduces a variety of convergence issues that must be carefully addressed to ensure accurate and reliable solutions. One of the primary challenges is the balancing of multiple loss terms, which can lead to unstable training dynamics and poor convergence behavior [63].

In typical PINN formulations, the loss function is composed of a combination of domain loss, boundary loss, and initial condition loss. Each of these terms contributes to the overall training objective, but their relative importance must be carefully tuned to ensure the network converges to a physically meaningful solution. However, due to the inherent non-convexity of the loss landscape, the training process can become highly sensitive to the scaling of these terms. For instance, if the domain loss dominates over the boundary or initial condition loss, the network may fail to satisfy the boundary conditions, leading to non-physical solutions [63]. This issue is particularly problematic in cases where the boundary conditions are complex or where the solution exhibits sharp gradients or discontinuities.

Another major convergence challenge is the influence of initial conditions and boundary constraints on the training process. In many fluid dynamics problems, the initial state of the system and the boundary conditions play a crucial role in determining the behavior of the solution. However, PINNs often struggle to effectively enforce these constraints, especially when the initial and boundary data are sparse or noisy. This can lead to the network converging to a solution that is physically inconsistent or fails to capture the correct dynamics of the system [149]. For example, in problems involving high Reynolds number flows, the initial conditions may have a strong impact on the long-term behavior of the solution, and any inaccuracies in the initial state can propagate through the training process, leading to significant errors in the final prediction.

The non-convexity of the loss landscape further exacerbates these convergence issues. PINNs are trained using gradient-based optimization methods, such as stochastic gradient descent (SGD) or Adam, which can get stuck in local minima or saddle points, especially when the loss function is highly non-convex. This is particularly problematic in high-dimensional PDEs, where the loss function can have a complex structure with many local minima. As a result, the training process may become very slow or even fail to converge to a meaningful solution [149]. Recent studies have shown that the choice of neural network architecture, activation functions, and optimizer can significantly impact the convergence behavior of PINNs, but the optimal configuration is often problem-specific and difficult to determine a priori [149].

Furthermore, the interplay between different loss terms can lead to instability during training. For example, if the physics-based loss (e.g., the residual of the PDE) and the data-based loss (e.g., the error between predicted and observed values) are not properly balanced, the network may prioritize one over the other, leading to suboptimal solutions. This issue is particularly pronounced in inverse problems, where the goal is to estimate unknown parameters or initial conditions based on limited observations. In such cases, the network may overfit to the available data while failing to satisfy the governing equations, resulting in inaccurate predictions [149].

To address these convergence challenges, several strategies have been proposed in the literature. One approach is to use adaptive loss weighting, where the relative contributions of different loss terms are dynamically adjusted during training to ensure a balanced optimization process [54]. Another strategy is to employ physics-informed activation functions, which incorporate physical knowledge into the neural network's architecture to improve the learning dynamics and reduce the risk of getting stuck in poor local minima [5]. Additionally, the use of curriculum learning, where the training problem is gradually increased in complexity, has been shown to improve convergence by allowing the network to first learn simpler aspects of the solution before tackling more challenging parts [53].

In summary, the convergence of PINNs remains a critical challenge in their application to fluid mechanics and other scientific domains. The difficulty in balancing multiple loss terms, the influence of initial and boundary conditions, and the non-convexity of the loss landscape all contribute to the instability and poor convergence behavior of these networks. Addressing these issues requires a combination of careful loss function design, adaptive training strategies, and the use of advanced optimization techniques. While significant progress has been made in recent years, further research is needed to develop more robust and efficient training methods that can ensure stable and accurate convergence in a wide range of applications.

### 5.3 Spectral Bias

Spectral bias is a well-documented phenomenon in Physics-Informed Neural Networks (PINNs), where neural networks tend to learn low-frequency components of the solution to a partial differential equation (PDE) more effectively than high-frequency components. This bias in learning can significantly impact the accuracy of fluid dynamics simulations, especially when dealing with problems that involve complex, high-frequency flow features such as turbulence, shock waves, or small-scale vortices. Understanding and addressing this issue is crucial for improving the reliability and effectiveness of PINNs in fluid mechanics applications.

The emergence of spectral bias in PINNs is rooted in the inherent properties of neural network architectures and the training process. Neural networks, particularly those used in PINNs, are generally more effective at approximating smooth, low-frequency functions, as these are easier to learn and generalize from limited data. High-frequency components, on the other hand, require more precise and detailed information to capture their behavior, which is often not available in sparse or noisy datasets [10]. This limitation becomes particularly evident when training PINNs on problems with highly oscillatory solutions, such as those found in turbulent flows or in the presence of discontinuities.

One of the key challenges posed by spectral bias is that it can lead to inaccurate or unphysical solutions, especially in regions of the domain where high-frequency dynamics are dominant. For example, in simulations of turbulent flows, the inability of PINNs to accurately capture the high-frequency components of the velocity field can result in poor predictions of turbulence statistics and flow structures. Similarly, in the context of shock wave propagation, PINNs may fail to resolve the sharp gradients and discontinuities that characterize such flows, leading to significant errors in the solution [1].

Several studies have explored the origins of spectral bias and proposed various strategies to mitigate its effects. For instance, the use of Fourier feature mappings has been shown to improve the ability of PINNs to learn high-frequency components of the solution [129]. By transforming the input coordinates using Fourier basis functions, the network is better equipped to capture the oscillatory nature of the solution, leading to improved accuracy in regions with high-frequency dynamics. Another approach involves the use of adaptive sampling techniques, which dynamically adjust the placement of collocation points to focus on areas of the domain where high-frequency features are expected to occur [49].

In addition to input transformations and sampling strategies, the choice of neural network architecture also plays a critical role in mitigating spectral bias. Studies have shown that certain activation functions, such as Gaussian or sine-based functions, can enhance the network's ability to learn high-frequency components by introducing more complex and varied nonlinearities [55]. For example, the use of Gaussian activations has been found to be particularly effective in training PINNs for problems involving high-frequency solutions, as these activations can better capture the smooth and oscillatory behavior of the solution.

Furthermore, the integration of physics-based constraints into the loss function can also help address spectral bias by guiding the network to better approximate the true solution. By incorporating additional terms that enforce physical consistency, such as conservation laws or boundary conditions, the network is encouraged to learn solutions that are not only accurate but also physically meaningful. This approach has been particularly effective in problems involving fluid dynamics, where the preservation of mass, momentum, and energy is essential for the accuracy of the simulation [61].

Despite these advances, spectral bias remains a significant challenge in the application of PINNs to fluid dynamics. One of the main reasons for this is the inherent difficulty in balancing the learning of low- and high-frequency components. In many cases, the network may prioritize learning the dominant low-frequency features at the expense of the high-frequency ones, leading to an overall degradation in the solution's accuracy. This issue is particularly pronounced in problems with a wide range of spatial and temporal scales, where the solution may exhibit both smooth and highly oscillatory behavior.

To address these challenges, recent research has focused on developing more sophisticated training strategies that can better balance the learning of different frequency components. For example, multi-task optimization has been proposed as a way to simultaneously train the network to learn both the low- and high-frequency components of the solution [150]. By splitting the training into multiple tasks, each focusing on a specific aspect of the solution, the network is better able to capture the full range of features present in the data. This approach has shown promise in improving the accuracy of PINNs for complex fluid dynamics problems.

Another promising direction is the use of transfer learning and meta-learning to enhance the ability of PINNs to generalize across different frequency components. By leveraging pre-trained models on similar problems, the network can benefit from the knowledge learned during the initial training phase, leading to improved performance on new tasks. This is particularly useful in scenarios where the solution exhibits different frequency characteristics, as the pre-trained model can provide a more robust starting point for the learning process [151].

In conclusion, spectral bias remains a critical challenge in the application of PINNs to fluid dynamics, as it can significantly impact the accuracy and reliability of the simulations. While various strategies have been proposed to mitigate this issue, including input transformations, adaptive sampling, and architecture design, further research is needed to develop more effective and efficient solutions. By addressing spectral bias, researchers can unlock the full potential of PINNs in simulating complex fluid flows and advancing the field of computational fluid dynamics.

### 5.4 Data Scarcity

Data scarcity is a critical challenge that significantly hampers the performance and reliability of Physics-Informed Neural Networks (PINNs) in fluid mechanics. In many practical scenarios, obtaining high-quality, dense, or labeled data for training neural networks is either prohibitively expensive, time-consuming, or infeasible due to the complexity of the physical systems involved. This limitation is particularly pronounced in fluid dynamics, where the governing equations are often nonlinear, and the underlying phenomena exhibit high-dimensional and multiscale behaviors. As a result, the lack of sufficient and diverse data can lead to poor generalization and inaccurate predictions by PINNs, thereby undermining their potential to revolutionize fluid flow simulations.  

One of the primary issues associated with data scarcity in PINNs is the reliance of these models on the availability of sufficient training data to learn the underlying physical laws and patterns. Unlike traditional data-driven approaches that require vast amounts of labeled data to achieve high accuracy, PINNs integrate physical constraints directly into the training process, reducing the dependency on large datasets. However, even with the incorporation of physics-based loss functions, the performance of PINNs can still be adversely affected when the training data is sparse or poorly distributed. This is because the neural network may struggle to capture the essential features of the fluid dynamics problem without sufficient examples to guide the learning process. For instance, in cases where the solution space is highly complex and includes multiple scales or regions of interest, the lack of data can lead to overfitting, underfitting, or the model failing to generalize well to unseen scenarios.  

The problem of data scarcity is further exacerbated in scenarios involving high-dimensional or non-idealized geometries, where the complexity of the problem increases exponentially. In such cases, the number of parameters that need to be accurately captured by the neural network can be enormous, making it difficult to obtain enough training data to cover the entire parameter space. As a result, PINNs may produce inaccurate or non-physical solutions, especially in regions where the training data is sparse or absent. For example, in problems involving turbulent flows or multiphase interactions, the presence of high-frequency dynamics and complex flow structures can make it challenging to collect sufficient data for training, leading to models that fail to capture the essential physical mechanisms.  

Moreover, the scarcity of data can also limit the ability of PINNs to handle inverse problems, where the goal is to infer unknown parameters or boundary conditions from limited observations. In such cases, the model must rely heavily on the physics-informed loss function to enforce the governing equations, which can be challenging when the available data is insufficient to constrain the solution space effectively. This issue has been highlighted in studies that demonstrate the limitations of PINNs in scenarios where the data is sparse or corrupted by noise [14].  

To address the challenge of data scarcity, several approaches have been proposed in the literature. One promising direction is the use of physics-informed activation functions (PAFs), which incorporate physical knowledge directly into the neural network's architecture. These functions can help the model learn the essential physical constraints more efficiently, even when the training data is limited [11]. Another approach is to leverage hybrid models that combine PINNs with traditional numerical methods, such as finite element or finite volume solvers. These hybrid models can provide additional constraints and guidance to the neural network, improving its accuracy and robustness in data-scarce scenarios [152].  

Additionally, the use of transfer learning and meta-learning techniques has shown potential in mitigating the effects of data scarcity. Transfer learning allows the model to leverage knowledge from related tasks or domains, reducing the need for large amounts of task-specific data [153]. Similarly, meta-learning techniques enable the model to adapt quickly to new problems by learning from a small number of examples, making them particularly useful in scenarios where data is scarce. However, these approaches often require careful design and tuning to ensure that the transferred knowledge is relevant and effective for the target problem.  

Another strategy to address data scarcity is to use synthetic data generated from high-fidelity simulations or analytical solutions. This approach can provide the model with a large and diverse set of training examples, even when real-world data is limited. However, the quality and relevance of the synthetic data are crucial, as poor-quality data can lead to suboptimal or inaccurate predictions. For instance, in studies involving the simulation of turbulent flows, the use of synthetic data generated from direct numerical simulations (DNS) has been shown to improve the performance of PINNs [14].  

Furthermore, the development of adaptive sampling techniques has been proposed as a way to improve the efficiency of training PINNs with limited data. These techniques dynamically adjust the distribution of training points to focus on regions where the model is uncertain or where the solution is expected to be more complex. For example, failure-informed adaptive sampling [34] has been shown to improve the accuracy of PINNs by prioritizing the sampling of regions that are likely to contribute the most to the solution's error. Similarly, R3 sampling [154] has been used to enhance the convergence of the model by distributing the training points more evenly across the domain.  

In summary, data scarcity remains a significant challenge for the application of PINNs in fluid mechanics, as it can lead to poor generalization and inaccurate predictions. While various strategies have been proposed to mitigate this issue, including the use of physics-informed activation functions, hybrid models, transfer learning, synthetic data, and adaptive sampling techniques, further research is needed to develop more robust and efficient methods for handling data-scarce scenarios. By addressing the challenges of data scarcity, PINNs can be further optimized to provide accurate and reliable solutions for a wide range of fluid dynamics problems.

### 5.5 Challenges in Capturing High-Frequency and Complex Dynamics

Physics-informed neural networks (PINNs) have shown promising capabilities in solving a wide range of partial differential equations (PDEs), including those governing fluid mechanics. However, one of the most significant challenges in applying PINNs to fluid dynamics is the difficulty in capturing high-frequency and complex dynamics. These dynamics, which are common in turbulent flows, unsteady flows, and flows with sharp gradients, pose significant challenges for existing PINN architectures and training strategies. High-frequency components of the solution, such as small-scale vortices or rapid changes in velocity and pressure, are particularly challenging for PINNs to resolve due to the inherent limitations of neural network architectures and the training processes employed.

One of the primary reasons for this challenge is the spectral bias of neural networks, which refers to their tendency to prioritize learning low-frequency components of the solution over high-frequency ones. This phenomenon is well documented in the literature, and it significantly affects the accuracy of PINNs in capturing detailed flow structures. For instance, in the context of turbulent flows, the presence of high-frequency fluctuations in the velocity and pressure fields can lead to inaccurate predictions if the PINN is unable to capture these features effectively [22]. Similarly, in the case of unsteady flows, where the solution evolves rapidly over time, the ability of PINNs to resolve the high-frequency temporal components is crucial for obtaining accurate and physically consistent solutions.

Current PINN architectures, which typically rely on fully connected neural networks, may not be well-suited for resolving high-frequency dynamics due to their limited capacity to represent complex spatial and temporal patterns. The use of standard activation functions, such as ReLU or tanh, can further exacerbate this issue, as these functions are not well-suited for capturing the intricate behavior of high-frequency solutions [61]. Moreover, the lack of explicit regularization mechanisms in the training process can lead to the PINN focusing on the low-frequency components of the solution, thereby neglecting the high-frequency components that are critical for accurate modeling.

The training strategies used in PINNs also play a crucial role in the ability to capture high-frequency and complex dynamics. The standard approach of minimizing the residual of the PDEs and the boundary conditions through gradient-based optimization algorithms may not be sufficient to ensure that the PINN learns the high-frequency components of the solution. This is particularly true for problems with complex geometries or multi-scale features, where the high-frequency components are distributed across the domain in a non-uniform manner. In such cases, the PINN may struggle to balance the contributions of different regions of the domain during training, leading to suboptimal performance [29].

To address these challenges, several strategies have been proposed to enhance the ability of PINNs to capture high-frequency and complex dynamics. One promising approach is the use of adaptive sampling techniques, which dynamically adjust the distribution of collocation points during training to focus on regions where high-frequency components are present. For example, adaptive collocation schemes [93] and failure-informed adaptive sampling [34] have been shown to improve the accuracy of PINNs in resolving high-frequency solutions. These methods aim to ensure that the PINN has sufficient information to learn the detailed features of the solution, even in regions with complex dynamics.

Another approach is the incorporation of physics-informed activation functions, which are designed to incorporate prior knowledge about the solution into the neural network architecture. These activation functions can help the PINN better capture the high-frequency components of the solution by leveraging the underlying physical principles. For instance, the use of physics-informed activation functions (PAFs) has been explored to improve the efficiency and validity of PINNs in capturing complex dynamics [72]. By encoding the physical constraints into the activation functions, these approaches can enhance the ability of the PINN to learn the high-frequency components of the solution.

Additionally, the use of hybrid models that combine PINNs with traditional numerical methods has been proposed to overcome the limitations of current PINN architectures in capturing high-frequency and complex dynamics. These hybrid models leverage the strengths of both approaches, with the traditional numerical methods providing accurate solutions in regions where the PINN struggles, while the PINN handles the rest of the domain. This approach has shown promise in improving the overall accuracy and efficiency of PINNs for complex fluid dynamics problems [80].

Despite these advancements, the challenge of capturing high-frequency and complex dynamics in fluid flows remains a significant limitation of PINNs. The limitations of current architectures and training strategies, combined with the inherent complexity of fluid dynamics problems, make it difficult to achieve accurate and reliable predictions for high-frequency solutions. Furthermore, the computational cost of training PINNs to capture these dynamics can be prohibitively high, especially for large-scale problems [135].

In conclusion, while PINNs have demonstrated remarkable potential in solving a wide range of PDEs, their ability to capture high-frequency and complex dynamics in fluid flows remains a significant challenge. Addressing this challenge requires further research into the development of more sophisticated architectures, advanced training strategies, and the integration of physics-informed techniques to enhance the accuracy and efficiency of PINNs for complex fluid dynamics problems.

### 5.6 Sensitivity to Noise and Errors

Physics-Informed Neural Networks (PINNs) have demonstrated remarkable potential in solving partial differential equations (PDEs) by integrating physical laws into their loss functions. However, one of the critical challenges they face is their sensitivity to noisy or erroneous data, which can lead to nonphysical solutions and unreliable predictions in fluid mechanics applications. This sensitivity is a significant limitation, especially in real-world scenarios where data quality is often compromised by measurement errors, incomplete information, or complex physical phenomena that are difficult to model accurately. 

The issue of noise sensitivity in PINNs is well-documented in the literature. For instance, the work presented in [10] highlights that PINNs can struggle to maintain accuracy when faced with noisy or sparse data. This is particularly problematic in fluid dynamics, where high-fidelity data is often difficult to obtain due to the complex and dynamic nature of the flow fields. When training data is noisy or incomplete, the PINN's ability to learn the underlying physical laws is compromised, leading to inaccurate predictions. This problem is exacerbated when the PDEs involved have high-frequency components or exhibit chaotic behavior, as the network may not be able to capture the essential features of the solution.

Another paper, [53], further explores the challenges that arise when PINNs are trained on noisy data. The authors identify that the soft regularization imposed by the physics-informed loss function can make the optimization landscape more complex, especially when the data is not clean. In such cases, the network may converge to local minima that do not satisfy the physical constraints, resulting in nonphysical solutions. This issue is particularly evident in problems involving turbulent flows, where the solution space is highly nonlinear and the presence of noise can significantly impact the network's ability to generalize.

Moreover, [61] discusses the importance of incorporating physics-informed augmentation to improve the robustness of PINNs against noisy data. The authors propose adding a pressure stabilization term to the PDE residuals, which enhances the accuracy of the pressure field. This approach demonstrates that the integration of additional physical constraints can help mitigate the effects of noise, but it also highlights the challenges associated with designing such augmentations for complex fluid dynamics problems.

The sensitivity of PINNs to noise is also a concern in inverse problems, where the goal is to infer unknown parameters or boundary conditions from limited measurements. In [10], the authors note that the accuracy of PINNs in solving inverse problems is heavily dependent on the quality of the input data. When the data is noisy or sparse, the network may fail to converge to the correct solution, leading to unreliable predictions. This issue is particularly challenging in applications such as blood flow simulations, where the accuracy of the model can have significant implications for clinical outcomes.

To address the sensitivity of PINNs to noise, researchers have proposed various strategies. One such approach is the use of robust training techniques that are less susceptible to the effects of noisy data. For example, [98] introduces an inverse-Dirichlet weighting strategy to alleviate the issues caused by scale imbalances and heteroscedasticity in the data. This method helps the network to focus more on the regions of the domain where the data is more reliable, thereby improving the overall accuracy of the predictions. The authors demonstrate that this approach can significantly improve the performance of PINNs, especially in cases where the data is highly noisy or imbalanced.

Another strategy to mitigate the effects of noise is the use of transfer learning. By leveraging pre-trained models, PINNs can benefit from the knowledge gained during the training of similar problems, reducing the need for extensive retraining on noisy data. This is particularly useful in fluid dynamics applications where the same physical principles apply across different scenarios. [35] illustrates the effectiveness of transfer learning in improving the performance of PINNs. The authors show that by transferring knowledge from simpler problems to more complex ones, the network can achieve better accuracy with fewer training samples, making it more robust to noisy data.

In addition to transfer learning, the use of ensemble learning techniques has also been proposed to enhance the robustness of PINNs. By training multiple models and combining their predictions, ensemble methods can reduce the impact of noise and improve the overall reliability of the predictions. [155] discusses the potential of ensemble learning in addressing the challenges associated with noisy data. The authors demonstrate that using an ensemble of PINNs can provide more accurate and reliable predictions, even in the presence of significant noise.

Furthermore, the paper [57] introduces a novel approach to training PINNs by leveraging sinusoidal mappings of the input data. The authors show that this method can improve the network's ability to capture high-frequency components of the solution, which are often lost in the presence of noise. By increasing the variability of the input gradients, the network becomes less susceptible to the effects of noise, leading to more accurate and stable predictions.

In conclusion, the sensitivity of PINNs to noisy or erroneous data is a critical challenge that must be addressed to ensure their reliability in fluid mechanics applications. While various strategies such as physics-informed augmentation, inverse-Dirichlet weighting, transfer learning, and ensemble learning have been proposed to mitigate this issue, the development of more robust and efficient training methods remains an active area of research. As the field of PINNs continues to evolve, it is essential to prioritize the investigation of these challenges to ensure the widespread adoption of PINNs in real-world fluid dynamics problems.

### 5.7 Scalability Limitations

The scalability of Physics-Informed Neural Networks (PINNs) remains a critical challenge when applied to large-scale or high-dimensional fluid dynamics problems. While PINNs have demonstrated promising performance in solving partial differential equations (PDEs) for relatively simple or small-scale scenarios, their effectiveness diminishes as the problem complexity increases. This limitation stems from several factors, including the computational demands of training, the difficulty in handling high-dimensional data, and the challenges of maintaining accuracy and stability in complex fluid flow simulations. These scalability issues highlight the need for further research into improving the efficiency and applicability of PINNs for real-world engineering and scientific applications [3].  

One of the primary challenges in scaling PINNs to larger problems is the computational cost associated with training. As the number of parameters in the neural network increases, the training process becomes more computationally intensive, requiring significant resources in terms of time and hardware. This is particularly problematic for high-dimensional PDEs, where the input space can be vast, and the solution space is complex. For instance, in the case of 3D flow-thermal problems with sparse domain data, PINNs face significant challenges in achieving accurate solutions without an excessive amount of computational effort [31]. The training process often requires a large number of iterations to converge, which can be impractical for real-time or resource-constrained applications.  

Another major limitation of PINNs is their difficulty in handling high-dimensional problems effectively. The representation of high-dimensional data in PINNs is inherently challenging due to the "curse of dimensionality," where the number of training samples required to achieve a certain level of accuracy grows exponentially with the dimensionality of the input space. This problem is exacerbated when dealing with complex fluid dynamics scenarios, where the solution space can involve multiple variables, such as velocity, pressure, and temperature, each of which may have its own spatial and temporal dependencies. In such cases, PINNs may struggle to capture the underlying physics accurately, leading to suboptimal performance. For example, in the context of multi-physics problems involving thermo-mechanical coupling, the complexity of the system increases significantly, making it difficult for PINNs to maintain both accuracy and efficiency [6].  

The limitations of existing training methodologies further compound the scalability issues of PINNs. Traditional training strategies, such as stochastic gradient descent (SGD) and its variants, are not always effective in optimizing the loss function of PINNs, especially when the loss landscape is non-convex and highly complex. The non-convexity of the loss function can lead to difficulties in finding optimal solutions, resulting in suboptimal models that fail to capture the underlying physics accurately. Additionally, the balance between different loss terms, such as the domain loss and boundary loss, is crucial for the training process. However, achieving this balance can be challenging, especially in high-dimensional or multi-physics scenarios where the contributions of different terms may vary significantly [55].  

Moreover, the scalability of PINNs is also limited by their inability to efficiently handle complex geometries and irregular domains. While PINNs are inherently mesh-free, which is a significant advantage over traditional numerical methods, they still face challenges in accurately representing the solution over complex geometries. This is particularly true for problems involving moving boundaries or deformable structures, where the solution space changes dynamically over time. In such cases, PINNs may struggle to adapt to the changing geometry, leading to inaccuracies in the predictions. For example, in the simulation of unsteady flows past moving bodies, the accuracy of PINNs can be compromised due to the difficulty in capturing the dynamic behavior of the flow field [86].  

The challenges of scalability are also evident in the context of distributed training and parallel computing. While distributed training techniques have been explored to improve the efficiency of PINNs, the inherent complexity of PDEs and the non-convex nature of the loss function can hinder the effectiveness of such approaches. For instance, in the case of distributed PINNs (DPINNs), the coordination between different subdomains and the synchronization of the training process can introduce additional computational overhead, making it difficult to achieve significant speedups [135]. Furthermore, the lack of standardized frameworks for distributed training and the limited availability of efficient hardware accelerators for PINNs further complicate the scalability of these models.  

In addition to these computational challenges, the scalability of PINNs is also affected by their limited ability to generalize to new or unseen scenarios. While PINNs can be trained on a specific set of data and PDEs, their performance may degrade when applied to new problems or different parameter configurations. This limitation is particularly problematic in applications where the physical system is subject to variations in operating conditions or where the solution space is highly nonlinear. For example, in the case of parametric Navier-Stokes equations, the ability of PINNs to generalize to new parameter values depends heavily on the quality of the training data and the design of the neural network architecture [3].  

Overall, the scalability limitations of PINNs in fluid mechanics highlight the need for further research into improving the efficiency, accuracy, and adaptability of these models. Addressing these challenges will require a combination of advances in neural network architecture, optimization algorithms, and distributed computing techniques, as well as a deeper understanding of the underlying physics and the specific characteristics of the problems being solved. As the field of PINNs continues to evolve, overcoming these scalability challenges will be crucial for realizing their full potential in solving complex fluid dynamics problems.

### 5.8 Generalization and Transfer Learning

Generalization and transfer learning are critical aspects in the application of Physics-Informed Neural Networks (PINNs) to fluid mechanics, as they determine the ability of these models to perform well on new or unseen scenarios. One of the primary challenges in this area is the limited generalization capability of PINNs, which often requires retraining from scratch for each new problem. This limitation is exacerbated by the complex and high-dimensional nature of fluid dynamics problems, which necessitate the development of robust transfer learning strategies to adapt trained models to new scenarios effectively [156].  

The difficulty in generalizing PINNs stems from the fact that they are trained on specific datasets that capture the physical laws governing a particular problem. When applied to new problems, these models may struggle to adapt due to differences in boundary conditions, initial states, or the underlying physics. This is particularly evident in scenarios where the solution domain or the governing equations differ significantly from the training data [156]. For instance, in turbulent flow simulations, where the dynamics are highly complex and sensitive to initial conditions, PINNs may fail to capture the necessary physical behavior without fine-tuning [29].  

To address these challenges, transfer learning strategies have been proposed to enhance the generalization capabilities of PINNs. These strategies involve leveraging pre-trained models that have been trained on a diverse set of PDEs to improve performance on new tasks. One such approach is the development of transferable neural feature spaces, which are constructed from a purely function approximation perspective without relying on PDE information [156]. This method enables the same feature space to be used across various PDEs with different domains and boundary conditions, significantly improving the transferability of the model. The theoretical analysis of this approach shows that the produced feature space is of high quality, with uniformly distributed neurons, leading to improved performance across different problems.  

Another promising direction is the use of meta-learning and meta-transfer learning (SMT) to adapt PINNs to new problems more efficiently [157]. Meta-learning involves training models to learn how to learn, allowing them to quickly adapt to new tasks with minimal data. This is particularly useful in scenarios where the amount of available data is limited, as it enables the model to generalize better by leveraging knowledge from previous tasks. Studies have shown that meta-learning can significantly reduce the training time required for new problems, making it a viable solution for real-world applications.  

Furthermore, the integration of physics-informed activation functions (PAFs) has been proposed to enhance the generalization of PINNs by incorporating domain-specific knowledge into the activation functions [29]. PAFs are designed to ensure that the neural network's output adheres to the physical laws governing the problem, thereby improving the model's ability to generalize to new scenarios. This approach not only enhances the accuracy of the predictions but also improves the robustness of the model in the face of noisy or incomplete data.  

In addition to transfer learning, the use of ensemble learning techniques and uncertainty quantification methods has been explored to improve the reliability and robustness of PINNs [138]. Ensemble methods involve training multiple models and combining their predictions to reduce the variance and improve the overall performance. This approach is particularly effective in scenarios where the data is noisy or the problem is highly complex, as it allows the model to capture a broader range of possible solutions. Uncertainty quantification methods, on the other hand, provide a measure of confidence in the model's predictions, which is essential for decision-making in real-world applications.  

The need for effective transfer learning strategies is further underscored by the fact that the training of PINNs is often computationally intensive and time-consuming. This is particularly true for complex fluid dynamics problems, where the high-dimensional nature of the data and the need for accurate predictions make the training process challenging [29]. To address this, researchers have explored the use of multi-fidelity learning approaches, which leverage data of varying fidelity levels to improve the training and generalization capabilities of PINNs [27]. By combining data from different sources, these methods can provide a more comprehensive understanding of the problem, leading to improved generalization and reduced computational costs.  

In conclusion, the challenges of generalizing PINNs to new or unseen scenarios highlight the need for effective transfer learning strategies to adapt trained models to new fluid mechanics problems. The development of transferable neural feature spaces, meta-learning techniques, physics-informed activation functions, and ensemble learning methods has shown promise in improving the generalization capabilities of PINNs. These approaches not only enhance the performance of the models but also make them more robust and reliable in real-world applications. As research in this area continues to advance, the integration of these strategies into the training and deployment of PINNs will play a crucial role in expanding their applicability to a wide range of fluid dynamics problems.

### 5.9 Training Pathologies

Training PINNs for solving partial differential equations (PDEs) presents a range of challenges, and one of the most critical is the occurrence of training pathologies. These pathologies refer to the various issues that can arise during the training process, such as getting stuck in local optima, failing to converge, or suffering from unstable updates, which can significantly impact the reliability and performance of PINNs. Understanding these pathologies is essential for developing more robust and efficient PINN architectures and training strategies.

One of the most common training pathologies in PINNs is the presence of non-convex loss landscapes, which make it difficult for optimization algorithms to find the global minimum of the loss function. This is particularly problematic when the loss landscape is highly non-convex and contains multiple local minima. As a result, the training process can become stuck in a local minimum, leading to suboptimal solutions that do not accurately represent the underlying PDE. This issue is often exacerbated by the fact that PINNs incorporate multiple loss terms—such as the domain loss, boundary loss, and initial condition loss—which can interact in complex ways, further complicating the optimization process [63].

Another significant challenge in training PINNs is the problem of convergence. In many cases, PINNs may fail to converge to the correct solution, even after extensive training. This can be attributed to several factors, including the choice of optimization algorithm, the learning rate, and the balance between different loss terms. For example, if the loss terms are not properly balanced, one term may dominate the training process, causing the network to converge to an inaccurate solution. Furthermore, the non-convex nature of the loss function can lead to unstable updates, making it difficult for the network to make progress during training [67].

A related issue is the problem of spectral bias, where PINNs tend to fit low-frequency components of the solution more accurately than high-frequency components. This can result in solutions that are smooth and accurate in regions with smooth behavior but fail to capture sharp transitions or discontinuities, which are common in many fluid dynamics problems. Spectral bias is particularly problematic in problems involving shocks, turbulence, or other high-frequency phenomena. This issue can be mitigated by using specialized activation functions or by incorporating additional constraints into the loss function, but it remains a significant challenge for many PINN applications [57].

Another training pathology is the phenomenon of vanishing or exploding gradients, which can occur during the backpropagation process. This is particularly problematic in deep PINN architectures, where the gradients can become extremely small or large as they are propagated through the network. Vanishing gradients can lead to slow or no learning, while exploding gradients can cause the weights to update in an unstable manner, resulting in poor convergence. Techniques such as gradient clipping, weight initialization, and the use of specialized activation functions can help mitigate these issues, but they require careful tuning and may not always be effective [32].

Moreover, PINNs can suffer from the problem of overfitting, where the network becomes too specialized to the training data and fails to generalize well to unseen data. This can be particularly problematic in scenarios where the training data is sparse or noisy. Overfitting can lead to solutions that are highly accurate on the training data but fail to capture the underlying physical laws or to generalize to new scenarios. To address this issue, techniques such as regularization, early stopping, and data augmentation can be employed. However, these methods often require careful tuning and may not always be sufficient to prevent overfitting in complex PDE problems [158].

Another critical pathology is the issue of initialization sensitivity. The performance of PINNs can be highly dependent on the initial values of the network weights and the choice of activation functions. Poor initialization can lead to slow convergence or failure to converge altogether. This is particularly problematic in high-dimensional problems, where the loss landscape is more complex and the optimization process is more challenging. Techniques such as Xavier initialization, He initialization, and the use of adaptive learning rates can help mitigate initialization sensitivity, but they do not guarantee success in all cases [55].

In addition to these pathologies, PINNs can also face issues related to the choice of loss function and the way in which the PDE is enforced. For instance, the use of a simple mean squared error (MSE) loss may not be sufficient to capture the complexity of the PDE, leading to inaccurate solutions. Alternatively, the inclusion of additional constraints or the use of more sophisticated loss functions, such as those based on the variational form of the PDE, can improve the accuracy of the solution but may also complicate the training process [159].

Overall, the training of PINNs is a complex and challenging process, and the occurrence of various pathologies can significantly impact the reliability and performance of these models. Addressing these issues requires a combination of theoretical analysis, empirical experimentation, and the development of new training strategies that can effectively navigate the non-convex and high-dimensional loss landscapes inherent in PINN training. By understanding and mitigating these training pathologies, researchers can continue to improve the accuracy, efficiency, and robustness of PINNs for solving a wide range of PDE problems in fluid mechanics and beyond.

### 5.10 Model Complexity and Interpretability

Model complexity and interpretability are critical challenges in the application of Physics-Informed Neural Networks (PINNs) to fluid mechanics. While PINNs have shown promise in solving partial differential equations (PDEs) by integrating physical laws into their training process, the trade-off between model complexity and interpretability remains a significant hurdle. As PINNs become more complex, their ability to provide meaningful insights into the underlying physical processes often diminishes. This is particularly problematic in fluid mechanics, where understanding the internal workings of the model is essential for validating predictions and making informed decisions.

One of the primary challenges in model complexity is the increasing number of parameters required to achieve high accuracy. As PINNs are designed to approximate solutions to PDEs, they often require deep architectures with numerous layers and neurons to capture the intricate dynamics of fluid flows. However, this complexity can lead to overfitting, where the model becomes too tailored to the training data and loses its ability to generalize to new scenarios. Studies have shown that overly complex models can struggle with convergence, leading to unstable training processes and unreliable predictions [55]. This issue is compounded by the fact that the loss function in PINNs is a combination of domain and boundary terms, which can create a highly non-convex optimization landscape, further complicating the training process.

Interpretability, on the other hand, is crucial for understanding how PINNs learn and apply the physical laws embedded in their architecture. While PINNs can generate accurate predictions, the lack of transparency in their decision-making process makes it difficult to verify whether the model is truly capturing the underlying physics. This is particularly concerning in fluid mechanics, where the accuracy of the solution is paramount. Several studies have highlighted the need for techniques that can provide insights into the model's internal workings, such as visualization of learned physics and feature importance analysis [146]. These techniques can help identify which parts of the model are contributing to the predictions and ensure that the model is not relying on spurious correlations in the data.

The trade-off between complexity and interpretability is further complicated by the fact that many PINN architectures are inspired by traditional numerical methods, such as finite element or finite volume methods, but lack the same level of interpretability. For example, while traditional numerical methods often provide a clear understanding of the solution process, PINNs can be seen as a "black box," making it challenging to validate their results. This is particularly problematic in applications where the accuracy of the solution is critical, such as in the design of aerospace systems or the simulation of blood flow in the human body. To address this issue, some researchers have proposed the use of brain-inspired neural network techniques, which aim to introduce sparsity and modularity into the PINN architecture [160]. These techniques can help reduce the complexity of the model while maintaining its accuracy, making it easier to interpret the results.

Another challenge in model complexity and interpretability is the difficulty of understanding how different components of the PINN contribute to the overall performance. For instance, the choice of activation function, the number of layers, and the architecture of the network can all have a significant impact on the model's ability to capture the physical laws governing the fluid flow. Studies have shown that certain activation functions, such as Gaussian activations, can lead to more stable training and better performance in PINNs [55]. However, the optimal choice of activation function can vary depending on the specific problem at hand, making it challenging to generalize across different applications.

The issue of model complexity and interpretability is also exacerbated by the fact that many PINN architectures are designed to handle high-dimensional problems, which can further increase the complexity of the model. In fluid mechanics, this is particularly relevant for problems involving multi-phase flows, turbulent flows, or high Reynolds number regimes, where the solution space is inherently complex. While some studies have proposed techniques such as domain decomposition or hybrid models to address these challenges, the overall complexity of the model remains a significant concern [70]. These approaches can help reduce the computational burden, but they also introduce additional layers of complexity that can make the model harder to interpret.

Moreover, the interpretability of PINNs is further complicated by the fact that the model's predictions are influenced by both the physical laws and the training data. This can lead to situations where the model's predictions are not fully aligned with the expected behavior of the physical system, especially when the training data is sparse or noisy. For example, studies have shown that PINNs can struggle to capture high-frequency components of the solution, leading to inaccuracies in the predictions [161]. This highlights the importance of not only understanding the model's complexity but also ensuring that the model is trained on sufficient and high-quality data.

In summary, the challenges of model complexity and interpretability in PINNs for fluid mechanics are multifaceted and require a careful balance between accuracy, efficiency, and transparency. While PINNs offer a powerful tool for solving complex fluid dynamics problems, their increasing complexity can make it difficult to validate their predictions and ensure that they are capturing the underlying physics. Addressing these challenges will require a combination of advanced architectural design, improved training strategies, and novel interpretability techniques that can provide deeper insights into the model's decision-making process. Future research should focus on developing more transparent and interpretable PINN architectures that can bridge the gap between data-driven learning and physical modeling, ultimately leading to more reliable and trustworthy solutions in fluid mechanics applications.

## 6 Comparative Analysis with Traditional and Data-Driven Methods

### 6.1 Accuracy Comparison with Traditional Numerical Methods

The accuracy of Physics-Informed Neural Networks (PINNs) compared to traditional numerical methods such as finite element and finite volume methods in solving fluid mechanics problems is a critical area of investigation. Traditional numerical methods have long been the standard for solving complex fluid dynamics problems, relying on discretization of the domain into a mesh and solving the governing equations using well-established algorithms. These methods, such as the finite element method (FEM) and finite volume method (FVM), are known for their robustness, accuracy, and ability to handle a wide range of physical phenomena. However, they often require significant computational resources, especially for high-resolution simulations or problems with complex geometries. In contrast, PINNs offer a data-driven approach that integrates physical laws directly into the neural network architecture, allowing for the solution of partial differential equations (PDEs) without the need for mesh generation.

One of the key advantages of PINNs is their ability to approximate solutions to PDEs with high accuracy, even in the presence of sparse or noisy data. This is particularly beneficial in fluid mechanics, where obtaining high-quality experimental data can be challenging and expensive. Studies have shown that PINNs can achieve comparable accuracy to traditional numerical methods while requiring less computational time and memory. For instance, the work by Raissi et al. [162] demonstrated that PINNs can accurately solve hyperbolic PDEs with discontinuities, a task that is notoriously difficult for traditional methods. The authors highlighted the ability of PINNs to handle shocks and discontinuities without the need for artificial dissipation, which is a common requirement in conventional numerical schemes.

However, the accuracy of PINNs can be influenced by several factors, including the choice of neural network architecture, the selection of collocation points, and the balance of the loss function. For example, the study by Zhang et al. [1] showed that the use of a mixed-variable scheme in PINNs can significantly improve the accuracy of the solutions by better capturing the physical constraints of the problem. The authors compared their PINN approach with traditional numerical methods and found that the PINN achieved high accuracy in predicting velocity and pressure fields for both steady and transient laminar flows.

On the other hand, traditional numerical methods like FEM and FVM have well-established error estimation techniques and a strong theoretical foundation, which makes them reliable for a wide range of applications. The finite element method, for instance, is known for its ability to handle complex geometries and provide accurate solutions for a variety of PDEs. However, the accuracy of FEM and FVM is often dependent on the quality of the mesh, which can be a limiting factor in terms of computational efficiency and scalability. The work by Liu et al. [70] explored the use of domain decomposition techniques to improve the accuracy and efficiency of PINNs. The authors demonstrated that by dividing the domain into subdomains and solving the PDEs in parallel, the accuracy of the solutions can be significantly enhanced.

Another critical aspect of accuracy comparison is the handling of high-frequency and complex dynamics in fluid flows. Traditional numerical methods often struggle with capturing these dynamics due to the limitations of their discretization schemes. In contrast, PINNs have shown promise in accurately capturing high-frequency components of solutions. The study by Li et al. [10] highlighted the ability of PINNs to accurately model turbulent flows, which are characterized by complex, high-frequency dynamics. The authors demonstrated that PINNs can effectively capture the Reynolds-averaged Navier–Stokes (RANS) equations without the need for explicit turbulence modeling, which is a significant advantage over traditional methods.

Despite the advantages of PINNs, there are still challenges in achieving the same level of accuracy as traditional numerical methods in certain scenarios. For instance, the work by Wang et al. [147] pointed out that the accuracy of PINNs can be affected by the complexity of the governing equations and the presence of multiple physical phenomena. The authors proposed a sequential training strategy to improve the accuracy of PINNs in solving multi-physics problems, which is a critical consideration for applications in geomechanics and other fields.

In terms of computational efficiency, traditional numerical methods have the advantage of being well-optimized for large-scale simulations. However, the advent of PINNs has introduced new possibilities for improving the efficiency of fluid dynamics simulations. The study by Zhang et al. [9] demonstrated that PINNs can serve as effective surrogate models for fluid dynamics problems, significantly reducing the computational cost compared to traditional methods. The authors showed that PINNs can achieve high accuracy with fewer training samples, making them particularly suitable for problems with limited data.

In conclusion, the accuracy of PINNs compared to traditional numerical methods in solving fluid mechanics problems is a complex and multifaceted issue. While traditional methods offer well-established accuracy and robustness, PINNs provide a flexible and data-driven approach that can achieve comparable accuracy with improved computational efficiency. The choice between the two approaches often depends on the specific requirements of the problem, including the complexity of the governing equations, the availability of data, and the computational resources. As research in this area continues to advance, it is likely that the gap between PINNs and traditional numerical methods will continue to narrow, leading to more efficient and accurate solutions for a wide range of fluid mechanics problems.

### 6.2 Computational Efficiency and Scalability

Computational efficiency and scalability are critical factors in evaluating the performance of physics-informed neural networks (PINNs) compared to traditional numerical methods and data-driven models, particularly in the context of large-scale and high-dimensional fluid flow problems. Traditional numerical methods, such as finite element and finite volume methods, are well-established for solving partial differential equations (PDEs), but they often require significant computational resources and time, especially for high-dimensional problems. In contrast, PINNs leverage the power of deep learning to approximate solutions to PDEs by embedding the governing equations directly into the loss function. This approach allows PINNs to bypass the need for explicit discretization of the domain, potentially reducing computational overhead. However, the efficiency and scalability of PINNs remain active areas of research and development.

One of the key advantages of PINNs is their ability to work with sparse data, which is particularly beneficial in fluid mechanics where data acquisition can be expensive or challenging. For instance, the paper "Physics Informed Deep Learning (Part I)" [5] highlights the effectiveness of PINNs in inferring solutions to PDEs using minimal data, demonstrating their potential for real-world applications. However, the computational efficiency of PINNs can be limited by the complexity of the PDEs they are tasked with solving. High-dimensional problems, such as those encountered in turbulence modeling, often require a large number of collocation points to achieve accurate solutions, which can increase training time and computational cost.

To address these challenges, several studies have explored techniques to improve the efficiency and scalability of PINNs. For example, the paper "Efficient training of physics-informed neural networks via importance sampling" [163] proposes an importance sampling approach to select collocation points more effectively, leading to faster convergence and reduced computational burden. This method leverages the distribution of the loss function to prioritize regions of the domain that contribute more significantly to the overall error, thereby optimizing the training process. Such strategies are particularly useful in high-dimensional problems where uniform sampling can be inefficient and lead to suboptimal solutions.

Another approach to enhancing the computational efficiency of PINNs is through the use of adaptive sampling techniques. The paper "Adaptive Sampling" [93] discusses the application of adaptive collocation schemes, such as R3 sampling and RANG, which dynamically adjust the placement of collocation points based on the local behavior of the solution. These methods aim to improve the accuracy of PINNs by focusing on regions where the solution exhibits complex or rapidly varying behavior, while reducing the number of points needed in regions where the solution is relatively smooth. This adaptive strategy can significantly reduce the computational cost associated with training PINNs, particularly for problems with intricate geometries or discontinuities.

In addition to sampling strategies, the choice of neural network architecture plays a crucial role in the computational efficiency and scalability of PINNs. The paper "Physics-Informed Neural Networks for Solving Forward and Inverse Problems in Complex Beam Systems" [43] demonstrates the effectiveness of using specialized architectures, such as convolutional neural networks (CNNs), for solving PDEs in complex beam systems. CNNs are well-suited for handling spatial data and can capture local patterns more efficiently than fully connected networks. This makes them a promising candidate for improving the scalability of PINNs in high-dimensional problems.

Furthermore, the paper "Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics" [164] introduces a multi-resolution approach to enhance the generalizability and long-term prediction accuracy of PINNs. This method leverages the connection between PDE operators and network structures to embed discretized PDEs into the neural network architecture, resulting in improved performance on spatiotemporal dynamical systems. By incorporating multi-resolution techniques, PINNs can better handle problems with varying scales and complexities, making them more scalable for large-scale applications.

Scalability is also a critical consideration when comparing PINNs with traditional numerical methods. The paper "Scalable algorithms for physics-informed neural and graph networks" [165] discusses the potential of graph neural networks (GNNs) for solving complex PDEs on unstructured meshes. GNNs offer distinct advantages in handling irregular domains and can be more efficient than traditional methods in certain scenarios. By integrating GNNs with PINNs, researchers can develop hybrid models that leverage the strengths of both approaches, leading to improved scalability and computational efficiency.

Despite these advancements, PINNs still face challenges in terms of computational efficiency and scalability, particularly for problems with high-frequency dynamics or complex geometries. The paper "Spectral Bias" [166] highlights the difficulty of capturing high-frequency components of solutions, which can limit the accuracy and efficiency of PINNs in certain applications. Additionally, the paper "Critical Investigation of Failure Modes in Physics-informed Neural Networks" [63] discusses the challenges of training PINNs on complex problems, where the loss landscape can be highly non-convex and difficult to optimize.

To overcome these limitations, ongoing research focuses on developing more robust and efficient training algorithms for PINNs. The paper "Loss Landscape Engineering via Data Regulation on PINNs" [69] explores the use of data regulation to improve the convergence of PINNs by shaping the loss landscape. By strategically selecting and regulating the data used for training, researchers can create a more favorable optimization environment, leading to faster convergence and improved performance.

In summary, while PINNs offer promising potential for solving large-scale and high-dimensional fluid flow problems, their computational efficiency and scalability require further improvements. Techniques such as adaptive sampling, multi-resolution approaches, and specialized neural network architectures are being explored to enhance the performance of PINNs. By addressing these challenges, researchers can unlock the full potential of PINNs in fluid mechanics and other complex scientific applications.

### 6.3 Data Requirements and Generalization Capabilities

Physics-Informed Neural Networks (PINNs) have demonstrated significant potential in solving partial differential equations (PDEs) by integrating physical laws into the neural network training process. However, their data requirements and generalization capabilities are crucial aspects to evaluate, especially when compared to purely data-driven models that rely heavily on large training datasets. This subsection explores the data requirements of PINNs and their ability to generalize to unseen scenarios, contrasting these with traditional data-driven approaches.

PINNs are designed to leverage both physical laws and data to solve PDEs. Unlike purely data-driven models, which require extensive and often high-quality datasets to achieve good performance, PINNs can operate with much smaller datasets. This is because the physical laws encoded in the PINN's loss function provide additional constraints that guide the learning process. For example, in the study "Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed Neural Networks," it was shown that PINNs can achieve significant improvements in predictive performance even with incomplete descriptions of the underlying physics or noisy datasets [9]. This suggests that PINNs are less dependent on the quantity of data and more on the quality and relevance of the data they are trained on.

However, the data requirements for PINNs can still be non-trivial, especially for complex fluid mechanics problems. The effectiveness of PINNs often depends on the strategic selection of collocation points, which are the points in the domain where the PDE residuals are evaluated. This is highlighted in the paper "Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory," where the authors proposed a technique called good lattice training (GLT) to select collocation points efficiently. They showed that GLT requires 2–20 times fewer collocation points than uniform random sampling or Latin hypercube sampling while achieving competitive performance [96]. This indicates that while PINNs can reduce the data requirements compared to purely data-driven models, the selection and distribution of data points remain critical.

In terms of generalization, PINNs have shown promising capabilities in extrapolating to unseen scenarios, particularly when the underlying physics is well-encapsulated in the loss function. For instance, "Learning solutions of parametric Navier-Stokes with physics-informed neural networks" demonstrated that PINNs can learn solution functions of parametric Navier-Stokes equations by incorporating parameters into the input of the network. This allowed the model to interpolate solutions for a range of parameters, showcasing the ability of PINNs to generalize beyond the training data [3]. In contrast, purely data-driven models often struggle with generalization, especially when the training data does not cover the full range of possible scenarios.

The ability of PINNs to generalize is also influenced by the choice of network architecture and training strategies. For example, the paper "Architectural Strategies for the optimization of Physics-Informed Neural Networks" highlights the importance of using Gaussian activations and preconditioned neural architectures to improve the training efficiency and accuracy of PINNs [55]. These architectural choices can significantly impact the model's ability to generalize, as they help the network learn the underlying physical laws more effectively.

Another critical factor in the generalization capabilities of PINNs is their ability to handle noisy or incomplete data. This is particularly relevant in real-world applications where data collection can be challenging. The study "Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration" demonstrated that PINNs can struggle with data-free scenarios, where no prior data is available [52]. However, when data is available, even if it is sparse or noisy, PINNs can still provide accurate solutions by leveraging the physical constraints encoded in the loss function. This is further supported by "Physics-informed deep learning for incompressible laminar flows," where the authors showed that PINNs can accurately predict velocity and pressure fields even with limited data [1].

In contrast, purely data-driven models often require large, high-quality datasets to achieve comparable performance. For example, "Physics-informed neural networks for solving Reynolds-averaged Navier-Stokes equations" highlighted the effectiveness of PINNs in solving turbulent flow problems with minimal data, whereas traditional data-driven models would require extensive training on large datasets [10]. This suggests that PINNs can be more efficient in terms of data requirements and can generalize better to new scenarios, especially when the physical laws are well understood.

Furthermore, the integration of domain knowledge and physical constraints in PINNs can enhance their generalization capabilities. This is evident in the paper "Knowledge-Based Convolutional Neural Network for the Simulation and Prediction of Two-Phase Darcy Flows," where the authors combined the discretized forms of the governing equations with neural networks to improve accuracy and reduce computational costs [167]. This approach not only leverages the data but also incorporates the physical principles, leading to better generalization and robustness.

In conclusion, while PINNs can operate with smaller datasets and leverage physical constraints to improve generalization, their performance is still dependent on the strategic selection of data points and the design of the neural network architecture. Compared to purely data-driven models, PINNs offer a more efficient and robust approach to solving PDEs, particularly in scenarios where the underlying physics is well understood. The ability of PINNs to generalize to unseen scenarios is a key advantage, making them a promising tool for fluid mechanics and other scientific domains.

### 6.4 Handling Complex and High-Frequency Dynamics

Physics-Informed Neural Networks (PINNs) have shown significant promise in capturing complex and high-frequency dynamics in fluid mechanics, which traditional numerical solvers often struggle with due to computational limitations and the need for extremely fine grids. High-frequency dynamics, such as turbulence, shock waves, and small-scale vortices, are inherently difficult to model because they require high-resolution discretization and significant computational resources. PINNs, by contrast, can potentially capture these dynamics without the same level of computational overhead by integrating physical laws directly into the learning process.

One of the key advantages of PINNs is their ability to approximate the solution to partial differential equations (PDEs) governing fluid dynamics without explicitly relying on mesh-based discretization. This is particularly beneficial in scenarios involving high-frequency dynamics, where traditional solvers may become computationally prohibitive. For instance, in the case of turbulent flows, PINNs can learn the underlying physics and generalize across different flow regimes, as demonstrated in the work by [14]. The study showed that PINNs could achieve high accuracy in simulating turbulent flows with significantly lower computational costs compared to conventional numerical solvers. The integration of physical laws into the neural network architecture allows PINNs to implicitly enforce conservation laws and boundary conditions, which is crucial for capturing the complex behavior of high-frequency dynamics.

In addition to handling turbulent flows, PINNs have also been applied to the simulation of shock waves and other high-frequency phenomena. For example, in the study titled [115], the authors proposed a fully-differentiable framework for solving compressible fluid flows using high-order numerical methods. This framework leverages the power of PINNs to approximate the solution of the Navier-Stokes equations, allowing for efficient and accurate simulations of high-frequency dynamics such as shock waves. The results demonstrated that PINNs could capture the intricate details of these phenomena with a high degree of fidelity, even on coarse grids.

Another area where PINNs excel is in the simulation of multiphase flows, where the interaction between different phases introduces additional complexity. Traditional solvers often struggle with capturing the dynamics of phase interfaces, especially when dealing with high-frequency oscillations and complex geometries. PINNs, on the other hand, can learn the underlying physics of the system and adaptively adjust their predictions based on the input data. The study [84] demonstrated the effectiveness of PINNs in simulating fluid-solid interactions, highlighting their ability to handle the high-frequency dynamics associated with such systems.

The ability of PINNs to handle complex and high-frequency dynamics is further supported by their flexibility in incorporating various physical constraints and boundary conditions. For instance, in the work [11], the authors explored the use of PINNs for both forward and inverse problems in fluid dynamics. The study showed that PINNs could accurately predict fluid behavior under a wide range of conditions, including those involving high-frequency oscillations and complex geometries. This adaptability is a significant advantage over traditional solvers, which often require extensive customization to handle such scenarios.

Furthermore, the integration of PINNs with other machine learning techniques, such as adaptive sampling and hybrid models, has further enhanced their ability to capture high-frequency dynamics. For example, the study [49] introduced adaptive sampling techniques that dynamically adjust the placement of collocation points to improve model accuracy and convergence. This approach allows PINNs to focus on regions of the domain where high-frequency dynamics are most prevalent, leading to more accurate predictions.

In addition to their ability to capture high-frequency dynamics, PINNs also show promise in handling complex, multi-scale problems. Traditional solvers often struggle with capturing the interactions between different scales of motion, which can lead to inaccuracies in the simulation results. PINNs, by contrast, can learn the underlying physics and generalize across different scales, as demonstrated in the work [168]. The study showed that PINNs could effectively simulate multi-scale problems with high accuracy, even when the underlying physics involved complex interactions between different scales.

The performance of PINNs in handling complex and high-frequency dynamics has also been validated through empirical studies. For instance, in the work [83], the authors demonstrated that PINNs could achieve significant speedups compared to traditional CFD solvers while maintaining high accuracy. The study showed that PINNs could capture the intricate details of fluid flows with a high degree of fidelity, even on coarse grids.

Overall, the ability of PINNs to capture complex and high-frequency dynamics in fluid mechanics is a significant advantage over traditional solvers. By integrating physical laws directly into the learning process, PINNs can efficiently and accurately simulate a wide range of fluid dynamics problems, including those involving turbulence, shock waves, and multiphase flows. The flexibility and adaptability of PINNs make them a powerful tool for addressing the challenges posed by high-frequency dynamics in fluid mechanics. As research in this area continues to advance, it is likely that PINNs will play an increasingly important role in the simulation and analysis of complex fluid dynamics problems.

### 6.5 Robustness and Stability

The robustness and stability of Physics-Informed Neural Networks (PINNs) in comparison to traditional numerical methods and data-driven models is a critical aspect of their evaluation, particularly when dealing with noisy or incomplete data and under varying boundary conditions. Traditional numerical methods, such as finite element or finite volume methods, are generally known for their robustness and stability due to their rigorous mathematical foundations and structured discretization of the domain. However, they often require significant computational resources and are limited in their adaptability to complex or irregular geometries. In contrast, PINNs offer a data-driven approach that integrates physical laws into the learning process, enabling them to handle scenarios where traditional methods may struggle.

In the presence of noisy or incomplete data, PINNs have demonstrated remarkable robustness. Unlike conventional data-driven models that rely heavily on the quality and quantity of training data, PINNs leverage the underlying physical laws to regularize the learning process. This is achieved by embedding the governing equations into the loss function, which ensures that the neural network adheres to the physical constraints of the problem even when the input data is sparse or corrupted [10]. For instance, studies have shown that PINNs can accurately approximate the solution of partial differential equations (PDEs) even when the available data is limited, making them particularly suitable for real-world applications where data acquisition is challenging [169].

Moreover, PINNs exhibit superior stability in handling problems with varying boundary conditions. Traditional numerical methods often require significant adjustments to the discretization scheme or the mesh when boundary conditions change, which can be computationally expensive and time-consuming. In contrast, PINNs can adapt to different boundary conditions without the need for retraining from scratch, thanks to their ability to learn the underlying physics. This adaptability is particularly beneficial in fluid dynamics, where boundary conditions can vary significantly depending on the flow regime and the geometry of the domain. For example, the use of PINNs in simulating unsteady flows past moving bodies has shown that the models can effectively capture the dynamics of the system even when the boundary conditions are not explicitly known [31].

Another critical aspect of robustness and stability is the ability of PINNs to maintain accuracy and reliability under different levels of noise and uncertainty. Traditional numerical methods, while robust in their own right, can suffer from inaccuracies when the input data is noisy or incomplete. PINNs, on the other hand, are designed to incorporate the physical laws into the learning process, which helps to mitigate the effects of noise and improve the generalization performance of the model. For instance, the use of physics-informed activation functions (PAFs) has been shown to enhance the robustness of PINNs by incorporating physical knowledge directly into the network's structure [31]. This approach not only improves the accuracy of the model but also ensures that the predictions remain physically meaningful even in the presence of noise.

In addition to robustness, PINNs also demonstrate stability in solving complex PDEs, particularly those with high-frequency components or chaotic dynamics. Traditional numerical methods often struggle with such problems due to the limitations of their discretization schemes and the potential for numerical instability. PINNs, however, have shown promising results in this area by leveraging the power of deep learning to capture the intricate dynamics of the system. For example, the use of spectral PINNs has been shown to effectively handle multiscale problems by capturing the small-scale dynamics with high accuracy [170]. This capability is particularly important in fluid mechanics, where the accurate simulation of high-frequency dynamics is crucial for understanding complex flow phenomena.

The stability of PINNs is further enhanced by the use of advanced training strategies and optimization techniques. Techniques such as adaptive sampling, multi-fidelity learning, and transfer learning have been shown to improve the convergence and stability of PINNs, making them more robust to variations in the input data and boundary conditions [34]. These methods allow the model to focus on the most relevant regions of the domain, thereby improving the accuracy of the solution while reducing the computational cost. Additionally, the integration of PINNs with traditional numerical methods has shown promise in combining the strengths of both approaches, leading to more accurate and stable solutions for complex problems [36].

Overall, the robustness and stability of PINNs in comparison to traditional numerical methods and data-driven models are significant advantages that make them a compelling choice for solving fluid dynamics problems. Their ability to handle noisy or incomplete data, adapt to varying boundary conditions, and maintain accuracy under different levels of uncertainty makes them particularly suitable for real-world applications. Furthermore, the integration of advanced training strategies and optimization techniques has further enhanced the stability of PINNs, making them a robust and reliable tool for solving complex PDEs in fluid mechanics. As research continues to advance, the potential of PINNs to revolutionize the field of computational fluid dynamics is becoming increasingly evident.

### 6.6 Hybrid Approaches and Synergies

Hybrid approaches that combine Physics-Informed Neural Networks (PINNs) with traditional numerical methods and data-driven models have emerged as a promising strategy to leverage the strengths of both paradigms. These synergies aim to enhance accuracy, efficiency, and reliability in fluid mechanics simulations, addressing the limitations of purely data-driven or purely physics-based approaches. By integrating PINNs with established numerical techniques, researchers have demonstrated the potential to overcome challenges such as computational costs, data scarcity, and the need for precise boundary conditions.

One prominent example of hybrid approaches is the integration of PINNs with traditional finite element or finite volume methods. Such combinations allow for the use of PINNs to handle complex, high-dimensional problems where traditional solvers may struggle, while benefiting from the structured grid-based discretization of classical numerical methods. For instance, a paper titled "Finite basis physics-informed neural networks as a Schwarz domain decomposition method" [70] explores the use of finite basis physics-informed neural networks (FBPINNs) in conjunction with domain decomposition techniques. This approach divides the computational domain into subdomains, each solved using PINNs, and then couples them via the Schwarz alternating method. The results demonstrate that this hybrid strategy improves the accuracy and efficiency of PINNs, particularly for large-scale and multi-scale fluid problems.

Another example of hybridization is the combination of PINNs with data-driven models. This synergy is particularly useful when the availability of high-quality data is limited, as the physics-informed constraints of PINNs can provide additional guidance to the model. A study titled "Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed Neural Networks" [9] discusses how PINNs can be used as surrogate models for fluid dynamics problems, even with incomplete physical knowledge. The paper highlights the effectiveness of incorporating a physics-based regularization term in the loss function, which significantly improves the robustness of the model to noisy data and reduces the computational effort required for convergence. This hybrid approach not only enhances predictive performance but also allows for the efficient extrapolation of results to new scenarios through transfer learning.

The use of PINNs in conjunction with traditional solvers has also been explored to address the limitations of purely data-driven methods. For instance, a paper titled "The Old and the New: Can Physics-Informed Deep-Learning Replace Traditional Linear Solvers" [58] investigates the potential of PINNs as linear solvers for the Poisson equation. The study shows that while PINNs can be effective in certain scenarios, their accuracy and computational performance are still limited for high-frequency components. However, by integrating PINNs with traditional linear solvers such as PETSc conjugate gradient solvers, the authors demonstrate that hybrid strategies can achieve performance comparable to established methods. This integration not only improves the accuracy of the solutions but also enhances the efficiency of the overall computational process.

Moreover, hybrid approaches that combine PINNs with other machine learning techniques, such as ensemble learning or Bayesian methods, have been shown to improve the reliability and robustness of fluid mechanics simulations. A paper titled "Ensemble Learning and Uncertainty Quantification in Physics-Informed Neural Networks" [155] discusses the use of ensemble learning to enhance the performance of PINNs. By combining multiple PINN models, the ensemble approach reduces the risk of overfitting and provides a more accurate representation of the solution space. Additionally, the paper highlights the importance of uncertainty quantification in PINNs, which is critical for applications where the reliability of the model is paramount.

Another significant development in hybrid approaches is the use of PINNs in conjunction with physics-informed activation functions. These functions incorporate physical constraints directly into the neural network architecture, allowing the model to learn the underlying physics more effectively. A paper titled "Physics-Informed Activation Functions for Enhanced Performance of PINNs" [67] demonstrates that the use of such activation functions can significantly improve the accuracy and stability of PINNs, particularly in problems with complex dynamics. This approach not only enhances the model's ability to capture high-frequency components but also improves the overall convergence behavior of the network.

The synergy between PINNs and traditional numerical methods is also evident in the context of multi-physics problems. A paper titled "Mixed formulation of physics-informed neural networks for thermo-mechanically coupled systems and heterogeneous domains" [6] presents a mixed formulation of PINNs for solving coupled systems of equations. This approach combines the strengths of PINNs with the flexibility of mixed finite element methods, enabling the accurate and efficient simulation of thermo-mechanically coupled systems. The results show that this hybrid strategy improves the accuracy of the model while reducing the computational cost, making it suitable for large-scale engineering applications.

In addition to traditional numerical methods, hybrid approaches have also been explored in the context of data-driven models. For instance, a paper titled "Physics-Informed Deep Learning for Incompressible Laminar Flows" [1] discusses the use of PINNs for simulating incompressible laminar flows, where the model is trained on sparse data. The study demonstrates that PINNs can achieve high accuracy even with limited data, thanks to the incorporation of physical constraints in the loss function. This hybrid approach is particularly useful for applications where high-quality data is scarce, such as in experimental fluid mechanics.

Overall, the integration of PINNs with traditional numerical methods and data-driven models offers a powerful framework for solving complex fluid mechanics problems. By combining the strengths of both paradigms, researchers can achieve higher accuracy, greater efficiency, and improved reliability in their simulations. These hybrid approaches not only address the limitations of individual methods but also open up new possibilities for the development of more advanced and versatile models for fluid dynamics. As research in this area continues to evolve, it is likely that even more sophisticated hybrid strategies will emerge, further expanding the capabilities of PINNs in the field of fluid mechanics.

### 6.7 Domain Decomposition and Parallelization

Domain decomposition and parallelization are critical strategies for improving the performance of numerical simulations, particularly for large-scale and high-dimensional problems. In traditional numerical solvers such as finite element and finite volume methods, domain decomposition techniques are extensively used to divide a complex computational domain into smaller, manageable subdomains that can be solved independently and in parallel. This approach not only reduces the computational load per subdomain but also enables efficient utilization of distributed computing resources, significantly accelerating the solution process [78]. In the context of Physics-Informed Neural Networks (PINNs), similar techniques have been explored to address the challenges of solving partial differential equations (PDEs) with high computational demands.

One of the key challenges in applying PINNs to large-scale problems is the computational cost associated with training deep neural networks on complex PDEs. Unlike traditional numerical methods, which are inherently parallelizable due to their structured grid-based formulations, PINNs often face difficulties in scaling due to the need to evaluate PDE residuals at numerous collocation points across the domain. This challenge has led researchers to explore domain decomposition strategies to partition the computational domain into subdomains, where each subdomain is handled by a separate neural network or a subset of the network's parameters. This approach not only reduces the computational burden but also facilitates parallelization, enabling faster training and improved scalability [48].

The effectiveness of domain decomposition in PINNs has been demonstrated in several studies. For instance, the MRF-PINN framework [48] proposed a multi-receptive-field architecture that allows PINNs to solve different types of PDEs on various mesh resolutions without manual tuning. This approach leverages parameter sharing and spatial feature extraction, making it more efficient and scalable than traditional PINN implementations. Moreover, by decomposing the domain into smaller subdomains, MRF-PINN can adapt to different PDE types and mesh resolutions without the need for extensive hyperparameter tuning, which is a common bottleneck in traditional PINN training.

Parallelization strategies in PINNs are also gaining traction, especially in the context of distributed computing. Traditional numerical solvers benefit from parallelization due to their structured grid-based formulations, which allow for efficient distribution of computations across multiple processors. In contrast, PINNs, which rely on neural networks to approximate the solution of PDEs, face unique challenges in parallelization. However, recent advances have shown that PINNs can be effectively parallelized by leveraging distributed computing frameworks such as TensorFlow and PyTorch. For example, the PINNs-TF2 framework [171] demonstrated significant speed improvements by utilizing XLA compilers and distributed training strategies. These strategies allow PINNs to handle large-scale problems with high-dimensional domains by distributing the computational load across multiple devices, thereby reducing training time and improving overall performance.

Another notable study that explores domain decomposition and parallelization in PINNs is the work on distributed PINNs [135]. This study proposed a novel distributed PINN approach, named DPINN, which decomposes the computational domain into subdomains and trains separate PINN models for each subdomain. The results showed that DPINN not only improves the accuracy of the solution but also enhances data efficiency, making it a promising approach for solving complex PDEs with limited data. Furthermore, the study demonstrated that DPINN can be effectively used to solve two-dimensional steady-state Navier-Stokes equations, which are a system of nonlinear PDEs. This approach is particularly useful in scenarios where data is scarce or expensive to obtain, as it allows for the efficient use of available data while maintaining high accuracy.

In addition to domain decomposition and parallelization, the integration of domain decomposition techniques with PINNs has been shown to improve the convergence and stability of the training process. The work by [172] highlighted the importance of combining domain decomposition with automated machine learning (AutoML) techniques to optimize the training of PINNs. This approach not only enhances the efficiency of the training process but also improves the generalization capabilities of the model. By decomposing the domain into smaller subdomains, the model can focus on learning the underlying physical laws more effectively, leading to more accurate and reliable solutions.

Moreover, the use of domain decomposition in PINNs has been shown to be effective in handling problems with discontinuities or high-frequency components. For instance, the work on RANS-PINN [22] demonstrated that domain decomposition can help in capturing the complex dynamics of turbulent flows by allowing the model to focus on specific regions of the domain where the solution exhibits high variability. This approach not only improves the accuracy of the solution but also enhances the robustness of the model, making it more suitable for real-world applications.

In comparison to traditional numerical solvers, the effectiveness of domain decomposition and parallelization in PINNs is still an active area of research. While traditional solvers benefit from well-established domain decomposition techniques, PINNs face unique challenges due to their reliance on neural networks for solving PDEs. However, the recent advancements in domain decomposition and parallelization strategies for PINNs suggest that these techniques can significantly enhance the performance of PINNs, making them a viable alternative to traditional numerical methods for solving complex fluid mechanics problems.

Overall, domain decomposition and parallelization strategies are essential for improving the performance of PINNs in solving large-scale and high-dimensional PDEs. By decomposing the computational domain into smaller subdomains and leveraging parallel computing resources, PINNs can achieve faster training times, improved accuracy, and better scalability. These strategies are particularly useful in scenarios where data is scarce or the computational domain is highly complex, as they allow for the efficient use of available resources while maintaining high accuracy. As research in this area continues to advance, it is expected that domain decomposition and parallelization will play an increasingly important role in the development and application of PINNs in fluid mechanics and other scientific domains.

### 6.8 Transfer Learning and Adaptability

Transfer learning and adaptability are crucial aspects in the evaluation of machine learning models, particularly when comparing Physics-Informed Neural Networks (PINNs) with traditional data-driven models. In the context of fluid mechanics, where problems often involve complex and varied physical conditions, the ability of a model to adapt to new scenarios without extensive retraining is a significant advantage. PINNs, by their design, are inherently equipped with the capacity to leverage prior knowledge from previously solved problems, making them particularly well-suited for transfer learning applications. This capability is not as pronounced in traditional data-driven models, which typically require complete retraining for each new scenario.

One of the primary reasons for the strong transfer learning capabilities of PINNs is their integration of physical laws into the training process. By incorporating the governing equations of fluid dynamics into the loss function, PINNs are able to generalize better across different problems. This is because the physical constraints act as a form of regularization, ensuring that the network learns solutions that are consistent with the underlying physics. For example, in the work by [156], the authors proposed a method to construct transferable neural feature spaces without using PDE information. This approach demonstrated that by reparameterizing the hidden neurons and using auxiliary functions, a feature space can be created that is highly transferable across various PDEs with different domains and boundary conditions. The results showed that the proposed method significantly improved transferability, highlighting the potential of PINNs to adapt to new problems efficiently.

In contrast, traditional data-driven models often rely solely on the data available for training, which can be limiting in scenarios where the data is scarce or not representative of the new problem. These models lack the built-in physical constraints that PINNs possess, making them more susceptible to overfitting and less effective in extrapolating to new conditions. For instance, in [173], the authors introduced a PINN extension that uses a Total-Variation penalty to accommodate multiple changepoints in the PDE dynamics. This method showed that by incorporating physical constraints, PINNs can adapt to changes in the dynamics of the system, a capability that is less common in traditional data-driven models.

Another aspect of transfer learning in PINNs is the use of pre-trained models. Research has shown that pre-training a neural network on a related problem can significantly improve its performance on a new task. In [156], the authors demonstrated that a pre-trained neural network can be used to solve a wide class of PDEs without requiring extensive retraining. This approach not only reduces the computational cost but also enhances the model's ability to generalize to new problems. The ability to transfer knowledge from one problem to another is a major advantage of PINNs, as it allows for the efficient reuse of learned features and reduces the need for large amounts of training data.

The adaptability of PINNs is also evident in their ability to handle different types of boundary conditions and initial conditions. Traditional data-driven models often struggle with these variations, as they are typically trained on a specific set of conditions. In contrast, PINNs can adapt to new conditions by incorporating the relevant physical laws into the loss function. This flexibility is demonstrated in [174], where the authors proposed a method that uses the variational form of PDEs to train neural networks. By using a loss function that is discretization-free and highly parallelizable, the model can efficiently adapt to new problems with different boundary conditions.

Moreover, the use of transfer learning in PINNs can be further enhanced by incorporating techniques such as domain decomposition and multi-fidelity learning. Domain decomposition allows the model to focus on specific regions of the domain, improving its ability to handle complex geometries and varying conditions. Multi-fidelity learning, on the other hand, leverages data of varying fidelity levels to improve the training and generalization capabilities of PINNs. This approach is discussed in [164], where the authors show that by combining data from different sources, PINNs can achieve better performance and robustness.

The adaptability of PINNs is also supported by the use of physics-informed activation functions. These functions incorporate physical knowledge into the neural network's activation functions, improving the efficiency and validity of PINNs. This concept is explored in [57], where the authors demonstrate that by designing activation functions that are consistent with the underlying physics, PINNs can better capture the complex dynamics of fluid mechanics problems.

In addition to these techniques, the use of ensemble learning and uncertainty quantification can further enhance the adaptability of PINNs. Ensemble learning involves training multiple models and combining their predictions to improve robustness and accuracy. This approach is discussed in [138], where the authors show that by using multiple models, PINNs can better handle noisy or uncertain data. This is particularly important in fluid mechanics, where the data can be sparse or noisy, and the ability to quantify uncertainty is crucial for reliable predictions.

Overall, the transfer learning capabilities and adaptability of PINNs make them a powerful tool for solving fluid mechanics problems. By leveraging physical laws and incorporating techniques such as domain decomposition, multi-fidelity learning, and physics-informed activation functions, PINNs can effectively handle new scenarios without extensive retraining. This is a significant advantage over traditional data-driven models, which often require retraining from scratch for each new problem. The ability to transfer knowledge and adapt to new conditions makes PINNs a promising approach for a wide range of applications in fluid mechanics and beyond.

### 6.9 Convergence and Training Challenges

The convergence behavior and training challenges of Physics-Informed Neural Networks (PINNs) remain significant areas of concern when compared to traditional numerical methods and purely data-driven models. While PINNs have shown great promise in solving partial differential equations (PDEs), they still face several limitations in terms of training stability, convergence speed, and robustness. These challenges are particularly pronounced when dealing with high-dimensional problems, complex geometries, and nonlinear PDEs.

One of the primary issues with PINNs is their susceptibility to non-convex loss landscapes, which can lead to difficulties in finding a global minimum during training. This is especially problematic for problems with highly nonlinear or chaotic dynamics, where the loss function can become extremely complex. Studies have shown that the training of PINNs is often hindered by the presence of multiple local minima, making it difficult for the network to converge to the true solution. For instance, a critical investigation of failure modes in PINNs [67] revealed that high-order PDEs can contaminate backpropagated gradients and hinder convergence. This issue is exacerbated when the network architecture is not well-suited to the problem at hand, leading to suboptimal performance and potential divergence.

In contrast, traditional numerical methods such as finite element or finite volume methods are typically more stable and reliable, especially for problems with well-defined boundary conditions and smooth solutions. These methods rely on structured discretization of the domain, which provides a clear path to convergence through established numerical techniques. However, they often require significant computational resources and may struggle with high-dimensional or complex geometries. Data-driven models, on the other hand, can sometimes outperform PINNs in terms of training speed and accuracy, particularly when a large amount of high-quality training data is available. However, they lack the ability to enforce physical constraints directly, which can lead to unphysical solutions in cases where the training data is sparse or noisy.

Another major challenge in training PINNs is the difficulty of balancing the different terms in the loss function. PINNs typically include terms related to the PDE residuals, boundary conditions, and initial conditions, each of which can have different scales and contribute differently to the overall loss. This imbalance can lead to poor convergence, as the optimizer may prioritize certain terms over others, resulting in suboptimal solutions. A study on the convergence of PINNs for linear second-order elliptic PDEs [175] highlighted that the convergence rate of PINNs depends on the number of training samples, the depth and width of the neural network, and the choice of activation functions. The paper also showed that the error in PINNs can be decomposed into approximation error and statistical error, with the former being influenced by the network's capacity and the latter by the quality and quantity of training data.

Furthermore, PINNs often require careful tuning of hyperparameters to achieve stable and accurate results. This includes selecting appropriate learning rates, network architectures, and optimization algorithms. The training process can be sensitive to these choices, and small adjustments can have a significant impact on the final solution. For example, the use of adaptive sampling strategies, such as importance sampling [163] and residual-based adaptive sampling [97], has been shown to improve convergence by focusing computational resources on regions of the domain where the solution is most uncertain. However, these techniques still require careful implementation and may not always be effective for all types of PDEs.

The training of PINNs can also be significantly affected by the presence of noise in the data or the initial conditions. This is particularly problematic for inverse problems, where the goal is to infer unknown parameters or boundary conditions from limited or noisy measurements. A study on the robustness of PINNs in the presence of noise [59] found that PINNs can be highly sensitive to errors in the training data, leading to overfitting and the propagation of these errors throughout the solution domain. The paper also highlighted the limitations of traditional physical regularization techniques, such as continuity and conservation laws, which may not be sufficient to prevent the network from converging to a local minimum that does not satisfy the true physical constraints.

In addition to these challenges, the training of PINNs can be computationally expensive, especially for large-scale problems. The use of automatic differentiation to compute higher-order derivatives is a key component of PINNs, but it can also be a source of inefficiency. Several studies have explored ways to improve the computational efficiency of PINNs, including the use of finite-difference approximations [74] and the development of specialized hardware accelerators. However, these approaches often come at the cost of reduced accuracy or increased complexity, making it difficult to strike the right balance between performance and computational feasibility.

Overall, while PINNs offer a powerful and flexible approach to solving PDEs, they still face significant challenges in terms of convergence and training stability. These challenges are particularly evident when compared to traditional numerical methods and data-driven models, which often provide more predictable and reliable results. Addressing these issues will require further research into the theoretical foundations of PINNs, the development of more efficient training algorithms, and the exploration of hybrid approaches that combine the strengths of different methodologies.

### 6.10 Practical Applications and Real-World Performance

Physics-Informed Neural Networks (PINNs) have shown significant promise in real-world applications within fluid mechanics, offering a unique blend of data-driven learning and physical constraints. Their practical performance in engineering problems has been evaluated against traditional numerical methods and purely data-driven models, revealing their feasibility and effectiveness in a variety of scenarios. This subsection explores the real-world performance of PINNs in fluid mechanics applications, highlighting their strengths, limitations, and comparative advantages.

One of the key practical applications of PINNs is in solving the Navier-Stokes equations, which are fundamental to fluid dynamics. PINNs have been used to approximate solutions to these equations with high accuracy, even in complex geometries where traditional numerical methods like finite element and finite volume methods may struggle. For instance, the work on *A physics-informed neural network framework for modeling obstacle-related equations* [25] demonstrated that PINNs can effectively model both two- and three-dimensional flows by approximating velocity components using a stream function or directly. This capability makes PINNs a valuable tool for simulating fluid flows in real-world engineering scenarios, such as aerodynamics and hydrodynamics.

In the context of real-world performance, PINNs have been applied to a variety of practical problems in fluid mechanics, including the simulation of turbulent flows, multiphase flows, and inverse problems. For example, the *RANS-PINN framework* [40] has been used to predict turbulent flows using Reynolds-averaged Navier-Stokes equations. This approach has shown that PINNs can effectively capture the complex dynamics of turbulent flows, which are challenging for traditional numerical methods due to their high computational cost and the need for detailed turbulence modeling. Similarly, the application of PINNs to inverse problems, such as estimating reduced-order model parameters and full velocity fields from noisy measurements [37], has demonstrated their effectiveness in scenarios where data is sparse or incomplete.

The practical performance of PINNs is also evident in their application to real-world engineering problems, such as the simulation of blood flow in the human body. The work on *SeqPINN and SP-PINN* [39] has shown that PINNs can be used for real-time training and accurate velocity recovery in ultrafast ultrasound blood flow imaging. This application highlights the potential of PINNs in medical diagnostics, where real-time data processing and accuracy are critical.

Another area where PINNs have demonstrated their practical performance is in the simulation of 3D flow-thermal problems with sparse data. The *hybrid data-PINNs approach* [31] has been used to develop surrogate models for complex designs in engineering, such as heat exchangers and cooling systems. This approach combines the strengths of PINNs with traditional numerical methods, allowing for accurate predictions even when data is limited. The results of these studies indicate that PINNs can be a viable alternative to traditional numerical methods in scenarios where data scarcity is a concern.

In the realm of practical applications, PINNs have also been used to address challenges in fluid mechanics that are difficult for traditional numerical methods. For example, the work on *A physics-informed neural network framework for modeling obstacle-related equations* [25] has shown that PINNs can accurately approximate solutions that lie above a given obstacle, which is a challenging task for traditional numerical methods. This capability makes PINNs a valuable tool for applications such as fluid-structure interaction and fluid flow around obstacles.

The real-world performance of PINNs is further supported by their ability to handle high-frequency and complex dynamics in fluid flows. The work on *Convolutional PINNs for PDEs* [100] has demonstrated that PINNs can effectively capture small-scale dynamics in fluid flows, which is crucial for applications such as weather prediction and turbulence modeling. Additionally, the use of attention mechanisms and residual-based methods [176] has shown that PINNs can improve their performance in capturing non-linear features of solutions, making them more effective for complex fluid dynamics problems.

In terms of feasibility, PINNs have been shown to be computationally efficient in many practical applications. The work on *Distributed PINNs for Data-Efficient Solutions* [177] has demonstrated that PINNs can be trained efficiently on large-scale problems, reducing the computational cost compared to traditional numerical methods. This efficiency is particularly important in real-world engineering scenarios where computational resources are limited.

The effectiveness of PINNs in real-world applications is also evident in their ability to generalize to new and unseen scenarios. The use of transfer learning and meta-transfer learning [178] has shown that PINNs can adapt to new problems with minimal retraining, making them a flexible tool for a wide range of fluid mechanics applications. This adaptability is crucial for real-world engineering problems where the conditions and parameters can vary significantly.

In summary, the practical applications of PINNs in real-world fluid mechanics problems have demonstrated their feasibility and effectiveness. Their ability to handle complex geometries, high-frequency dynamics, and sparse data, combined with their computational efficiency and adaptability, makes them a valuable tool for solving a wide range of engineering problems. While there are still challenges to overcome, the results from various studies [179] indicate that PINNs are well-positioned to play a significant role in the future of fluid mechanics and engineering simulations.

## 7 Case Studies and Empirical Validation

### 7.1 Solving Navier-Stokes Equations with EPINN-NSE

The EPINN-NSE (Enhanced Physics-Informed Neural Networks for Solving Navier-Stokes Equations) model is a notable advancement in the field of physics-informed neural networks (PINNs), specifically tailored for solving the Navier-Stokes equations. This model's approach to approximating velocity components using either a stream function or directly is a key innovation, as it effectively addresses the complexities of fluid dynamics in both two- and three-dimensional scenarios [20]. The EPINN-NSE model builds upon traditional PINN frameworks by incorporating enhanced techniques that improve the accuracy and efficiency of solutions to the Navier-Stokes equations, which are fundamental in fluid mechanics [20].

In the context of solving the Navier-Stokes equations, the EPINN-NSE model leverages the concept of a stream function to simplify the system while ensuring that the velocity field remains divergence-free. This approach is particularly advantageous in two-dimensional flows, where the stream function can accurately represent the velocity components without the need for explicit computation of all three velocity components. By doing so, the model reduces the complexity of the problem and enhances computational efficiency. This method is especially effective in scenarios where the flow is incompressible, as the divergence-free condition is inherently satisfied, which is a critical requirement for accurate simulations [20].

For three-dimensional flows, the EPINN-NSE model directly approximates the velocity components, which allows for a more comprehensive representation of the fluid dynamics. This approach is essential for capturing the intricate interactions and variations in velocity that occur in three-dimensional spaces. The model employs a flexible architecture that can adapt to the specific requirements of the problem, ensuring that it can handle both laminar and turbulent flows with high accuracy. The direct approximation of velocity components enables the model to maintain the necessary physical constraints, such as the conservation of mass and momentum, which are crucial for the validity of the simulations [20].

The effectiveness of the EPINN-NSE model in solving the Navier-Stokes equations is further enhanced by its ability to handle complex geometries and boundary conditions. Traditional numerical methods often struggle with these aspects, particularly when dealing with irregular domains or varying boundary conditions. The EPINN-NSE model, however, demonstrates superior performance in these scenarios due to its flexible and data-driven nature. This adaptability is a result of the model's ability to learn from the underlying physical laws while simultaneously incorporating the specific details of the problem at hand. The model's capacity to generalize across different scenarios makes it a powerful tool for a wide range of fluid mechanics applications [20].

In addition to its ability to handle complex geometries, the EPINN-NSE model also excels in capturing the transient behavior of fluid flows. This is particularly important in applications where the flow dynamics change over time, such as in unsteady flows or flows with time-dependent boundary conditions. The model's ability to accurately predict the evolution of the flow field over time is a significant advantage, as it allows for more realistic and accurate simulations. This capability is achieved through the integration of time-dependent terms in the loss function, which ensures that the model can learn and adapt to the temporal variations in the flow [20].

The EPINN-NSE model's performance is further validated through a series of empirical studies and case studies that demonstrate its effectiveness in both two- and three-dimensional scenarios. These studies highlight the model's ability to achieve high accuracy with relatively few training data points, which is a significant advantage in scenarios where data is scarce or expensive to obtain. The model's ability to generalize well across different problems and conditions is another key factor in its success, as it reduces the need for extensive retraining when applied to new scenarios [20].

Moreover, the EPINN-NSE model incorporates advanced optimization techniques that enhance its training efficiency and convergence properties. These techniques include adaptive learning rates, regularization strategies, and the use of advanced optimization algorithms. By leveraging these methods, the model can achieve faster convergence and more stable training, which is crucial for solving complex fluid dynamics problems. The model's ability to maintain a balance between the physical constraints and the data-driven aspects of the problem is another critical factor in its success, as it ensures that the solutions are both accurate and physically meaningful [20].

In conclusion, the EPINN-NSE model represents a significant advancement in the application of physics-informed neural networks to the solution of the Navier-Stokes equations. Its approach to approximating velocity components using a stream function or directly, combined with its ability to handle complex geometries and boundary conditions, makes it a powerful tool for fluid mechanics simulations. The model's effectiveness in both two- and three-dimensional scenarios, as well as its ability to capture the transient behavior of fluid flows, further underscores its value in the field. As the field of physics-informed neural networks continues to evolve, the EPINN-NSE model serves as a benchmark for future research and development in this area [20].

### 7.2 Physics-Informed Neural Networks for Obstacle-Related Equations

Physics-Informed Neural Networks (PINNs) have been extended to address obstacle-related partial differential equations (PDEs), which pose unique challenges due to the need for accurate approximations of solutions above a given obstacle. These types of PDEs are commonly encountered in fluid dynamics, where obstacles such as walls, surfaces, or irregular geometries can significantly affect the flow field. Traditional numerical methods often struggle with these problems due to the complexity of handling boundaries and the need for high-resolution meshing. PINNs, on the other hand, offer a flexible and data-driven approach that can adapt to both regular and irregular obstacles without the need for explicit meshing, making them a promising tool for solving obstacle-related PDEs.

In the context of obstacle-related PDEs, PINNs are typically trained to enforce the governing equations of the physical system while also satisfying the boundary conditions imposed by the obstacle. This is achieved by incorporating the residual of the PDE into the loss function, along with terms that enforce the boundary constraints. For instance, in linear PDEs such as the Laplace equation, the PINN can be trained to approximate the solution in the domain, with the obstacle acting as a constraint that the solution must respect. By leveraging automatic differentiation, the network can compute the necessary derivatives of the solution at the boundary, ensuring that the physical laws are accurately enforced even in the presence of obstacles [5].

In nonlinear scenarios, the challenges of solving obstacle-related PDEs increase significantly. For example, in the case of nonlinear diffusion or reaction-diffusion equations, the presence of an obstacle can lead to complex interactions between the solution and the boundary. Traditional methods often require careful treatment of the nonlinear terms and the boundary conditions, which can be computationally intensive. PINNs, however, can handle these challenges by incorporating the nonlinear PDEs directly into the loss function and using the network's capacity to approximate highly nonlinear relationships. This approach has been demonstrated to be effective in solving nonlinear PDEs with obstacles, as shown in [45], where PINNs were used to model complex physical systems governed by nonlinear PDEs.

The performance of PINNs in solving obstacle-related PDEs has been validated in both regular and irregular obstacle scenarios. In regular obstacles, such as flat walls or simple geometric shapes, PINNs have demonstrated high accuracy and efficiency. For instance, in the case of the Poisson equation with a flat obstacle, the PINN can be trained to enforce the boundary conditions at the obstacle surface, ensuring that the solution satisfies the governing equation in the rest of the domain. This is particularly useful in applications such as heat conduction or electrostatics, where the presence of an obstacle modifies the distribution of the physical quantity being modeled [6].

In more complex, irregular obstacle scenarios, the flexibility of PINNs becomes even more apparent. For example, in fluid dynamics, obstacles such as rough surfaces or irregularly shaped objects can significantly alter the flow field. PINNs can be trained to approximate the solution in such cases by incorporating the geometry of the obstacle directly into the loss function. This allows the network to adapt to the complex boundary conditions imposed by the obstacle and produce accurate solutions even in the presence of high curvature or irregular surfaces. The ability of PINNs to handle such scenarios has been demonstrated in [164], where the model was trained to solve the Navier-Stokes equations with obstacles of varying shapes and sizes, showcasing its robustness and adaptability.

Another key advantage of PINNs in solving obstacle-related PDEs is their ability to handle high-frequency dynamics and complex flow patterns. Traditional numerical methods often struggle with these aspects, particularly when dealing with turbulent flows or high Reynolds number regimes. PINNs, however, can capture these dynamics by leveraging the expressive power of deep neural networks and the integration of physical constraints. This has been demonstrated in [180], where the model was used to simulate flows around obstacles with complex geometries, showing high accuracy and stability even in the presence of strong vortices and turbulence.

Furthermore, the adaptability of PINNs allows for efficient training even when dealing with sparse or noisy data. In many real-world applications, obtaining high-quality data for training is challenging, especially in cases where the obstacle is not well-characterized. PINNs can mitigate this issue by relying on the physical laws encoded in the loss function, reducing the dependence on large datasets. This is particularly beneficial in scenarios where data collection is expensive or impractical, such as in underground flow simulations or complex geological formations [180].

The use of PINNs for obstacle-related PDEs also extends to multi-physics problems, where the interaction between different physical processes must be accounted for. For example, in coupled systems involving fluid-structure interaction or thermo-mechanical coupling, the presence of an obstacle can lead to complex interactions between the different physical fields. PINNs can be trained to handle these interactions by incorporating the governing equations for each physical process into the loss function, allowing the model to capture the coupled dynamics accurately. This has been demonstrated in [54], where the model was used to simulate fluid-structure interaction problems with obstacles, showing excellent performance and accuracy.

In conclusion, the extension of PINNs to solve obstacle-related PDEs represents a significant advancement in the field of computational physics. The ability of PINNs to handle both regular and irregular obstacles, along with their flexibility in dealing with nonlinear and high-frequency dynamics, makes them a powerful tool for a wide range of applications. The success of PINNs in these scenarios highlights their potential to revolutionize traditional numerical methods, offering a more efficient and accurate approach to solving complex PDEs in the presence of obstacles. As research in this area continues to evolve, further improvements in the training and optimization of PINNs are expected to enhance their performance and broaden their applicability to even more challenging problems.

### 7.3 Transfer Learning in Branched Flow Simulations

Transfer learning has emerged as a powerful technique in the realm of physics-informed neural networks (PINNs), particularly for simulating complex and stochastic branched flows. Branched flows, characterized by their intricate and dynamic nature, pose significant challenges for traditional numerical methods and even for conventional PINN approaches. The ability of transfer learning to leverage pre-trained models and adapt them to new tasks offers a promising solution to these challenges, providing substantial computational speedups over traditional PINN training methods [134]. 

One of the primary advantages of transfer learning in the context of branched flows is its ability to reduce the computational burden associated with training PINNs from scratch for each new problem. In traditional PINN training, the model must learn the underlying physical laws and the specific characteristics of the flow problem from scratch, which can be computationally expensive and time-consuming. By contrast, transfer learning allows the model to start with a pre-trained network that has already learned the fundamental physics of fluid dynamics, enabling it to converge much faster on new, unseen problems. This is particularly beneficial in scenarios where the branched flow patterns are similar but not identical to those encountered in the training data, as the pre-trained model can quickly adapt to the new conditions without relearning the entire physics from scratch [134].

The application of transfer learning in branched flow simulations has been demonstrated in several studies, where the approach has shown significant improvements in both speed and accuracy. For instance, in the work titled "SVD-PINNs: Transfer Learning of Physics-Informed Neural Networks via Singular Value Decomposition," the authors applied transfer learning to simulate stochastic branched flows, demonstrating that the approach can achieve computational speedups of up to 3x compared to traditional PINN training [134]. This is attributed to the fact that the pre-trained model already has a good understanding of the physical laws governing the flow, allowing it to focus on learning the specific characteristics of the branched flow rather than relearning the basic physics. 

Moreover, transfer learning can also improve the robustness of PINNs in the face of noisy or incomplete data, which is a common issue in real-world fluid dynamics problems. In many cases, the data available for training PINNs may be sparse, noisy, or incomplete, making it difficult for the model to learn the underlying physical laws accurately. By starting with a pre-trained model that has already learned the basic physics, transfer learning can help the model to better handle these data challenges, leading to more accurate and reliable predictions [134].

Another benefit of transfer learning in branched flow simulations is its ability to handle different flow conditions and geometries more efficiently. Branched flows often occur in complex geometries where the flow patterns can vary significantly depending on the specific configuration. Traditional PINN training would require retraining the model for each new geometry, which can be time-consuming and resource-intensive. However, with transfer learning, the model can be adapted to new geometries by fine-tuning the pre-trained network on the new data, which is much faster and more efficient than starting from scratch. This is particularly useful in applications such as biomedical flow simulations, where the geometry of the flow domain can vary significantly between different patients or even within the same patient over time [134].

The effectiveness of transfer learning in branched flow simulations has also been validated through empirical studies. In one such study, the authors compared the performance of transfer learning-based PINNs with traditional PINNs on a set of branched flow problems, including the simulation of stochastic branched flows in random wave dynamics. The results showed that the transfer learning approach not only achieved faster convergence but also produced more accurate predictions, especially in cases where the flow patterns were complex and difficult to capture with traditional methods [134]. This suggests that transfer learning can be a valuable tool for improving the accuracy and efficiency of PINN-based simulations in real-world applications.

Furthermore, transfer learning can also be used to improve the generalization capabilities of PINNs, allowing them to perform well on a wide range of branched flow problems. By leveraging the knowledge gained from previous training sessions, transfer learning enables the model to generalize better to new, unseen scenarios, reducing the need for extensive retraining. This is particularly important in fluid dynamics, where the flow patterns can vary significantly depending on the specific conditions and parameters. The ability of transfer learning to enhance the generalization performance of PINNs makes it a valuable approach for solving a wide range of fluid dynamics problems, including those involving branched flows [134].

In addition to improving speed and accuracy, transfer learning can also reduce the computational costs associated with training PINNs for branched flow simulations. Traditional PINN training often requires a large number of training samples and a significant amount of computational resources, which can be prohibitive for large-scale problems. By using transfer learning, the number of training samples required can be significantly reduced, as the pre-trained model already has a good understanding of the underlying physics. This not only reduces the computational burden but also makes it possible to train PINNs on smaller, more manageable datasets, which is particularly useful in resource-constrained environments [134].

In conclusion, the application of transfer learning in branched flow simulations represents a significant advancement in the use of PINNs for fluid dynamics problems. By leveraging pre-trained models and adapting them to new tasks, transfer learning offers substantial computational speedups over traditional PINN training methods, while also improving the accuracy, robustness, and generalization capabilities of the models. This makes it a powerful tool for simulating complex and stochastic branched flows, and it has the potential to revolutionize the way we approach fluid dynamics problems in both research and real-world applications.

### 7.4 Unsteady Flow Modeling with Moving Boundaries

Unsteady flow modeling around moving boundaries presents a significant challenge in computational fluid dynamics (CFD) due to the dynamic nature of the domain and the necessity for accurate and efficient simulation of complex interactions between fluid and solid structures. Traditional numerical solvers, such as Arbitrary Lagrangian-Eulerian (ALE) methods, have been widely used for this purpose. However, these methods often suffer from limitations in terms of computational cost, accuracy, and adaptability. Recent advancements in Physics-Informed Neural Networks (PINNs) have introduced new possibilities for addressing these challenges, particularly in the context of moving-boundary problems. 

One of the key developments in this area is the introduction of moving-boundary-enabled PINN variants, which are specifically designed to handle unsteady flows past moving bodies. These variants incorporate the physics of fluid dynamics and the movement of boundaries directly into the neural network architecture, enabling the model to learn and predict the complex interactions between the fluid and the moving structures. The performance of these PINN variants has been compared against ALE-based solvers, highlighting their potential for improved accuracy and efficiency.

In a study by [19], the authors developed a moving-boundary-enabled PINN variant for unsteady flows past moving bodies. They demonstrated that this approach could achieve comparable or even superior accuracy to ALE-based solvers while significantly reducing computational costs. The study highlighted the ability of PINNs to capture the intricate dynamics of unsteady flows, including the formation and evolution of vortices, boundary layer separation, and other complex flow features. This is particularly important for applications such as aerospace engineering, where accurate predictions of unsteady flow behavior are critical for design optimization and performance assessment.

The data efficiency strategies employed in moving-boundary-enabled PINN variants are another key aspect of their performance. Traditional ALE-based solvers often require extensive mesh refinement and high-resolution simulations to accurately capture the dynamics of moving boundaries. In contrast, PINNs leverage their ability to learn from sparse data, making them particularly suitable for scenarios where high-quality data is limited or expensive to obtain. This data efficiency is achieved through the integration of physical constraints and the use of adaptive sampling techniques, which allow the model to focus on regions of the domain that are most critical for accurate predictions.

A notable example of this data efficiency is demonstrated in the work by [131], where the authors applied PINNs to digital twin applications. The study showed that PINNs could accurately predict the behavior of fluid systems with minimal data, making them ideal for real-time monitoring and control. This capability is particularly valuable in industrial settings, where the ability to quickly adapt to changing conditions can lead to significant improvements in efficiency and performance.

Moreover, the development of moving-boundary-enabled PINN variants has also addressed the challenge of handling complex geometries and dynamic boundary conditions. Traditional numerical solvers often struggle with these aspects, requiring extensive mesh generation and refinement. PINNs, on the other hand, can naturally handle irregular and moving boundaries without the need for complex meshing. This is due to their ability to learn the underlying physical laws and incorporate them directly into the model, allowing for a more flexible and robust approach to fluid dynamics simulations.

The work by [181] further emphasizes the importance of integrating physical constraints into PINN architectures. By embedding the governing equations of fluid dynamics into the loss function, these models can ensure that the predictions are not only accurate but also physically meaningful. This is particularly crucial for unsteady flow modeling, where the dynamics of the system can change rapidly and unpredictably.

In addition to their accuracy and efficiency, moving-boundary-enabled PINN variants have also demonstrated strong potential for scalability and adaptability. As highlighted in [182], the ability to handle large-scale simulations with high-resolution data is a significant advantage of these models. This scalability is essential for applications involving complex flow phenomena, such as turbulent flows and multiphase interactions, where the computational demands can be extremely high.

Another important aspect of moving-boundary-enabled PINN variants is their ability to handle transient and time-dependent problems. Traditional numerical solvers often require time-stepping procedures, which can be computationally expensive and may introduce numerical instabilities. PINNs, however, can naturally handle time-dependent problems by incorporating temporal derivatives into the loss function, allowing for more efficient and stable solutions. This is particularly beneficial for applications such as aeroelasticity, where the interaction between fluid and structural dynamics can lead to complex and nonlinear behaviors.

The study by [51] further illustrates the versatility of PINNs in handling various types of physical systems. By learning the local physics of magnetohydrodynamics (MHD), these models can accurately predict the behavior of complex fluid systems, including those with moving boundaries. This capability is essential for applications such as plasma physics and astrophysics, where the dynamics of the system can be highly complex and challenging to model.

In conclusion, the development of moving-boundary-enabled PINN variants has introduced a new paradigm for unsteady flow modeling around moving bodies. These models offer significant advantages over traditional ALE-based solvers in terms of accuracy, efficiency, and adaptability. By leveraging the power of physics-informed learning, they can capture the intricate dynamics of unsteady flows while maintaining data efficiency and scalability. The ongoing research in this area continues to push the boundaries of what is possible in fluid dynamics simulations, opening up new opportunities for innovation and advancement in various engineering and scientific applications.

### 7.5 Multiscale Flow Simulations with Spectral PINNs

Multiscale flow simulations present a significant challenge in fluid mechanics due to the coexistence of phenomena occurring over vastly different spatial and temporal scales. Traditional numerical methods often struggle with capturing these dynamics due to the high computational cost and the need for fine resolution in all regions of interest. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, and recent advancements in spectral PINNs have further enhanced their capability to address multiscale problems in fluid dynamics. Spectral PINNs leverage the spectral representation of solutions, enabling them to efficiently capture both small-scale and large-scale features of the flow [183].

The use of spectral PINNs for multiscale flow simulations has been demonstrated in the context of the Kuramoto-Sivashinsky equation (KSE) and the Navier-Stokes equations (NSE). The KSE is a well-known partial differential equation that models the dynamics of a thin fluid film and is particularly challenging due to its chaotic and turbulent behavior, which spans multiple spatial scales [183]. Spectral PINNs have been applied to solve the KSE by leveraging the Fourier spectral method, which allows for the efficient representation of the solution in the frequency domain. This approach enables the network to capture the intricate patterns and interactions that occur at different scales, significantly improving the accuracy and efficiency of the solution [183].

In addition to the KSE, spectral PINNs have been applied to the Navier-Stokes equations, which govern the motion of incompressible fluids. The NSE is a complex system of equations that describe the conservation of mass and momentum in fluid flow. Solving the NSE for multiscale flows is particularly challenging due to the presence of both large-scale vortices and small-scale turbulence. Spectral PINNs address this by incorporating a spectral representation of the solution, which allows the network to resolve fine-scale features without requiring an excessive number of collocation points [183]. This is particularly beneficial in scenarios where high-resolution data is limited or unavailable, as the spectral approach enables the network to generalize well across different scales.

The application of spectral PINNs to multiscale flow simulations has been further supported by the use of Fourier features. Fourier features are a technique that enhances the ability of neural networks to capture high-frequency components of the solution by embedding frequency information directly into the input. This has been shown to improve the convergence and accuracy of PINNs, particularly in problems where the solution contains rapidly varying components [120]. In the context of multiscale flows, the inclusion of Fourier features allows the network to better resolve small-scale structures while maintaining the ability to capture the overall behavior of the flow.

One of the key advantages of spectral PINNs is their ability to handle complex geometries and boundary conditions without the need for mesh generation. Traditional numerical methods, such as finite element or finite volume methods, require a mesh to discretize the domain, which can be computationally expensive and time-consuming, especially for complex geometries. In contrast, PINNs, and specifically spectral PINNs, can operate on a mesh-free basis, making them particularly well-suited for problems with irregular or moving boundaries [10]. This feature is especially valuable in fluid dynamics applications where the geometry of the domain may change over time, such as in the simulation of moving boundaries or deformable structures.

The effectiveness of spectral PINNs in multiscale flow simulations has been validated through a series of numerical experiments. For example, in the case of the KSE, spectral PINNs have been shown to accurately capture the chaotic behavior of the solution, even when the initial conditions are not fully resolved [183]. Similarly, in the case of the NSE, spectral PINNs have been able to resolve the intricate interactions between large-scale vortices and small-scale turbulence, demonstrating their capability to handle the multiscale nature of fluid flow [183]. These results highlight the potential of spectral PINNs to provide accurate and efficient solutions for a wide range of fluid dynamics problems.

In addition to their accuracy, spectral PINNs also offer significant advantages in terms of computational efficiency. By leveraging the spectral representation of the solution, these networks can achieve faster convergence and reduced computational costs compared to traditional PINNs. This is particularly important in large-scale simulations, where the computational resources required can be prohibitive [183]. The use of spectral methods also allows for the incorporation of advanced techniques, such as adaptive sampling and multi-fidelity learning, which further enhance the performance of the network.

Overall, the application of spectral PINNs to multiscale flow simulations represents a significant advancement in the field of fluid mechanics. By combining the power of deep learning with the principles of physics, spectral PINNs offer a robust and efficient approach to solving complex fluid dynamics problems. The successful application of these networks to the KSE and NSE demonstrates their versatility and potential for broader use in a variety of fluid flow scenarios. As research in this area continues to evolve, it is likely that spectral PINNs will play an increasingly important role in the development of next-generation computational tools for fluid mechanics.

### 7.6 Parametric Navier-Stokes Solutions with PINNs

Physics-Informed Neural Networks (PINNs) have shown remarkable potential in solving parametric Navier-Stokes equations, where the solution depends on a set of parameters such as Reynolds number, boundary conditions, or geometric configurations. By incorporating these parameters into the input of the neural network, PINNs can learn solution functions that are valid across a range of parameter values, making them highly versatile for engineering applications where parametric studies are essential. This approach enables the prediction of flow fields for new parameter configurations without the need for retraining from scratch, thus significantly reducing computational costs and improving the efficiency of fluid dynamics simulations.

One of the key advantages of using PINNs for parametric Navier-Stokes problems is their ability to enforce physical laws directly during the training process. The governing equations, including the Navier-Stokes equations, are embedded into the loss function of the neural network, ensuring that the solution adheres to the fundamental conservation laws of mass, momentum, and energy. This physical constraint not only improves the accuracy of the predictions but also enhances the generalizability of the model to unseen parameter values. The combination of data-driven learning and physical knowledge allows PINNs to outperform traditional data-driven models, especially in scenarios where the available data is sparse or noisy.

A notable approach in this domain is the integration of parameters into the input of the neural network, effectively transforming the problem into a parameterized PDE. For example, in the case of the Navier-Stokes equations, the parameters such as Reynolds number and boundary conditions are included as part of the input vector. This enables the PINN to learn a mapping from the parameter space to the solution space, capturing the complex dependencies between the parameters and the flow behavior. The ability to handle such parameterized problems is crucial in applications like aerodynamic design, where the performance of a system needs to be evaluated for a wide range of operating conditions.

The accuracy of gradient predictions is another critical aspect of PINNs in parametric Navier-Stokes solutions. In traditional numerical methods, the computation of gradients is often computationally expensive and time-consuming, especially for high-dimensional problems. PINNs, on the other hand, leverage automatic differentiation to compute gradients efficiently, even for complex parametric problems. This capability is essential for applications that require sensitivity analysis, such as optimization and uncertainty quantification. The gradients computed by PINNs are not only accurate but also consistent with the conservation laws, ensuring that the solutions are physically meaningful.

The compliance with conservation laws is a fundamental requirement for any numerical method, and PINNs are no exception. By embedding the Navier-Stokes equations into the loss function, PINNs ensure that the solutions satisfy the conservation of mass and momentum. This is particularly important in turbulent flow simulations, where the accuracy of the solution is critical for reliable predictions. The ability of PINNs to enforce these conservation laws during the training process significantly enhances their robustness and reliability.

Empirical studies have demonstrated the effectiveness of PINNs in learning parametric Navier-Stokes solutions. For instance, the work by [3] highlights the successful application of PINNs to the classical 2D flow past a cylinder problem, where the Reynolds number is treated as a parameter. The study shows that PINNs can accurately predict the velocity and pressure fields for a range of Reynolds numbers, demonstrating their capability to interpolate between different parameter configurations. Moreover, the results indicate that PINNs can capture the complex flow behavior, including the formation of vortices and the transition to turbulence, with high accuracy.

Another significant contribution in this area is the development of a parametric PINN framework that incorporates the parameters of interest as inputs to the neural network. This approach allows for the direct estimation of the solution functions for a range of parameter values, making it highly suitable for applications that require parametric studies. The study by [3] demonstrates that this approach can achieve high accuracy in predicting the solution functions, even for complex parameter configurations. The results also show that the PINN models are able to maintain the conservation laws, ensuring that the solutions are physically consistent.

The integration of parameters into the input of the neural network also enables the use of transfer learning techniques to improve the efficiency of training. Transfer learning allows the PINN to leverage knowledge from previously trained models, reducing the amount of training data required and accelerating the convergence of the model. This is particularly useful in scenarios where the parameter space is large, and the available data is limited. The study by [3] shows that transfer learning can significantly improve the performance of PINNs, making them more practical for real-world applications.

The accuracy of gradient predictions is another critical factor in the success of PINNs for parametric Navier-Stokes problems. The ability to compute gradients efficiently and accurately is essential for applications that require sensitivity analysis and optimization. The study by [3] demonstrates that PINNs can accurately predict the gradients of the solution functions, even for high-dimensional parameter spaces. This capability is crucial for applications such as shape optimization, where the gradients are used to guide the optimization process.

In addition to the accuracy of gradients, the compliance with conservation laws is a key requirement for any numerical method. The study by [3] shows that PINNs can enforce the conservation laws during the training process, ensuring that the solutions are physically consistent. This is particularly important in turbulent flow simulations, where the accuracy of the solution is critical for reliable predictions. The ability of PINNs to maintain the conservation laws significantly enhances their robustness and reliability.

Overall, the use of PINNs for solving parametric Navier-Stokes equations has shown great promise in recent studies. By incorporating parameters into the input of the neural network, PINNs can learn solution functions that are valid across a range of parameter values, making them highly versatile for engineering applications. The accuracy of gradient predictions and compliance with conservation laws are critical aspects that make PINNs a powerful tool for fluid dynamics simulations. The empirical results from various studies demonstrate the effectiveness of PINNs in solving parametric Navier-Stokes problems, highlighting their potential for future applications in computational fluid dynamics.

### 7.7 Architectural Strategies for PINN Optimization

The performance of Physics-Informed Neural Networks (PINNs) is significantly influenced by the choice of neural architecture, including the activation functions and the overall design of the network. Recent studies have focused on optimizing PINN architectures to enhance training efficiency, accuracy, and robustness. One of the key areas of investigation has been the impact of different activation functions on PINN performance. Among the various activation functions explored, Gaussian activations have shown promising results in improving the training dynamics of PINNs. This is because Gaussian activations can better capture the smooth and continuous nature of the solutions to partial differential equations (PDEs), which is critical for ensuring accurate and stable training processes. In particular, the use of Gaussian activations has been shown to enhance the convergence of PINNs by reducing the likelihood of getting stuck in local minima and improving the overall stability of the training process [55].

The impact of neural architecture on PINN training is further exemplified by the introduction of preconditioned neural architectures. Preconditioning refers to the process of modifying the neural network to improve the conditioning of the optimization problem, thereby facilitating faster and more stable convergence. In the context of PINNs, preconditioned architectures have been proposed to address the challenges associated with the non-convex nature of the loss landscape and the difficulties in balancing multiple loss terms during training. By introducing a preconditioned neural architecture, researchers have demonstrated improved performance in solving complex PDEs, particularly in scenarios where traditional PINNs struggle with convergence and accuracy. The preconditioned architecture is designed to account for the specific characteristics of the PDEs being solved, such as the spatial and temporal scales, and the nature of the physical constraints. This approach has been shown to significantly enhance the training efficiency of PINNs and improve their ability to generalize to new problems [55].

The theoretical analysis of neural architectures in the context of PINNs has provided valuable insights into the relationship between network structure and training performance. For instance, the study on the impact of neural architecture on PINN training has revealed that the choice of activation function and the design of the network can have a profound effect on the convergence behavior of the model. The use of Gaussian activations has been found to be particularly effective in improving the trainability of PINNs, as they enable the network to better approximate the underlying physical laws and enforce the constraints imposed by the PDEs. This is especially important in scenarios where the solution to the PDE is highly non-linear or exhibits complex behavior, such as in turbulent flows or high-frequency oscillations [55].

In addition to the choice of activation functions, the structure of the neural network itself plays a crucial role in determining the effectiveness of PINN training. Recent research has explored the use of various architectural designs to optimize PINN performance, including the use of deeper networks, the introduction of residual connections, and the incorporation of attention mechanisms. For example, the study on the impact of neural architecture on PINN training has shown that deeper networks can provide better approximation capabilities, but they also introduce additional challenges in terms of training stability and convergence. To address these challenges, researchers have proposed the use of residual connections, which help to alleviate the vanishing gradient problem and improve the overall training dynamics of the network. Additionally, attention mechanisms have been introduced to enable the network to focus on the most relevant parts of the input data, thereby improving the accuracy and efficiency of the model [55].

Another important aspect of architectural strategies for PINN optimization is the use of specialized activation functions that incorporate physical knowledge into the neural network. These physics-informed activation functions (PAFs) are designed to encode the physical constraints and properties of the system being modeled, thereby enhancing the performance of the PINN. For instance, the study on the impact of neural architecture on PINN training has demonstrated that the use of PAFs can significantly improve the accuracy of the model by ensuring that the network respects the underlying physical laws. This is particularly important in scenarios where the solution to the PDE is highly sensitive to small changes in the input data or the boundary conditions [55].

The effectiveness of different architectural strategies in optimizing PINN training has been validated through a series of empirical studies. For example, the use of Gaussian activations has been shown to lead to faster convergence and improved accuracy in solving a variety of PDEs, including the Navier-Stokes equations and the heat equation. Similarly, the introduction of preconditioned architectures has been found to enhance the stability and efficiency of PINN training, particularly in complex and high-dimensional problems. These findings highlight the importance of carefully selecting and designing the neural architecture to ensure optimal performance in the context of PINNs [55].

In conclusion, the impact of neural architecture on PINN training is a critical factor in determining the success of PINN-based solutions for fluid mechanics and other scientific applications. The use of Gaussian activations and the introduction of preconditioned neural architectures have been shown to significantly enhance the training efficiency, accuracy, and robustness of PINNs. These architectural strategies provide valuable insights into the design of effective PINN models and highlight the importance of considering the specific characteristics of the PDEs being solved when developing and optimizing neural network architectures. As the field of PINNs continues to evolve, further research into architectural strategies will be essential to address the challenges of training PINNs on complex and high-dimensional problems.

### 7.8 3D Flow-Thermal Problems with Sparse Data

[82]

The application of Physics-Informed Neural Networks (PINNs) to three-dimensional (3D) flow-thermal problems with sparse data represents a significant advancement in the field of computational fluid dynamics (CFD). These problems, which involve the coupled simulation of fluid flow and heat transfer, are inherently complex and often require high computational resources to solve accurately. The integration of PINNs into such simulations offers a promising alternative to traditional numerical methods, especially when dealing with sparse or limited data. This subsection explores how PINNs, particularly the hybrid data-PINNs approach, are being used to address 3D incompressible Navier-Stokes equations with sparse domain data, emphasizing their potential in surrogate modeling for electronics design.

One of the primary challenges in solving 3D flow-thermal problems is the scarcity of high-quality, densely sampled data. Traditional methods often rely on extensive computational grids and large datasets to capture the intricate interactions between fluid flow and thermal dynamics. However, in real-world applications, such as electronics cooling, obtaining dense data can be impractical due to cost and time constraints. PINNs offer a way to overcome this limitation by leveraging the physical laws governing the problem, allowing the network to learn the solution even with sparse data points.

The hybrid data-PINNs approach, as discussed in the paper titled "A data-driven PINN approach for the solution of Navier-Stokes equations" [184], combines the strengths of data-driven methods with the physical constraints imposed by the Navier-Stokes equations. This method uses a PINN to approximate the solution of the Navier-Stokes equations, while also incorporating data from experiments or simulations to improve the accuracy of the model. By embedding the governing equations into the loss function, the PINN ensures that the solution adheres to the physical laws, even when the data is sparse.

In the context of 3D flow-thermal problems, the hybrid data-PINNs approach is particularly effective because it allows the model to learn the complex interactions between fluid flow and heat transfer without requiring an extensive number of data points. This is crucial in applications such as electronics design, where the geometry and boundary conditions can be highly complex, and the availability of data is often limited. The ability of PINNs to generalize from sparse data makes them well-suited for such scenarios, where traditional methods might struggle to produce accurate results.

The paper titled "Surrogate-data-enriched Physics-Aware Neural Networks" [139] further highlights the potential of PINNs in handling sparse data by demonstrating how physics-aware models can be enriched with computationally cheaper, but inexact, data from other surrogate models like Reduced-Order Models (ROMs). This approach helps to mitigate the challenges associated with data scarcity by leveraging the knowledge embedded in these surrogate models. The results show that incorporating inexact data from ROMs can significantly improve the training accuracy of the PINN, making it a valuable tool for solving 3D flow-thermal problems with sparse data.

Another critical aspect of the hybrid data-PINNs approach is its ability to handle the non-linear and high-dimensional nature of 3D flow-thermal problems. The paper titled "Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics" [164] discusses how PINNs can be used to learn solutions to partial differential equations (PDEs) with a multi-resolution approach. This method enables the PINN to capture both the large-scale and small-scale features of the solution, which is essential for accurately modeling the complex dynamics of 3D flow-thermal problems.

The paper titled "Solving multiscale elliptic problems by sparse radial basis function neural networks" [185] presents a sparse radial basis function (RBF) neural network method that can solve elliptic PDEs with multiscale coefficients. This approach is particularly useful for 3D flow-thermal problems, where the solution may involve multiple scales of variation. By using an $\ell_1$ regularization term in the loss function, the method ensures that the solution is represented with fewer RBFs, which reduces the computational complexity and improves the efficiency of the model.

The application of PINNs to 3D flow-thermal problems with sparse data is also supported by the paper titled "A data-driven PINN approach for the solution of Navier-Stokes equations" [184], which demonstrates the effectiveness of PINNs in capturing the essential features of the solution even with limited data. This is particularly important in electronics design, where the geometry and boundary conditions can vary significantly, and the availability of data is often limited. The results show that PINNs can accurately predict the flow and thermal behavior of complex systems, even when the data is sparse.

Furthermore, the paper titled "A data-driven PINN approach for the solution of Navier-Stokes equations" [184] highlights the importance of using physics-informed loss functions to guide the training of the PINN. By incorporating the governing equations into the loss function, the PINN is trained to produce solutions that are consistent with the physical laws, even when the data is sparse. This approach ensures that the model is not just fitting the data but is also capturing the underlying physics, which is crucial for accurate predictions.

In conclusion, the application of PINNs to 3D flow-thermal problems with sparse data represents a significant advancement in the field of computational fluid dynamics. The hybrid data-PINNs approach, as demonstrated in several studies, offers a powerful tool for solving complex 3D flow-thermal problems with limited data. By combining the strengths of data-driven methods with the physical constraints imposed by the Navier-Stokes equations, PINNs can accurately model the complex interactions between fluid flow and heat transfer, even when the data is sparse. This makes them an invaluable tool for surrogate modeling in electronics design, where the availability of data is often limited, and the complexity of the problem requires an efficient and accurate solution.

### 7.9 Adaptive Collocation Point Sampling in PINNs

Adaptive collocation point sampling has emerged as a critical technique in improving the accuracy and efficiency of Physics-Informed Neural Networks (PINNs). The selection of collocation points, which are the points where the PDE residuals are evaluated during training, significantly affects the performance of PINNs. Traditional approaches often use uniform or random sampling, which can lead to suboptimal performance due to the uneven distribution of residuals across the domain. Adaptive sampling strategies, on the other hand, dynamically adjust the placement of collocation points based on the current state of the network, leading to more efficient and accurate solutions.

One notable approach is the use of importance sampling, where collocation points are selected based on the magnitude of the residual loss function. This method ensures that points with higher residuals, which are more critical for convergence, are sampled more frequently. According to the paper titled "Efficient training of physics-informed neural networks via importance sampling", this approach can significantly improve the convergence behavior of PINNs by focusing the training on regions where the model needs the most improvement [163]. The study demonstrated that importance sampling not only enhances the training efficiency but also leads to more accurate solutions compared to uniform sampling methods.

Another significant contribution in this area is the development of the Residual-based Adaptive Node Generation (RANG) method. This approach is based on a variable density nodal distribution method for Radial Basis Function Finite Differences (RBF-FD) and is designed to optimize the placement of collocation points. As discussed in the paper "RANG: A Residual-based Adaptive Node Generation Method for Physics-Informed Neural Networks", RANG improves the training stability by dynamically adjusting the number of points based on the residual distribution [97]. The numerical experiments conducted on various PDEs showed that RANG outperforms traditional uniform sampling in terms of both accuracy and efficiency.

The concept of adaptive sampling has also been extended to include temporal and spatial considerations, particularly in the context of time-dependent PDEs. The paper "A Novel Adaptive Causal Sampling Method for Physics-Informed Neural Networks" introduced a causal sampling approach that takes into account the temporal dynamics of the problem, ensuring that the sampling strategy respects the causality of the underlying physics [49]. This method not only improves the accuracy of the solution but also enhances the convergence properties of the PINN by focusing on regions that are more likely to contribute to the overall solution.

In addition to importance sampling and RANG, the paper "Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory" proposed a technique that uses good lattice points to select collocation points. This method leverages number theory to generate a set of points that are uniformly distributed across the domain, which can significantly reduce the number of points needed for accurate solutions [96]. The experiments showed that this approach can achieve competitive performance with fewer collocation points, leading to faster training times and lower computational costs.

The paper "Loss Landscape Engineering via Data Regulation on PINNs" also highlighted the impact of collocation point selection on the training dynamics of PINNs. The study demonstrated that by regulating the data used in the training process, the loss landscape can be made more navigable, leading to faster convergence and improved accuracy [69]. The authors found that the use of sparse or coarse data as a regulator can morph the loss landscape, making it easier for the optimizer to find the solution.

Another innovative approach is the use of adaptive sampling in conjunction with domain decomposition methods. The paper "Domain decomposition-based coupling of physics-informed neural networks via the Schwarz alternating method" explored how domain decomposition can be used to improve the training of PINNs by dividing the problem into smaller subdomains [73]. This method not only enhances the accuracy of the solution but also allows for more efficient parallelization of the training process.

The paper "Adversarial Training for Physics-Informed Neural Networks" also discussed the role of adaptive sampling in improving the robustness of PINNs. By using adversarial samples, the training process can be enhanced to better handle complex PDEs, particularly those with sharp or oscillatory solutions [186]. This approach not only improves the accuracy of the solution but also enhances the stability of the training process.

In summary, adaptive collocation point sampling has become a crucial technique in improving the performance of PINNs. By dynamically adjusting the placement of collocation points based on the current state of the network, these methods can significantly enhance the accuracy and efficiency of PINNs. The various approaches discussed in the literature, such as importance sampling, RANG, causal sampling, good lattice training, and domain decomposition, demonstrate the effectiveness of adaptive sampling in addressing the challenges associated with collocation point selection. As the field of PINNs continues to evolve, the development of more sophisticated adaptive sampling strategies will play a vital role in advancing the capabilities of these networks in solving complex PDEs.

### 7.10 Challenges in Data-Free PINN Simulations

The use of Physics-Informed Neural Networks (PINNs) without given data, commonly referred to as data-free PINN simulations, presents significant challenges that have been highlighted through experimental studies on complex fluid dynamics problems such as the Taylor-Green vortex and cylinder flow [25]. Data-free PINN simulations rely solely on the governing physical equations, such as the Navier-Stokes equations, to train the neural network, eliminating the need for labeled data. While this approach has the potential to reduce data dependency and enable simulations in data-scarce environments, it introduces several fundamental difficulties that need to be addressed for successful implementation.

One of the primary challenges in data-free PINN simulations is the lack of explicit data to guide the training process. Traditional PINN methods incorporate both the physical laws and observational data into the loss function, ensuring that the model is trained to satisfy both the governing equations and the observed behavior. However, in data-free scenarios, the absence of such data means that the network must rely entirely on the residual of the PDEs to inform its training. This can lead to poor convergence and inaccurate solutions, as the network may not have sufficient constraints to learn the correct physical behavior. Studies on the Taylor-Green vortex and cylinder flow have shown that data-free PINNs often struggle to capture the complex dynamics of these flows, resulting in solutions that deviate significantly from the expected behavior [25].

Another significant challenge is the sensitivity of data-free PINNs to the initial conditions and the choice of hyperparameters. The absence of data means that the training process is more susceptible to the initial configuration of the neural network and the optimization strategy used. For instance, the choice of activation functions, network depth, and learning rate can greatly influence the performance of the PINN. In the case of the Taylor-Green vortex, experiments have shown that even small variations in the initial conditions can lead to vastly different solutions, highlighting the need for robust and well-tuned hyperparameters [25]. Additionally, the lack of data makes it difficult to validate the model's performance, as there are no ground-truth solutions to compare against.

The issue of spectral bias is another critical challenge in data-free PINN simulations. Spectral bias refers to the tendency of neural networks to learn low-frequency components of the solution more efficiently than high-frequency components. This can be particularly problematic in fluid dynamics simulations, where high-frequency dynamics are often essential for accurate predictions. In data-free scenarios, the absence of data exacerbates this issue, as the network has no external guidance to learn the high-frequency features. Experiments on the Taylor-Green vortex and cylinder flow have demonstrated that data-free PINNs often fail to capture the fine-scale structures of the flow, leading to solutions that are less accurate and less physically meaningful [25].

Furthermore, the computational cost of training data-free PINNs can be prohibitively high. Without the guidance of data, the training process must rely on the PDE residuals to iteratively refine the solution. This can lead to longer training times and higher computational resources, making it less practical for large-scale simulations. The complexity of the PDEs being solved also plays a significant role in the computational burden, as more complex equations require more extensive training to achieve accurate solutions. In the context of the Taylor-Green vortex and cylinder flow, the high nonlinearity and multiscale nature of the equations further complicate the training process, making it challenging to achieve convergence within a reasonable timeframe [25].

The lack of data also poses challenges in terms of generalization and robustness. Data-free PINNs must be able to generalize across different scenarios and maintain accuracy under varying conditions. However, without the presence of data to guide the training, the network may struggle to adapt to new or unseen situations. This is particularly evident in the case of the Taylor-Green vortex, where the flow exhibits complex, chaotic behavior that is difficult to capture without sufficient training data. The inability to generalize effectively limits the practical applicability of data-free PINNs, as they may not perform well in real-world scenarios where conditions can vary significantly [25].

To address these challenges, further research is needed to develop more robust and efficient training strategies for data-free PINNs. One potential approach is to incorporate physics-informed constraints that can guide the training process without the need for explicit data. Techniques such as the use of variational formulations, energy-based methods, and hybrid approaches that combine PINNs with traditional numerical solvers may offer promising solutions. Additionally, the development of adaptive sampling strategies and improved optimization algorithms could help to enhance the performance of data-free PINNs by providing better guidance during the training process [25].

In summary, while data-free PINN simulations offer an attractive alternative to traditional data-driven approaches, they come with significant challenges that need to be addressed. The lack of data, sensitivity to hyperparameters, spectral bias, high computational costs, and issues with generalization and robustness all contribute to the difficulty of implementing data-free PINNs successfully. Future research should focus on developing innovative methods to overcome these challenges, ensuring that data-free PINNs can achieve accurate and reliable solutions for complex fluid dynamics problems.

### 7.11 Inverse Problems in Engineering Structures

Inverse problems in engineering structures involve determining unknown parameters or conditions based on observed data, often requiring the solution of complex partial differential equations (PDEs) that describe the physical behavior of the system. Physics-informed neural networks (PINNs) have emerged as a powerful tool for addressing these inverse problems, particularly in the context of structural analysis, where they can be used to estimate boundary conditions, material properties, or internal forces from limited experimental data [187]. The integration of physics-based constraints into the loss function of PINNs enables these networks to learn solutions that are consistent with the underlying physical laws, making them well-suited for inverse problems that require both accuracy and physical plausibility.

One of the key challenges in solving inverse problems with PINNs is the potential for non-physical solutions or poor convergence due to the ill-posed nature of the problem. This is especially true when the available data is sparse or noisy, as the network may struggle to distinguish between physical and non-physical solutions [63]. To address this, researchers have explored the use of transfer learning and uncertainty weighting, which can significantly enhance the performance of PINNs in inverse problem scenarios.

Transfer learning is particularly useful in inverse problems where prior knowledge or pre-trained models can be leveraged to improve the training process. For example, in structural analysis, a PINN trained on a related problem—such as a similar type of structure or a different set of boundary conditions—can be fine-tuned to solve the specific inverse problem at hand. This approach not only accelerates the training process but also improves the robustness of the solution by reducing the risk of overfitting to the limited data [178]. By initializing the network with weights from a pre-trained model, transfer learning enables the PINN to start from a more informed state, thereby increasing the likelihood of converging to a physically meaningful solution.

Uncertainty weighting is another technique that has gained attention for its ability to improve the performance of PINNs in inverse problems. In traditional PINN formulations, the loss function is typically composed of a weighted sum of the PDE residuals and boundary conditions. However, this approach can be problematic when the data is sparse or the problem is ill-conditioned, as the weights may not accurately reflect the relative importance of each term. To address this, researchers have proposed methods that dynamically adjust the weights based on the uncertainty of the data or the model's predictions [54]. For instance, uncertainty weighting can be used to assign higher weights to regions where the model is less confident, allowing the PINN to focus on areas that require more accurate predictions.

A recent study demonstrated the effectiveness of combining transfer learning with uncertainty weighting in solving inverse problems in structural analysis [178]. The authors proposed a meta-transfer learning (SMT) framework that leverages prior knowledge from related tasks to accelerate the training of PINNs on new inverse problems. In their experiments, the SMT approach significantly reduced the computational cost of training while maintaining high accuracy. By decomposing the problem into smaller, more manageable subproblems and using meta-learners to initialize the training process, the framework was able to achieve faster convergence and better generalization, even in cases where the data was limited or noisy.

Another important aspect of inverse problems in engineering structures is the need for robust and reliable predictions, particularly in safety-critical applications. To this end, researchers have explored the use of Bayesian neural networks and other uncertainty quantification techniques to provide probabilistic estimates of the solution [188]. These methods can help quantify the uncertainty in the PINN's predictions, providing valuable insights into the reliability of the results. For example, by incorporating Bayesian inference into the training process, PINNs can estimate the distribution of possible solutions rather than just a single point estimate, which is particularly useful in cases where the data is sparse or the problem is highly nonlinear.

In addition to transfer learning and uncertainty weighting, other techniques such as domain decomposition and multi-fidelity learning have also been applied to improve the performance of PINNs in inverse problems. Domain decomposition involves dividing the problem into smaller subdomains, which can be solved independently and then combined to obtain the final solution. This approach can significantly reduce the computational burden and improve the scalability of PINNs, especially for large-scale engineering structures [73]. Multi-fidelity learning, on the other hand, leverages data of varying fidelity levels to improve the training of PINNs. By incorporating both high-fidelity and low-fidelity data, multi-fidelity learning can enhance the accuracy of the solution while reducing the need for extensive computational resources.

A notable example of the application of PINNs to inverse problems in engineering structures is the use of PINNs to estimate material properties from experimental data. In this context, PINNs can be trained to predict the mechanical response of a structure based on a set of input parameters, such as the material's Young's modulus or Poisson's ratio. By minimizing the residual of the governing PDEs and incorporating the experimental data into the loss function, PINNs can effectively learn the relationship between the input parameters and the observed response. This approach has been successfully applied to problems such as the estimation of crack propagation in composite materials and the identification of damage in reinforced concrete structures [187].

Overall, the application of PINNs to inverse problems in engineering structures is a rapidly evolving field, with significant potential for improving the accuracy and efficiency of structural analysis. By leveraging techniques such as transfer learning, uncertainty weighting, and multi-fidelity learning, researchers are able to overcome the challenges associated with sparse data and ill-posed problems. As the field continues to advance, it is likely that PINNs will play an increasingly important role in solving complex inverse problems in engineering, contributing to the development of more robust and reliable structural models.

### 7.12 Solving Hyperbolic PDEs with PINNs

The solution of hyperbolic partial differential equations (PDEs) presents unique challenges due to the presence of shocks, discontinuities, and non-linear wave propagation phenomena. Traditional numerical methods, such as finite volume and finite difference schemes, often struggle with these problems due to the need for careful treatment of discontinuities and the computational cost of resolving sharp gradients. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative for solving hyperbolic PDEs, leveraging their ability to integrate physical laws with data-driven learning and offering a flexible framework for handling complex dynamics [189]. This subsection discusses the development of a PINN framework for solving hyperbolic PDEs, focusing on the handling of shocks and discontinuities and its performance on analytical solutions.

Hyperbolic PDEs, such as the Euler equations, the Burgers equation, and the shallow water equations, are characterized by their wave-like solutions, which can exhibit discontinuities (e.g., shocks) and complex interactions. The solution of such equations typically requires numerical methods that can resolve these features without introducing unphysical oscillations or excessive diffusion. PINNs offer a novel approach by embedding the governing PDEs directly into the loss function, allowing the network to learn the solution while respecting the physical constraints. This approach enables PINNs to handle hyperbolic PDEs without the need for explicit mesh generation or discretization, as the neural network can approximate the solution over a continuous domain [189].

One of the main challenges in solving hyperbolic PDEs with PINNs is the handling of shocks and discontinuities, which can lead to poor convergence and inaccurate solutions. This is due to the fact that neural networks tend to smooth out sharp features, making it difficult to capture the abrupt changes in the solution. To address this issue, recent studies have introduced various strategies to improve the ability of PINNs to resolve discontinuities. For example, a study proposed a PINN framework that incorporates discontinuity-aware loss functions, which penalize deviations from the expected discontinuity behavior and help the network better approximate shocks [189]. Additionally, some approaches have used adaptive sampling techniques, such as residual-based adaptive sampling, to focus the training on regions of high residual error, which often correspond to areas with sharp gradients or discontinuities [189]. These strategies have shown promise in improving the accuracy of PINNs for hyperbolic PDEs.

Another critical aspect of solving hyperbolic PDEs with PINNs is their ability to handle non-linear wave propagation. Unlike elliptic or parabolic PDEs, hyperbolic equations do not necessarily have smooth solutions, and their behavior can be highly sensitive to initial and boundary conditions. PINNs have demonstrated the ability to capture the non-linear dynamics of hyperbolic PDEs by learning the underlying physical laws through the residual loss function. For example, a study applied PINNs to solve the Burgers equation, a classic example of a hyperbolic PDE, and showed that the network could accurately predict the formation of shocks and the evolution of the solution over time [189]. The results were compared with analytical solutions and traditional numerical methods, demonstrating the effectiveness of PINNs in capturing the complex behavior of hyperbolic PDEs.

The performance of PINNs on analytical solutions of hyperbolic PDEs has been a key area of investigation. Analytical solutions provide a ground truth for evaluating the accuracy of PINNs and offer insights into their ability to resolve complex dynamics. For instance, a study used the method of characteristics to derive analytical solutions for the one-dimensional Euler equations and compared them with the predictions of a PINN. The results showed that the PINN could accurately capture the propagation of waves and the formation of shocks, even in the presence of strong discontinuities [189]. This study also highlighted the importance of appropriate initialization and training strategies in ensuring the convergence of PINNs for hyperbolic PDEs.

In addition to handling shocks and discontinuities, PINNs have shown promise in solving high-dimensional hyperbolic PDEs, which are often challenging for traditional numerical methods. The ability of PINNs to learn the solution over a continuous domain without the need for explicit meshing makes them well-suited for problems with complex geometries and high-dimensional state spaces. For example, a study demonstrated the application of PINNs to solve the two-dimensional shallow water equations, which describe the motion of fluid flows in open channels and are widely used in oceanography and meteorology. The results showed that the PINN could accurately capture the dynamics of the flow, including the formation of hydraulic jumps and the propagation of waves, and outperformed traditional finite volume methods in terms of computational efficiency [189].

The development of PINNs for solving hyperbolic PDEs has also benefited from advances in optimization techniques and adaptive sampling strategies. For instance, studies have explored the use of adaptive learning rates and momentum-based optimization to improve the convergence of PINNs for hyperbolic PDEs [189]. These techniques have been shown to help PINNs navigate the complex loss landscape associated with hyperbolic PDEs and avoid getting stuck in local minima. Additionally, the use of importance sampling and residual-based adaptive sampling has been shown to improve the accuracy of PINNs by focusing the training on regions of high residual error, which often correspond to areas with sharp gradients or discontinuities [189].

In summary, the development of PINNs for solving hyperbolic PDEs represents a significant advancement in the field of computational fluid dynamics. By integrating physical laws into the learning process, PINNs offer a flexible and efficient framework for solving complex hyperbolic PDEs, including the handling of shocks and discontinuities. The performance of PINNs on analytical solutions and their ability to capture the non-linear dynamics of hyperbolic PDEs have been demonstrated in several studies, highlighting their potential as a powerful tool for fluid dynamics simulations. As research in this area continues to advance, the integration of PINNs with other machine learning techniques and optimization strategies will likely further improve their performance and expand their applicability to a wider range of hyperbolic PDEs.

### 7.13 Multi-Task Optimization in Traffic Density Prediction

The integration of multi-task optimization in training Physics-Informed Neural Networks (PINNs) for traffic density prediction has emerged as a promising strategy to enhance the performance of the main task by leveraging auxiliary tasks. This approach not only improves the generalization and robustness of the model but also facilitates the learning of more accurate and stable solutions for complex traffic scenarios. By incorporating multiple tasks, the model is encouraged to learn shared representations and features that are beneficial across different objectives, leading to improved efficiency and accuracy in traffic density prediction.

Multi-task optimization in the context of PINNs involves the simultaneous training of the neural network to solve multiple related tasks. These tasks can include predicting traffic density, estimating vehicle speeds, and forecasting traffic flow patterns. By training the network on these auxiliary tasks alongside the main task of traffic density prediction, the model gains a more comprehensive understanding of the underlying dynamics of traffic systems. This approach is particularly beneficial in scenarios where the data for the main task is scarce or noisy, as the auxiliary tasks can provide additional constraints and information to guide the learning process.

One of the key advantages of multi-task optimization in PINNs is the ability to leverage the correlations between different tasks. For example, traffic density and vehicle speed are inherently related, and predicting both can lead to more accurate and coherent solutions. By optimizing for multiple tasks, the model can learn to balance the trade-offs between different objectives, resulting in a more holistic understanding of the traffic system. This is especially important in real-world applications where traffic conditions are dynamic and influenced by a multitude of factors.

Recent studies have demonstrated the effectiveness of multi-task optimization in PINNs for traffic density prediction. A notable example is the work presented in the paper titled "Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction" [150]. In this study, the authors proposed a novel framework that incorporates multiple auxiliary tasks to improve the performance of the main task. The framework is designed to address the challenges of training PINNs, such as the imbalance between different loss terms and the difficulty in converging to an optimal solution.

The proposed framework in the study involves the creation of multiple auxiliary tasks that are solved alongside the main task. These tasks are designed to provide additional constraints and information to the training process. For instance, one auxiliary task could be to predict the velocity of vehicles, while another could be to forecast the traffic flow. By training the network on these tasks, the model is encouraged to learn more robust and generalizable features that are beneficial for the main task.

The results of the study showed that the multi-task optimization framework significantly improved the performance of the main task compared to traditional training methods. The model achieved higher accuracy and better generalization, even when the data for the main task was limited. This is attributed to the fact that the auxiliary tasks provide additional information that helps the model to better understand the underlying physics of the traffic system.

Moreover, the study also highlighted the importance of carefully designing the auxiliary tasks to ensure they are relevant and complementary to the main task. The authors emphasized that the auxiliary tasks should be chosen based on their potential to provide additional insights and constraints that can improve the performance of the main task. This requires a deep understanding of the traffic system and the relationships between different tasks.

Another benefit of multi-task optimization in PINNs is the potential to reduce the computational cost of training. By leveraging the shared representations learned from the auxiliary tasks, the model can converge faster and require fewer training iterations. This is particularly important in real-time applications where computational resources are limited. The study demonstrated that the multi-task optimization framework achieved faster convergence and lower computational costs compared to traditional methods.

In addition to improving the performance of the main task, multi-task optimization in PINNs can also enhance the interpretability of the model. By learning shared representations across different tasks, the model can provide more insights into the underlying dynamics of the traffic system. This can be particularly useful for traffic management and planning, where understanding the relationships between different traffic variables is crucial.

The effectiveness of multi-task optimization in PINNs for traffic density prediction has been further validated through empirical studies. For instance, the study presented in "Training Physics-Informed Neural Networks via Multi-Task Optimization for Traffic Density Prediction" [150] demonstrated that the proposed framework outperformed traditional methods in terms of accuracy and robustness. The results showed that the model was able to handle complex traffic scenarios and provide accurate predictions even in the presence of noise and uncertainty.

In conclusion, the use of multi-task optimization in training PINNs for traffic density prediction offers significant benefits. By leveraging auxiliary tasks, the model can learn more robust and generalizable features, leading to improved performance and accuracy. The framework presented in the study [150] demonstrates the effectiveness of this approach and highlights the potential for further research and development in this area. As the field of traffic prediction continues to evolve, the integration of multi-task optimization in PINNs is likely to play a crucial role in advancing the capabilities of these models.

### 7.14 Frequency-Compensated PINNs for Fluid-Dynamic Design

Frequency-Compensated Physics-Informed Neural Networks (PINNs) represent an innovative approach to improve the generalization capabilities of fluid-dynamic simulations, particularly when predicting flow velocity and pressure over novel designs. This approach leverages Fourier features to address the limitations of traditional PINNs in capturing high-frequency components of the solution, which is critical in fluid mechanics where high-frequency dynamics and complex geometries are prevalent. By integrating Fourier features into the PINN architecture, these models can better generalize to unseen scenarios and provide more accurate predictions, even when the input data is sparse or noisy.

The integration of Fourier features into PINNs is rooted in the concept of frequency compensation, which aims to address the spectral bias phenomenon. Spectral bias refers to the tendency of neural networks to prioritize learning low-frequency components of the solution, often leading to suboptimal performance in capturing high-frequency details [57]. This limitation is particularly problematic in fluid dynamics, where high-frequency components such as turbulence and vortices play a crucial role in determining the overall flow behavior. By incorporating Fourier features, Frequency-Compensated PINNs can effectively learn both low- and high-frequency components of the solution, thereby improving their ability to generalize to new and complex scenarios.

One of the key advantages of using Fourier features in PINNs is their ability to enhance the representational power of the neural network. Fourier features provide a rich set of basis functions that can capture a wide range of spatial and temporal patterns, making them particularly suitable for problems with complex and multi-scale dynamics. This is evident in the work of [57], where the authors demonstrate that the use of Fourier features can significantly improve the performance of PINNs in solving a variety of PDEs. The authors show that the Fourier features allow the neural network to better approximate the true solution, especially in regions where the solution exhibits high-frequency variations.

In the context of fluid-dynamic design, Frequency-Compensated PINNs have shown promising results in predicting flow velocity and pressure over novel designs. For instance, in the study by [57], the authors applied Fourier features to PINNs for solving the Navier-Stokes equations, which are fundamental to fluid dynamics. The results showed that the Frequency-Compensated PINNs were able to accurately capture the flow behavior, even in cases where the geometry of the domain was significantly different from the training data. This is a crucial advantage, as it allows the model to generalize to new and unseen designs without requiring extensive retraining.

Moreover, the use of Fourier features in PINNs has been shown to improve the convergence properties of the training process. Traditional PINNs often struggle with slow convergence, especially when dealing with high-dimensional and complex PDEs. However, the integration of Fourier features can help the network converge faster by providing a more structured representation of the input space. This is supported by the findings in [57], where the authors demonstrate that the use of Fourier features leads to a more stable and efficient training process. The improved convergence properties of Frequency-Compensated PINNs make them particularly suitable for real-time applications, where rapid and accurate predictions are essential.

Another significant benefit of Frequency-Compensated PINNs is their ability to handle noisy and sparse data more effectively. In many fluid dynamics applications, the available data may be limited or contaminated with noise, which can lead to poor generalization and inaccurate predictions. By incorporating Fourier features, these models can better capture the underlying physical laws and maintain their accuracy even in the presence of noise. This is particularly important in scenarios where the data is scarce or where the measurement process introduces significant errors. The robustness of Frequency-Compensated PINNs to noise and sparse data has been demonstrated in several studies, including [57], where the authors show that the model can achieve high accuracy even when the input data is highly noisy.

In addition to improving generalization and handling noise, Frequency-Compensated PINNs also offer enhanced interpretability and physical consistency. The use of Fourier features ensures that the model's predictions are not only accurate but also physically meaningful. This is crucial in fluid dynamics, where the solutions must adhere to the fundamental principles of conservation of mass, momentum, and energy. By incorporating Fourier features, the model can better enforce these physical constraints, leading to more reliable and physically consistent predictions. This is highlighted in [57], where the authors demonstrate that the Frequency-Compensated PINNs are able to capture the essential physics of the problem while maintaining high accuracy.

The application of Frequency-Compensated PINNs in fluid-dynamic design has also been explored in the context of surrogate modeling. Surrogate models are used to approximate the behavior of complex systems, enabling faster and more efficient simulations. In [57], the authors show that Frequency-Compensated PINNs can be used as effective surrogate models for fluid dynamics problems, significantly reducing the computational cost compared to traditional numerical methods. The ability of these models to generalize to new designs and handle complex geometries makes them particularly suitable for applications such as aerodynamic design and turbulence modeling.

Furthermore, the use of Fourier features in PINNs has been shown to improve the model's ability to capture the effects of boundary conditions and initial conditions. In fluid dynamics, the behavior of the solution is highly dependent on the boundary and initial conditions, and any inaccuracies in these can lead to significant errors in the predictions. By incorporating Fourier features, Frequency-Compensated PINNs can more accurately enforce these conditions, leading to more reliable and accurate predictions. This is demonstrated in [57], where the authors show that the model can effectively capture the boundary conditions and initial conditions, even in complex scenarios.

In summary, Frequency-Compensated PINNs offer a powerful approach to improve the generalization capabilities of fluid-dynamic simulations, particularly in predicting flow velocity and pressure over novel designs. By integrating Fourier features, these models can effectively address the spectral bias phenomenon, enhance their representational power, and improve their convergence properties. The ability of Frequency-Compensated PINNs to handle noisy and sparse data, maintain physical consistency, and capture the effects of boundary and initial conditions makes them a promising tool for a wide range of applications in fluid dynamics. As research in this area continues to evolve, Frequency-Compensated PINNs are likely to play an increasingly important role in the development of more accurate and efficient fluid-dynamic simulations.

### 7.15 Temporal Consistency Loss for PINNs

Temporal consistency loss has emerged as a critical component in enhancing the performance of Physics-Informed Neural Networks (PINNs) when solving time-dependent problems, particularly the Navier-Stokes equations. These equations describe the motion of fluid substances and are fundamental in fluid dynamics. The introduction of a temporal consistency loss aims to address the challenges posed by the dynamic and transient nature of fluid flows, ensuring that the solutions adhere to the temporal evolution of the system. This section delves into the concept of temporal consistency loss, its implementation, and its effectiveness in improving the accuracy and robustness of PINNs in solving Navier-Stokes equations.

The Navier-Stokes equations are a set of nonlinear partial differential equations that govern the motion of viscous fluid substances. Solving these equations numerically is a challenging task, especially when dealing with complex flows characterized by turbulence, shock waves, and other dynamic phenomena. Traditional numerical methods such as finite element and finite volume methods are often employed for this purpose, but they can be computationally expensive and may struggle with high-dimensional problems. PINNs, on the other hand, offer a data-driven approach that can potentially overcome some of these limitations by integrating physical laws into the training process of neural networks. However, the challenge lies in ensuring that the neural networks capture the temporal evolution of the solutions accurately.

To address this, the concept of temporal consistency loss has been introduced. This loss function is designed to enforce consistency in the temporal behavior of the solutions generated by the PINNs. By incorporating the temporal derivative of the solution into the loss function, the PINN is trained to ensure that the solution evolves correctly over time. This is particularly important in problems where the solution exhibits rapid changes or where the temporal behavior is crucial for the accuracy of the results. The temporal consistency loss can be formulated as a function of the time derivative of the neural network's output, ensuring that the solution adheres to the physical laws governing the system over time.

The implementation of temporal consistency loss involves modifying the standard PINN loss function, which typically includes terms for the PDE residuals and boundary conditions. In addition to these terms, the temporal consistency loss introduces a new term that penalizes deviations from the expected temporal behavior. This term is computed as the difference between the time derivative of the neural network's output and the time derivative predicted by the Navier-Stokes equations. By minimizing this loss, the PINN is encouraged to produce solutions that not only satisfy the spatial constraints but also exhibit the correct temporal evolution.

Several studies have demonstrated the effectiveness of temporal consistency loss in improving the performance of PINNs. For instance, the work by [190] highlights the importance of this approach in solving the Navier-Stokes equations. The authors introduce a temporal consistency loss that ensures the solutions produced by the PINN are consistent with the temporal evolution of the fluid flow. Their experiments show that this approach leads to significant improvements in the accuracy of the solutions, particularly for problems involving complex flows.

Sensitivity analysis is a crucial step in evaluating the effectiveness of the temporal consistency loss. By varying the parameters of the loss function, researchers can assess how sensitive the solutions are to changes in the temporal constraints. This analysis helps in understanding the trade-offs between the accuracy of the solutions and the computational cost of the training process. The study by [190] conducts a thorough sensitivity analysis, demonstrating that the temporal consistency loss can significantly enhance the accuracy of the solutions while maintaining computational efficiency.

Real-world data validation is another essential aspect of evaluating the performance of the temporal consistency loss. By applying the PINN with temporal consistency loss to real-world fluid dynamics problems, researchers can assess its practical applicability and robustness. The study by [190] validates their approach on a variety of real-world scenarios, including flows with complex geometries and varying boundary conditions. The results show that the PINN with temporal consistency loss outperforms traditional PINN approaches, providing more accurate and reliable solutions.

The effectiveness of temporal consistency loss is further supported by the theoretical analysis presented in [190]. The authors derive a priori error estimates for the PINN with temporal consistency loss, demonstrating that the solutions converge to the true solutions of the Navier-Stokes equations as the number of training samples increases. This theoretical foundation provides confidence in the reliability and accuracy of the approach.

In addition to improving the accuracy of the solutions, the temporal consistency loss also enhances the robustness of the PINN in the face of noisy or incomplete data. By enforcing temporal consistency, the PINN is less likely to produce nonphysical solutions that may arise due to the presence of noise or errors in the training data. The study by [190] shows that the PINN with temporal consistency loss is more robust to such issues, leading to more reliable predictions.

The introduction of temporal consistency loss also has implications for the training process of PINNs. By incorporating this loss function, the training dynamics of the PINN are altered, potentially leading to faster convergence and better optimization. The study by [190] investigates the training dynamics of the PINN with temporal consistency loss, showing that the loss function can improve the training efficiency and stability. This is particularly important for large-scale problems where the training process can be computationally intensive.

Moreover, the temporal consistency loss can be combined with other techniques to further enhance the performance of PINNs. For example, the use of adaptive sampling strategies, as discussed in [34], can be integrated with the temporal consistency loss to ensure that the PINN is trained on the most relevant regions of the solution domain. This combination can lead to more efficient training and better overall performance.

In conclusion, the introduction of temporal consistency loss has proven to be a valuable approach in improving the performance of PINNs when solving the Navier-Stokes equations. By ensuring that the solutions adhere to the temporal evolution of the fluid flow, this loss function enhances the accuracy, robustness, and reliability of the PINN. The theoretical analysis and empirical validation presented in [190] provide strong support for the effectiveness of this approach, highlighting its potential for a wide range of fluid dynamics applications. As research in this area continues to advance, the integration of temporal consistency loss into PINNs is expected to play a crucial role in addressing the challenges of solving complex, time-dependent fluid flow problems.

### 7.16 PINNs as Linear Solvers

Physics-Informed Neural Networks (PINNs) have shown promise in solving partial differential equations (PDEs), but their application as linear solvers for problems like the Poisson equation has been a topic of increasing interest. The Poisson equation is a fundamental second-order elliptic PDE that arises in various physical contexts, such as electrostatics, heat conduction, and fluid mechanics. The potential of PINNs as linear solvers for the Poisson equation has been explored, with a focus on the benefits of transfer learning and the limitations in resolving high-frequency components.

The use of PINNs as linear solvers has been studied in several papers, including "The Old and the New: Can Physics-Informed Deep-Learning Replace Traditional Linear Solvers" [58]. This paper evaluated the potential of PINNs as linear solvers for the Poisson equation, focusing on their accuracy and performance under different network configurations. The authors found that while PINNs can achieve reasonable accuracy for low-frequency components of the solution, they struggle with high-frequency components, requiring significantly more time to converge. This is due to the spectral bias inherent in neural networks, where the networks are more adept at capturing low-frequency patterns than high-frequency ones. The study highlighted the critical role of transfer learning in improving the performance of PINNs, as it allows the networks to leverage knowledge from previously solved problems to accelerate training for new tasks.

Transfer learning has been shown to be particularly effective in reducing the computational cost and improving the accuracy of PINNs when applied to the Poisson equation. In "An Expert's Guide to Training Physics-informed Neural Networks" [56], the authors discussed best practices for training PINNs, including the use of transfer learning to improve convergence and stability. They demonstrated that by pre-training the network on similar problems, the network can generalize better to new scenarios, leading to faster and more accurate solutions. This is especially important for the Poisson equation, where the solution may vary significantly depending on the boundary conditions and the geometry of the domain.

Despite the benefits of transfer learning, PINNs still face challenges in resolving high-frequency components of the solution. In "PINNs and GaLS: A Priori Error Estimates for Shallow Physics Informed Neural Networks Applied to Elliptic Problems" [191], the authors analyzed the convergence properties of PINNs for elliptic PDEs, including the Poisson equation. They found that while PINNs can achieve good accuracy for low-frequency solutions, the accuracy decreases for high-frequency components, which require more complex network architectures and extensive training. This limitation is exacerbated by the fact that the spectral bias of neural networks makes it difficult for them to learn high-frequency patterns, even with a large number of training points.

To address the limitations of PINNs in resolving high-frequency components, several studies have proposed various techniques. For example, "A certified wavelet-based physics-informed neural network for the solution of parameterized partial differential equations" [192] introduced a wavelet-based approach to improve the accuracy of PINNs for high-frequency solutions. By incorporating wavelet expansions into the loss function, the authors were able to achieve better error bounds and more accurate solutions for parameterized PDEs, including the Poisson equation. This approach demonstrates that combining PINNs with wavelet transforms can help overcome the limitations of spectral bias, making PINNs more effective as linear solvers for problems with high-frequency components.

Another approach to improving the performance of PINNs for the Poisson equation involves the use of more sophisticated network architectures. In "Brain-Inspired Physics-Informed Neural Networks: Bare-Minimum Neural Architectures for PDE Solvers" [160], the authors proposed a brain-inspired architecture that mimics the sparsity and modularity of biological neural networks. They demonstrated that by using compact, modular architectures, PINNs can achieve higher accuracy with fewer parameters, making them more efficient as linear solvers. This approach is particularly useful for the Poisson equation, where the solution may involve multiple scales and complex geometries.

In addition to architectural improvements, the choice of loss function and training strategy plays a crucial role in the performance of PINNs as linear solvers. In "Error Estimates and Physics Informed Augmentation of Neural Networks for Thermally Coupled Incompressible Navier Stokes Equations" [61], the authors introduced a physics-informed augmentation to the loss function, which improved the accuracy of the pressure field in thermally coupled Navier-Stokes equations. While this study focused on fluid dynamics, the principles of physics-informed augmentation can be extended to the Poisson equation, where the inclusion of additional physical constraints can enhance the accuracy of the solution.

Despite these advancements, the application of PINNs as linear solvers for the Poisson equation still faces several challenges. One of the main limitations is the computational cost associated with training PINNs, especially for high-dimensional problems. In "Efficient training of physics-informed neural networks via importance sampling" [163], the authors proposed an importance sampling approach to improve the efficiency of PINN training. By sampling collocation points according to the loss function, they were able to achieve faster convergence and better performance, which is particularly important for the Poisson equation, where the solution may involve complex spatial variations.

Overall, the potential of PINNs as linear solvers for the Poisson equation is promising, with transfer learning offering significant benefits in terms of accuracy and efficiency. However, the limitations in resolving high-frequency components and the computational cost of training remain challenges that need to be addressed. Future research should focus on developing more efficient training strategies, improving the ability of PINNs to capture high-frequency patterns, and exploring hybrid approaches that combine PINNs with traditional numerical methods to achieve better performance. By addressing these challenges, PINNs can be further optimized as linear solvers for a wide range of PDEs, including the Poisson equation.

### 7.17 Surrogate Modeling of Turbulent Natural Convection

Surrogate modeling of turbulent natural convection is a critical challenge in fluid mechanics, particularly for applications such as heat transfer in complex geometries or energy systems. Turbulent Rayleigh-Bénard convection, a canonical problem in this domain, involves buoyancy-driven flows between two horizontal plates with a temperature difference, leading to complex, multiscale dynamics. Traditional numerical methods for simulating such flows often suffer from high computational costs, especially when dealing with high Rayleigh numbers and large spatial domains. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering a data-driven yet physically consistent framework for solving these challenging problems [10].  

In the context of surrogate modeling, PINNs have demonstrated their ability to approximate the solution of the Navier-Stokes equations while respecting the underlying physical laws. For turbulent natural convection, the challenge lies in capturing the highly nonlinear and chaotic nature of the flow, along with the associated heat transfer. One of the key approaches to improve the accuracy of PINNs in this context is the use of padding techniques and the relaxation of incompressibility conditions [61]. These strategies address the limitations of standard PINN formulations, which often struggle with the high-frequency dynamics and spatial variability inherent in turbulent flows.  

The padding technique involves extending the computational domain beyond the physical boundaries to improve the accuracy of the solution near the edges. This is particularly useful for problems where the flow field is not well-resolved at the boundaries, leading to inaccuracies in the predicted velocity and temperature fields. By padding the domain, the PINN can better approximate the solution in regions where the physical constraints are less stringent, allowing the network to learn the underlying physics more effectively. This technique has been shown to enhance the predictive capability of PINNs in turbulent Rayleigh-Bénard convection simulations [61].  

Another important aspect of improving the accuracy of PINNs for turbulent natural convection is the relaxation of incompressibility conditions. In traditional computational fluid dynamics (CFD), the incompressibility condition is enforced through the continuity equation, which ensures that the divergence of the velocity field is zero. However, in practice, this condition can be challenging to satisfy accurately, especially in regions with strong gradients or incompressible turbulence. By relaxing the incompressibility constraint in the PINN formulation, the model can better account for small deviations from the incompressibility assumption, leading to more accurate predictions. This approach has been explored in the context of thermally coupled incompressible Navier-Stokes equations, where the inclusion of a pressure stabilization term significantly improved the accuracy of the pressure field [61].  

The combination of the padding technique and the relaxation of incompressibility conditions has been shown to significantly enhance the performance of PINNs in surrogate modeling of turbulent natural convection. For example, in the case of Rayleigh-Bénard convection, the use of padding techniques allows the PINN to better capture the thermal plumes and vortical structures that are characteristic of this type of flow. At the same time, the relaxation of incompressibility conditions ensures that the velocity and pressure fields are consistent with the physical laws governing the problem, even in regions where the flow is highly complex [61].  

Moreover, the application of PINNs to surrogate modeling of turbulent natural convection has been extended to include the use of hybrid data-PINN approaches. These methods combine the strengths of PINNs with traditional numerical data, allowing for the creation of more accurate and robust surrogate models. For instance, in the case of Rayleigh-Bénard convection, a hybrid data-PINN approach can leverage high-fidelity simulation data from finite element or finite volume methods to train the neural network, while still incorporating the physical constraints of the Navier-Stokes equations [10]. This approach has been shown to improve the generalization capabilities of the PINN, particularly in cases where the training data is sparse or noisy.  

Another important consideration in the application of PINNs to turbulent natural convection is the choice of activation functions and the network architecture. The effectiveness of a PINN in capturing the complex dynamics of turbulent flows depends on the ability of the neural network to approximate the underlying PDEs accurately. Studies have shown that certain activation functions, such as the Gaussian or hyperbolic tangent, can improve the performance of PINNs in capturing the high-frequency components of the solution [193]. Additionally, the use of deeper networks with more hidden layers can enhance the capacity of the PINN to learn the intricate features of the flow field.  

In addition to these technical aspects, the training process of PINNs for turbulent natural convection requires careful consideration of the loss function. The loss function in a PINN typically consists of terms that enforce the satisfaction of the PDEs and the boundary conditions. However, in the case of turbulent flows, the presence of multiple scales and the sensitivity to initial conditions can make the training process more challenging. To address this, researchers have explored the use of adaptive sampling strategies, such as R3 sampling or failure-informed adaptive sampling, to improve the efficiency and accuracy of the training process [34]. These techniques dynamically adjust the placement of collocation points during training, ensuring that the PINN focuses on regions where the solution is most uncertain or where the PDE residuals are large.  

Overall, the application of PINNs to the surrogate modeling of turbulent natural convection represents a significant advancement in the field of computational fluid dynamics. By combining the strengths of data-driven learning with the physical constraints of the Navier-Stokes equations, PINNs offer a powerful tool for simulating complex, turbulent flows with high accuracy and efficiency. The use of padding techniques, relaxation of incompressibility conditions, and adaptive sampling strategies has further enhanced the performance of PINNs in this domain, making them a viable alternative to traditional numerical methods for a wide range of applications. As research in this area continues to evolve, the integration of PINNs with other advanced techniques, such as multi-fidelity learning or hybrid models, is expected to further expand their applicability and effectiveness.

### 7.18 Error Estimates and Physics Augmentation in PINNs

Error estimates and physics augmentation in Physics-Informed Neural Networks (PINNs) have become a critical area of research, especially when dealing with complex fluid dynamics problems such as thermally coupled Navier-Stokes equations. These equations govern the behavior of incompressible flows where thermal effects are significant, and their accurate solution is crucial for applications ranging from heat exchanger design to geophysical flows. The integration of physics into neural network training through residual-based loss functions has shown promise, but the reliability of these solutions and the ability to quantify their errors remain areas of active investigation. In this subsection, we discuss the convergence analysis and error estimates for PINNs applied to thermally coupled Navier-Stokes equations, focusing on the role of physics-based regularization and the impact of pressure stabilization terms.

A key aspect of error estimation in PINNs is understanding how the network's predictions align with the true solution of the underlying partial differential equations (PDEs). In the context of thermally coupled Navier-Stokes equations, the governing equations are a system of nonlinear PDEs that describe the conservation of mass, momentum, and energy. PINNs approximate these solutions by minimizing a loss function that incorporates the residual of the PDEs at collocation points, as well as boundary conditions. However, the accuracy of the solution depends on the quality of the training data, the architecture of the neural network, and the balance between different loss terms. Recent studies have shown that PINNs can achieve high accuracy for steady-state and transient flows, but the theoretical guarantees for their convergence and error bounds are still under development.

One of the challenges in error estimation for PINNs is the lack of a direct measure of the solution's global error. Instead, researchers often rely on local error indicators, such as the magnitude of the PDE residual at collocation points. These indicators can provide insights into regions where the network's predictions deviate from the expected behavior, but they do not necessarily reflect the overall accuracy of the solution. To address this, some studies have proposed post-processing techniques that refine the PINN predictions based on known physical constraints or additional data. For example, the work in [61] demonstrates how incorporating a pressure stabilization term into the loss function can improve the accuracy of the pressure field, which is often a challenging component to predict in incompressible flows.

The impact of physics augmentation on error estimates is another important consideration. Physics-informed regularization terms, such as the PDE residual and boundary condition terms, help ensure that the network's predictions satisfy the underlying physical laws. This is particularly critical in thermally coupled flows, where the energy equation must be accurately resolved to capture the interaction between fluid motion and temperature fields. Studies have shown that the inclusion of physics-based terms in the loss function not only improves the accuracy of the solution but also enhances the robustness of the PINN to noisy or sparse data. For instance, the work in [61] highlights the importance of a pressure stabilization term in the form of a pressure Poisson equation, which significantly improves the accuracy of the pressure field compared to the case without augmentation.

In addition to the physical constraints, the choice of neural network architecture and training strategy also plays a role in error estimation. The study in [61] shows that the convergence of the PINN solution can be improved by using a mixed-variable scheme, where the neural network is trained to approximate both the velocity and pressure fields simultaneously. This approach not only enhances the trainability of the PINN but also ensures that the solution satisfies the incompressibility constraint more accurately. Furthermore, the study demonstrates that the accuracy of the PINN solution is closely related to the number of collocation points and the distribution of these points in the domain. A higher density of collocation points in regions with strong gradients or sharp transitions leads to a more accurate solution, but it also increases the computational cost.

The convergence analysis of PINNs for thermally coupled Navier-Stokes equations is another area of active research. While PINNs have shown promising results in solving steady and transient flows, the convergence behavior can be affected by the non-convexity of the loss landscape and the presence of multiple local minima. Recent studies have explored the use of adaptive sampling techniques, such as failure-informed adaptive sampling, to improve the convergence of the PINN training process. These techniques dynamically adjust the placement of collocation points based on the current state of the network, ensuring that the most challenging regions of the domain are adequately sampled. This approach has been shown to improve the convergence rate and reduce the overall training time, as demonstrated in [61].

In summary, the error estimates and physics augmentation in PINNs for thermally coupled Navier-Stokes equations are critical for ensuring the accuracy and reliability of the solutions. The integration of physics-based terms into the loss function not only improves the accuracy of the predictions but also enhances the robustness of the PINN to noisy or sparse data. The impact of pressure stabilization terms, such as the pressure Poisson equation, has been shown to significantly improve the accuracy of the pressure field, which is essential for capturing the complex interactions in thermally coupled flows. Furthermore, the choice of neural network architecture, training strategy, and adaptive sampling techniques plays a crucial role in achieving convergence and minimizing errors. As the field of PINNs continues to evolve, further research is needed to develop more rigorous error estimation methods and to explore the potential of physics-informed regularization in improving the accuracy and efficiency of fluid dynamics simulations.

### 7.19 MRF-PINN for Solving PDEs

The MRF-PINN (Multi-Receptive-Field Physics-Informed Neural Network) model is a groundbreaking approach that addresses the challenges of solving different types of partial differential equations (PDEs) without the need for manual tuning of critical hyperparameters [48]. This innovative model leverages the advantages of convolutional neural networks (CNNs), such as parameter sharing, spatial feature extraction, and low inference cost, while addressing the limitations that have hindered their application in physics-informed neural networks (PINNs) [48]. The MRF-PINN is designed to solve various PDEs on different mesh resolutions without manual tuning, thereby offering a more flexible and generalizable framework for solving PDEs.

One of the key innovations of the MRF-PINN model is the use of the dimensional balance method to estimate the loss weights when solving Navier-Stokes equations [48]. This method allows for a more balanced and effective training process by ensuring that the loss terms are appropriately weighted, which is crucial for achieving accurate and stable solutions. By addressing the imbalance between the PDE loss and boundary loss, the dimensional balance method contributes to the overall success of the MRF-PINN model in solving complex PDEs.

Another significant contribution of the MRF-PINN model is the implementation of high-order finite differences to improve the accuracy of the solutions [48]. The use of high-order finite differences allows the model to capture the finer details of the solution, especially in regions where the solution exhibits rapid variations or complex behavior. This is particularly important for problems with high-frequency components or multi-scale dynamics, where traditional low-order methods may fail to resolve the solution accurately. The high-order finite difference approach also enables the MRF-PINN to handle larger channel numbers and higher mesh resolutions, leading to more precise and reliable predictions.

The MRF-PINN model has been extensively tested on a variety of PDEs, including linear and nonlinear equations, to demonstrate its generality and superiority [48]. The results show that the MRF-PINN is capable of adapting to completely different equation types and mesh resolutions without any hyperparameter tuning, making it a versatile and robust solution for a wide range of PDE problems. This adaptability is a significant advantage over traditional numerical methods, which often require extensive parameter tuning and customization for each specific problem.

The effectiveness of the MRF-PINN model is further enhanced by the use of a multi-receptive-field architecture, which allows the model to capture spatial features at multiple scales [48]. This architecture is inspired by the way biological neural networks process information, where different regions of the network are responsible for extracting features at different scales. By incorporating this multi-scale feature extraction capability, the MRF-PINN can effectively handle problems with complex spatial structures and varying resolutions.

In addition to the dimensional balance method and high-order finite differences, the MRF-PINN model also employs a novel padding technique to handle the virtual nodes near the boundaries [48]. This technique is particularly useful for problems with complex geometries or irregular domains, where the traditional boundary conditions may be difficult to enforce. The padding technique ensures that the boundary conditions are accurately represented, leading to more accurate and physically consistent solutions.

The MRF-PINN model has also been applied to a wide range of practical problems, demonstrating its applicability and effectiveness in real-world scenarios [48]. The model has been used to solve problems in fluid dynamics, heat transfer, and other areas of scientific computing, showing its potential as a general-purpose solver for PDEs. The results of these applications highlight the model's ability to handle complex and high-dimensional problems with high accuracy and efficiency.

Moreover, the MRF-PINN model's performance is further enhanced by its ability to integrate with traditional numerical methods and data-driven approaches. This hybrid approach allows for the combination of the strengths of different methods, leading to more accurate and reliable solutions [48]. For example, the MRF-PINN can be combined with finite element or finite volume methods to improve the accuracy of the solutions, particularly in regions with sharp gradients or discontinuities.

In conclusion, the MRF-PINN model represents a significant advancement in the field of physics-informed neural networks for solving PDEs. Its ability to solve different types of PDEs without manual tuning, combined with the use of the dimensional balance method and high-order finite differences, makes it a powerful and versatile tool for a wide range of applications. The MRF-PINN's effectiveness has been demonstrated through extensive numerical experiments, and its potential for real-world applications is promising. As research in this area continues to evolve, the MRF-PINN model is likely to play a crucial role in the development of more accurate and efficient methods for solving PDEs in fluid mechanics and other scientific domains.

### 7.20 Improved Surrogate Modeling with PINNs

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for surrogate modeling in fluid mechanics, offering a unique combination of data-driven learning and physics-based constraints. Surrogate modeling is essential in scenarios where direct numerical simulations are computationally prohibitive, and high-fidelity models are impractical. PINNs provide a robust and efficient alternative by integrating the governing equations of fluid dynamics into the training process, enabling them to generalize well even in the presence of noisy or sparse data. This subsection explores the advantages of using PINNs for surrogate modeling, including their improved robustness to noise, reduced convergence effort through transfer optimization, and enhanced predictive performance.

One of the primary benefits of PINNs for surrogate modeling is their robustness to noise. Traditional data-driven models, such as purely supervised neural networks, often struggle with noisy data, leading to inaccurate predictions and overfitting. In contrast, PINNs incorporate the physical laws of the system into the loss function, which acts as a regularization term that helps the network learn the underlying physics rather than relying solely on the data. This is particularly advantageous in fluid mechanics, where experimental measurements are often contaminated with noise. For instance, in the context of blood flow imaging, PINNs have been shown to recover accurate velocity fields from noisy measurements, demonstrating their ability to maintain performance even when the data quality is suboptimal [24]. Similarly, in the study on super-resolution of flow-field data, PINNs were able to reconstruct high-resolution velocity fields from low-resolution and noisy measurements, outperforming traditional data-driven models in terms of accuracy and physical consistency [11].

Another significant advantage of PINNs is their ability to reduce the convergence effort through transfer optimization. Surrogate modeling often requires training models for a variety of scenarios, which can be computationally expensive. However, transfer learning, where a pre-trained model is fine-tuned on a new task, has shown promise in accelerating the training process. In the context of PINNs, transfer optimization leverages the knowledge gained from previous training tasks to improve the convergence of new models. For example, in the case of predicting blood flow in the ascending aorta, a pre-trained PINN model was fine-tuned to estimate unknown parameters and velocity fields, significantly reducing the training time and improving the accuracy of the results [194]. Similarly, in the study on parametric Navier-Stokes solutions, transfer optimization was used to improve the convergence of new scenarios, achieving a 3x improvement in speed to convergence and an order of magnitude improvement in predictive performance [9]. These findings highlight the potential of transfer learning in enhancing the efficiency of PINN-based surrogate modeling.

In addition to robustness and efficiency, PINNs offer enhanced predictive performance compared to traditional data-driven models. This is primarily due to the incorporation of physical constraints, which ensures that the solutions are consistent with the underlying physics. For instance, in the case of simulating turbulent flows, PINNs have been shown to accurately predict the velocity and pressure fields, even in complex geometries and high Reynolds number regimes [10]. The use of physics-informed loss functions allows PINNs to maintain the conservation laws of mass and momentum, which is crucial for accurate fluid dynamics simulations. Furthermore, the integration of domain knowledge into the neural network architecture has been shown to improve the generalization capabilities of PINNs, making them suitable for a wide range of applications, including biomedical flows and industrial fluid dynamics [22].

Moreover, the ability of PINNs to handle complex, high-dimensional problems makes them a compelling choice for surrogate modeling in fluid mechanics. Traditional numerical methods often struggle with high-dimensional problems due to the curse of dimensionality, but PINNs can efficiently navigate the solution space by leveraging the power of deep learning. For example, in the case of 3D flow-thermal problems with sparse data, PINNs were able to provide accurate solutions using a hybrid data-PINNs approach, demonstrating their effectiveness in handling complex, high-dimensional PDEs [29]. The ability of PINNs to work with sparse data is particularly valuable in real-world applications where obtaining high-quality data is challenging.

In conclusion, PINNs offer a range of advantages for surrogate modeling in fluid mechanics, including improved robustness to noise, reduced convergence effort through transfer optimization, and enhanced predictive performance. These benefits make PINNs a valuable tool for a wide range of applications, from biomedical flows to industrial fluid dynamics. As research in this area continues to advance, the potential of PINNs for surrogate modeling is expected to grow, paving the way for more efficient and accurate solutions to complex fluid dynamics problems.

### 7.21 Provably Correct PINNs

The concept of provably correct Physics-Informed Neural Networks (PINNs) has gained significant attention in recent years as researchers seek to address the limitations of traditional PINN approaches in terms of reliability and robustness. While PINNs have shown remarkable success in solving partial differential equations (PDEs), their predictions are often treated as black-box solutions, making it difficult to guarantee their correctness in practical applications. This is where the idea of provably correct PINNs comes into play, aiming to provide rigorous guarantees on the accuracy of the solutions by incorporating tolerance-based correctness conditions and post-training frameworks to bound residual errors.

A key paper that explores this concept is "Provably Correct Physics-Informed Neural Networks" [195], which introduces a framework to ensure that PINNs produce solutions that are not only accurate but also verifiably correct. The authors emphasize that previous works on PINNs have primarily focused on point-wise comparisons between the neural network's output and the solution obtained by traditional solvers on a set of inputs. However, in real-world applications, it is not sufficient to evaluate performance on a finite set of points because the performance could be significantly worse on a different set. To address this, the authors establish tolerance-based correctness conditions for PINNs over the entire input domain. They propose a post-training framework, named $\partial$-CROWN, which can bound the residual errors of PINNs, ensuring that the solutions remain within acceptable error margins across the entire domain.

The $\partial$-CROWN framework is particularly effective because it provides a general, efficient, and scalable way to bound the residual errors of PINNs. This framework is not only applicable to PINNs but also to other deep learning models that incorporate physical constraints. By applying $\partial$-CROWN to two classically studied PDEs—Burgers' and Schrödinger's equations—and two more challenging ones with real-world applications—Allan-Cahn and Diffusion-Sorption equations—the authors demonstrate its effectiveness in obtaining tight certificates. This approach ensures that the PINN solutions are not only accurate but also certified to be within a specified tolerance, making them suitable for safety-critical applications.

Another important aspect of provably correct PINNs is the need to address the issue of residual error bounds. Traditional PINNs often rely on minimizing a loss function that includes the PDE residual and boundary conditions. However, without a rigorous theoretical foundation, it is difficult to guarantee that the minimized loss translates to a solution that satisfies the PDE with a high degree of accuracy. The paper "On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs" [175] provides a theoretical analysis of the convergence of PINNs for linear second-order elliptic and parabolic PDEs. The authors show that the sequence of minimizers generated by PINNs strongly converges to the true solution of the PDE in $C^0$, and if the initial and boundary conditions are satisfied, the convergence becomes $H^1$. This theoretical foundation is essential for understanding the behavior of PINNs and developing provably correct models.

Moreover, the paper "Error Estimates and Physics Informed Augmentation of Neural Networks for Thermally Coupled Incompressible Navier Stokes Equations" [61] presents a detailed analysis of error estimation and convergence properties of PINNs for multi-physics problems. The authors show that a small training error implies a small generalization error, which is crucial for ensuring the reliability of PINN solutions. They also introduce a pressure stabilization term in the form of a pressure Poisson equation, which significantly improves the accuracy of the pressure field in thermally coupled Navier-Stokes simulations. This approach not only enhances the performance of PINNs but also contributes to the development of provably correct models by providing additional constraints that improve the quality of the solutions.

The concept of provably correct PINNs is also closely related to the idea of verifying the correctness of neural network predictions through rigorous mathematical analysis. This is particularly important in applications where the accuracy of the solution is critical, such as in aerospace engineering, climate modeling, and biomedical simulations. The paper "Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next" [146] provides a comprehensive review of the current state of PINNs and highlights the need for theoretical guarantees. The authors point out that while PINNs have shown great potential in solving a wide range of PDEs, there are still many theoretical issues that remain unresolved, such as the convergence properties of PINNs for non-linear and high-dimensional problems.

In addition to theoretical guarantees, the development of provably correct PINNs requires the integration of advanced optimization techniques and post-training verification methods. The paper "Improved Training of Physics-Informed Neural Networks with Model Ensembles" [138] proposes using ensemble learning to stabilize PINN training and improve the reliability of the solutions. By training an ensemble of PINNs, the authors show that the agreement among the models can be used as a criterion for gradually expanding the solution interval, leading to more accurate and robust predictions. This approach can be combined with post-training verification techniques to ensure that the solutions meet the required accuracy and correctness criteria.

Overall, the concept of provably correct PINNs represents a significant step forward in the development of reliable and trustworthy neural network models for solving PDEs. By incorporating tolerance-based correctness conditions and post-training frameworks to bound residual errors, researchers can ensure that the solutions produced by PINNs are not only accurate but also verifiably correct. This is essential for the widespread adoption of PINNs in real-world applications where the accuracy and reliability of the solutions are critical. As the field continues to evolve, further research into the theoretical foundations and practical implementation of provably correct PINNs will be necessary to fully realize their potential.

### 7.22 PINNs for Flow Prediction in Stirred Tanks

Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for flow prediction in stirred tank reactors, demonstrating their potential as reduced order models (ROMs). Stirred tank reactors (STRs) are commonly used in chemical and pharmaceutical industries for mixing and reaction processes, and accurate prediction of the flow field within these systems is crucial for optimizing performance. Traditional numerical methods, such as finite element and finite volume methods, often require significant computational resources and time, especially for complex geometries and transient flow conditions. PINNs offer a data-driven alternative that can efficiently approximate the solution of the Navier-Stokes equations while incorporating physical constraints, making them an attractive option for flow prediction in stirred tanks.

In the context of stirred tank reactors, PINNs have been applied to solve the two-dimensional stationary Navier-Stokes equations within geometrically complex domains [196]. This approach leverages the ability of PINNs to embed the governing equations directly into the loss function, ensuring that the neural network learns a solution that satisfies the underlying physical laws. By doing so, PINNs can capture the intricate flow patterns and turbulence characteristics that are essential for accurate predictions. The integration of domain decomposition techniques further enhances the effectiveness of PINNs in handling the complexity of stirred tank geometries. Domain decomposition involves dividing the computational domain into smaller subdomains, each of which can be solved independently. This not only reduces the computational burden but also allows for the application of different sets of equations in different regions based on the local flow characteristics. The Extended Physics-Informed Neural Network (XPINN) approach has been adapted to solve different sets of equations in distinct subdomains, enabling more accurate and efficient flow predictions [196].

One of the key advantages of using PINNs for flow prediction in stirred tanks is their ability to incorporate parametric inputs, which allows for the interpolation of solutions across a wide range of operating conditions. By treating parameters such as the stirring rate as inputs, PINNs can learn a mapping from these parameters to the flow field, enabling rapid predictions without the need for retraining from scratch. This parametric approach is particularly useful in industrial applications where the operating conditions may vary significantly. The use of parametric inputs also enhances the generalizability of PINNs, allowing them to adapt to new scenarios with minimal additional training. For instance, by incorporating the stirring rate as a parametric input, researchers have developed fast-to-evaluate models of the flow that can interpolate across a wide range of Reynolds numbers, providing accurate predictions for different flow regimes [196].

The application of PINNs in stirred tank flow prediction also highlights the importance of domain decomposition in improving the accuracy and efficiency of the model. Domain decomposition allows for the treatment of different regions of the tank with tailored strategies, ensuring that the solution is accurate across the entire domain. This approach is particularly beneficial in regions where the flow characteristics differ significantly, such as near the impeller and in the wake region. By decomposing the domain into overlapping or non-overlapping subdomains, PINNs can focus on the critical regions where the flow is more complex, leading to improved predictions. The use of domain decomposition in conjunction with PINNs has been shown to reduce the l1 prediction errors for both pressure and velocity to less than 1%, demonstrating the effectiveness of this approach [196].

Another significant aspect of using PINNs for flow prediction in stirred tanks is their ability to handle sparse and noisy data. Traditional numerical methods often require high-quality, dense data to achieve accurate results, which can be challenging to obtain in practical applications. PINNs, on the other hand, can learn from limited data by incorporating physical constraints into the training process. This makes them particularly suitable for situations where data collection is constrained by experimental limitations or financial considerations. The ability of PINNs to learn from sparse data is further enhanced by the use of adaptive sampling techniques, which dynamically adjust the placement of collocation points to improve the accuracy of the solution. These techniques ensure that the model focuses on regions where the solution is most uncertain, leading to more efficient training and better generalization [196].

The effectiveness of PINNs in predicting flow in stirred tanks has been validated through a series of numerical experiments and empirical studies. For example, the application of PINNs to the Navier-Stokes equations within a geometrically complex domain has demonstrated their ability to accurately capture the flow field, even in the presence of complex geometries and transient conditions [196]. These studies have shown that PINNs can achieve high accuracy with relatively few training data points, making them a cost-effective alternative to traditional numerical methods. Additionally, the use of PINNs as reduced order models has been shown to significantly reduce the computational time required for flow simulations, enabling real-time predictions and faster design optimization.

In conclusion, the application of PINNs for flow prediction in stirred tanks highlights their potential as a powerful tool for simulating complex fluid dynamics problems. By leveraging the integration of physical laws with data-driven learning, PINNs offer a flexible and efficient approach to solving the Navier-Stokes equations. The use of domain decomposition and parametric inputs further enhances the accuracy and generalizability of PINNs, making them well-suited for practical applications in the chemical and pharmaceutical industries. As research in this area continues to advance, PINNs are expected to play an increasingly important role in the development of efficient and accurate flow prediction models for stirred tank reactors.

### 7.23 PINNs with Skip Connections for Gas-Lifted Oil Wells

The application of Physics-Informed Neural Networks (PINNs) in modeling gas-lifted oil wells has demonstrated significant potential in enhancing the accuracy and efficiency of dynamic system simulations. One notable advancement in this domain is the integration of skip connections into PINN architectures. Skip connections, a concept originally introduced in deep learning to address the vanishing gradient problem, have been adapted to improve the gradient flow during training, leading to better convergence and performance in complex fluid dynamics scenarios, particularly in gas-lifted oil well systems [197]. This subsection delves into the use of skip connections in PINNs for gas-lifted oil wells, emphasizing their impact on gradient flow and their application in Model Predictive Control (MPC).

Gas-lifted oil wells involve the injection of gas into the wellbore to reduce the density of the fluid column, thereby enhancing the flow of crude oil to the surface. The dynamics of these systems are governed by complex, nonlinear partial differential equations (PDEs) that describe the interactions between gas and liquid phases, as well as the effects of pressure and temperature variations. Traditional numerical methods for solving these PDEs often suffer from computational inefficiencies and difficulties in capturing the transient behavior of the system. PINNs, on the other hand, offer a data-driven approach that incorporates physical constraints into the neural network's loss function, enabling the network to learn the underlying dynamics while respecting the governing equations.

The introduction of skip connections in PINNs for gas-lifted oil wells addresses a critical challenge in training deep neural networks: the degradation of gradient flow during backpropagation. In standard PINN architectures, the network's depth can lead to vanishing gradients, making it difficult for the network to learn long-range dependencies and complex patterns. Skip connections mitigate this issue by providing a direct pathway for gradients to flow through the network, bypassing intermediate layers. This not only accelerates the training process but also improves the network's ability to capture the intricate dynamics of gas-lifted oil wells.

The benefits of skip connections in PINNs for gas-lifted oil wells are evident in the improved gradient flow during training. By allowing gradients to propagate more effectively through the network, skip connections enable the model to converge faster and achieve higher accuracy in its predictions. This is particularly important in applications where the system's behavior is highly nonlinear and sensitive to initial conditions, such as in gas-lifted oil wells. The enhanced gradient flow also contributes to the stability of the training process, reducing the likelihood of the network getting stuck in local optima or failing to converge altogether.

In addition to improving gradient flow, skip connections have been successfully applied in the context of Model Predictive Control (MPC) for gas-lifted oil wells. MPC is a control strategy that uses a predictive model to optimize the system's performance over a finite horizon. In this context, the PINN with skip connections serves as a dynamic model that predicts the future state of the system based on current and past inputs. The incorporation of skip connections into the PINN architecture enhances the model's ability to capture the temporal dependencies and nonlinear interactions within the system, leading to more accurate predictions and improved control performance.

The application of skip connections in PINNs for gas-lifted oil wells has been validated through experimental studies. For instance, in a study focused on modeling and control of gas-lifted oil wells, the authors introduced skip connections into the PINC (Physics-Informed Neural Control) framework and demonstrated significant improvements in both the accuracy of the model and the effectiveness of the control strategy [197]. The results showed that the improved PINC model achieved a 67% reduction in validation prediction error compared to the original PINC model, highlighting the practical benefits of incorporating skip connections into PINN architectures.

Furthermore, the use of skip connections in PINNs for gas-lifted oil wells has been shown to enhance the model's robustness to noisy and incomplete data. In real-world applications, the availability of high-quality data is often limited, and the presence of noise can significantly impact the performance of traditional models. By improving the gradient flow and stability of the training process, skip connections enable the PINN to learn more reliably from sparse and noisy data, making it a valuable tool for practical applications in the oil and gas industry.

Another advantage of using skip connections in PINNs for gas-lifted oil wells is the ability to handle complex, high-dimensional problems. The gas-lifted oil well system involves multiple interacting variables, such as pressure, temperature, and flow rates, which can be challenging to model using conventional methods. The incorporation of skip connections into the PINN architecture allows the network to effectively capture the relationships between these variables, leading to more accurate and reliable predictions.

In summary, the integration of skip connections into PINN architectures has significantly advanced the modeling and control of gas-lifted oil wells. By improving gradient flow and stability during training, skip connections enhance the accuracy and efficiency of the models, making them more suitable for real-world applications. The application of these models in conjunction with Model Predictive Control further demonstrates their potential to optimize the performance of gas-lifted oil well systems. As the field of physics-informed neural networks continues to evolve, the use of skip connections and other architectural innovations will play a crucial role in addressing the challenges of complex, dynamic systems in fluid mechanics and beyond.

### 7.24 Limitations of PINNs in Dynamical PDEs

Physics-Informed Neural Networks (PINNs) have shown great promise in solving partial differential equations (PDEs) by integrating physical laws into the training process. However, their effectiveness in tackling dynamical PDEs, especially those with strong advection and long time durations, remains a critical challenge. While PINNs have demonstrated success in solving steady-state and simple transient problems, their performance in complex, long-term simulations of fluid dynamics is often limited by several fundamental issues. These limitations necessitate the development of improved optimization strategies and more robust architectures to address the unique challenges posed by dynamical PDEs.

One of the primary challenges in applying PINNs to dynamical PDEs is their inability to effectively handle strong advection. Advection-dominated flows, such as those encountered in high-speed aerodynamics or rapidly evolving turbulence, require the model to capture and propagate information over long distances and time intervals. PINNs often struggle with this due to the inherent limitations of their architecture and training dynamics. For instance, in the case of advection-dominated flows, the network may fail to accurately track the movement of fluid structures, leading to significant errors in the predicted velocity and pressure fields [91]. This is particularly problematic when the flow dynamics involve sharp gradients or discontinuities, as the neural network may not be able to resolve these features effectively without additional regularization or specialized training strategies.

Another significant limitation of PINNs in dynamical PDEs is their performance over long time durations. PINNs are typically trained using a fixed set of collocation points and boundary conditions, which may not adequately capture the evolving nature of the solution over extended periods. As a result, the model may lose accuracy as the simulation progresses, especially in cases where the solution exhibits chaotic or highly nonlinear behavior. This issue is exacerbated in problems involving transient phenomena, where the solution evolves rapidly and the model must adapt to changing conditions. For example, in the context of turbulent flows, PINNs often fail to maintain stability over long time horizons, leading to divergence or inaccurate predictions [11]. This suggests that current PINN formulations may not be well-suited for simulating such complex, long-term dynamics without additional constraints or modifications.

The training process itself presents another major limitation for PINNs in solving dynamical PDEs. The non-convex nature of the loss landscape and the difficulty in balancing the different terms in the loss function can lead to convergence issues. In particular, the integration of physical constraints through the residual terms may result in an unbalanced optimization process, where some parts of the PDE are prioritized over others. This can lead to suboptimal solutions, especially in cases where the dynamics are highly sensitive to small changes in the initial conditions or boundary data. Furthermore, the use of auto-differentiation to compute derivatives of the neural network output with respect to the input variables can introduce numerical instabilities, particularly when dealing with high-frequency components of the solution [90]. These challenges highlight the need for more sophisticated optimization techniques, such as adaptive sampling, multi-fidelity learning, and physics-informed activation functions, to improve the training efficiency and accuracy of PINNs.

In addition to these issues, PINNs often struggle with capturing the high-frequency components of the solution in dynamical PDEs. This is known as the spectral bias problem, where the network tends to prioritize learning low-frequency features of the solution while neglecting high-frequency details. This is particularly problematic in problems involving complex, multiscale dynamics, where high-frequency components play a crucial role in determining the overall behavior of the system. For example, in the case of turbulent flows, the network may fail to capture the small-scale eddies and vortices that are essential for accurate predictions [170]. This limitation suggests that current PINN architectures may not be well-suited for simulating such complex systems without additional modifications or the use of more advanced network designs.

The limitations of PINNs in dynamical PDEs also extend to their ability to generalize to unseen scenarios. While PINNs can be trained to approximate the solution of a given PDE, they often struggle to generalize to new initial or boundary conditions, especially when the problem involves significant changes in the physical parameters or geometry. This is a critical issue in real-world applications, where the model must be able to handle a wide range of scenarios without requiring retraining from scratch. For instance, in the case of unsteady flows past moving bodies, the network may fail to accurately predict the solution when the motion of the body deviates from the training data [198]. This highlights the need for more flexible and adaptive architectures that can generalize effectively to new configurations.

Finally, the sensitivity of PINNs to noisy or incomplete data is another critical limitation in dynamical PDEs. While PINNs are designed to incorporate physical constraints into the training process, they may still be vulnerable to the effects of noise and missing data, especially in high-dimensional or complex problems. This can lead to nonphysical solutions or inaccurate predictions, particularly in cases where the data is sparse or corrupted. For example, in experimental fluid mechanics, the network may fail to accurately reconstruct the flow field from low-resolution or noisy measurements [11]. This suggests that additional strategies, such as uncertainty quantification and robust training techniques, are needed to improve the reliability and accuracy of PINNs in such scenarios.

In conclusion, while PINNs have shown great promise in solving a wide range of PDEs, their performance in dynamical PDEs, particularly those involving strong advection and long time durations, is still limited by several critical challenges. These include difficulties in capturing high-frequency components, handling long-term stability, and adapting to new scenarios. Addressing these limitations requires the development of improved optimization strategies, more robust architectures, and advanced training techniques that can better handle the complexities of dynamical PDEs. As research in this area continues to evolve, the integration of these improvements will be essential to fully realize the potential of PINNs in fluid mechanics and other complex scientific domains.

### 7.25 RANS-PINN for Turbulent Flow Prediction

The RANS-PINN framework represents a significant advancement in the application of Physics-Informed Neural Networks (PINNs) for predicting turbulent flows. By leveraging the Reynolds-averaged Navier-Stokes (RANS) formulation, RANS-PINN addresses the complexities of turbulent flow simulations, which are typically challenging due to the high computational costs and the need for accurate turbulence models. This framework not only integrates the fundamental physics of fluid dynamics but also introduces a novel training approach that enhances the accuracy and efficiency of the predictions [22].

Turbulent flows are characterized by chaotic and unpredictable fluctuations in velocity and pressure, making their accurate prediction a formidable task. Traditional numerical methods, such as finite volume and finite element methods, require substantial computational resources and often rely on empirical turbulence models, which may not always be reliable. In contrast, RANS-PINN offers a data-driven approach that leverages the power of deep learning while ensuring that the underlying physical laws are respected. This is achieved by embedding the RANS equations into the loss function of the neural network, thereby enforcing the conservation of mass and momentum during training.

One of the key innovations of RANS-PINN is its novel training approach, which addresses the challenges of balancing the various components of the loss function. In conventional PINN training, the loss function typically includes terms for the PDE residuals, boundary conditions, and data fitting. However, in the context of RANS-PINN, the complexity of the turbulence modeling introduces additional layers of intricacy. The RANS-PINN framework employs a 2-equation eddy viscosity model based on the RANS formulation, which is integrated into the training process. This model accounts for the additional complexity introduced by turbulence, ensuring that the predictions are not only accurate but also physically meaningful.

The training process of RANS-PINN involves a careful balance between the different components of the loss function. This is achieved through a novel training approach that ensures effective initialization and balance among the various components. The framework is designed to handle the challenges of high Reynolds number turbulent flow regimes, where the flow dynamics are highly nonlinear and complex. By leveraging the RANS formulation, RANS-PINN is capable of capturing the essential features of turbulent flows while maintaining computational efficiency.

Empirical validations of RANS-PINN have demonstrated its effectiveness in predicting turbulent flow fields, including velocity and pressure distributions. The framework has been tested on a variety of turbulent flow cases, such as zero-pressure-gradient boundary layers, adverse-pressure-gradient boundary layers, and turbulent flows over a NACA4412 airfoil and periodic hills. The results show that RANS-PINN can achieve very good accuracy on simulation results, even for the Reynolds-stress components, which are notoriously difficult to predict. This level of accuracy is particularly impressive given the inherent challenges of turbulence modeling.

Another notable aspect of RANS-PINN is its ability to handle the challenges of sparse data. In many practical scenarios, the availability of high-quality, high-resolution data is limited, making it difficult to train traditional numerical models. RANS-PINN addresses this issue by incorporating the RANS equations into the loss function, which allows the model to learn the underlying physics without relying heavily on data. This approach not only improves the generalization capabilities of the model but also enhances its robustness to noisy or incomplete data.

The RANS-PINN framework also demonstrates the potential of PINNs in solving inverse problems in fluid dynamics. By training the model on sparse data, RANS-PINN can infer the underlying flow characteristics, such as the velocity and pressure fields, from limited measurements. This capability is particularly valuable in applications such as biomedical flows, where obtaining high-resolution data is often challenging. The ability to predict turbulent flow fields from sparse data opens up new possibilities for real-time monitoring and control of fluid systems.

Furthermore, RANS-PINN's integration of the RANS formulation and the novel training approach makes it a powerful tool for parametric studies. The framework can be used to simulate a wide range of turbulent flow scenarios by adjusting the parameters of the RANS equations. This flexibility is crucial for understanding the behavior of turbulent flows under different operating conditions and for optimizing fluid system designs. The results of such studies can provide valuable insights into the complex dynamics of turbulence, which are often difficult to capture using traditional numerical methods.

In addition to its accuracy and efficiency, RANS-PINN also highlights the potential of PINNs in reducing the computational costs associated with turbulent flow simulations. Traditional numerical methods often require extensive computational resources to achieve high accuracy, particularly for high Reynolds number flows. By leveraging the power of deep learning, RANS-PINN can achieve similar levels of accuracy with significantly lower computational overhead. This makes it a viable alternative for applications where computational efficiency is a critical factor.

The effectiveness of RANS-PINN is further supported by its ability to handle complex geometries and boundary conditions. The framework's incorporation of the RANS equations ensures that the model respects the physical constraints of the problem, even in the presence of complex geometries. This is particularly important in applications such as aerospace engineering and automotive design, where the accuracy of turbulent flow predictions can have a significant impact on the performance and efficiency of the system.

Overall, the RANS-PINN framework represents a promising approach to predicting turbulent flows using Physics-Informed Neural Networks. By integrating the RANS formulation and introducing a novel training approach, RANS-PINN addresses the challenges of turbulent flow simulations and offers a powerful tool for both forward and inverse problems in fluid dynamics. The empirical validations and theoretical analyses conducted on RANS-PINN demonstrate its potential to revolutionize the way we approach turbulent flow predictions, making it an essential component of the evolving landscape of computational fluid dynamics.

### 7.26 PINNs for Securing Water Distribution Systems

---
The application of Physics-Informed Neural Networks (PINNs) in securing water distribution systems represents a novel and promising approach to address critical challenges in urban infrastructure. Water distribution systems are essential for providing safe and reliable water to communities, but they are increasingly vulnerable to cyber-physical attacks, which can disrupt the supply, compromise water quality, and lead to severe economic and health consequences. PINNs, which integrate physical laws with data-driven learning, offer a unique opportunity to detect anomalies, predict system behaviors, and enhance the resilience of water distribution systems against such threats [24].

One of the key applications of PINNs in water distribution systems is their use in detecting and mitigating cyber-physical attacks. Traditional methods for monitoring and securing water distribution systems often rely on heuristic rules or data-driven models that may not account for the complex dynamics of fluid flow and pressure distribution. PINNs, on the other hand, can be trained to solve the governing equations of fluid dynamics, such as the Navier-Stokes equations, while incorporating real-time sensor data. This allows them to accurately model the behavior of the system under normal conditions and detect deviations that may indicate an attack [24]. For instance, PINNs can be used to monitor pressure and flow rate anomalies in real-time, enabling early detection of tampering or unauthorized access to the system.

Another important application of PINNs in water distribution systems is their ability to predict the impact of potential attacks and optimize mitigation strategies. By integrating the physical laws of fluid dynamics with machine learning, PINNs can simulate various attack scenarios, such as pipe bursts, valve malfunctions, or contamination events. These simulations can help engineers and operators understand the potential consequences of an attack and develop robust response plans. For example, a PINN-based model can be trained to predict the spread of contaminants in a water distribution network, allowing for the implementation of targeted remediation strategies to minimize the impact on public health [24].

Despite their potential, the application of PINNs in securing water distribution systems is not without challenges. One of the primary challenges is the need for high-quality and diverse training data. Water distribution systems are complex and dynamic, and the training data must capture a wide range of operating conditions and potential attack scenarios. However, obtaining such data can be difficult, as it often requires extensive monitoring and data collection over extended periods. Moreover, the data may be incomplete or noisy, which can affect the accuracy and reliability of the PINN model [24].

Another challenge is the computational complexity of training PINNs for large-scale water distribution systems. While PINNs offer a flexible and data-efficient approach to modeling fluid dynamics, they can be computationally intensive, especially when dealing with high-dimensional and time-dependent problems. This is particularly true for systems with complex geometries, such as those with multiple branches, valves, and storage tanks. To address this challenge, researchers are exploring techniques such as domain decomposition, parallel computing, and adaptive sampling to improve the efficiency and scalability of PINN training [73].

Furthermore, the integration of PINNs into existing water distribution systems requires careful consideration of the interface between the neural network and the physical system. The PINN model must be able to communicate with the sensors and actuators in the system, which can be challenging due to differences in data formats, sampling rates, and communication protocols. Additionally, the model must be robust to sensor noise and other uncertainties that are inherent in real-world systems. To overcome these challenges, researchers are developing hybrid models that combine PINNs with traditional numerical methods or other machine learning techniques, such as domain decomposition methods and multi-fidelity learning [73].

The applicability of PINNs in securing water distribution systems also depends on the availability of computational resources and the expertise of the operators. While PINNs can provide valuable insights and predictions, their deployment requires a certain level of technical proficiency, which may not be readily available in all water utilities. This highlights the need for user-friendly tools and interfaces that can facilitate the adoption of PINNs by non-expert users. Additionally, the development of standardized benchmarks and evaluation metrics will be crucial for assessing the performance of PINNs in real-world scenarios [73].

In addition to these challenges, there are also ethical and regulatory considerations that must be addressed when applying PINNs to water distribution systems. For instance, the use of machine learning models in critical infrastructure raises concerns about transparency, accountability, and the potential for bias. It is essential to ensure that PINN models are transparent, interpretable, and auditable, so that their predictions can be trusted and validated by stakeholders. Moreover, the deployment of PINNs in water distribution systems must comply with relevant regulations and standards to ensure the safety and security of the water supply [73].

Despite these challenges, the potential benefits of using PINNs in securing water distribution systems are significant. By combining the power of machine learning with the accuracy of physical models, PINNs can provide a robust and adaptive solution for detecting and mitigating threats to water infrastructure. As research in this area continues to advance, it is expected that PINNs will play an increasingly important role in enhancing the security and reliability of water distribution systems [73].
---

### 7.27 Solving Seepage Problems with PINNs

Seepage problems, which involve the flow of fluids through porous media, are a critical aspect of various engineering and environmental applications, including groundwater management, oil recovery, and geotechnical engineering. Traditional methods for solving these problems often rely on numerical techniques such as the finite element method (FEM), which, while effective, can be computationally expensive and time-consuming. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative, offering a data-driven approach that integrates physical laws into the learning process. This subsection explores the use of PINNs in solving seepage problems, with a focus on the comprehensive loss function and a comparison with conventional finite element methods.

One of the key advantages of PINNs in solving seepage problems is their ability to incorporate the governing equations of fluid flow in porous media directly into the loss function. This approach ensures that the neural network not only learns from available data but also adheres to the physical principles that govern the problem. A comprehensive loss function in PINNs typically includes terms that enforce the satisfaction of the governing partial differential equations (PDEs), as well as boundary and initial conditions. For seepage problems, this means that the PINN must accurately approximate the flow of fluid through a porous medium, accounting for the effects of permeability, porosity, and pressure gradients. The integration of these physical constraints into the loss function allows PINNs to produce solutions that are not only data-driven but also physically consistent [10].

The comprehensive loss function in PINNs is crucial for ensuring the accuracy of the solution. In the context of seepage problems, this function typically includes a residual term that measures the deviation of the neural network's predictions from the governing PDEs. By minimizing this residual, the PINN learns to approximate the solution to the PDEs while adhering to the physical constraints. Additionally, the loss function may include terms that enforce the satisfaction of boundary conditions, ensuring that the neural network's predictions are consistent with the known physical behavior at the boundaries of the domain. This multi-faceted approach allows PINNs to produce accurate and physically consistent solutions, even in complex and heterogeneous porous media [11].

Comparing the performance of PINNs with conventional finite element methods (FEM) provides valuable insights into the strengths and limitations of each approach. FEM is a well-established numerical technique that discretizes the domain into a mesh of elements, allowing for the accurate approximation of the solution to the PDEs. However, the process of mesh generation can be time-consuming and computationally intensive, particularly for complex geometries and heterogeneous media. In contrast, PINNs do not require a mesh, making them more flexible and efficient for problems with irregular or complex geometries. This advantage is particularly relevant for seepage problems, where the porous media can have a highly irregular structure and varying properties [31].

The performance of PINNs in solving seepage problems has been validated through various empirical studies. One such study demonstrated the effectiveness of PINNs in solving the 3D incompressible Navier-Stokes equations at moderate to high Reynolds numbers for complex geometries. The study showed that PINNs could be effective when used in conjunction with sparse data for solving 3D nonlinear PDEs or for surrogate modeling of design spaces governed by them. This finding is particularly relevant for seepage problems, where the availability of high-resolution data may be limited [31].

In addition to their ability to handle sparse data, PINNs have shown promise in capturing the complex dynamics of fluid flow in porous media. The use of a comprehensive loss function that includes terms for the PDE residuals, boundary conditions, and initial conditions allows PINNs to learn the underlying physics of the problem. This approach is particularly effective for seepage problems, where the flow can be influenced by a variety of factors, including the permeability of the porous medium, the presence of obstacles, and the effects of gravity. By integrating these factors into the loss function, PINNs can produce solutions that are not only accurate but also physically meaningful [11].

Another advantage of PINNs is their ability to handle nonlinear and multiphysics problems. Seepage problems often involve the coupling of multiple physical processes, such as fluid flow, heat transfer, and chemical reactions. Traditional numerical methods may struggle to capture these complex interactions, particularly in cases where the governing equations are highly nonlinear. PINNs, on the other hand, can be designed to handle such problems by incorporating the relevant physics into the loss function. This flexibility makes PINNs a powerful tool for solving seepage problems in a wide range of applications [10].

The comparison between PINNs and conventional finite element methods also highlights the potential of PINNs to reduce computational costs. While FEM requires the generation of a mesh and the solution of a large system of equations, PINNs can be trained using a relatively small number of data points. This makes PINNs particularly suitable for problems where the availability of high-resolution data is limited. Additionally, the use of a comprehensive loss function ensures that the PINN's predictions are consistent with the physical laws governing the problem, reducing the need for post-processing and validation [10].

In summary, the use of PINNs for solving seepage problems offers a promising alternative to traditional numerical methods. The comprehensive loss function in PINNs ensures that the solutions are not only data-driven but also physically consistent, making them suitable for a wide range of applications. Empirical studies have demonstrated the effectiveness of PINNs in capturing the complex dynamics of fluid flow in porous media, even in the presence of sparse data. By comparing PINNs with conventional finite element methods, it is evident that PINNs offer significant advantages in terms of flexibility, efficiency, and accuracy. These findings underscore the potential of PINNs to revolutionize the way seepage problems are solved, paving the way for more efficient and accurate simulations in engineering and environmental applications.

### 7.28 Adversarial Training for Robust PINNs

Adversarial training has emerged as a promising technique to enhance the robustness and reliability of Physics-Informed Neural Networks (PINNs) in solving complex partial differential equations (PDEs). Traditional PINNs often struggle with issues such as convergence instability, sensitivity to noisy data, and the inability to accurately capture high-frequency or multi-scale dynamics. Adversarial training addresses these challenges by incorporating adversarial examples into the training process, thereby improving the model's ability to generalize and maintain accuracy in the presence of perturbations. This technique has been particularly effective in scenarios where PINNs face difficulties due to the complexity of the PDEs or the presence of sharp gradients, oscillatory solutions, or chaotic behavior.

One of the key contributions of adversarial training in PINNs is the identification of failure regions within the solution domain. In many cases, PINNs may fail to accurately approximate the solution in specific areas, such as near boundaries or in regions with steep gradients. Adversarial training helps uncover these failure regions by generating adversarial samples that target the model's weaknesses. These samples are designed to maximize the loss function, thereby forcing the network to adapt and improve its performance in these critical areas. For example, in the context of solving PDEs with sharp solutions, such as the Burgers equation or the Allen-Cahn equation, adversarial training has been shown to significantly reduce the error in the regions where the PINN previously struggled [186].

Moreover, adversarial training enhances the convergence of PINNs by promoting more stable and efficient training dynamics. Traditional PINN training can suffer from the issue of local minima, where the network gets stuck in suboptimal solutions that do not fully satisfy the PDE constraints. By introducing adversarial examples, the training process becomes more robust, as the network is forced to adjust its parameters to minimize the loss not only for the original data points but also for the adversarial samples. This results in a more balanced training process and better overall convergence. For instance, in the case of solving the Poisson equation with multi-peak solutions, adversarial training has been shown to significantly improve the convergence behavior of PINNs, enabling them to reach accurate solutions with fewer training iterations [186].

The effectiveness of adversarial training in PINNs is also evident in its ability to handle PDEs with multi-scale or turbulent dynamics. These types of problems often require the model to capture both large-scale trends and small-scale fluctuations, which can be challenging for standard PINNs. Adversarial training addresses this by encouraging the network to focus on the most critical regions of the solution domain. This is achieved by generating adversarial samples that emphasize the high-frequency components of the solution, thereby forcing the network to refine its approximation in those areas. This approach has been successfully applied to the simulation of turbulent flows, where the PINN's ability to capture the intricate dynamics of the flow is crucial for accurate predictions [186].

In addition to improving robustness and convergence, adversarial training also helps in mitigating the issue of overfitting in PINNs. Overfitting can occur when the network becomes too focused on fitting the training data, leading to poor generalization to new or unseen scenarios. Adversarial training introduces a form of regularization by forcing the network to consider adversarial examples, which act as a form of perturbation that prevents the model from becoming overly sensitive to the training data. This is particularly important in applications where the PDEs are governed by complex physical laws, and the training data may be limited or noisy. By incorporating adversarial examples into the training process, the PINN becomes more resilient to such challenges, leading to more reliable and accurate predictions [186].

Another advantage of adversarial training is its ability to enhance the interpretability of PINNs. While PINNs are inherently data-driven, they can sometimes produce solutions that are difficult to interpret in terms of the underlying physical laws. Adversarial training provides a way to ensure that the network's predictions are not only accurate but also physically consistent. By training the network to handle adversarial examples that challenge the physical constraints of the problem, the model is forced to produce solutions that adhere to the governing equations. This leads to more physically meaningful results and improves the overall trustworthiness of the PINN's predictions [186].

The application of adversarial training in PINNs has also been extended to problems involving parameterized PDEs, where the solution depends on a set of parameters that may vary over time or across different scenarios. In such cases, the PINN must not only solve the PDE but also generalize to new parameter values. Adversarial training helps in this regard by ensuring that the network can handle variations in the parameters without losing accuracy. This is achieved by generating adversarial examples that perturb the parameters, thereby forcing the network to learn a more robust and generalizable solution. This approach has been particularly useful in the context of inverse problems, where the goal is to estimate the parameters of the PDE based on observed data [186].

Overall, adversarial training has proven to be a valuable tool for improving the robustness and reliability of PINNs in solving complex PDEs. By identifying failure regions, enhancing convergence, mitigating overfitting, and improving interpretability, adversarial training addresses many of the key challenges associated with traditional PINN approaches. The effectiveness of this technique has been demonstrated in a variety of applications, from solving PDEs with sharp or oscillatory solutions to simulating turbulent flows and parameterized PDEs. As the field of PINNs continues to evolve, adversarial training is likely to play an increasingly important role in ensuring that these models can be applied to a wide range of real-world problems with high accuracy and reliability.

### 7.29 Hypernetwork-Based Meta-Learning for Low-Rank PINNs

The use of hypernetworks and low-rank adaptation in Physics-Informed Neural Networks (PINNs) represents a significant advancement in the quest for efficient and scalable deep learning solutions for fluid mechanics problems. Traditional PINNs, while effective in integrating physical laws into the learning process, often face challenges related to computational cost, training efficiency, and model complexity. These challenges are particularly pronounced when dealing with high-dimensional problems or when requiring repeated simulations for varying input parameters. The introduction of hypernetworks and low-rank adaptation techniques offers a promising solution to these issues, enabling the development of PINNs with a reduced number of model parameters and enhanced training efficiency.

Hypernetworks, which are neural networks that generate the weights of another neural network, have been explored as a means to reduce the number of parameters in PINNs. This approach leverages the concept of meta-learning, where the hypernetwork learns to generate the parameters of the main PINN model based on the input data and the physical constraints of the problem. By doing so, hypernetworks can significantly reduce the number of trainable parameters in the PINN, leading to faster training times and improved generalization capabilities. This technique has been shown to be particularly effective in scenarios where the PINN needs to be trained for multiple configurations or parameter settings, as the hypernetwork can adaptively generate the necessary weights without requiring full retraining of the model [145].

Low-rank adaptation (LoRA) is another technique that has gained attention for its potential to reduce model complexity in PINNs. LoRA involves introducing low-rank matrices into the existing neural network architecture, which can capture the essential features of the solution while significantly reducing the number of parameters. This method is particularly useful in scenarios where the problem involves a large number of parameters or where the solution requires high-dimensional representations. By integrating LoRA into the PINN framework, researchers have been able to achieve comparable or even superior performance to traditional PINNs with a fraction of the computational resources [145].

The combination of hypernetworks and low-rank adaptation in PINNs offers a powerful approach to enhance the efficiency and effectiveness of these models in solving fluid mechanics problems. One of the key advantages of this approach is the ability to maintain the physical constraints of the problem while reducing the model's complexity. The inclusion of a physics-informed loss component in the training process ensures that the PINN remains aligned with the underlying physical laws, even as the model parameters are reduced. This is crucial for maintaining the accuracy and reliability of the solutions, especially in complex and high-dimensional scenarios where traditional PINNs may struggle with convergence and stability [145].

In practice, the integration of hypernetworks and low-rank adaptation into PINNs has been demonstrated to be highly effective in a variety of applications. For instance, the use of hypernetworks has been shown to significantly reduce the computational cost of training PINNs for problems involving varying input parameters, such as different Reynolds numbers or boundary conditions [145]. This is particularly relevant in fluid dynamics, where the ability to quickly adapt to new scenarios is essential for real-time applications and predictive modeling.

Moreover, the use of low-rank adaptation in conjunction with hypernetworks has been found to improve the convergence behavior of PINNs. By reducing the number of parameters, the training process becomes more efficient, allowing for faster convergence and improved stability. This is especially beneficial in scenarios where the PINN is required to solve complex PDEs with high-frequency components or where the solution exhibits multi-scale behavior [145].

Another significant advantage of using hypernetworks and low-rank adaptation in PINNs is their ability to handle noisy or incomplete data. The reduced model complexity allows the PINN to generalize better from limited data, making it more robust to variations in the input. This is particularly important in fluid mechanics, where experimental data can be sparse and noisy, and the ability to extract meaningful insights from such data is critical [145].

Furthermore, the use of hypernetworks and low-rank adaptation in PINNs can lead to significant improvements in the scalability of these models. Traditional PINNs often struggle with large-scale problems due to the high computational cost and the difficulty in managing the large number of parameters. By reducing the number of parameters and leveraging the efficiency of hypernetworks, PINNs can be scaled to handle more complex and larger problems, making them more suitable for real-world applications in fluid mechanics [145].

The integration of a physics-informed loss component is a critical aspect of the hypernetwork-based meta-learning approach for low-rank PINNs. This loss component ensures that the PINN not only fits the data but also satisfies the physical constraints of the problem. By incorporating this loss into the training process, the model is able to maintain the physical consistency of the solution, even when the number of parameters is reduced. This is essential for ensuring the accuracy and reliability of the PINN's predictions, particularly in scenarios where the solution is sensitive to small changes in the input or the physical conditions [145].

In summary, the use of hypernetworks and low-rank adaptation in PINNs represents a significant step forward in the development of efficient and scalable deep learning models for fluid mechanics. By reducing the number of parameters and improving the training efficiency, these techniques enable the creation of PINNs that are more robust, accurate, and capable of handling complex and high-dimensional problems. The inclusion of a physics-informed loss component ensures that the models remain aligned with the underlying physical laws, making them more reliable and effective for real-world applications in fluid dynamics [145].

### 7.30 Best Practices for Training PINNs

[82]
Training Physics-Informed Neural Networks (PINNs) effectively requires a combination of careful architecture choices, strategic training methods, and the use of efficient computational tools. As a survey, we explore the best practices that have emerged from empirical validation and case studies, emphasizing the importance of these practices in achieving stable and accurate solutions for complex fluid dynamics problems.

One of the most critical aspects of training PINNs is the selection of the neural network architecture. Studies have demonstrated that the choice of activation functions significantly impacts the training performance of PINNs. For instance, the use of Gaussian activations has been shown to improve the trainability and accuracy of PINNs, particularly in problems where the solution requires high precision and stability [55]. This is because Gaussian activations provide a smooth and differentiable function, allowing for more effective gradient flow during backpropagation. Another notable approach is the use of a preconditioned neural architecture, which leverages insights from numerical linear algebra to enhance the optimization process [55]. This approach helps in reducing the non-convexity of the loss landscape, making it easier for the network to converge to a global minimum.

In addition to the choice of activation functions, the design of the neural network's layers and the number of hidden layers also play a crucial role in training PINNs. Empirical studies have shown that increasing the depth of the network can improve the model's ability to capture complex patterns in the data, but it also increases the risk of overfitting. Therefore, it is important to strike a balance between network depth and the complexity of the problem at hand. For example, the use of a shallow network might be sufficient for simpler problems, while more complex problems may require deeper architectures to achieve the desired accuracy [9].

Another essential consideration in training PINNs is the choice of training strategies. One of the most effective strategies is the use of transfer learning, where a pre-trained model is fine-tuned on a specific problem. This approach has been shown to significantly reduce the training time and improve the convergence of PINNs, especially in scenarios where the dataset is sparse or noisy [9]. Transfer learning allows the model to leverage the knowledge gained from previous tasks, leading to better generalization and robustness. Moreover, the use of adaptive sampling techniques, such as failure-informed adaptive sampling, has been shown to improve the accuracy of PINNs by dynamically adjusting the placement of collocation points based on the network's performance [49]. This approach ensures that the model focuses on regions of the domain where the solution is less accurate, thereby improving the overall performance.

The training process itself also benefits from the use of efficient optimization algorithms. Stochastic gradient descent (SGD) and its variants, such as Adam, are commonly used in training PINNs due to their ability to handle non-convex loss functions. However, the choice of learning rate and the use of learning rate schedules can significantly impact the convergence of the model. Studies have shown that using a learning rate that decreases over time can help the model converge more smoothly and avoid getting stuck in local minima [9]. Additionally, the use of momentum and adaptive learning rate methods can further enhance the training process by accelerating convergence and reducing oscillations.

The use of the JAX library has also emerged as a best practice in training PINNs, particularly for ensuring reproducibility and optimization. JAX provides a powerful set of tools for automatic differentiation and vectorization, making it easier to implement and optimize complex neural network architectures [55]. By leveraging JAX's capabilities, researchers can efficiently compute gradients and perform vectorized operations, which are essential for training large-scale PINNs. The JAX library also supports GPU and TPU acceleration, which can significantly reduce the training time for large datasets and complex models. This makes JAX an indispensable tool for researchers and practitioners working with PINNs.

Another critical aspect of training PINNs is the management of the loss function. The loss function in PINNs typically consists of multiple terms, including the data loss, the residual loss, and the boundary condition loss. Balancing these terms is essential to ensure that the model learns the underlying physical laws without overfitting to the data. Studies have shown that using a weighted sum of these terms can help in achieving a better balance, but the choice of weights requires careful tuning [9]. Moreover, the use of a pressure stabilization term in the loss function has been shown to improve the accuracy of the pressure field, particularly in problems involving thermally coupled incompressible Navier-Stokes equations [61]. This approach helps in reducing the error in the pressure field, leading to more accurate predictions.

In conclusion, the training of PINNs requires a careful consideration of architecture choices, training strategies, and computational tools. By following the best practices outlined in this section, researchers can improve the performance and reliability of their PINN models, leading to more accurate and robust solutions for complex fluid dynamics problems. These practices are supported by empirical validation and case studies, ensuring that they are effective in real-world applications. As the field of PINNs continues to evolve, it is essential to stay updated with the latest research and incorporate these best practices into future studies.

### 7.31 PINNs for Solving RANS Equations

Physics-Informed Neural Networks (PINNs) have demonstrated significant potential in solving complex fluid dynamics problems, including the Reynolds-Averaged Navier-Stokes (RANS) equations for incompressible turbulent flows. Traditional numerical methods, such as finite element and finite volume methods, often struggle with the high computational costs and complexity associated with solving RANS equations. These equations are derived by averaging the Navier-Stokes equations over time, which introduces additional terms to account for turbulent stresses. The challenge lies in accurately modeling these terms, which are typically approximated using turbulence models such as the k-ε or k-ω models. PINNs offer a promising alternative by integrating the physics of the RANS equations directly into the loss function, allowing for the learning of the solution while adhering to the underlying physical laws [22].

In the context of RANS equations, PINNs are designed to approximate the velocity and pressure fields of incompressible turbulent flows by minimizing a loss function that incorporates the residual of the RANS equations. This approach not only ensures that the solution satisfies the governing equations but also enforces boundary conditions, leading to more accurate and physically consistent results. The integration of RANS equations into the PINN framework allows for the direct prediction of turbulent flow fields without the need for explicit turbulence modeling, which is a significant advantage over traditional methods [22].

One of the key applications of PINNs in solving RANS equations is in the simulation of turbulent flows over complex geometries, such as airfoils and walls. Traditional CFD solvers often require extensive mesh generation and computational resources to resolve the turbulent scales, which can be computationally prohibitive. PINNs, on the other hand, offer a mesh-free solution, enabling the simulation of complex flows with fewer computational resources. This is particularly beneficial for problems involving high Reynolds numbers, where the turbulent flow structures are highly complex and require fine resolution [22].

The RANS-PINN framework, introduced in [22], is a modified PINN approach that specifically addresses the challenges of solving RANS equations. This framework employs a 2-equation eddy viscosity model based on the RANS formulation, which allows for the accurate prediction of turbulent flow fields. The RANS-PINN method also incorporates a novel training approach that ensures effective initialization and balance among the various components of the loss function, which is crucial for achieving stable and accurate solutions. This approach has been successfully applied to simulate turbulent flows in high Reynolds number regimes, demonstrating the effectiveness of PINNs in capturing the complex dynamics of turbulent flows [22].

Another significant advantage of PINNs in solving RANS equations is their ability to handle sparse and noisy data. Traditional CFD solvers often require high-fidelity data for accurate predictions, which can be challenging to obtain in practical scenarios. PINNs, however, can learn the underlying physical laws from limited data, making them suitable for applications where data is scarce or noisy. This is particularly important in real-world engineering problems, where the availability of high-resolution data is often limited [22].

The accuracy of PINNs in solving RANS equations has been validated through various benchmark problems, including the simulation of zero-pressure-gradient boundary layers, adverse-pressure-gradient boundary layers, and turbulent flows over a NACA4412 airfoil. In these cases, PINNs have demonstrated excellent accuracy, with predictions that are in close agreement with experimental and high-fidelity numerical simulations. For instance, the RANS-PINN framework has been shown to produce accurate predictions of Reynolds-stress components, which are essential for understanding the turbulent flow behavior [22].

Moreover, the applicability of PINNs in solving RANS equations extends beyond just the simulation of turbulent flows. PINNs can also be used for inverse problems, where the goal is to estimate unknown parameters or boundary conditions from observed data. This is particularly useful in applications such as flow control and optimization, where the accurate estimation of parameters is critical for achieving desired flow characteristics. The ability of PINNs to handle both forward and inverse problems makes them a versatile tool for fluid dynamics simulations [22].

In addition to their accuracy and applicability, PINNs offer several advantages over traditional numerical methods in terms of computational efficiency and scalability. Traditional CFD solvers often require significant computational resources to resolve the turbulent scales, especially in high Reynolds number flows. PINNs, on the other hand, can be trained efficiently using modern deep learning frameworks, making them suitable for large-scale simulations. This is particularly beneficial for problems involving complex geometries and multi-physics interactions, where traditional methods may struggle with computational costs [22].

The integration of PINNs with RANS equations also opens up new possibilities for the development of more accurate turbulence models. By learning the underlying physical laws from data, PINNs can potentially identify new relationships between turbulence parameters and flow characteristics, leading to improved turbulence models. This is a significant advantage over traditional turbulence models, which are often based on empirical correlations and may not capture the full complexity of turbulent flows [22].

In conclusion, the application of PINNs to solving RANS equations for incompressible turbulent flows has shown great promise. PINNs offer a mesh-free, data-driven approach that integrates the physics of the RANS equations into the loss function, leading to accurate and physically consistent solutions. The RANS-PINN framework, in particular, has demonstrated the effectiveness of PINNs in capturing the complex dynamics of turbulent flows, making them a valuable tool for fluid dynamics simulations. As research in this area continues to advance, PINNs are expected to play an increasingly important role in the simulation and analysis of turbulent flows, providing a powerful alternative to traditional numerical methods.

### 7.32 Knowledge-Based Convolutional Neural Networks for Two-Phase Darcy Flows

The integration of discretized governing equations into the Physics-Informed Neural Network (PINN) framework for simulating two-phase Darcy flows represents a significant advancement in computational fluid dynamics. Two-phase Darcy flows involve the simultaneous movement of two immiscible fluids, such as oil and water, through a porous medium. These flows are governed by a set of partial differential equations (PDEs) that describe the conservation of mass and momentum for each phase, along with constitutive relationships that define the interaction between the fluids and the porous medium. Traditional numerical methods, such as finite element or finite volume methods, are often used to solve these PDEs, but they can be computationally expensive and challenging to apply to complex, heterogeneous domains. The use of PINNs offers a promising alternative by leveraging the power of deep learning to approximate the solution of these equations while incorporating the underlying physical laws as constraints during training.

The key idea behind this approach is to integrate the discretized governing equations directly into the PINN framework. This is achieved by defining a neural network that approximates the solution of the PDEs, and then using the residual of these equations as part of the loss function during training. By doing so, the PINN is forced to satisfy the physical laws that govern the flow, even in the presence of sparse or noisy data. This integration of physics into the learning process ensures that the network's predictions are not only data-driven but also physically consistent, which is crucial for accurately modeling complex two-phase flow phenomena.

One of the main advantages of using PINNs for two-phase Darcy flows is the potential for reduced computational cost. Traditional numerical methods require the discretization of the domain into a grid, which can be computationally intensive, especially for high-resolution simulations. PINNs, on the other hand, do not require a fixed grid, allowing for a more flexible and efficient representation of the solution. Additionally, the use of convolutional neural networks (CNNs) in conjunction with PINNs can further enhance the efficiency of the method by exploiting the spatial structure of the problem. CNNs are particularly well-suited for handling grid-like data and can capture local patterns and features that are important for accurately modeling the flow dynamics.

The integration of discretized governing equations into the PINN framework is not without its challenges. One of the primary difficulties is ensuring that the network can accurately capture the complex interactions between the two phases, particularly in regions where the flow is highly nonlinear or discontinuous. To address this, researchers have proposed various strategies, including the use of specialized activation functions, adaptive sampling techniques, and the incorporation of prior knowledge into the network architecture. For example, the paper titled "Frame-independent vector-cloud neural network for nonlocal constitutive modeling on arbitrary grids" [199] introduces a vector-cloud neural network that can effectively model the nonlocal interactions between the fluids and the porous medium, leading to more accurate and physically consistent predictions.

Another important aspect of integrating discretized governing equations into the PINN framework is the choice of the loss function. The loss function must be carefully designed to balance the contributions from the PDE residuals, boundary conditions, and any additional constraints that may be imposed. This requires a deep understanding of the underlying physics and the mathematical structure of the equations. For instance, the paper "VW-PINNs: A volume weighting method for PDE residuals in physics-informed neural networks" [200] proposes a volume-weighted residual approach that accounts for the spatial distribution of the collocation points, leading to improved convergence and accuracy in the solution of the PDEs.

In addition to the technical challenges, there are also practical considerations that must be addressed when applying PINNs to two-phase Darcy flows. For example, the choice of hyperparameters, such as the learning rate, the number of layers, and the type of activation function, can have a significant impact on the performance of the network. Furthermore, the availability and quality of training data can also affect the accuracy of the predictions, as PINNs rely on data to learn the underlying patterns and relationships. To mitigate these issues, researchers have explored various strategies, including the use of transfer learning and the incorporation of domain knowledge into the network architecture.

Despite these challenges, the integration of discretized governing equations into the PINN framework has shown promising results in the simulation of two-phase Darcy flows. The approach has been successfully applied to a variety of problems, including the prediction of fluid saturation, pressure distribution, and flow rates in porous media. These results demonstrate the potential of PINNs to provide accurate and efficient solutions to complex two-phase flow problems, particularly in situations where traditional numerical methods may be computationally prohibitive.

In summary, the integration of discretized governing equations into the PINN framework for simulating two-phase Darcy flows offers a powerful and flexible approach to solving these complex problems. By combining the strengths of deep learning with the physical laws that govern the flow, PINNs can provide accurate and physically consistent predictions, while also reducing the computational cost associated with traditional numerical methods. As research in this area continues to advance, it is likely that PINNs will play an increasingly important role in the simulation and analysis of two-phase Darcy flows in both academic and industrial applications.

### 7.33 Failure-Informed Adaptive Sampling for PINNs

Failure-informed adaptive sampling has emerged as a promising strategy to enhance the accuracy and reliability of Physics-Informed Neural Networks (PINNs), particularly in solving partial differential equations (PDEs) that involve complex, high-frequency dynamics or regions with significant solution variation. Traditional PINNs often struggle with convergence, especially when the solution exhibits intricate spatial and temporal patterns or when the initial and boundary conditions are not well aligned with the underlying physical laws. One of the key challenges in training PINNs is the uneven distribution of collocation points across the solution domain, which can lead to suboptimal learning and inaccurate predictions. Adaptive sampling strategies have been proposed to dynamically adjust the placement of collocation points during training, ensuring that the network focuses on regions where the solution is most uncertain or where the PDE residuals are high. Among these, failure-informed adaptive sampling takes this concept a step further by incorporating failure probability as a posterior error indicator to guide the sampling process.

The failure-informed adaptive sampling strategy is designed to detect and address regions of the solution domain where the PINN is prone to making errors. This approach is rooted in the observation that PINNs often fail to converge to the correct solution when the training data is imbalanced or when the PDE residuals are not uniformly distributed across the domain. By monitoring the failure probability—defined as the likelihood that the PINN produces nonphysical or incorrect predictions in a given region—this strategy dynamically adjusts the collocation point distribution to focus on areas where the model is most likely to fail. The failure probability is calculated based on the residuals of the PDEs and the consistency of the solution with the physical constraints. This allows the model to prioritize sampling in regions with high failure risk, thereby improving the overall accuracy and convergence of the PINN.

A key advantage of failure-informed adaptive sampling is its ability to address the spectral bias problem that is commonly encountered in PINNs. Spectral bias refers to the tendency of PINNs to preferentially learn low-frequency components of the solution, which can lead to inaccurate predictions in regions with high-frequency variations. By incorporating failure probability as a posterior error indicator, failure-informed adaptive sampling ensures that the network is exposed to a diverse set of samples, including those in high-frequency regions where the solution is most uncertain. This helps to mitigate the spectral bias and improves the model's ability to capture the full range of solution dynamics. Moreover, the use of failure probability as an error indicator provides a principled way to determine when and where additional samples are needed, leading to more efficient training and better generalization performance.

The effectiveness of failure-informed adaptive sampling has been demonstrated in several studies. For example, in a study on solving the Navier-Stokes equations, failure-informed adaptive sampling was shown to significantly improve the accuracy of the PINN solution by focusing on regions with high failure probability, such as near the boundaries or in regions of high shear stress [124]. The study also found that the approach led to faster convergence and reduced training time compared to traditional fixed sampling strategies. Another study on solving the Kuramoto-Sivashinsky equation, a nonlinear PDE with chaotic behavior, demonstrated that failure-informed adaptive sampling was able to stabilize the training process and produce more accurate solutions, even in the presence of noisy or incomplete data [124]. These results highlight the potential of failure-informed adaptive sampling to enhance the robustness and reliability of PINNs in complex fluid dynamics problems.

In addition to improving accuracy and convergence, failure-informed adaptive sampling also addresses the issue of data scarcity in PINN training. Traditional PINNs often require a large amount of training data to achieve good performance, which can be a significant limitation in practical applications where data is limited or expensive to obtain. By dynamically adjusting the sampling strategy based on failure probability, failure-informed adaptive sampling ensures that the model makes the most efficient use of the available data. This is particularly beneficial in scenarios where the solution domain is large or the PDEs are highly nonlinear, as it allows the PINN to focus on the most informative regions of the domain. Furthermore, the approach can be combined with other techniques such as transfer learning and domain decomposition to further enhance the model's performance in data-scarce settings [124].

Another important aspect of failure-informed adaptive sampling is its ability to handle the non-convexity of the loss landscape in PINN training. The loss function in PINNs is typically non-convex, which can make the optimization process challenging and lead to the model getting stuck in local optima. By using failure probability as a guide for adaptive sampling, the strategy helps the model avoid regions of the loss landscape that are likely to lead to poor solutions. This is achieved by ensuring that the collocation points are distributed in a way that promotes a more uniform and well-conditioned loss landscape. As a result, failure-informed adaptive sampling can improve the stability and reliability of PINN training, even in the presence of complex and high-dimensional PDEs.

The failure-informed adaptive sampling strategy also has implications for the design of future PINN architectures. By providing a dynamic and data-driven approach to collocation point selection, this strategy can inform the development of more efficient and scalable PINN models. For instance, it can be used to guide the design of neural network architectures that are better suited to capturing the solution dynamics in high-frequency regions or to handle complex boundary conditions. Additionally, the integration of failure-informed adaptive sampling with other advanced training techniques, such as multi-fidelity learning and physics-informed activation functions, could further enhance the performance of PINNs in a wide range of applications.

In conclusion, failure-informed adaptive sampling represents a significant advancement in the training of Physics-Informed Neural Networks (PINNs). By incorporating failure probability as a posterior error indicator, this strategy enables the model to dynamically adjust the placement of collocation points, leading to improved accuracy, convergence, and robustness. The approach has been shown to be effective in a variety of fluid dynamics problems, including the Navier-Stokes equations and the Kuramoto-Sivashinsky equation, and has the potential to address the challenges of spectral bias, data scarcity, and non-convex optimization in PINN training. As the field of PINNs continues to evolve, the development of more sophisticated adaptive sampling strategies will be essential to unlocking their full potential in solving complex and high-dimensional PDEs.

### 7.34 Transport Equations with PINNs for Yield Strength Prediction

---
Transport equations are a class of partial differential equations (PDEs) that describe the transport of physical quantities such as mass, momentum, and energy in a medium. These equations are widely used in engineering and physics to model phenomena such as fluid flow, heat transfer, and diffusion. In the context of yield strength prediction, transport equations are particularly relevant for understanding the behavior of materials under stress, where the transport of dislocations and other microstructural features plays a critical role. Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving such transport equations, offering a data-driven approach that incorporates the physical laws governing the system into the neural network's architecture.

One of the key challenges in applying PINNs to transport equations is the choice of activation functions, which significantly influences the network's ability to approximate the solution and generalize to new data. Activation functions determine how the network processes input data and, consequently, affect the model's capacity to capture the complex dynamics of the underlying physical system. The paper titled "Transport Equation based Physics Informed Neural Network to predict the Yield Strength of Architected Materials" [201] explores the impact of different activation functions on the predictive performance of PINNs in the context of yield strength prediction. The study investigates how the choice of activation function affects the model's ability to learn the relationship between input parameters, such as strut diameter and unit cell size, and the corresponding yield stress values.

The results of the study indicate that the choice of activation function may have a minimal influence on the model's predictive accuracy for this particular problem. However, the PINN model demonstrates exceptional generalization capabilities, indicating its ability to avoid overfitting with the provided dataset. The study highlights the importance of striking a balance between performance and computational efficiency when selecting an activation function for specific real-world applications. The findings suggest that while the choice of activation function is crucial, the overall effectiveness of PINNs in solving transport equations for yield strength prediction is relatively robust to variations in this aspect.

In addition to the choice of activation functions, the accuracy of PINNs in solving transport equations also depends on the quality and quantity of the training data. The study emphasizes the importance of using a diverse set of input parameters to ensure that the model can generalize well to unseen scenarios. The dataset used in the study consists of a varied set of input parameters related to strut diameter, unit cell size, and the corresponding yield stress values, which allows for a comprehensive evaluation of the model's performance. The results show that the PINN model can effectively capture the underlying relationships between these parameters and the yield stress, demonstrating its potential as a reliable tool for predicting the mechanical properties of architected materials.

Another critical factor in the success of PINNs for solving transport equations is the design of the loss function, which incorporates the governing equations of the system into the training process. The loss function in PINNs is typically a combination of the residuals of the PDEs and the boundary conditions, ensuring that the network's predictions satisfy the physical laws of the problem. This approach allows PINNs to learn the solution to the PDEs without the need for labeled data, making them particularly useful for problems where experimental data is scarce or expensive to obtain. The paper titled "Physics-informed neural networks for solving thermo-mechanics problems of functionally graded material" [169] demonstrates the effectiveness of PINNs in solving complex PDEs by incorporating the governing equations into the loss function. The study highlights the potential of PINNs to handle multi-physics problems, where the solution of the PDEs must account for multiple interacting physical processes.

The application of PINNs to transport equations for yield strength prediction also involves addressing the challenges associated with high-frequency dynamics and complex flow behaviors. The study shows that PINNs can effectively capture the high-frequency components of the solution, which are critical for accurately predicting the mechanical behavior of materials under stress. The paper titled "Adversarial Training for Physics-Informed Neural Networks" [186] presents a method to improve the robustness of PINNs in solving complex PDEs by incorporating adversarial training. This approach enhances the network's ability to handle problems with sharp or oscillatory solutions, which are common in transport equations.

Furthermore, the use of PINNs in solving transport equations for yield strength prediction can benefit from techniques such as adaptive sampling and multi-fidelity learning. These methods aim to improve the efficiency and accuracy of PINNs by dynamically adjusting the training process based on the complexity of the problem. The paper titled "Adaptive quadratures for nonlinear approximation of low-dimensional PDEs using smooth neural networks" [202] discusses the use of adaptive quadratures to improve the integration of neural networks, which is essential for obtaining accurate solutions. The study demonstrates that adaptive quadratures can significantly enhance the convergence of PINNs while reducing the computational cost.

In conclusion, the application of PINNs to solve transport equations for yield strength prediction represents a promising direction in the field of computational mechanics. The results of the studies highlight the effectiveness of PINNs in capturing the complex dynamics of transport equations and their potential to provide accurate predictions of material properties. The choice of activation functions, the design of the loss function, and the use of advanced training techniques all play a critical role in the success of PINNs. As research in this area continues to advance, the integration of PINNs with other machine learning methods and traditional numerical solvers is expected to further enhance their capabilities, paving the way for more efficient and accurate solutions to complex physical problems.
---

### 7.35 PINNs for Strongly Degenerate Parabolic Problems

Strongly degenerate parabolic problems are a class of partial differential equations (PDEs) that exhibit significant challenges due to their nonlinear and degenerate nature. These problems often arise in various fields, including fluid dynamics, where they describe phenomena such as diffusion in porous media or transport in heterogeneous environments. The complexity of these equations stems from the fact that the diffusion coefficient can vanish in certain regions of the domain, leading to a loss of parabolicity and requiring specialized numerical techniques for their solution. Physics-Informed Neural Networks (PINNs) have emerged as a promising approach to tackle these challenges, offering a data-driven yet physically consistent method for solving such problems. 

The application of PINNs to strongly degenerate parabolic problems is particularly significant because traditional numerical methods, such as finite difference or finite element methods, often struggle with the nonlinearity and degeneracy of the equations. PINNs address this issue by embedding the governing PDEs directly into the loss function of the neural network, ensuring that the solution adheres to the underlying physical laws. This approach not only enhances the accuracy of the solution but also provides a more flexible framework for handling complex and ill-posed problems, which are common in strongly degenerate parabolic systems [10].

One of the key advantages of using PINNs for strongly degenerate parabolic problems is their ability to handle sparse or noisy data. In many real-world scenarios, the availability of high-quality data is limited, making it difficult to train traditional models effectively. PINNs, however, can leverage the physical constraints encoded in the PDEs to guide the learning process, even in the absence of extensive data. This is particularly valuable in applications where data collection is costly or impractical. For instance, in the context of porous media flow, where the governing equations may be highly degenerate, PINNs can provide accurate approximations of the solution by relying on the inherent structure of the PDEs [21].

The effectiveness of PINNs in solving strongly degenerate parabolic problems is further enhanced by the use of advanced training strategies and architectures. Techniques such as adaptive sampling, physics-informed activation functions, and hybrid models have been shown to improve the performance of PINNs in these challenging scenarios. Adaptive sampling, for example, dynamically adjusts the distribution of collocation points during training to focus on regions where the PDE residuals are high, thereby improving the overall accuracy of the solution. This is particularly beneficial in problems with sharp gradients or discontinuities, which are often associated with strongly degenerate parabolic equations [93].

Moreover, the integration of PINNs with other machine learning techniques, such as ensemble learning and uncertainty quantification, can further enhance their robustness and reliability. Ensemble learning, for instance, involves training multiple PINNs and combining their predictions to reduce the risk of overfitting and improve generalization. This approach is especially useful in problems where the solution is sensitive to small variations in the input parameters or initial conditions. Uncertainty quantification, on the other hand, allows for the assessment of the confidence in the PINN predictions, providing valuable insights into the reliability of the solution [203].

Recent studies have demonstrated the effectiveness of PINNs in solving strongly degenerate parabolic problems through a series of empirical validations. For example, in the context of subsurface flow simulation, PINNs have been used to model the transport of contaminants in porous media, where the governing equations are highly nonlinear and degenerate. The results show that PINNs can accurately capture the complex dynamics of the flow, even in the presence of high uncertainty and sparse data. This is a significant achievement, as traditional numerical methods often require extensive computational resources and may not be feasible for large-scale problems [204].

Another important aspect of PINNs in solving strongly degenerate parabolic problems is their ability to handle multi-physics and multi-scale phenomena. In many real-world applications, the governing equations involve multiple interacting physical processes, such as heat transfer, fluid flow, and mechanical deformation. PINNs can seamlessly integrate these processes into a single framework, allowing for the simultaneous solution of coupled PDEs. This is particularly advantageous in problems where the interactions between different physical processes are critical to the overall behavior of the system, such as in thermo-hydro-mechanical (THM) processes in porous media [10].

In conclusion, the use of PINNs for solving strongly degenerate parabolic problems represents a significant advancement in the field of computational fluid dynamics and related areas. By embedding the governing PDEs directly into the training process, PINNs provide a powerful and flexible framework for addressing the challenges associated with these complex equations. The empirical validation of PINNs in various applications demonstrates their effectiveness in producing accurate and reliable solutions, even in the presence of sparse or noisy data. As research in this area continues to evolve, the integration of advanced training strategies and hybrid models will further enhance the capabilities of PINNs, making them an indispensable tool for solving a wide range of physical problems.

### 7.36 Multi-Case PINNs for Biomedical Tube Flows

The application of multi-case Physics-Informed Neural Networks (PINNs) in biomedical tube flows represents a significant advancement in the integration of machine learning with fluid dynamics, particularly in scenarios where real-time predictions and parameterization of complex geometries are essential. Biomedical tube flows, such as those in blood vessels or respiratory systems, involve highly variable geometries and dynamic fluid behaviors, making traditional numerical methods computationally intensive and less adaptable to real-world scenarios. Multi-case PINNs have emerged as a powerful solution by enabling the simultaneous training of a single neural network to generalize across multiple cases, thereby reducing the need for retraining from scratch for each new geometry or parameter configuration [48].

One of the primary advantages of multi-case PINNs is their ability to parameterize the problem domain. By incorporating key parameters such as geometric dimensions, boundary conditions, and fluid properties into the input of the neural network, multi-case PINNs can learn to predict flow fields across a wide range of scenarios. This parameterization is crucial in biomedical applications, where patient-specific geometries and physiological conditions vary significantly. For instance, in the context of arterial blood flow, a multi-case PINN can be trained on a dataset that includes various vessel geometries, flow rates, and pressure profiles, allowing it to accurately predict flow patterns for new, unseen geometries [20].

Real-time prediction is another critical application of multi-case PINNs in biomedical tube flows. Traditional solvers often require significant computational resources and time to simulate fluid dynamics, especially in complex 3D geometries. In contrast, multi-case PINNs can be trained to approximate the solution of the governing equations (e.g., Navier-Stokes equations) with high accuracy and efficiency. This makes them suitable for applications such as real-time monitoring of blood flow in vascular systems, where rapid and accurate predictions are essential for clinical decision-making [205].

The parameterization of multi-case PINNs is often achieved through a combination of input encoding and loss function design. Input encoding involves transforming geometric and flow-related parameters into a format that can be effectively processed by the neural network. This can include scaling, normalization, or the use of feature engineering techniques to extract relevant information from the input data. The loss function, on the other hand, is designed to incorporate the physical constraints of the problem, such as the conservation of mass and momentum, as well as the boundary and initial conditions. By combining these elements, multi-case PINNs can learn to capture the underlying physics of the flow while maintaining the flexibility to adapt to new cases [48].

A key challenge in developing multi-case PINNs for biomedical tube flows is ensuring that the model remains robust and generalizable across different geometries and flow conditions. This requires careful selection of training data and the inclusion of a diverse range of scenarios in the training set. For instance, a multi-case PINN trained on a variety of vascular geometries, including both healthy and diseased vessels, can better generalize to new cases and provide more reliable predictions. Additionally, techniques such as data augmentation, transfer learning, and domain adaptation can be employed to enhance the model's ability to handle unseen geometries and flow conditions [48].

Recent studies have demonstrated the effectiveness of multi-case PINNs in biomedical tube flow simulations. For example, the use of PINNs in solving the Navier-Stokes equations for blood flow in arteries has shown that multi-case PINNs can achieve high accuracy while significantly reducing computational costs compared to traditional finite element methods [20]. In another study, the application of PINNs to the simulation of turbulent flows in biomedical tube geometries highlighted the potential of these models to capture complex flow phenomena, such as vortices and recirculation zones, with high fidelity [22]. These findings underscore the versatility and power of multi-case PINNs in addressing the challenges of biomedical fluid dynamics.

Moreover, the integration of parameterization and real-time prediction capabilities in multi-case PINNs has opened up new possibilities for personalized medicine and clinical applications. By leveraging the predictive power of these models, healthcare professionals can gain deeper insights into the hemodynamics of individual patients, enabling more accurate diagnoses and tailored treatment strategies. For instance, a multi-case PINN trained on a large dataset of patient-specific geometries can be used to predict blood flow patterns in real-time during surgical planning, helping surgeons to identify potential complications and optimize their procedures [48].

In summary, the application of multi-case PINNs in biomedical tube flows represents a promising approach for parameterizing complex geometries and enabling real-time predictions. By combining the strengths of physics-informed neural networks with the flexibility of parameterized models, these approaches offer a powerful tool for advancing the field of biomedical fluid dynamics. The success of multi-case PINNs in this domain highlights their potential to revolutionize the way we simulate and analyze fluid flows in medical applications, paving the way for more accurate, efficient, and personalized solutions.

### 7.37 Data-Driven PINNs for Digital Twins

Digital twins, which are virtual replicas of physical systems, have gained significant attention in recent years due to their potential to enhance system monitoring, prediction, and optimization. One of the most promising approaches for developing digital twins is the use of data-driven physics-informed neural networks (PINNs). These networks integrate physical laws with data-driven learning, enabling the creation of accurate and efficient digital twins for complex fluid mechanics systems. In this subsection, we explore the use of data-driven PINNs for digital twins, emphasizing adaptive sampling, multi-fidelity learning, and uncertainty quantification.

Adaptive sampling is a critical technique in the development of data-driven PINNs for digital twins. Traditional numerical methods often require a large number of collocation points to ensure the accuracy of the solution. However, this can be computationally expensive, especially for high-dimensional problems. Data-driven PINNs address this issue by dynamically adjusting the placement of collocation points based on the solution's complexity. This approach ensures that the model focuses on regions where the solution is more uncertain or complex, leading to improved accuracy and efficiency. For instance, the RANG method, which is a residual-based adaptive node generation approach, has been shown to significantly reduce the number of collocation points required for accurate solutions while maintaining high accuracy [189]. This adaptive sampling strategy is particularly beneficial for digital twins, as it allows for the efficient use of computational resources while ensuring the model's reliability.

Multi-fidelity learning is another important aspect of data-driven PINNs for digital twins. In many real-world applications, data of varying fidelity levels are available, ranging from high-fidelity simulations to low-fidelity measurements. Multi-fidelity learning leverages this information to improve the accuracy and efficiency of the PINN model. By incorporating data from multiple sources, the model can learn to approximate the solution more effectively, even in the presence of noisy or incomplete data. The SVD-PINNs method, which uses singular value decomposition to transfer knowledge between different PDEs, is an example of a multi-fidelity learning approach that can be applied to digital twins [134]. This method has been shown to be effective in solving a class of PDEs with different but close right-hand-side functions, making it a valuable tool for digital twin applications.

Uncertainty quantification is essential for ensuring the reliability and robustness of data-driven PINNs in digital twin applications. Physical systems are often subject to uncertainties due to factors such as measurement errors, model inaccuracies, and environmental variations. Data-driven PINNs can incorporate these uncertainties into the model by using probabilistic methods and Bayesian inference. This allows the model to not only predict the solution but also quantify the uncertainty associated with the predictions. The use of Bayesian PINNs, which incorporate prior knowledge and update the model based on new data, is a promising approach for uncertainty quantification in digital twins. By providing a measure of confidence in the predictions, Bayesian PINNs can help users make more informed decisions and improve the overall reliability of the digital twin.

The integration of adaptive sampling, multi-fidelity learning, and uncertainty quantification in data-driven PINNs has the potential to significantly enhance the capabilities of digital twins in fluid mechanics. For example, in the context of digital twins for turbulent flow simulations, data-driven PINNs can leverage adaptive sampling to focus on regions with high turbulence intensity, multi-fidelity learning to incorporate data from both high-fidelity simulations and low-fidelity measurements, and uncertainty quantification to provide confidence intervals for the predictions. This combination of techniques can lead to more accurate and reliable digital twins, which can be used for real-time monitoring, predictive maintenance, and optimization of complex fluid systems.

Recent studies have demonstrated the effectiveness of data-driven PINNs in various digital twin applications. For instance, the use of PINNs in the simulation of blood flow in arteries has shown promising results, with the model being able to accurately predict velocity fields and pressure distributions [39]. The application of PINNs in the context of digital twins for complex fluid systems has also been explored in the literature, with the focus on improving the accuracy and efficiency of the models through the use of advanced techniques such as adaptive sampling and multi-fidelity learning [12]. These studies highlight the potential of data-driven PINNs to revolutionize the development of digital twins in fluid mechanics.

In conclusion, the use of data-driven PINNs for digital twins in fluid mechanics presents a powerful approach to enhance the accuracy, efficiency, and reliability of virtual replicas of physical systems. By incorporating adaptive sampling, multi-fidelity learning, and uncertainty quantification, data-driven PINNs can address the challenges associated with complex fluid dynamics problems. The successful application of these techniques in various studies demonstrates their potential to transform the way digital twins are developed and utilized in the field of fluid mechanics. As research in this area continues to advance, the integration of data-driven PINNs into digital twin frameworks is expected to lead to significant improvements in system monitoring, prediction, and optimization.

### 7.38 Mixed Formulation of PINNs for Thermo-Mechanical Systems

The mixed formulation of Physics-Informed Neural Networks (PINNs) for thermo-mechanically coupled systems represents a significant advancement in the integration of physical principles with deep learning techniques. Thermo-mechanical systems involve the coupling of thermal and mechanical behaviors, which often leads to complex, nonlinear partial differential equations (PDEs) that are challenging to solve using traditional numerical methods. PINNs offer a promising alternative by embedding the governing equations directly into the neural network architecture, allowing for the simultaneous approximation of both thermal and mechanical fields while ensuring the satisfaction of physical constraints.

In the context of thermo-mechanical systems, the mixed formulation of PINNs typically involves the use of first-order derivatives to capture the interactions between the thermal and mechanical components of the system. This approach leverages the ability of neural networks to approximate functions and their derivatives, which is critical for enforcing the PDEs that govern the system's behavior. By incorporating first-order derivatives into the loss function, the PINN can effectively minimize the residual of the governing equations across the entire domain, ensuring that the solution adheres to the underlying physical laws. This method is particularly effective in scenarios where the thermal and mechanical fields are strongly coupled, as it allows for a more accurate representation of the system's behavior.

The mixed formulation of PINNs for thermo-mechanical systems is often implemented through a sequential training strategy. Sequential training involves breaking down the complex problem into a series of simpler subproblems that can be addressed individually. This approach allows the PINN to progressively learn the solution to the coupled system by first focusing on the thermal field and then incorporating the mechanical field into the training process. This stepwise refinement ensures that the model can handle the intricate dependencies between the thermal and mechanical components, leading to more accurate and stable solutions.

One of the key advantages of the mixed formulation of PINNs is its ability to handle high-dimensional and multi-physics problems efficiently. By leveraging the power of neural networks, PINNs can approximate solutions to complex PDEs with a relatively small number of parameters, making them computationally efficient compared to traditional numerical methods. This efficiency is particularly beneficial in scenarios where the system's behavior is influenced by multiple physical phenomena, as it allows for the simultaneous solution of the governing equations without the need for extensive computational resources.

The mixed formulation of PINNs also benefits from the use of first-order derivatives in the loss function, which helps to ensure that the model accurately captures the gradients of the solution. This is crucial for problems involving thermal expansion, where the mechanical response is directly influenced by the temperature distribution. By explicitly incorporating the first-order derivatives into the loss function, the PINN can ensure that the solution satisfies the necessary conditions for continuity and differentiability, leading to more reliable and physically consistent results.

In addition to the use of first-order derivatives, the mixed formulation of PINNs often incorporates a sequential training approach to address the challenges of solving coupled thermo-mechanical systems. Sequential training involves training the PINN in stages, starting with the thermal field and then progressing to the mechanical field. This approach allows the model to gradually build up its understanding of the system's behavior, ensuring that each component is accurately captured before moving on to the next. This step-by-step process helps to prevent the model from becoming overwhelmed by the complexity of the coupled system, leading to more stable and accurate solutions.

The effectiveness of the mixed formulation of PINNs for thermo-mechanical systems has been demonstrated in various studies. For example, the paper titled "Error Estimates and Physics Informed Augmentation of Neural Networks for Thermally Coupled Incompressible Navier Stokes Equations" [61] presents a detailed analysis of the application of PINNs to coupled thermo-mechanical problems, highlighting the benefits of using first-order derivatives and sequential training. The authors demonstrate that the mixed formulation of PINNs can achieve high accuracy and efficiency, making it a viable alternative to traditional numerical methods for solving complex PDEs.

Moreover, the mixed formulation of PINNs offers several advantages over conventional numerical methods in terms of flexibility and adaptability. Traditional numerical methods often require extensive mesh generation and refinement, which can be computationally expensive and time-consuming. In contrast, PINNs can operate on unstructured grids and do not require a predefined mesh, making them more suitable for problems with complex geometries and varying boundary conditions. This flexibility is particularly important in thermo-mechanical systems, where the geometry and boundary conditions can be highly variable and difficult to model using traditional methods.

Another benefit of the mixed formulation of PINNs is its ability to handle high-frequency and high-dimensional problems. The use of first-order derivatives in the loss function allows the model to capture the rapid changes in the solution, which is essential for accurately representing the behavior of thermo-mechanical systems. Additionally, the sequential training approach ensures that the model can effectively learn the solution to the coupled system, even in the presence of complex interactions between the thermal and mechanical fields.

In conclusion, the mixed formulation of PINNs for thermo-mechanical systems represents a powerful approach to solving complex, coupled PDEs. By leveraging the use of first-order derivatives and sequential training, PINNs can accurately capture the interactions between thermal and mechanical fields, leading to more reliable and physically consistent solutions. The flexibility and adaptability of PINNs make them a promising alternative to traditional numerical methods, particularly in scenarios where the system's behavior is influenced by multiple physical phenomena. As research in this area continues to advance, the mixed formulation of PINNs is expected to play an increasingly important role in the simulation and analysis of thermo-mechanical systems.

### 7.39 Physics-Informed Deep Learning for Experimental Fluid Mechanics

Physics-Informed Deep Learning (PIDL) has emerged as a powerful tool in experimental fluid mechanics, particularly for the super-resolution of flow-field data. Traditional experimental techniques often face limitations in resolving high-resolution data due to the constraints of measurement devices, which can result in noisy and sparse data. Physics-Informed Neural Networks (PINNs) have been proposed as a solution to this challenge by leveraging both the physical laws governing fluid dynamics and the available data to reconstruct high-resolution flow fields. This approach has demonstrated significant potential in enhancing the accuracy and reliability of experimental fluid mechanics analyses by providing physically consistent predictions even from limited and noisy data.

One of the primary challenges in experimental fluid mechanics is the reconstruction of high-resolution flow data from sparse and noisy measurements. Techniques such as particle image velocimetry (PIV) or laser Doppler anemometry (LDA) often provide limited spatial and temporal resolution, making it difficult to capture the full complexity of fluid flows. PINNs address this issue by integrating the governing equations of fluid dynamics, such as the Navier-Stokes equations, into the neural network architecture. This integration ensures that the reconstructed flow fields not only match the available data but also adhere to the fundamental physical laws. For instance, the work by [11] demonstrated that PINNs can effectively reconstruct high-resolution flow fields from low-resolution and noisy measurements without requiring any high-resolution reference data. This capability is particularly valuable in experimental settings where obtaining high-resolution data is either impractical or impossible.

The application of PINNs in experimental fluid mechanics involves training the neural network to approximate the solution of the governing PDEs while minimizing the discrepancy between the predicted flow field and the available measurements. This is achieved through a loss function that incorporates both the data fidelity term and the physics-informed term. The data fidelity term ensures that the network accurately reproduces the observed data, while the physics-informed term enforces the constraints imposed by the governing equations. By combining these two components, PINNs can produce solutions that are not only consistent with the measurements but also physically meaningful. The study by [11] showcased the effectiveness of this approach by applying PINNs to three canonical cases: Burgers' equation, two-dimensional vortex shedding behind a circular cylinder, and the minimal turbulent channel flow. The results demonstrated that PINNs can accurately reconstruct the flow fields, even in the presence of synthetic Gaussian noise, highlighting their robustness and reliability in experimental settings.

Another key advantage of PINNs in experimental fluid mechanics is their ability to handle the inherent uncertainties in measurement data. Experimental measurements are often subject to noise and errors, which can significantly affect the accuracy of the reconstructed flow fields. PINNs mitigate this issue by incorporating the physical constraints into the training process, which helps to regularize the solution and reduce the impact of noise. For example, [11] demonstrated that PINNs can effectively reduce the noise in experimental datasets, such as hot-wire anemometry measurements, by leveraging the physical laws governing the flow. This capability is crucial for applications where the quality of the data is limited, as it ensures that the reconstructed flow fields remain physically consistent and reliable.

The use of PINNs for super-resolution of flow-field data has also been extended to more complex and realistic scenarios. For instance, the study by [11] explored the application of PINNs to the super-resolution of flow-field data in time and space, demonstrating their effectiveness in reconstructing high-resolution data from a limited set of measurements. This work highlighted the ability of PINNs to capture the temporal and spatial variations of the flow field, which is essential for understanding the dynamics of complex fluid systems. The results showed that PINNs can provide continuous and physically consistent predictions of the flow field, enabling researchers to analyze the flow behavior in greater detail.

In addition to their ability to reconstruct high-resolution flow fields, PINNs have also been used to enhance the resolution and accuracy of experimental data. By integrating the physical laws into the neural network architecture, PINNs can identify and correct for errors in the measurements, leading to more accurate and reliable results. This is particularly beneficial in applications where the quality of the data is critical, such as in biomedical fluid mechanics or environmental fluid dynamics. The work by [11] demonstrated that PINNs can significantly improve the resolution and accuracy of experimental datasets, making them a valuable tool for researchers in these fields.

The effectiveness of PINNs in experimental fluid mechanics is further supported by their ability to generalize to new and unseen scenarios. Unlike traditional data-driven models, which often require large amounts of training data to achieve good performance, PINNs can leverage the physical constraints to learn the underlying patterns in the data more efficiently. This makes them particularly well-suited for applications where the available data is limited or sparse. For example, the study by [11] showed that PINNs can achieve accurate predictions even when trained on a small subset of the data, demonstrating their potential for real-world applications where data collection is challenging.

In conclusion, the application of PINNs in experimental fluid mechanics has shown great promise in addressing the challenges of super-resolution of flow-field data. By integrating the physical laws governing fluid dynamics with the available measurements, PINNs can reconstruct high-resolution flow fields that are both accurate and physically consistent. This capability is particularly valuable in experimental settings where the quality and resolution of the data are limited, as it enables researchers to gain deeper insights into the complex dynamics of fluid flows. The work by [11] and other related studies has demonstrated the effectiveness of PINNs in this domain, highlighting their potential to revolutionize the way experimental fluid mechanics is conducted.

### 7.40 Metalearning for Parameterized PDEs with PINNs

[82]

Metalearning, also known as "learning to learn," has emerged as a powerful paradigm for accelerating the training of physics-informed neural networks (PINNs), especially when dealing with parameterized partial differential equations (PDEs). The core idea of metalearning is to leverage prior knowledge from related tasks to improve the performance of a model on new, unseen tasks. In the context of PINNs, this means training a model on a set of related PDEs or parameterized problems, and then using that knowledge to rapidly adapt to new parameterized PDEs with minimal additional training. This approach is particularly valuable in fluid mechanics, where the same governing equations may be solved with varying boundary conditions, initial conditions, or parameters, such as different Reynolds numbers or material properties [148].

One of the key challenges in training PINNs for parameterized PDEs is the high computational cost associated with retraining the network from scratch for each new set of parameters or conditions. This is especially problematic when dealing with high-dimensional or complex PDEs, where the training process can be both time-consuming and resource-intensive. To address this, researchers have proposed metalearning strategies that allow PINNs to learn the underlying patterns and relationships across different parameterized PDEs, thereby reducing the need for extensive retraining [148].

A metalearning approach for PINNs involves two main phases: the meta-training phase and the meta-testing phase. During the meta-training phase, the model is exposed to a diverse set of parameterized PDE problems. The goal is to learn a generalizable representation of the solution space, which can then be used to quickly adapt to new problems. This is typically achieved by training the PINN on a variety of PDEs with different parameter configurations and using techniques such as gradient-based metalearning or model-agnostic metalearning (MAML) to optimize the learning process. The meta-testing phase involves applying the learned model to new, unseen parameterized PDEs, where the model should be able to generalize and provide accurate solutions with minimal additional training.

The integration of metalearning with PINNs has been shown to significantly improve the efficiency and effectiveness of solving parameterized PDEs. For instance, in a study focused on parameterized PDEs, researchers proposed a metalearning framework that leverages the concept of transfer learning to accelerate the training of PINNs. By pretraining the model on a set of related PDEs, the PINN is able to converge faster and achieve higher accuracy when applied to new problems [35]. This approach not only reduces the computational burden but also enhances the robustness of the model, as it learns to handle a wide range of parameter configurations.

One of the key advantages of using metalearning for parameterized PDEs is the ability to generalize across different domains and conditions. For example, a PINN trained on a set of fluid flow problems with varying Reynolds numbers can be adapted to new flow scenarios with minimal additional training. This is particularly useful in real-world applications, where the parameters of the system may change over time or vary between different scenarios. By leveraging metalearning, PINNs can quickly adapt to these changes, making them more versatile and efficient for practical use.

Another benefit of metalearning is its potential to reduce the amount of data required for training. Traditional PINNs often rely on large datasets to achieve accurate results, which can be challenging to obtain in certain applications. However, metalearning allows the model to learn from a smaller number of examples by leveraging the knowledge gained from previous tasks. This is particularly valuable in scenarios where data collection is expensive or difficult, such as in experimental fluid dynamics or biomedical applications [35].

In addition to improving the training efficiency and generalization capability of PINNs, metalearning also opens up new possibilities for exploring complex and high-dimensional parameter spaces. By training the model on a diverse set of parameterized PDEs, researchers can gain insights into the relationships between different parameters and their impact on the solution. This can lead to the development of more accurate and efficient models, as well as a deeper understanding of the underlying physical processes.

Furthermore, metalearning can be combined with other techniques, such as transfer learning and domain adaptation, to further enhance the performance of PINNs. For example, a PINN trained on a specific set of parameterized PDEs can be fine-tuned on a related but slightly different problem, allowing it to adapt to new conditions while retaining the knowledge gained from the original training. This approach not only improves the model's performance but also reduces the amount of data required for the new task [35].

In conclusion, the application of metalearning to parameterized PDEs with PINNs represents a promising direction for improving the efficiency, accuracy, and versatility of these models. By leveraging prior knowledge and learning from a diverse set of tasks, metalearning enables PINNs to adapt quickly to new problems and achieve high performance with minimal additional training. As research in this area continues to evolve, we can expect to see even more advanced and sophisticated metalearning techniques that further enhance the capabilities of PINNs in fluid mechanics and other scientific domains.

### 7.41 Causality in PINN Training

Causality in PINN Training is a critical aspect that has gained increasing attention in the realm of physics-informed neural networks. The integration of physical laws into neural networks through the formulation of loss functions is essential for ensuring accurate and reliable solutions to partial differential equations (PDEs). However, the training of PINNs often encounters challenges related to the causality of the solutions, particularly in problems where the underlying physical processes exhibit complex and non-linear behaviors. Respecting causality in the training of PINNs is not just a theoretical consideration; it has practical implications for the accuracy and robustness of the solutions generated by these models.

One of the primary challenges in PINN training is the potential for the model to learn non-physical solutions, which can lead to incorrect predictions and undermine the reliability of the model. This issue arises because the loss function in PINNs typically includes terms that penalize deviations from the governing PDEs and boundary conditions. However, these terms may not always account for the physical causality inherent in the problem. For instance, in problems involving fluid dynamics, the flow field must satisfy the conservation laws of mass, momentum, and energy. If the training process does not properly account for these physical constraints, the resulting solutions may not reflect the true physical behavior of the system.

To address this issue, researchers have proposed various strategies to reformulate the loss functions of PINNs to better account for physical causality. One such approach is the use of causality-aware loss functions, which explicitly incorporate the temporal and spatial dependencies of the physical processes into the training objective. By doing so, these loss functions can ensure that the model's predictions are consistent with the physical principles governing the problem, thereby improving the accuracy and reliability of the solutions.

For example, the paper titled "Temporal Consistency Loss for Physics-Informed Neural Networks" highlights the importance of considering the temporal causality in the training of PINNs for time-dependent PDEs. The authors emphasize that the loss function should not only penalize deviations from the PDE residuals but also account for the temporal evolution of the solution. By reformulating the loss function to incorporate causality, the model can better capture the dynamic behavior of the physical system, leading to more accurate and physically consistent predictions.

Another approach to addressing causality in PINN training is the use of causal inference techniques. These techniques aim to identify and incorporate the causal relationships between different variables in the physical system. By explicitly modeling these causal relationships, the loss function can be designed to prioritize solutions that are consistent with the underlying physical laws. For instance, the paper titled "Causality in PINN Training" discusses the application of causal inference to PINNs for solving problems involving fluid dynamics. The authors propose a causal loss function that explicitly accounts for the causal relationships between the velocity, pressure, and other physical quantities in the system, leading to more accurate and physically consistent predictions.

In addition to reformulating the loss function, the design of the neural network architecture can also play a role in respecting causality in PINN training. For instance, the use of recurrent neural networks (RNNs) or other architectures that are capable of capturing temporal dependencies can help ensure that the model's predictions are consistent with the temporal causality of the physical system. The paper titled "Causality in PINN Training" explores the use of RNNs in PINNs for solving time-dependent PDEs. The authors demonstrate that the use of RNNs can improve the model's ability to capture the temporal dynamics of the system, leading to more accurate and physically consistent predictions.

Moreover, the choice of training data and the way in which it is processed can also impact the ability of PINNs to respect causality. For example, the use of data that is representative of the physical system's behavior and that captures the temporal and spatial dependencies of the system can help ensure that the model's predictions are consistent with the underlying physical laws. The paper titled "Causality in PINN Training" highlights the importance of using high-quality data that reflects the true physical behavior of the system. The authors argue that the use of noisy or incomplete data can lead to the model learning non-physical solutions, which can undermine the reliability of the predictions.

Furthermore, the paper titled "Causality in PINN Training" discusses the importance of incorporating domain knowledge into the training process. This can be achieved by using prior knowledge about the physical system to guide the training of the model. For instance, the use of physical constraints and conservation laws can help ensure that the model's predictions are consistent with the underlying physical principles. The authors demonstrate that the incorporation of domain knowledge can significantly improve the accuracy and reliability of the predictions generated by the model.

In conclusion, respecting causality in PINN training is essential for ensuring the accuracy and reliability of the solutions generated by these models. By reformulating the loss functions to account for physical causality, incorporating causal inference techniques, and designing the neural network architecture to capture temporal dependencies, researchers can improve the ability of PINNs to generate physically consistent predictions. The insights and methodologies presented in the paper titled "Causality in PINN Training" provide valuable guidance for the development of more robust and accurate PINNs for solving complex PDEs in fluid dynamics and other fields [206].

### 7.42 PINNs for THM Processes in Porous Media

Physics-Informed Neural Networks (PINNs) have shown great promise in solving complex problems governed by partial differential equations (PDEs), particularly in the realm of multi-physics simulations. One such area where PINNs have been applied is in the simulation of thermo-hydro-mechanical (THM) processes in porous media. THM processes involve the coupled interactions of thermal, hydraulic, and mechanical phenomena, and they are critical in various engineering and geoscientific applications, such as geothermal energy extraction, carbon sequestration, and reservoir engineering. The integration of these coupled processes into a single modeling framework presents significant challenges, but PINNs offer a promising approach by leveraging their ability to encode physical laws directly into the neural network structure. 

A notable contribution in this area is the work described in the paper titled "Physics-informed neural network solution of thermo-hydro-mechanical (THM) processes in porous media" [147]. This paper explores the application of PINNs to the forward solution of THM problems in porous media, which exhibit disparate spatial and temporal scales in thermal conductivity, hydraulic permeability, and elasticity. The authors address the challenges of the multi-objective and non-convex nature of the optimization problem by proposing a sequential training strategy. This strategy circumvents the need for a simultaneous solution of the multi-physics problem and facilitates the task of optimizers in the solution search. Additionally, the authors leverage adaptive weight strategies to overcome the stiffness in the gradient flow of the multi-objective optimization problem, demonstrating that this approach significantly improves the robustness and accuracy of the PINN solutions.

One of the primary challenges in applying PINNs to THM processes is the need to handle multiple, often conflicting, objectives simultaneously. THM processes involve a combination of thermal, hydraulic, and mechanical equations, each with its own set of boundary conditions and initial conditions. The integration of these equations into a single PINN framework requires careful balancing of the respective loss terms to ensure that all physical constraints are satisfied. The paper titled "Multi-Objective Loss Balancing for Physics-Informed Deep Learning" [54] discusses the importance of correctly weighting the combination of multiple competitive loss functions for training PINNs effectively. The authors implement and evaluate different methods to balance the contributions of multiple terms of the PINNs loss function and their gradients, ultimately proposing a novel self-adaptive loss balancing scheme that consistently outperforms existing scaling methods in terms of accuracy and computational efficiency.

Another challenge in the application of PINNs to THM processes is the need for accurate and efficient numerical methods to handle the complexity of the coupled equations. Traditional numerical methods, such as finite element or finite volume methods, are well-suited for solving such problems but can be computationally expensive, especially for large-scale simulations. The paper "Physics-informed neural networks for solving thermo-mechanics problems of functionally graded material" [169] highlights the potential of PINNs to provide mesh-free solutions that are both accurate and efficient. However, the authors note that achieving this level of accuracy requires careful design of the neural network architecture and the loss function to ensure that the underlying physics is properly captured.

To address these challenges, the authors in [147] propose a dimensionless form of the coupled governing equations that is best suited for deep-learning algorithms. This approach helps to normalize the problem and makes it more amenable to the training of PINNs. Additionally, the sequential training strategy allows the PINN to gradually build up the solution, starting with simpler subproblems and progressively tackling more complex ones. This method not only improves the convergence of the PINN but also enhances the accuracy of the solution by allowing the model to learn the interactions between the different physical processes in a structured manner.

The paper also emphasizes the importance of incorporating hard boundary constraints to ensure valid predictions. By enforcing these constraints, the PINN can avoid physically unrealistic solutions and improve the overall accuracy of the model. This approach is particularly important in THM processes, where the boundary conditions can have a significant impact on the solution. The authors demonstrate that the inclusion of these constraints leads to more reliable and accurate results, even in the presence of noisy or incomplete data.

Furthermore, the paper discusses the use of adaptive weight strategies to overcome the stiffness in the gradient flow of the multi-objective optimization problem. By dynamically adjusting the weights assigned to different loss terms, the PINN can effectively balance the competing objectives and achieve a more stable and accurate solution. This strategy is particularly useful in THM processes, where the different physical phenomena can have vastly different scales and sensitivities, making it difficult to find a single set of weights that works well for all terms.

The application of PINNs to THM processes in porous media represents a significant advancement in the field of computational geomechanics. By leveraging the power of deep learning and the ability to encode physical laws directly into the model, PINNs offer a flexible and efficient alternative to traditional numerical methods. The strategies proposed in the literature, such as sequential training, adaptive weight strategies, and the use of dimensionless forms, provide a robust framework for solving these complex multi-physics problems. As research in this area continues to evolve, it is expected that PINNs will play an increasingly important role in the simulation and analysis of THM processes in porous media.

## 8 Future Directions and Research Opportunities

### 8.1 Integration of PINNs with Quantum Computing

The integration of Physics-Informed Neural Networks (PINNs) with quantum computing presents a promising frontier for advancing the solution of complex fluid mechanics problems. Quantum computing, with its inherent ability to perform parallel computations and represent states in superposition, offers unique advantages that could significantly enhance the efficiency and accuracy of PINNs. This subsection explores the potential of this integration, the challenges it poses, and the opportunities it presents for solving complex fluid dynamics problems.

One of the primary motivations for exploring this integration is the ability of quantum computing to handle high-dimensional data and complex mathematical structures more efficiently than classical computing. PINNs, which rely on deep neural networks to approximate solutions to partial differential equations (PDEs), often face limitations in terms of computational efficiency and scalability, particularly for problems involving high-dimensional spaces or complex geometries. Quantum circuits, which can be designed to perform operations in parallel, have the potential to accelerate the training process of PINNs. For instance, quantum circuits can be used to compute gradients more efficiently, which is a critical step in the training of neural networks. This could lead to faster convergence and better performance of PINNs in solving fluid dynamics problems [8].

Furthermore, the concept of quantum neural networks (QNNs) could be leveraged to enhance the capabilities of PINNs. QNNs are neural networks that utilize quantum mechanics to process information, potentially leading to more efficient and accurate solutions. By integrating QNNs with PINNs, the resulting architecture could benefit from both the expressive power of deep learning and the parallelism offered by quantum computing. For example, QNNs could be used to approximate the solution of PDEs more effectively, especially for problems involving non-linear dynamics or high-frequency components. The ability of QNNs to represent complex probability distributions could also be advantageous in capturing the uncertainties inherent in fluid dynamics simulations [8].

Hybrid quantum-classical architectures also present an exciting opportunity for enhancing PINNs. These architectures combine the strengths of classical computing and quantum computing, allowing for the efficient execution of tasks that are infeasible for either system alone. In the context of PINNs, hybrid architectures could be used to handle the training of neural networks on classical hardware while leveraging quantum computing for specific tasks such as solving PDEs or optimizing the loss function. This approach could lead to a more efficient and scalable solution for fluid mechanics problems, as quantum computing can handle certain aspects of the problem more efficiently while classical computing manages the rest [8].

However, the integration of PINNs with quantum computing also presents several challenges. One of the primary challenges is the current limitations of quantum hardware. Most quantum computers today are noisy and have limited qubit counts, which can restrict the complexity of the problems that can be solved. This means that the full potential of quantum computing in enhancing PINNs may not be realized until more advanced quantum hardware becomes available. Additionally, the development of quantum algorithms that can effectively solve PDEs is still in its early stages, and further research is needed to establish a robust theoretical foundation [8].

Another challenge is the need for new algorithms and frameworks that can seamlessly integrate quantum computing with PINNs. This requires a deep understanding of both quantum computing and deep learning, as well as the ability to design algorithms that can leverage the strengths of both domains. For example, the development of quantum-enhanced optimization techniques could be crucial in improving the training efficiency of PINNs. These techniques could involve the use of quantum annealing or other quantum optimization methods to find the optimal parameters for the neural network more efficiently [8].

Despite these challenges, the opportunities for integrating PINNs with quantum computing are vast. One of the most promising areas is the potential for quantum computing to enable the solution of problems that are currently intractable for classical computing. For instance, quantum computing could be used to solve complex fluid dynamics problems that involve high-dimensional spaces or intricate geometries, which are challenging for classical PINNs. The ability of quantum computing to handle such problems could lead to breakthroughs in the field of fluid mechanics, enabling the simulation of complex flows and the prediction of fluid behavior with greater accuracy [8].

Moreover, the integration of PINNs with quantum computing could lead to the development of new methods for solving inverse problems in fluid mechanics. Inverse problems, which involve determining the parameters or inputs of a system based on observed outputs, are often challenging for classical methods. Quantum computing could provide a new approach to solving these problems by enabling the efficient exploration of the parameter space and the accurate estimation of the unknown parameters. This could lead to more robust and accurate models for fluid dynamics, with applications in a wide range of engineering and scientific domains [8].

In conclusion, the integration of PINNs with quantum computing holds significant promise for advancing the solution of complex fluid mechanics problems. While there are challenges to overcome, such as the limitations of current quantum hardware and the need for new algorithms, the potential benefits of this integration are substantial. By leveraging the strengths of both quantum computing and PINNs, researchers can develop more efficient and accurate methods for solving PDEs and other complex problems in fluid mechanics. This interdisciplinary approach could lead to new breakthroughs in the field, opening up new avenues for research and application in scientific computing and engineering.

### 8.2 Enhanced Interpretability of PINNs

Enhancing the interpretability of Physics-Informed Neural Networks (PINNs) is a critical research direction that can significantly improve their utility in fluid mechanics applications. While PINNs have demonstrated remarkable success in solving partial differential equations (PDEs) and integrating physical laws into data-driven models, their "black-box" nature often limits the ability to understand how these networks learn and apply physical principles. This lack of interpretability can hinder the adoption of PINNs in safety-critical applications, where transparency and accountability are essential. Therefore, improving the interpretability of PINNs is a key step toward making them more trustworthy, reliable, and widely applicable in scientific and engineering domains.

One of the primary approaches to enhancing the interpretability of PINNs is through the visualization of learned physics. By visualizing the outputs of PINNs, researchers can gain insights into how the networks approximate physical laws and identify potential discrepancies between the learned solutions and the expected physical behavior. For instance, visualization techniques can be used to analyze the spatial and temporal distribution of physical quantities such as velocity, pressure, and temperature in fluid flow simulations. This not only helps in validating the accuracy of PINN solutions but also provides a deeper understanding of the underlying physical phenomena. As noted in the work by [64], visualization of the solution fields can reveal regions where the network struggles to capture high-frequency dynamics or where the residual errors are concentrated, offering valuable feedback for model refinement.

Another promising technique for improving interpretability is feature importance analysis, which aims to identify the most influential input features or physical constraints that contribute to the PINN's predictions. In fluid mechanics, this could involve analyzing how different components of the PDE, such as the convection, diffusion, or reaction terms, influence the final solution. By quantifying the importance of these terms, researchers can better understand the relative contributions of different physical mechanisms to the overall behavior of the system. For example, [63] highlights how imbalances in the loss terms can lead to suboptimal training, and feature importance analysis can help in diagnosing such issues by revealing which physical constraints are most critical for the model's performance.

Model-agnostic interpretability tools also play a significant role in enhancing the transparency of PINNs. These tools, which are not tied to the specific architecture of the network, can be used to analyze the decision-making process of PINNs regardless of their design. Techniques such as saliency maps, SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations) have been widely used in the field of explainable AI to provide insights into how neural networks make predictions. For instance, saliency maps can highlight the regions of the input domain that have the greatest impact on the network's output, while SHAP and LIME can provide local explanations that help in understanding how different input features contribute to the final solution. The application of such tools to PINNs can help researchers identify whether the networks are correctly incorporating the physical laws or if certain constraints are being overemphasized or neglected. As discussed in [149], gradient-based interpretability techniques can also be used to analyze the training dynamics of PINNs and detect potential issues such as vanishing gradients or unbalanced loss terms.

In addition to these techniques, the development of physics-informed interpretability frameworks that explicitly encode physical knowledge into the model's architecture can further enhance the interpretability of PINNs. For example, [128] introduces a framework that ensures the preservation of energy and stability properties, making it easier to interpret the network's behavior in terms of the underlying physical principles. Such frameworks not only improve the reliability of PINNs but also provide a more transparent representation of the physical laws that govern the system under study.

Another important aspect of enhancing interpretability is the integration of domain-specific knowledge into the training process. By explicitly incorporating known physical constraints, such as conservation laws or symmetry conditions, into the loss function, PINNs can be guided to learn solutions that are not only accurate but also physically meaningful. This can be achieved through the use of physics-informed loss functions, which penalize deviations from the governing equations and ensure that the network's predictions are consistent with the underlying physics. As demonstrated in [43], such an approach can lead to more interpretable solutions by ensuring that the network adheres to the known physical laws.

Finally, the use of hybrid models that combine PINNs with traditional numerical methods can also enhance interpretability by providing a bridge between data-driven and physics-based approaches. These hybrid models can leverage the strengths of both methods, allowing researchers to validate the predictions of PINNs against established numerical solutions and gain a deeper understanding of the physical mechanisms at play. For example, [207] discusses how PINNs can be combined with finite element or finite volume methods to improve their accuracy and reliability, while also providing a more interpretable framework for analyzing the results.

In conclusion, the enhancement of interpretability in PINNs is a crucial step toward making these models more transparent, reliable, and widely applicable in fluid mechanics. By leveraging visualization techniques, feature importance analysis, model-agnostic interpretability tools, and physics-informed frameworks, researchers can gain deeper insights into how PINNs learn and apply physical laws. These efforts not only improve the trustworthiness of PINNs but also pave the way for their broader adoption in scientific and engineering applications. As the field of physics-informed machine learning continues to evolve, the development of more interpretable models will be essential in realizing the full potential of PINNs in solving complex fluid dynamics problems.

### 8.3 Scalability for Large-Scale Fluid Mechanics Problems

Scaling Physics-Informed Neural Networks (PINNs) to handle large-scale and high-dimensional fluid mechanics problems remains a critical challenge in the field. While PINNs have shown great promise in solving partial differential equations (PDEs) with sparse data and integrating physical laws into neural networks, their scalability is limited by computational costs, training complexity, and the need for efficient architectures. Addressing these challenges is essential to enable PINNs to solve real-world engineering and scientific problems, particularly those involving complex geometries, multi-scale dynamics, and large domains.

One of the primary barriers to scaling PINNs is the computational cost associated with training large neural networks on high-dimensional PDEs. As the number of collocation points increases, the training time and memory requirements grow substantially. This is particularly problematic for high-Reynolds-number turbulent flows, where the solution requires resolving small-scale structures. To mitigate this, researchers have explored distributed training and parallel computing strategies. For example, the study titled *Distributed PINNs for Data-Efficient Solutions to PDEs* [135] proposes a distributed PINN (DPINN) framework that partitions the computational domain into subdomains and trains separate neural networks on each subdomain. This approach not only reduces the computational load on individual networks but also allows for parallel processing, significantly improving training efficiency. By leveraging distributed computing frameworks, DPINN demonstrates the potential for scaling PINNs to handle large-scale problems.

Another approach to improving scalability is the design of efficient neural architectures tailored for fluid mechanics problems. Traditional PINN architectures often rely on fully connected networks, which may not be optimal for high-dimensional PDEs. Studies such as *MRF-PINN: A Multi-Receptive-Field Convolutional Physics-Informed Neural Network for Solving Partial Differential Equations* [48] propose convolutional PINNs (MRF-PINN) that exploit parameter sharing and spatial feature extraction. This architecture improves performance on PDEs with different mesh resolutions and can handle high-order finite differences without manual tuning. By incorporating convolutional layers, MRF-PINN achieves better scalability and accuracy compared to conventional fully connected PINNs. Such architectures are particularly useful for problems with spatially varying solution features, such as turbulent flows or multi-phase systems.

Additionally, techniques such as domain decomposition and adaptive sampling have been explored to enhance the scalability of PINNs. For instance, the *Finite Basis Physics-Informed Neural Networks (FBPINNs)* [208] paper introduces a domain decomposition approach inspired by classical finite element methods. In FBPINNs, the solution is approximated as a sum of basis functions defined over overlapping subdomains, and neural networks learn these basis functions. This approach reduces the complexity of the optimization problem by breaking it into smaller subproblems, making it more tractable for large-scale PDEs. Furthermore, the paper highlights that FBPINNs can address the spectral bias of neural networks by normalizing inputs across subdomains, leading to improved convergence and accuracy.

Another critical aspect of scalability is the efficiency of training algorithms. PINNs often suffer from non-convex loss landscapes, where the training process can get stuck in local minima or converge slowly. To address this, researchers have developed methods such as adaptive sampling and physics-informed activation functions. For example, *A Novel Adaptive Causal Sampling Method for Physics-Informed Neural Networks* [49] introduces a causal sampling strategy that incorporates temporal consistency into the selection of collocation points. This approach ensures that the training process remains stable and efficient, even when dealing with high-dimensional PDEs. Similarly, *Physics-Informed Deep Learning for Incompressible Laminar Flows* [1] demonstrates the effectiveness of mixed-variable PINN schemes in improving trainability and solution accuracy, making it more scalable for complex flow simulations.

The integration of PINNs with traditional numerical methods also offers promising avenues for improving scalability. Hybrid models that combine PINNs with finite element or finite volume methods can leverage the strengths of both approaches. For example, the *Hybrid PINNs for Multi-Physics Problems* [29] paper discusses the use of domain decomposition and sequential training strategies to solve coupled systems of PDEs. By combining PINNs with classical solvers, these hybrid models can reduce computational costs while maintaining the accuracy of the solution. This approach is particularly useful for problems involving multiple physical phenomena, such as thermo-mechanical coupling or multi-phase flows.

In addition, the use of transfer learning and meta-learning has shown potential in enhancing the scalability of PINNs. *Hypernetwork-Based Meta-Learning for Low-Rank Physics-Informed Neural Networks* [145] proposes a low-rank PINN architecture that can be efficiently trained for different PDE input parameters. This approach significantly reduces the training time and computational resources required, making it suitable for large-scale problems with varying parameters. Similarly, *A Metalearning Approach for Physics-Informed Neural Networks (PINNs) Application to Parameterized PDEs* [148] explores the use of meta-learning to accelerate the training process for parameterized PDEs, further improving the scalability of PINNs.

Overall, the scalability of PINNs for large-scale fluid mechanics problems requires a multi-faceted approach that includes efficient architectures, distributed training, adaptive sampling, and the integration of classical numerical methods. As research in this area continues to advance, the development of scalable PINN frameworks will play a crucial role in enabling their widespread adoption in engineering and scientific applications.

### 8.4 Development of Robust and Efficient Training Algorithms

[82]

The development of robust and efficient training algorithms for Physics-Informed Neural Networks (PINNs) is a critical area of research in the field of fluid mechanics. PINNs have shown great promise in solving partial differential equations (PDEs) by integrating physical laws into the neural network training process. However, the training of PINNs is often plagued by issues such as non-convex loss landscapes, slow convergence, and instability, which hinder their practical applicability. To address these challenges, researchers have been exploring various emerging techniques to enhance the convergence and reliability of PINN training, including adaptive sampling, multi-fidelity learning, physics-informed activation functions, and advanced optimization strategies.

Adaptive sampling is one of the most promising approaches to improve the training efficiency of PINNs. Traditional PINNs rely on uniformly distributed collocation points, which may not adequately capture the complex behavior of PDEs, especially in regions with high gradients or sharp transitions. Adaptive sampling techniques dynamically adjust the placement of collocation points based on the solution's characteristics, ensuring that the neural network focuses on critical regions where the physical constraints are most challenging to satisfy. For instance, the failure-informed adaptive sampling strategy [34] dynamically identifies failure regions in the solution domain and refines the sampling density in those areas to improve accuracy and convergence. Similarly, the R3 sampling method [34] and RANG [97] adaptively select collocation points to enhance the PINN's ability to learn the underlying physical laws. These techniques help to balance the trade-off between computational cost and solution accuracy, making PINNs more practical for real-world fluid mechanics applications.

Another significant advancement in improving PINN training is the integration of multi-fidelity learning. Multi-fidelity learning leverages data of varying fidelity levels to enhance the training process, particularly in scenarios where high-fidelity data is scarce or expensive to obtain. By combining low-fidelity and high-fidelity data, multi-fidelity learning enables PINNs to learn more effectively from limited or noisy data while maintaining accuracy. This approach has been shown to be particularly effective in fluid dynamics problems, where high-fidelity simulations can be computationally expensive. For example, the work by [27] demonstrates how multi-fidelity learning can be used to improve the generalization and accuracy of PINNs for solving Navier-Stokes equations. By training on a combination of low-fidelity and high-fidelity data, PINNs can better approximate the physical behavior of fluid flows without requiring excessive computational resources.

Physics-informed activation functions are another innovative approach to enhance the robustness and efficiency of PINN training. Traditional neural networks use generic activation functions, such as ReLU or sigmoid, which may not incorporate the underlying physical laws that govern the system. Physics-informed activation functions (PAFs) are designed to encode the physical constraints directly into the neural network architecture, leading to more accurate and stable solutions. For instance, the work by [72] explores the use of PAFs that incorporate the conservation laws and boundary conditions of the fluid dynamics problem. These activation functions help the network to respect the physical properties of the system, resulting in more reliable and accurate predictions. Additionally, the use of PAFs can reduce the number of training iterations required to achieve convergence, making the training process more efficient.

Advanced optimization strategies are also essential for improving the training stability and efficiency of PINNs. The non-convex nature of the loss function in PINNs can lead to issues such as getting stuck in local optima or failing to converge. To address these challenges, researchers have been exploring various optimization techniques, including adaptive learning rate methods, second-order optimization algorithms, and hybrid optimization strategies. For example, the work by [34] demonstrates how the use of adaptive learning rates can significantly improve the convergence behavior of PINNs by dynamically adjusting the step size based on the gradient information. Additionally, the integration of second-order optimization methods, such as the Newton-Raphson method, can help to accelerate convergence and improve the stability of the training process. These advanced optimization strategies enable PINNs to better navigate the complex loss landscape, leading to more robust and efficient training.

In addition to these techniques, the development of hybrid models that combine PINNs with traditional numerical methods has shown great potential in improving the training efficiency and accuracy of PINNs. Hybrid models leverage the strengths of both data-driven and physics-based approaches, allowing PINNs to benefit from the accuracy of traditional solvers while maintaining the flexibility of neural networks. For instance, the work by [36] presents a framework that integrates PINNs with finite element methods to solve complex fluid dynamics problems. By combining the high accuracy of traditional solvers with the adaptability of neural networks, hybrid models can achieve better performance and stability in challenging scenarios.

The development of robust and efficient training algorithms for PINNs is a rapidly evolving field, with ongoing research exploring new techniques to address the limitations of current methods. As the application of PINNs in fluid mechanics continues to grow, the need for more reliable and efficient training strategies becomes increasingly important. By incorporating adaptive sampling, multi-fidelity learning, physics-informed activation functions, and advanced optimization strategies, researchers are making significant strides in improving the convergence and reliability of PINN training. These advancements will play a crucial role in enabling PINNs to become a mainstream tool for solving complex fluid dynamics problems, paving the way for more accurate and efficient simulations in the future.

### 8.5 Hybrid Models Combining PINNs with Traditional Numerical Methods

Hybrid models that combine Physics-Informed Neural Networks (PINNs) with traditional numerical methods such as finite element and finite volume methods represent a promising direction in fluid mechanics simulations. These hybrid approaches aim to leverage the strengths of both methodologies, combining the flexibility and data-driven nature of neural networks with the robustness and accuracy of conventional solvers. By integrating PINNs into traditional frameworks, researchers can achieve more accurate, efficient, and scalable solutions to complex fluid dynamics problems, particularly those involving high-dimensional and nonlinear partial differential equations (PDEs).

One of the key advantages of hybrid models is their ability to address the limitations of both PINNs and traditional numerical methods. For instance, PINNs are known for their ability to handle sparse data and incorporate physical constraints directly into the training process, which is particularly useful for inverse problems and scenarios where data is limited [22]. However, they often struggle with high-frequency components and may require extensive training time to converge. On the other hand, traditional numerical methods such as finite element and finite volume methods are well-established for solving PDEs, but they can be computationally expensive and require careful mesh design, which may not be feasible for complex geometries or high-dimensional problems. By integrating PINNs into these traditional methods, hybrid models can benefit from the accuracy of numerical solvers while improving efficiency through data-driven learning.

A notable example of such a hybrid approach is the integration of PINNs into finite element solvers. In this context, PINNs are used to approximate the solution of the PDEs, while the finite element method provides a structured framework for discretizing the domain and enforcing boundary conditions. This combination allows for the development of more accurate and efficient solvers, particularly for problems involving complex geometries and multiscale dynamics [48]. For instance, the MRF-PINN model leverages the parameter-sharing and spatial feature extraction capabilities of convolutional neural networks while incorporating the physical constraints of the PDEs through a loss function. This hybrid approach not only improves the accuracy of the solution but also reduces the computational cost associated with traditional solvers.

Another significant application of hybrid models is in the context of domain decomposition. Domain decomposition methods are widely used in traditional numerical methods to divide large problems into smaller, more manageable subproblems. By integrating PINNs into domain decomposition frameworks, hybrid models can further enhance the scalability and efficiency of fluid mechanics simulations. For example, the distributed PINN (DPINN) approach uses a parallel computing framework to solve non-linear PDEs such as the Navier-Stokes equations. This method demonstrates improved accuracy and efficiency compared to traditional PINN approaches, particularly in handling complex flow dynamics [135]. The integration of PINNs into domain decomposition strategies not only allows for the efficient distribution of computational resources but also ensures that the physical constraints of the PDEs are accurately enforced across the entire domain.

In addition to improving accuracy and efficiency, hybrid models also enhance the robustness of fluid mechanics simulations. PINNs are known to be sensitive to noise and errors in the data, which can lead to nonphysical solutions and unreliable predictions. By combining PINNs with traditional numerical methods, hybrid models can mitigate these issues by leveraging the stability and accuracy of the conventional solvers. For instance, the use of finite element methods as a reference can help in validating the results obtained from PINNs, ensuring that the solutions are physically consistent [61]. This approach not only improves the reliability of the predictions but also provides a framework for error estimation and convergence analysis, which are critical for ensuring the validity of the simulations.

The integration of PINNs with traditional numerical methods also opens up new possibilities for solving inverse problems in fluid mechanics. Inverse problems, such as estimating unknown parameters or boundary conditions from limited measurements, are often challenging due to the ill-posed nature of the problem. Hybrid models can address these challenges by combining the data-driven capabilities of PINNs with the structured approach of traditional solvers. For example, the use of PINNs in conjunction with finite element methods has been shown to be effective in solving inverse problems involving fluid flow and heat transfer [24]. By incorporating the physical constraints of the PDEs into the loss function, these hybrid models can accurately estimate the unknown parameters while ensuring that the solutions are consistent with the underlying physics.

Furthermore, hybrid models offer a flexible framework for handling multi-physics problems, where multiple physical phenomena are coupled and interact simultaneously. In such cases, traditional numerical methods often require complex formulations and specialized solvers, which can be computationally intensive. By integrating PINNs into these frameworks, hybrid models can leverage the adaptability of neural networks to handle different types of PDEs and coupling conditions. For instance, the mixed formulation of PINNs for thermo-mechanically coupled systems demonstrates how the integration of PINNs with traditional solvers can improve the accuracy and efficiency of simulations involving coupled heat and fluid flow [6]. This approach not only enhances the performance of the simulations but also provides a more comprehensive understanding of the underlying physical processes.

In conclusion, hybrid models that combine PINNs with traditional numerical methods represent a powerful approach for improving the accuracy, efficiency, and robustness of fluid mechanics simulations. By leveraging the strengths of both methodologies, these hybrid models can address the limitations of individual approaches and provide more reliable and scalable solutions to complex fluid dynamics problems. As research in this area continues to evolve, the integration of PINNs with traditional numerical methods is expected to play a crucial role in advancing the field of computational fluid dynamics and enabling new applications in engineering and scientific research.

### 8.6 Application of PINNs in Quantum-Enhanced Fluid Dynamics

The intersection of Physics-Informed Neural Networks (PINNs) and quantum-enhanced fluid dynamics represents a promising frontier where quantum computing technologies could significantly enhance the performance, accuracy, and efficiency of PINN-based simulations. Quantum computing, with its potential to process vast amounts of information in parallel and solve complex problems exponentially faster than classical computers, offers a unique opportunity to address some of the limitations of PINNs in fluid mechanics. This subsection explores how quantum computing can be harnessed to accelerate or enhance the performance of PINNs in fluid dynamics applications, focusing on quantum-assisted partial differential equation (PDE) solving, quantum-enhanced optimization, and the development of new computational paradigms for fluid flow analysis.

One of the most direct applications of quantum computing in the context of PINNs is in the acceleration of PDE solving. Traditional PINNs rely heavily on gradient-based optimization techniques, which can be computationally expensive, particularly for high-dimensional and complex fluid dynamics problems. Quantum computing, through quantum algorithms like the Quantum Phase Estimation (QPE) or Variational Quantum Eigensolver (VQE), can potentially speed up the solution of linear and nonlinear PDEs by leveraging quantum parallelism and interference effects. For instance, the quantum-assisted solution of the Navier-Stokes equations, a fundamental challenge in fluid mechanics, could benefit from quantum algorithms that efficiently solve the underlying systems of equations. This would not only reduce the computational time required for PINN training but also allow for the simulation of more complex and realistic fluid dynamics scenarios [10].

Another significant area where quantum computing can enhance PINNs is in optimization. PINNs often face challenges in training, such as non-convex loss landscapes, local minima, and poor convergence, especially when dealing with high-frequency or multi-scale problems. Quantum-enhanced optimization methods, such as Quantum Neural Networks (QNNs) or quantum-inspired variants of gradient descent algorithms, could provide more efficient ways to navigate these loss landscapes. For example, the use of quantum annealing or quantum-inspired metaheuristics could help PINNs converge faster and achieve better solutions, even with limited training data. Moreover, the integration of quantum computing with transfer learning techniques, as explored in some PINN studies [35], could further enhance the adaptability and generalization capabilities of PINNs in fluid dynamics.

Furthermore, quantum computing opens up the possibility of developing new computational paradigms for fluid flow analysis. Traditional PINNs are typically trained on classical hardware, which imposes limitations on the size and complexity of problems they can handle. Quantum computing, however, could enable the development of hybrid quantum-classical PINN frameworks, where the quantum computer handles the most computationally intensive parts of the problem, while the classical computer manages the rest. This approach could lead to the creation of more powerful and scalable PINN models for fluid mechanics. For instance, quantum circuits could be used to represent and manipulate the high-dimensional solution spaces of PDEs, while classical PINNs could handle the physics-informed constraints and boundary conditions. Such hybrid systems could revolutionize the way fluid dynamics problems are solved, enabling the simulation of previously intractable problems.

In addition to accelerating PDE solving and enhancing optimization, quantum computing could also play a role in the development of new types of PINN architectures. For example, quantum-inspired activation functions or quantum-based feature mapping techniques could be explored to improve the representational power of PINNs. These quantum-inspired techniques could help PINNs better capture the intricate and nonlinear relationships inherent in fluid dynamics problems, leading to more accurate and reliable simulations. Moreover, the integration of quantum computing with PINNs could also lead to the development of novel loss functions that incorporate quantum-inspired regularization terms, further improving the robustness and generalization capabilities of PINNs in fluid dynamics.

The potential of quantum-enhanced fluid dynamics with PINNs is not limited to theoretical exploration. Several studies have already begun to investigate the feasibility of integrating quantum computing with PINNs. For instance, research on the application of quantum computing to scientific machine learning [10] has shown that quantum algorithms can significantly improve the accuracy and efficiency of solving PDEs, particularly in the context of turbulent flows and high Reynolds number simulations. These findings suggest that the combination of quantum computing and PINNs could lead to breakthroughs in the field of computational fluid dynamics, enabling more accurate and efficient simulations of complex fluid flows.

Another promising direction is the use of quantum computing to address the challenges of data scarcity and noise in PINN-based fluid dynamics simulations. Many fluid mechanics problems involve sparse or noisy data, which can lead to poor generalization and inaccurate predictions by PINNs. Quantum computing, with its ability to process and analyze large volumes of data efficiently, could help improve the robustness of PINNs in such scenarios. By leveraging quantum algorithms for data processing and noise reduction, PINNs could be trained more effectively, even with limited or imperfect data. This could be particularly useful in applications such as real-time blood flow imaging or biomedical fluid dynamics, where high-quality data may be difficult to obtain.

In conclusion, the application of PINNs in quantum-enhanced fluid dynamics represents a promising and exciting area of research. By integrating quantum computing technologies with PINNs, researchers can potentially overcome many of the limitations of traditional PINN-based approaches, leading to more accurate, efficient, and scalable simulations of fluid dynamics problems. As quantum computing continues to advance, the collaboration between quantum computing and PINNs is likely to yield groundbreaking results, transforming the way we model and understand complex fluid flows. The future of fluid mechanics may well lie at the intersection of these two powerful technologies.

### 8.7 Exploitation of Quantum Feature Spaces in PINNs

The integration of quantum computing with Physics-Informed Neural Networks (PINNs) presents a promising avenue for enhancing the representation power and generalization capabilities of these models. Quantum feature spaces, which leverage the principles of quantum mechanics to expand the expressive capacity of neural networks, offer a unique opportunity to improve the performance of PINNs in solving complex fluid mechanics problems. By utilizing quantum computing, PINNs can potentially access higher-dimensional feature spaces, leading to more accurate and robust solutions for partial differential equations (PDEs) that govern fluid flow.

One of the key advantages of quantum feature spaces is their ability to represent complex, high-dimensional data in a more compact and efficient manner. Quantum computing provides a natural framework for this through the use of qubits, which can exist in superposition states and entangle with each other. This inherent parallelism allows for the exploration of a vast number of potential solutions simultaneously, which is particularly beneficial for solving complex PDEs that require significant computational resources [209]. In the context of PINNs, the incorporation of quantum feature spaces could lead to a more efficient exploration of the solution space, thereby improving the convergence and accuracy of the model.

Furthermore, quantum feature spaces can be used to encode physical laws and constraints directly into the neural network architecture. This is achieved by leveraging the unique properties of quantum states, which can be designed to represent specific physical conditions and interactions. For instance, the use of quantum circuits to encode the Navier-Stokes equations or other governing equations of fluid mechanics could enable PINNs to learn the underlying physics more effectively. This approach would allow PINNs to not only approximate the solution of PDEs but also enforce the physical constraints more rigorously, leading to more reliable and physically consistent predictions [3].

The potential of quantum feature spaces in PINNs is not limited to the representation of physical laws; it also extends to the generalization capabilities of the model. Traditional PINNs often struggle with generalizing to unseen scenarios, particularly when the input data is sparse or noisy. Quantum feature spaces, by their very nature, can provide a more robust and flexible representation of the input data. This is because quantum states can be designed to be resilient to noise and perturbations, which is a critical requirement for solving fluid mechanics problems where the input data may be incomplete or uncertain [209]. By leveraging the inherent noise resilience of quantum states, PINNs can be made more robust to data imperfections, leading to more accurate predictions even in challenging conditions.

Moreover, the integration of quantum feature spaces into PINNs can also enhance the model's ability to capture high-frequency and complex dynamics in fluid flows. Traditional PINNs often face challenges in accurately representing high-frequency components due to the spectral bias phenomenon, where the network struggles to learn the high-frequency parts of the solution. Quantum feature spaces, with their ability to represent a wide range of frequencies, can mitigate this issue by providing a more comprehensive and balanced representation of the solution space. This could lead to significant improvements in the accuracy of PINNs when applied to problems involving turbulent flows, shock waves, and other complex fluid dynamics phenomena [209].

Another promising direction is the use of quantum feature spaces to enhance the training dynamics of PINNs. The training process of PINNs is often challenging due to the non-convex nature of the loss landscape and the difficulty in balancing the different terms in the loss function. Quantum feature spaces can potentially alleviate these issues by providing a more favorable optimization landscape. By encoding the physical constraints into the quantum feature space, the loss function can be designed to have a more structured and convex form, which can facilitate the optimization process and lead to faster convergence [209].

In addition to these technical benefits, the integration of quantum feature spaces into PINNs also opens up new research opportunities in the field of quantum machine learning. The development of hybrid quantum-classical algorithms that combine the strengths of both quantum computing and deep learning could lead to significant advancements in the solution of complex fluid mechanics problems. For example, quantum-inspired neural networks could be used to pre-process the data or to guide the training of classical PINNs, leading to more efficient and effective models [209].

However, the exploitation of quantum feature spaces in PINNs also presents several challenges that need to be addressed. One of the main challenges is the current limitations of quantum computing hardware. While theoretical studies have shown the potential benefits of quantum feature spaces, practical implementations are still in their infancy due to the limited qubit counts and high error rates of current quantum processors. This necessitates the development of efficient quantum algorithms and error mitigation techniques that can be integrated with PINNs to make them viable in real-world applications [209].

Another challenge is the need for a deeper understanding of how quantum feature spaces can be effectively integrated into the architecture of PINNs. This requires interdisciplinary research that combines expertise from quantum computing, machine learning, and fluid mechanics. The development of novel architectures and training strategies that leverage the unique properties of quantum feature spaces will be crucial for realizing the full potential of this approach.

In conclusion, the exploitation of quantum feature spaces in PINNs offers a promising avenue for enhancing the representation power and generalization capabilities of these models. By leveraging the principles of quantum computing, PINNs can potentially overcome some of the limitations of traditional approaches and achieve more accurate and robust solutions for complex fluid mechanics problems. While there are still significant challenges to be addressed, the ongoing research in this area is expected to lead to exciting new developments in the field of scientific machine learning.

### 8.8 Quantum-Inspired PINN Architectures

Quantum-inspired Physics-Informed Neural Networks (PINNs) represent an intriguing frontier in the integration of quantum computing principles with machine learning for fluid mechanics applications. This emerging approach seeks to leverage the unique features of quantum mechanics, such as superposition, entanglement, and quantum gates, to enhance the efficiency and accuracy of PINNs. By drawing inspiration from quantum computing, researchers are exploring novel architectures that could potentially overcome some of the limitations faced by traditional PINNs in solving complex fluid dynamics problems.

One of the key principles of quantum computing is superposition, which allows quantum systems to exist in multiple states simultaneously. This concept can be translated into PINN designs by enabling neural networks to process multiple input states at once, thereby improving their ability to capture the intricate and often nonlinear behavior of fluid flows. For instance, a quantum-inspired PINN might be designed to represent the solution space of a partial differential equation (PDE) in a superposition of states, allowing the network to explore a broader range of possible solutions more efficiently. This could be particularly beneficial in high-dimensional problems, where traditional PINNs may struggle due to the curse of dimensionality. The concept of superposition in PINNs can be further enhanced by incorporating quantum gates, which are the fundamental building blocks of quantum circuits. These gates can be used to manipulate the quantum states of the neural network, enabling it to perform complex transformations that are difficult to achieve with classical neural networks [210].

Another important concept in quantum computing is entanglement, which refers to the phenomenon where quantum particles become interconnected and the state of one particle instantaneously influences the state of another, regardless of the distance between them. In the context of PINNs, entanglement can be harnessed to create correlations between different parts of the neural network, allowing for more efficient information sharing and propagation. This could be particularly useful in fluid mechanics, where the solution to a PDE often involves complex interactions between different regions of the domain. By designing PINNs with entangled layers, researchers aim to improve the network's ability to capture these interactions, leading to more accurate and robust solutions. The integration of entanglement into PINN architectures could also help in reducing the computational overhead associated with training large networks, as entangled layers may require fewer parameters to achieve the same level of performance [210].

Quantum gates, which are the fundamental operations in quantum computing, can also be incorporated into PINN designs to enhance their computational capabilities. These gates can be used to manipulate the quantum states of the neural network, allowing it to perform complex transformations that are difficult to achieve with classical neural networks. For example, a quantum gate could be used to adjust the weights of the network in a way that mimics the behavior of a quantum system, leading to more efficient optimization. This could be particularly useful in scenarios where the solution to a PDE requires precise control over the network's parameters. The use of quantum gates in PINNs could also help in addressing the issue of spectral bias, where traditional neural networks struggle to learn high-frequency components of the solution. By incorporating quantum gates, researchers aim to create PINNs that are better equipped to capture these high-frequency components, leading to more accurate solutions [210].

The design of quantum-inspired PINN architectures is still in its early stages, but several studies have begun to explore the potential of these approaches. For example, some researchers have proposed the use of quantum-inspired neural networks that mimic the behavior of quantum systems by incorporating elements of quantum mechanics into the network's architecture. These networks aim to leverage the principles of superposition and entanglement to improve their ability to capture complex patterns in the data. In one study, a quantum-inspired PINN was used to solve a class of nonlinear PDEs, demonstrating the potential of these architectures to handle challenging fluid dynamics problems [210].

Despite the promising potential of quantum-inspired PINN architectures, there are several challenges that need to be addressed. One of the main challenges is the integration of quantum computing principles into existing neural network frameworks. This requires a deep understanding of both quantum mechanics and machine learning, as well as the development of new algorithms and techniques to implement these ideas in practice. Additionally, the computational resources required to train and optimize quantum-inspired PINNs may be significantly higher than those required for traditional PINNs, which could limit their applicability in real-world scenarios.

In conclusion, the design of quantum-inspired PINN architectures represents a promising direction for the future of fluid mechanics simulations. By incorporating principles from quantum computing, such as superposition, entanglement, and quantum gates, researchers aim to create more efficient and accurate PINNs that can better handle the complexities of fluid dynamics. While there are still many challenges to overcome, the potential benefits of these architectures make them an exciting area of research that could lead to significant advancements in the field of computational fluid dynamics. As the field continues to evolve, it will be interesting to see how these quantum-inspired approaches are further developed and applied to real-world problems. [210]

### 8.9 Quantum-Resilient PINN Training

The emergence of quantum computing has opened new frontiers in computational science, promising exponential speedups for certain types of problems. However, the integration of Physics-Informed Neural Networks (PINNs) with quantum computing environments presents unique challenges, particularly due to the inherent noise and errors in quantum systems [76]. Quantum noise, such as decoherence and gate errors, can severely impact the training and performance of PINNs, which rely on precise calculations and stable gradients. Therefore, developing strategies to make PINNs robust against quantum noise is crucial for their practical application in quantum computing environments.

One of the primary challenges in training PINNs in quantum environments is the instability caused by noise. Quantum systems are prone to errors that can disrupt the training process, leading to suboptimal or incorrect solutions. To address this, researchers have explored error mitigation techniques. These techniques aim to reduce the impact of noise on the training process by adjusting the model's parameters or modifying the training strategy. For instance, techniques such as zero-noise extrapolation and probabilistic error cancellation have been proposed to mitigate the effects of noise in quantum computations [163]. These methods can be adapted to PINNs to improve their resilience against quantum noise, ensuring that the trained models remain accurate and reliable.

Another approach to enhancing the robustness of PINNs in quantum environments is noise-adaptive training. This involves adjusting the training process to account for the presence of noise, thereby making the PINNs more resilient to its effects. For example, by incorporating noise into the training data, PINNs can learn to generalize better and perform more reliably in the presence of quantum noise. This can be achieved through techniques such as data augmentation, where additional noise is introduced into the training data to simulate the effects of quantum noise. By training PINNs on such augmented data, the models can become more robust and capable of handling the uncertainties inherent in quantum systems [74].

Quantum-safe loss functions are another important strategy for making PINNs resilient to quantum noise. Traditional loss functions used in PINNs may not be suitable for quantum environments due to their sensitivity to noise. Therefore, developing loss functions that are robust to quantum noise is essential. One approach is to use loss functions that are less sensitive to small perturbations, such as those based on robust statistics. These loss functions can help PINNs maintain their performance even in the presence of noise. Additionally, researchers have proposed loss functions that incorporate uncertainty quantification, allowing PINNs to account for the variability introduced by quantum noise [158].

In addition to these strategies, the development of quantum-inspired PINN architectures is also a promising direction. These architectures leverage the principles of quantum computing to design PINNs that are more resilient to noise. For example, quantum-inspired PINNs may use entanglement and superposition to enhance their representational power and robustness. By incorporating these principles, PINNs can better handle the complexities of quantum environments and maintain their accuracy and efficiency.

Another important consideration is the use of hybrid quantum-classical training strategies. These strategies combine the strengths of classical and quantum computing to improve the training of PINNs. For instance, classical computing can be used to handle the more stable and predictable aspects of the training process, while quantum computing can be leveraged to perform complex calculations that benefit from quantum speedup. This hybrid approach can help mitigate the effects of noise and improve the overall performance of PINNs in quantum environments.

Furthermore, the integration of quantum error correction techniques into PINN training is a critical area of research. Quantum error correction involves encoding quantum information in a way that can detect and correct errors caused by noise. By incorporating these techniques into PINN training, it is possible to create models that are more resilient to the effects of noise. This can be achieved through the use of error-correcting codes and other quantum error correction methods, which can help ensure the accuracy and reliability of PINNs in quantum environments.

In conclusion, the development of quantum-resilient PINN training strategies is essential for the successful integration of PINNs with quantum computing environments. By addressing the challenges posed by quantum noise through error mitigation techniques, noise-adaptive training, quantum-safe loss functions, quantum-inspired architectures, hybrid training strategies, and quantum error correction, researchers can create PINNs that are robust and reliable in the face of the inherent uncertainties of quantum systems. These strategies not only enhance the performance of PINNs but also pave the way for their broader application in scientific and engineering domains that leverage the power of quantum computing.

### 8.10 Quantum-Enhanced Uncertainty Quantification in PINNs

Quantum-enhanced uncertainty quantification in Physics-Informed Neural Networks (PINNs) represents a promising frontier in the integration of quantum computing with scientific machine learning. As PINNs become increasingly prevalent in fluid mechanics simulations, the challenge of quantifying uncertainty in their predictions remains a critical issue. Traditional methods for uncertainty quantification (UQ) often rely on Monte Carlo simulations or polynomial chaos expansions, which can be computationally intensive, especially when dealing with high-dimensional problems. Quantum computing offers a paradigm shift by leveraging quantum algorithms and quantum-enhanced probabilistic models to improve the reliability and confidence in PINN predictions, particularly in scenarios involving uncertain inputs or boundary conditions.

One of the key advantages of quantum computing in UQ is its ability to efficiently represent and manipulate probability distributions. Quantum algorithms, such as the Quantum Monte Carlo method [211], can sample complex probability distributions more efficiently than classical methods. This is particularly beneficial in fluid mechanics, where uncertainties in initial conditions, boundary conditions, or material properties can significantly affect the accuracy of PINN predictions. By integrating quantum algorithms into the UQ framework of PINNs, it becomes possible to quantify uncertainties with higher precision and lower computational cost, thereby enhancing the robustness of PINN-based simulations.

Another promising direction is the use of quantum-enhanced probabilistic models to improve the reliability of PINN predictions. Quantum neural networks (QNNs) and variational quantum circuits have shown potential in capturing complex probabilistic relationships in data [212]. These models can be trained to approximate the posterior distribution of PINN parameters, providing a more accurate representation of uncertainties in the predictions. By incorporating quantum-enhanced probabilistic models, PINNs can not only make predictions but also provide confidence intervals, which are crucial for decision-making in engineering and scientific applications.

The integration of quantum computing with PINNs for UQ also opens up new possibilities for handling high-dimensional problems. Classical methods for UQ often suffer from the "curse of dimensionality," where the computational complexity grows exponentially with the number of input variables. Quantum computing, on the other hand, can leverage quantum parallelism to explore the high-dimensional space more efficiently. For instance, quantum amplitude estimation [213] can be used to estimate the expectation value of a function with a quadratic speedup over classical methods. This can significantly reduce the computational burden of UQ in fluid mechanics simulations, where the state space is often high-dimensional.

Moreover, quantum-enhanced UQ methods can improve the interpretability of PINN predictions. Traditional UQ methods often provide a posterior distribution that is difficult to interpret, especially in complex models like PINNs. Quantum computing, however, can enable the development of interpretable quantum models that provide insights into the uncertainty distribution. For example, quantum principal component analysis (QPCA) [214] can be used to identify the most significant sources of uncertainty in a PINN prediction, allowing researchers to focus on the critical factors that affect the simulation outcomes.

The application of quantum computing to UQ in PINNs is not without challenges. One of the main obstacles is the current limitations of quantum hardware, which is still in its early stages of development. Quantum computers are prone to noise and errors, which can affect the accuracy of UQ calculations. However, recent advances in error mitigation techniques [215] have shown promise in improving the reliability of quantum computations. These techniques can be combined with PINNs to develop robust UQ frameworks that are resilient to quantum noise.

Another challenge is the integration of quantum computing with existing PINN architectures. While PINNs are primarily designed for classical computing, the adaptation of these models to quantum computing requires significant changes in the network architecture and training procedures. For instance, the use of quantum neural networks (QNNs) as a component of PINNs [212] may require the development of new training algorithms that are compatible with quantum hardware. Additionally, the design of quantum circuits that can efficiently encode the physical constraints of PDEs remains an open research problem.

Despite these challenges, the potential benefits of quantum-enhanced UQ in PINNs make it a worthwhile area of research. The ability to quantify uncertainties in fluid mechanics simulations with higher accuracy and efficiency can have far-reaching implications for engineering and scientific applications. For example, in the context of climate modeling, where uncertainties in initial conditions and boundary conditions are significant, quantum-enhanced UQ can provide more reliable predictions of climate change scenarios. Similarly, in the field of aerospace engineering, where the accuracy of fluid flow simulations is critical for design optimization, quantum-enhanced UQ can help engineers make more informed decisions.

In conclusion, the integration of quantum computing with PINNs for uncertainty quantification represents a transformative approach to improving the reliability and confidence in fluid mechanics simulations. By leveraging quantum algorithms and quantum-enhanced probabilistic models, it is possible to address the limitations of traditional UQ methods and enhance the robustness of PINN-based simulations. While challenges remain, the ongoing advancements in quantum computing and machine learning provide a strong foundation for further research in this exciting and promising area [211; 212; 213; 214; 215].

## 9 Conclusion

### 9.1 Recap of Key Findings

Physics-Informed Neural Networks (PINNs) have emerged as a transformative approach in fluid mechanics, revolutionizing the way partial differential equations (PDEs) are solved and enabling the integration of physical laws with data-driven learning. The key discoveries and advancements in the application of PINNs to fluid mechanics are extensive, and they highlight the effectiveness of PINNs in solving PDEs, handling complex geometries, and improving computational efficiency [20; 4]. 

One of the most significant contributions of PINNs to fluid mechanics is their ability to solve PDEs, including the Navier-Stokes equations, without requiring extensive meshing or computational resources. Traditional numerical methods, such as finite element and finite volume methods, often demand significant computational power to handle complex fluid dynamics problems. In contrast, PINNs leverage neural networks to approximate solutions, embedding the governing PDEs into the loss function through automatic differentiation. This approach enables PINNs to solve PDEs with high accuracy, even in scenarios with sparse data [20; 1]. For instance, the EPINN-NSE model demonstrates the capability to solve two- and three-dimensional Navier-Stokes equations effectively by approximating velocity components through a stream function or directly [20].

Another major advancement is the ability of PINNs to handle complex geometries, which are often challenging for traditional numerical methods. Fluid mechanics problems often involve irregular domains, and traditional methods require extensive meshing and computational effort to resolve such geometries. PINNs, on the other hand, do not rely on a mesh, making them particularly suitable for problems with complex or irregular domains. The paper "A physics-informed neural network framework for modeling obstacle-related equations" demonstrates how PINNs can be extended to solve obstacle-related PDEs, which present a significant computational challenge due to the need for accurate approximations of solutions above a given obstacle [25]. Furthermore, the application of PINNs in simulating branched flows and other complex fluid dynamics scenarios shows their adaptability and effectiveness in capturing the dynamics of intricate systems [35].

In terms of computational efficiency, PINNs offer a promising alternative to traditional numerical methods by reducing the computational cost and time required for simulations. This is particularly beneficial in scenarios where high-resolution simulations are needed, as PINNs can provide accurate solutions with fewer computational resources. For example, the paper "Physics-informed deep learning for incompressible laminar flows" discusses the use of a mixed-variable scheme to improve the trainability and solution accuracy of PINNs for simulating steady and transient laminar flows at low Reynolds numbers [1]. Similarly, the "Distributed physics informed neural network for data-efficient solution to partial differential equations" paper presents a novel distributed PINN (DPINN) that improves the accuracy and efficiency of solving non-linear PDEs like the Navier-Stokes equation [135]. These advancements highlight the potential of PINNs to reduce computational costs while maintaining high accuracy.

PINNs have also demonstrated their effectiveness in handling high-frequency and complex dynamics, which are often challenging for traditional numerical methods. The spectral bias issue, where PINNs struggle to learn high-frequency components of solutions, has been a significant challenge. However, recent studies have introduced strategies to overcome this limitation. For instance, the paper "A Novel Adaptive Causal Sampling Method for Physics-Informed Neural Networks" proposes an adaptive causal sampling method that improves the performance and efficiency of PINNs, particularly for PDEs with high-order derivatives and strong nonlinearity [49]. Additionally, the "Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning" paper discusses the use of transfer learning to enhance the robustness and convergence of PINNs when solving high-frequency and multi-scale problems [30]. These innovations demonstrate the potential of PINNs to handle complex dynamics more effectively.

Moreover, PINNs have shown promise in solving inverse problems in fluid mechanics, where the goal is to estimate parameters or reconstruct solutions from limited or noisy data. The paper "Physics-informed neural networks for solving Reynolds-averaged Navier-Stokes equations" demonstrates how PINNs can be used to simulate turbulent flows without any specific model or assumption for turbulence, using only data on the domain boundaries [10]. This approach is particularly useful in scenarios where traditional methods struggle due to the complexity of the problem. Furthermore, the paper "Physics-Informed Neural Networks for Securing Water Distribution Systems" explores the use of PINNs in enhancing the security of intelligent cyberphysical systems, showcasing their versatility in real-world applications [8].

In summary, the key findings in the application of PINNs to fluid mechanics underscore their effectiveness in solving PDEs, handling complex geometries, and improving computational efficiency. These advancements have been driven by innovations in network architecture, training strategies, and the integration of physical laws into the learning process. As research in this area continues to evolve, PINNs are poised to play an increasingly important role in fluid mechanics and other scientific disciplines. The ongoing development of PINNs will further enhance their capabilities, enabling them to address a broader range of challenges in fluid dynamics and beyond.

### 9.2 Transformative Potential of PINNs

The transformative potential of Physics-Informed Neural Networks (PINNs) in fluid mechanics is profound, offering a paradigm shift in how we approach the modeling and simulation of complex fluid systems. By seamlessly integrating physical laws with data-driven learning, PINNs have the capability to revolutionize traditional simulation and modeling practices, providing a more accurate, efficient, and adaptable framework for solving fluid dynamics problems. Unlike conventional numerical methods, which often require extensive computational resources and meticulous grid generation, PINNs leverage the power of deep learning to approximate solutions to partial differential equations (PDEs) with minimal data requirements. This integration not only enhances the accuracy of predictions but also broadens the scope of applications in fluid mechanics, from incompressible flows to turbulent systems and beyond [5].

One of the most significant contributions of PINNs is their ability to incorporate physical constraints directly into the learning process. By embedding the governing equations of fluid dynamics into the loss function, PINNs ensure that the neural network learns solutions that are consistent with the underlying physics. This approach not only improves the reliability of the predictions but also enables the model to generalize better, even when faced with limited or noisy data. For instance, the development of mixed formulation PINNs has demonstrated that using first-order derivatives and combining equations from both strong and weak forms can lead to much better accuracy, especially when there are heterogeneity and variable jumps in the domain [6]. This capability is particularly valuable in real-world applications where the data may be sparse or incomplete, making traditional numerical methods less effective.

Moreover, the potential of PINNs to transform traditional simulation practices is evident in their ability to handle complex geometries and high-dimensional problems. Traditional methods often struggle with these challenges due to the limitations of mesh-based discretization techniques. In contrast, PINNs are mesh-free and can handle irregular domains without the need for complex grid generation. This makes them particularly suitable for applications involving complex fluid-structure interactions, where the geometry of the system can vary significantly. For example, the use of physics-informed convolutional networks (PICNs) has shown promise in generating physical fields from shallow neural networks, demonstrating the ability to capture multi-frequency components of the solution [180]. This flexibility and adaptability make PINNs a powerful tool for simulating a wide range of fluid dynamics problems.

Another transformative aspect of PINNs is their potential to accelerate the discovery of new physical models and the development of reduced-order models (ROMs). By leveraging the ability of PINNs to learn the underlying physics from data, researchers can develop more efficient and accurate models that capture the essential dynamics of complex systems. For instance, the application of PINNs in solving parametric Navier-Stokes equations has shown that they can learn solution functions for a range of parameters, enabling interpolation of solution functions and improving the efficiency of simulations [45]. This capability is particularly valuable in scenarios where multiple simulations are required, such as in design optimization and uncertainty quantification.

The transformative potential of PINNs is also evident in their ability to handle inverse problems in fluid mechanics, where the goal is to estimate unknown parameters or initial conditions from observed data. Traditional methods for solving inverse problems often require extensive computational resources and are sensitive to noise and errors in the data. In contrast, PINNs can efficiently estimate these parameters by incorporating the physical constraints into the loss function, leading to more robust and accurate solutions. For example, the use of PINNs in solving inverse problems in hemodynamics has demonstrated their ability to estimate boundary conditions and full velocity fields from scattered measurements, providing valuable insights into the dynamics of blood flow [37]. This capability opens up new possibilities for real-time monitoring and control of fluid systems, particularly in biomedical applications.

Furthermore, the integration of PINNs with other machine learning techniques and traditional numerical methods has the potential to create hybrid models that combine the strengths of both approaches. These hybrid models can leverage the data-driven learning capabilities of PINNs while maintaining the accuracy and robustness of traditional numerical methods. For instance, the development of hybrid models that combine PINNs with finite element and finite volume methods has shown promise in improving the accuracy and efficiency of simulations for complex fluid systems [80]. This synergy between different approaches can lead to more reliable and efficient solutions for a wide range of fluid dynamics problems.

In addition to their technical advantages, the transformative potential of PINNs lies in their ability to democratize access to advanced simulation tools. By reducing the computational costs and technical barriers associated with traditional numerical methods, PINNs make it possible for researchers and engineers to explore complex fluid systems with greater ease and efficiency. This democratization is particularly important in fields where computational resources are limited, such as in educational institutions and small research groups. The availability of open-source tools like IDRLnet further enhances the accessibility of PINNs, enabling a broader community of researchers to contribute to and benefit from their development [216].

In conclusion, the transformative potential of PINNs in fluid mechanics is vast and multifaceted. By integrating physical laws with data-driven learning, PINNs offer a powerful alternative to traditional numerical methods, enabling more accurate, efficient, and adaptable solutions to a wide range of fluid dynamics problems. Their ability to handle complex geometries, high-dimensional problems, and inverse problems, coupled with their potential for integration with other machine learning techniques, makes them a promising tool for the future of fluid mechanics research and applications. As the field continues to evolve, the continued development and refinement of PINNs will undoubtedly play a crucial role in advancing our understanding and ability to model complex fluid systems.

### 9.3 Current Limitations and Challenges

Despite the promising advancements of Physics-Informed Neural Networks (PINNs) in fluid mechanics, several critical challenges and limitations persist, hindering their widespread adoption and practical application. These challenges include computational inefficiencies, convergence issues, and limitations in handling high-frequency dynamics and complex flows. Addressing these challenges is essential for realizing the full potential of PINNs in solving fluid dynamics problems.

One of the primary limitations of PINNs is their computational inefficiency, especially for large-scale and high-dimensional problems. Training PINNs typically involves solving partial differential equations (PDEs) over a wide range of spatiotemporal domains, which demands significant computational resources. The need to evaluate PDE residuals at numerous collocation points increases the computational burden, making it challenging to scale PINNs for real-world engineering applications. For instance, in the context of solving incompressible Navier-Stokes equations, the number of collocation points required can grow exponentially with the complexity of the flow, leading to long training times and high computational costs [29]. Furthermore, the reliance on automatic differentiation to compute derivatives of the solution adds to the computational overhead, particularly for higher-order PDEs [57].

Another significant challenge is the issue of convergence during the training process. PINNs often face difficulties in achieving stable and accurate convergence due to the non-convex nature of the loss landscape. This is exacerbated by the need to balance multiple loss terms, including the PDE residual loss, boundary condition loss, and data fidelity loss. For example, in the case of solving obstacle-related PDEs, the presence of irregular boundaries and complex geometries makes it difficult to ensure that the PINN converges to the correct solution [25]. Additionally, the spectral bias of neural networks, where they tend to learn low-frequency components of the solution more easily than high-frequency ones, can lead to poor performance in capturing fine-scale features of the flow [71]. This limitation is particularly relevant for applications such as turbulent flow modeling, where high-frequency dynamics play a crucial role [10].

Handling high-frequency dynamics and complex flows remains a major challenge for PINNs. Turbulent flows, which are characterized by chaotic and multiscale behavior, are particularly difficult to model with PINNs. The inherent nonlinearity and instabilities in such flows make it challenging to ensure that the PINN accurately captures the underlying physics. For instance, in the case of simulating high Reynolds number turbulent flows, PINNs often struggle to reproduce the correct energy spectra and statistical properties of the flow [1]. Moreover, the presence of shocks and discontinuities in hyperbolic PDEs, such as those encountered in compressible flow simulations, further complicates the training process, as the PINN must learn to resolve these features accurately [4].

The limitations of PINNs in handling complex flows are also evident in their performance on real-world applications. While PINNs have shown success in solving idealized problems, their performance in more realistic scenarios, such as those involving multiphase flows or multi-physics interactions, is less well understood. For example, in the context of simulating multiphase flows, the need to accurately model the interactions between different phases and the associated interface dynamics poses significant challenges for PINNs [4]. Similarly, in the case of thermo-mechanically coupled systems, the coupling between different physical phenomena adds to the complexity of the problem, making it difficult for PINNs to learn the underlying relationships [6].

Another key challenge is the sensitivity of PINNs to noise and errors in the input data. In many real-world applications, the data used for training PINNs is often noisy, incomplete, or sparse, which can lead to nonphysical solutions and unreliable predictions. For example, in the case of reconstructing Rayleigh-Bénard flows from temperature-only measurements, the accuracy of the PINN predictions is significantly affected by the quality and density of the input data [204]. This sensitivity to data quality highlights the need for robust data preprocessing and regularization techniques to improve the reliability of PINN predictions.

Finally, the scalability of PINNs remains a significant challenge, particularly for problems involving large domains or high-dimensional parameter spaces. While PINNs have shown promise in solving small-scale problems, their performance on large-scale problems is still limited by the computational resources required for training and inference. For instance, in the context of solving 3D flow-thermal problems with sparse domain data, the effectiveness of PINNs is constrained by the availability of sufficient training data and the computational complexity of the problem [31]. Addressing these scalability issues is crucial for extending the application of PINNs to more complex and realistic fluid mechanics problems.

In summary, while PINNs have demonstrated great potential in fluid mechanics, they still face several challenges that need to be addressed to ensure their practical viability. These challenges include computational inefficiencies, convergence issues, limitations in handling high-frequency dynamics and complex flows, sensitivity to noise and errors, and scalability issues. Overcoming these challenges will require further research and innovation in the design and training of PINNs, as well as the development of new methodologies to enhance their accuracy, efficiency, and robustness.

### 9.4 Comparative Advantages and Trade-offs

Physics-Informed Neural Networks (PINNs) have emerged as a transformative tool in fluid mechanics, offering a unique blend of data-driven learning and physics-based constraints. Compared to traditional numerical methods and purely data-driven models, PINNs present several comparative advantages, particularly in terms of adaptability, scalability, and potential for real-time applications. However, they also have notable limitations in certain scenarios, especially where high-frequency dynamics, sparse data, or complex geometries are involved.

One of the most significant advantages of PINNs is their adaptability to a wide range of fluid flow problems. Unlike traditional numerical methods, which often require extensive grid generation and meshing, PINNs can operate on irregular or sparse data without the need for a predefined computational grid [19]. This makes them particularly effective in scenarios where the geometry is complex or evolving, such as in simulations involving moving boundaries or multi-phase flows [217]. This adaptability is also beneficial for inverse problems in fluid mechanics, where the goal is to estimate unknown parameters or boundary conditions from limited measurements [24]. Traditional methods, such as finite element or finite volume methods, often struggle with these types of problems due to their reliance on structured grids and the need for accurate initial guesses.

Another key advantage of PINNs is their scalability. While traditional numerical methods face challenges in scaling to large computational domains, PINNs can be trained using distributed computing strategies or parallel architectures, enabling efficient handling of high-dimensional problems. For instance, in the context of high-fidelity simulations of turbulent flows, PINNs can be combined with domain decomposition techniques to reduce the computational burden [178]. This makes them suitable for large-scale simulations, such as those encountered in climate modeling or aerospace engineering, where the complexity of the flow field increases with the size of the domain [177]. Moreover, PINNs can be trained on relatively small datasets and still achieve high accuracy, making them more data-efficient compared to fully data-driven models that require extensive labeled data for training [50].

In terms of real-time applications, PINNs hold great promise. Their ability to approximate solutions to partial differential equations (PDEs) with high accuracy and at reduced computational costs makes them ideal for real-time fluid dynamics simulations, such as in autonomous systems, robotics, or medical imaging. For example, in the field of biomedical engineering, PINNs have been used to enable real-time blood flow imaging, where the model can quickly recover velocity fields from sparse measurements [39]. This is particularly advantageous in scenarios where traditional numerical solvers would be too slow or computationally expensive. Similarly, PINNs have been successfully applied to fluid-structure interaction problems, allowing for real-time simulation of complex flows with moving boundaries [107]. This real-time capability is a significant advantage over traditional methods, which often require extensive computational resources and time to produce accurate results.

Despite these advantages, PINNs also have limitations that need to be acknowledged. One of the most notable challenges is their performance in capturing high-frequency dynamics, which are often critical in turbulent or chaotic flows. Traditional numerical methods, such as finite difference or finite volume schemes, are specifically designed to resolve such dynamics with high accuracy, whereas PINNs may struggle with these tasks due to issues like spectral bias or the difficulty of enforcing high-frequency constraints [71]. Moreover, in scenarios where the data is sparse or noisy, PINNs may produce nonphysical solutions, as their accuracy depends heavily on the quality and quantity of the training data [218]. This is a significant limitation, as many real-world fluid mechanics problems involve incomplete or uncertain data.

Another limitation of PINNs is their computational efficiency in certain scenarios. While they can be highly effective for low-dimensional problems or simplified flow configurations, their performance may degrade in high-dimensional or multi-physics problems. For example, in simulations involving complex interactions between fluid and solid domains, the computational cost of training PINNs can become prohibitive, especially when high accuracy is required [219]. Traditional numerical methods, on the other hand, are often better suited for such problems due to their established efficiency and stability in handling multi-scale and multi-physics interactions.

Furthermore, the interpretability of PINNs remains a challenge. While they are capable of producing accurate solutions, understanding the internal mechanics of how they enforce physical constraints and learn the underlying flow dynamics is not always straightforward. This lack of transparency can be a barrier in applications where interpretability is crucial, such as in safety-critical systems or regulatory compliance scenarios. In contrast, traditional numerical methods, although more rigid in their approach, are often more transparent and easier to validate [188].

In summary, PINNs offer a compelling alternative to traditional numerical methods and data-driven models, with significant advantages in adaptability, scalability, and real-time applications. However, their limitations in handling high-frequency dynamics, sparse data, and complex multi-physics problems must be carefully considered. By acknowledging these trade-offs, researchers can better leverage the strengths of PINNs while addressing their shortcomings through hybrid approaches, enhanced training strategies, and continued methodological improvements.

### 9.5 Empirical Validation and Case Studies

Empirical validation and case studies play a pivotal role in demonstrating the practical success of Physics-Informed Neural Networks (PINNs) in fluid mechanics. Through rigorous testing on real-world and synthetic problems, PINNs have been shown to achieve accurate, reliable, and computationally efficient solutions. These validations not only confirm the theoretical foundations of PINNs but also highlight their applicability in complex engineering scenarios. A variety of case studies and empirical investigations have showcased the ability of PINNs to solve challenging fluid dynamics problems, including the Navier-Stokes equations, turbulent flows, inverse problems, and multi-phase systems, with high accuracy and robustness.

One of the earliest empirical validations of PINNs in fluid mechanics involved solving the Navier-Stokes equations using the Extended Physics-Informed Neural Network (EPINN-NSE) approach [20]. The EPINN-NSE model demonstrated the capability to solve both two-dimensional and three-dimensional Navier-Stokes problems with high accuracy, even when the velocity field was approximated using a stream function. The method provided a divergence-free velocity field, which is essential for incompressible flows, and showed promising results in capturing complex flow dynamics with minimal data. The accuracy of the model was further validated through comparisons with traditional numerical solvers, showcasing the potential of PINNs as a viable alternative to conventional methods.

Another critical case study involved the application of PINNs to turbulent flow simulations using the RANS-PINN framework [40]. This model integrated a 2-equation eddy viscosity model based on the Reynolds-Averaged Navier-Stokes (RANS) formulation to simulate high Reynolds number turbulent flows. The results demonstrated that RANS-PINN could accurately predict velocity and pressure fields, even for complex turbulent flow regimes. The model’s performance was particularly notable in capturing the Reynolds stress components, which are crucial for understanding the behavior of turbulent flows. This study highlighted the feasibility of PINNs in solving high-fidelity fluid dynamics problems where traditional methods often struggle.

In the context of inverse problems, PINNs have shown remarkable success in estimating reduced-order model parameters and the full velocity field from scatter 2D noisy measurements in the ascending aorta [24]. The results showed stable and accurate parameter estimations when using the method with simulated data, while the velocity reconstruction showed dependence on the measurement quality and the flow pattern complexity. The method allows for solving clinical-relevant inverse problems in hemodynamics and complex coupled physical systems.

Another significant empirical validation involved the use of PINNs for solving the 3D incompressible Navier-Stokes equations with sparse domain data [29]. The study demonstrated that PINNs could effectively handle sparse data by incorporating a hybrid data-PINN approach. The model was tested on a realistic flow-thermal electronics design problem, where it outperformed standard data-driven neural networks in terms of accuracy and computational efficiency. This case study highlighted the ability of PINNs to generalize well from limited data and their potential for surrogate modeling in complex engineering systems.

The effectiveness of PINNs in solving obstacle-related PDEs was also empirically validated [25]. The study demonstrated that PINNs could accurately approximate the solution to PDEs involving obstacles, even when the obstacles were irregular or complex. The model’s performance was evaluated on multiple scenarios, including linear and nonlinear PDEs, and the results confirmed the robustness of PINNs in handling such challenging problems.

In the realm of multi-phase and multi-scale flows, PINNs have shown significant potential. The MRF-PINN (Multi-Receptive-Field PINN) model was tested on a variety of linear and nonlinear PDEs, including the Navier-Stokes equations [48]. The results demonstrated that MRF-PINN could adapt to different types of equations and mesh resolutions without the need for manual tuning. The model’s performance was further enhanced through the use of a dimensional balance method and high-order finite differences, which significantly reduced the solving error under various conditions.

Empirical validation also extended to the simulation of branched flows using transfer learning with PINNs [35]. The study demonstrated that transfer learning could significantly improve the efficiency of PINNs in solving stochastic branched flows. The results showed that the model could achieve accurate solutions in a fraction of the time required by conventional PINN training, making it a promising approach for real-time fluid dynamics simulations.

The case of solving the Navier-Stokes equations in the context of ultrafast ultrasound blood flow imaging further highlighted the practical success of PINNs [28]. The study introduced a novel training framework, SeqPINN, which enabled real-time training and accurate velocity recovery. The results demonstrated that SeqPINN could achieve high accuracy with significantly reduced training time, making it suitable for real-world medical imaging applications.

In summary, the empirical validation and case studies of PINNs in fluid mechanics have consistently demonstrated their ability to solve complex and challenging problems with high accuracy and efficiency. From turbulent flows and inverse problems to multi-phase systems and obstacle-related PDEs, PINNs have shown their versatility and robustness. These case studies not only highlight the current capabilities of PINNs but also provide a foundation for future research and development in this promising field.

### 9.6 Future Research Directions

Future research directions for Physics-Informed Neural Networks (PINNs) in fluid mechanics are vast and promising, with opportunities to push the boundaries of both theoretical understanding and practical applications. Emerging research directions include the integration of PINNs with quantum computing, improving interpretability, enhancing scalability, and developing more robust training algorithms for complex problems. These areas represent critical pathways for advancing the capabilities and applicability of PINNs in the field of fluid mechanics.

One of the most intriguing directions for future research is the integration of PINNs with quantum computing. Quantum computing has the potential to revolutionize the way we solve complex problems, including those governed by partial differential equations (PDEs) [58]. The high computational power of quantum systems could be leveraged to accelerate the training of PINNs or to solve problems that are currently infeasible with classical computing. For example, quantum circuits could be used to encode the physics-based constraints of PINNs, potentially leading to more efficient and accurate solutions [33]. Moreover, hybrid quantum-classical architectures could be explored to combine the strengths of both paradigms, creating new opportunities for solving PDEs in fluid mechanics.

Improving the interpretability of PINNs is another important direction for future research. While PINNs have demonstrated remarkable success in solving PDEs, their black-box nature often makes it difficult to understand how they learn and apply physical laws. Enhancing interpretability would not only increase the trustworthiness of PINNs but also enable researchers to refine their models and improve their performance. Techniques such as visualization of learned physics, feature importance analysis, and model-agnostic interpretability tools could be employed to provide insights into how PINNs learn and apply physical laws [220]. For instance, recent studies have shown that the use of attention mechanisms and residual-based methods can improve the performance of PINNs by focusing on critical regions or features in the solution domain [221].

Enhancing the scalability of PINNs is crucial for their application to large-scale and high-dimensional fluid mechanics problems. While PINNs have shown promise in solving PDEs on small to medium domains, their performance on large-scale problems remains a challenge. Future research could explore the use of distributed training, parallel computing, and efficient architectures to ensure that PINNs can be applied to real-world engineering and scientific problems [222]. For example, domain decomposition techniques and parallelization strategies could be adapted to improve the performance of PINNs on large-scale problems [223]. Moreover, the development of efficient training algorithms that can handle the complexity of large-scale PDEs would be essential.

Developing more robust training algorithms for complex problems is another key area for future research. PINNs often face challenges in achieving stable and accurate convergence, particularly when dealing with high-frequency and complex dynamics [224]. Future research could focus on improving the training stability and efficiency of PINNs by exploring emerging techniques such as adaptive sampling, multi-fidelity learning, physics-informed activation functions, and advanced optimization strategies [225]. For instance, the use of adaptive sampling techniques has been shown to improve the accuracy and convergence of PINNs by dynamically adjusting the placement of collocation points [93]. Additionally, the introduction of physics-informed activation functions that incorporate physical knowledge into the neural network's activation functions could improve the efficiency and validity of PINNs [72].

In addition to these technical advancements, future research could also explore the integration of PINNs with traditional numerical methods and data-driven models. Hybrid models that combine PINNs with traditional numerical methods like finite element and finite volume methods could leverage the strengths of both approaches to improve accuracy, efficiency, and robustness in fluid mechanics simulations [36]. Similarly, the integration of PINNs with data-driven models could enhance their ability to handle complex and high-dimensional problems by leveraging the strengths of both paradigms [207]. For example, the use of multi-fidelity learning approaches that leverage data of varying fidelity levels could improve the training and generalization capabilities of PINNs [27].

Furthermore, future research could also focus on improving the robustness of PINNs in the presence of noisy or erroneous data. PINNs are often sensitive to noise, which can lead to nonphysical solutions and unreliable predictions in fluid mechanics applications [218]. Techniques such as uncertainty quantification and ensemble learning could be explored to enhance the robustness and reliability of PINNs [188]. For instance, the use of Bayesian approaches and multiple models could provide a more comprehensive understanding of the uncertainties associated with PINN predictions [188].

In summary, the future research directions for PINNs in fluid mechanics are diverse and promising. By integrating PINNs with quantum computing, improving interpretability, enhancing scalability, and developing more robust training algorithms for complex problems, researchers can push the boundaries of what is possible with this powerful technique. These efforts will not only advance the theoretical understanding of PINNs but also enable their practical application to a wide range of fluid mechanics problems. The continued exploration of these research directions will be crucial for realizing the full potential of PINNs in the field of fluid mechanics.

### 9.7 Broader Implications and Impact

Physics-Informed Neural Networks (PINNs) have emerged as a transformative paradigm in scientific computing and engineering, offering a novel approach to solving complex problems in fluid dynamics and beyond. Their ability to seamlessly integrate physical laws with data-driven learning opens up new frontiers in multiscale and multiphysics simulations, and their potential to drive innovation in fluid dynamics is profound. By bridging the gap between traditional numerical methods and modern machine learning, PINNs are reshaping the landscape of computational science and engineering. 

One of the most significant implications of PINNs is their capacity to handle multiscale problems, which are ubiquitous in fluid mechanics. Traditional numerical methods often struggle with the computational demands of simulating systems that span multiple spatial and temporal scales, such as the interaction between turbulent flows and fine-scale structures in complex geometries. PINNs, on the other hand, leverage the flexibility of neural networks to approximate solutions across different scales without explicitly discretizing the entire domain. This is particularly advantageous in problems like the simulation of turbulent flows, where high-resolution data is often sparse or unavailable. For example, the work presented in [22] demonstrates how PINNs can be trained to capture the intricate behavior of turbulent flows without the need for expensive high-fidelity simulations. This capability not only reduces computational costs but also allows for the exploration of scenarios that were previously infeasible with traditional methods.

Moreover, PINNs have the potential to revolutionize multiphysics simulations, where multiple physical phenomena interact simultaneously. Fluid dynamics problems often involve coupled processes such as thermal conduction, mechanical deformation, and chemical reactions. The traditional approach to solving such problems involves the integration of multiple solvers, each tailored to a specific physics, which can be computationally intensive and prone to errors. PINNs, by contrast, can handle these coupled systems within a unified framework, as shown in [6]. By embedding the governing equations of each physics into the loss function of the neural network, PINNs can learn the interactions between different physical processes in a more coherent and efficient manner. This not only simplifies the computational workflow but also enhances the accuracy of the solutions by ensuring that the physical constraints are consistently enforced throughout the simulation.

The broader impact of PINNs extends beyond fluid dynamics into other domains of engineering and science. In the field of materials science, for instance, PINNs have been applied to model the behavior of functionally graded materials, where the material properties vary spatially. The work in [169] highlights how PINNs can accurately capture the complex interactions between thermal and mechanical responses in such materials. This capability is crucial for the design and optimization of advanced materials, where traditional methods often require extensive experimental data or high-fidelity simulations. Similarly, in the realm of biomedical engineering, PINNs have been used to simulate blood flow in vascular systems, enabling the development of patient-specific models for diagnostic and therapeutic purposes. The study in [9] demonstrates the effectiveness of PINNs in capturing the intricate dynamics of blood flow, even in the presence of noisy and sparse data.

Another critical implication of PINNs is their potential to enable real-time simulations and decision-making in complex systems. Traditional numerical methods are often limited by their computational cost, making real-time applications challenging. PINNs, however, can be trained to provide rapid approximations of solutions, as demonstrated in [39]. This is particularly valuable in applications such as aerodynamic design, where the ability to quickly evaluate different design configurations can significantly accelerate the development process. Moreover, PINNs can be integrated with digital twin technologies to create virtual replicas of physical systems, allowing for continuous monitoring and predictive maintenance. The work in [226] illustrates how PINNs can be used to construct accurate and efficient digital twins, enabling real-time insights into the behavior of complex systems.

The integration of PINNs into existing computational frameworks also has the potential to drive innovation in the field of scientific computing. By combining the strengths of deep learning with the rigorous mathematical foundations of partial differential equations (PDEs), PINNs offer a new paradigm for solving problems that were previously considered intractable. For instance, the work in [130] introduces a novel framework that leverages the flexibility of graph neural networks to solve PDEs on irregular geometries. This approach not only improves the scalability of PINNs but also expands their applicability to a broader range of problems. Similarly, the use of PINNs in solving inverse problems, as explored in [10], highlights their potential to extract valuable information from limited data, which is crucial in scenarios where experimental measurements are scarce.

In addition to their technical capabilities, PINNs have the potential to democratize access to high-fidelity simulations. Traditional numerical methods often require specialized expertise and significant computational resources, making them inaccessible to many researchers and engineers. PINNs, by contrast, can be trained using relatively simple architectures and can leverage the power of modern machine learning libraries to achieve high accuracy with minimal computational overhead. The work in [78] showcases how PINNs can be implemented in a user-friendly manner, making them accessible to a wider audience. This democratization of computational tools is essential for fostering innovation and accelerating the development of new applications in fluid dynamics and beyond.

In conclusion, the broader implications and impact of PINNs in scientific computing and engineering are profound. Their ability to handle multiscale and multiphysics problems, their potential to enable real-time simulations, and their capacity to democratize access to high-fidelity simulations make them a powerful tool for advancing the field of fluid dynamics. As research in this area continues to evolve, the integration of PINNs with other emerging technologies such as quantum computing and advanced optimization algorithms will further expand their capabilities, paving the way for new innovations in scientific computing and engineering.

### 9.8 Call to Action for the Research Community

The field of Physics-Informed Neural Networks (PINNs) has made significant strides in recent years, demonstrating immense potential in revolutionizing fluid mechanics simulations and related domains. However, despite these achievements, several challenges and limitations remain, which call for continued research and collaboration within the scientific and engineering communities. The emergence of PINNs has opened up new possibilities for integrating physical laws with data-driven learning, allowing for the development of more accurate and robust models. However, to fully harness the potential of PINNs, it is crucial to address several key issues and push the boundaries of current methodologies.

One of the most pressing challenges is the issue of computational efficiency. While PINNs have shown promise in solving complex fluid dynamics problems, they often require substantial computational resources, especially for high-dimensional and large-scale problems. This has been highlighted in several studies, where the training of PINNs can be computationally expensive and time-consuming [63]. To overcome this, there is a need for the development of more efficient training algorithms and optimization strategies that can reduce the computational burden without sacrificing accuracy. For instance, techniques such as adaptive sampling and multi-fidelity learning have shown promise in improving the training efficiency of PINNs [93]. These methods need to be further refined and extended to handle the complexities of fluid mechanics problems, such as turbulence and multi-phase flows.

Another critical area that requires attention is the issue of convergence. PINNs often struggle with convergence, particularly when dealing with complex and high-frequency dynamics. This problem has been extensively studied, with researchers highlighting the non-convexity of the loss landscape and the challenges in balancing different loss terms [227]. To address this, there is a need for the development of more robust training algorithms and loss function formulations that can guide the network towards the correct solution. Techniques such as the use of physics-informed activation functions and the incorporation of residual-based adaptive refinement have shown promise in improving the convergence behavior of PINNs [62]. These approaches need to be further explored and integrated into existing PINN frameworks to enhance their reliability and performance.

The issue of data scarcity and the need for robust data augmentation strategies also warrant further investigation. In many fluid mechanics applications, the availability of high-quality and diverse data is limited, which can hinder the training and generalization capabilities of PINNs [228]. To address this, there is a need for the development of more effective data augmentation techniques that can generate synthetic data while preserving the underlying physical laws. Methods such as the use of generative models and the incorporation of physical constraints into the loss function have shown promise in this regard [229]. These approaches need to be further refined and validated on a wide range of fluid mechanics problems to ensure their effectiveness and robustness.

The integration of PINNs with other advanced computational techniques, such as quantum computing and high-performance computing, presents another exciting avenue for future research. The potential of quantum computing to enhance the efficiency and accuracy of PINNs has been explored in several studies, with researchers highlighting the need for the development of hybrid quantum-classical architectures [33]. These approaches can potentially leverage the unique properties of quantum systems to solve complex fluid dynamics problems more efficiently. However, the practical implementation of these ideas requires significant advancements in both quantum computing and machine learning, and there is a need for collaborative efforts between researchers in these fields.

Furthermore, the need for more interpretable and explainable PINN models cannot be overstated. While PINNs have shown impressive performance in solving fluid dynamics problems, their black-box nature often makes it difficult to understand how they arrive at their solutions. This lack of interpretability can be a barrier to their adoption in critical applications where transparency and trust are essential. To address this, there is a need for the development of techniques that can provide insights into the internal workings of PINNs. Methods such as the use of visualization tools, feature importance analysis, and model-agnostic interpretability techniques have shown promise in this regard [220]. These approaches need to be further developed and integrated into existing PINN frameworks to enhance their interpretability and usability.

The importance of collaboration and knowledge sharing within the scientific and engineering communities cannot be overstated. The challenges and limitations of PINNs are complex and multifaceted, requiring a multidisciplinary approach to address them effectively. Researchers from different domains, including fluid mechanics, computer science, and applied mathematics, need to come together to share their expertise and insights. This collaboration can lead to the development of more innovative and effective solutions that can push the boundaries of current methodologies. For instance, the integration of PINNs with traditional numerical methods and data-driven models has shown promise in improving the accuracy and efficiency of fluid dynamics simulations [36].

In addition to technical challenges, there is a need for the development of standardized benchmarks and evaluation metrics to assess the performance of PINNs in fluid mechanics applications. The lack of standardized benchmarks makes it difficult to compare the effectiveness of different PINN approaches and to identify areas for improvement. To address this, there is a need for the creation of comprehensive benchmark datasets and evaluation frameworks that can provide a fair and objective assessment of PINN performance. These benchmarks should cover a wide range of fluid mechanics problems, including both forward and inverse problems, and should be made publicly available to facilitate research and development.

Finally, the need for continued research and innovation in the field of PINNs is essential to fully realize their potential in fluid mechanics and related domains. While significant progress has been made, there are still many open questions and challenges that need to be addressed. Researchers should focus on developing more efficient training algorithms, improving the convergence behavior of PINNs, and enhancing their interpretability and robustness. Additionally, the integration of PINNs with other advanced computational techniques, such as quantum computing and high-performance computing, presents exciting opportunities for future research. By fostering collaboration, sharing knowledge, and addressing the challenges head-on, the scientific and engineering communities can continue to advance the field of PINNs and unlock new possibilities in fluid mechanics and beyond.


## References

[1] Physics-informed deep learning for incompressible laminar flows

[2] Towards Physics-informed Deep Learning for Turbulent Flow Prediction

[3] Learning solutions of parametric Navier-Stokes with physics-informed  neural networks

[4] Physics-informed attention-based neural network for solving non-linear  partial differential equations

[5] Physics Informed Deep Learning (Part I)  Data-driven Solutions of  Nonlinear Partial Differential Equations

[6] Mixed formulation of physics-informed neural networks for  thermo-mechanically coupled systems and heterogeneous domains

[7] An extended physics informed neural network for preliminary analysis of  parametric optimal control problems

[8] Physics-Informed Neural Networks for Securing Water Distribution Systems

[9] Improved Surrogate Modeling of Fluid Dynamics with Physics-Informed  Neural Networks

[10] Physics-informed neural networks for solving Reynolds-averaged  Navier$\unicode{x2013}$Stokes equations

[11] Physics-informed deep-learning applications to experimental fluid  mechanics

[12] Data-Driven Physics-Informed Neural Networks  A Digital Twin Perspective

[13] Deep Continuous Networks

[14] Machine learning accelerated computational fluid dynamics

[15] Computational Tools for Cardiac Simulation -- GPU-Parallel Multiphysics

[16] Performance of high-order Godunov-type methods in simulations of  astrophysical low Mach number flows

[17] Machine learning for rapid discovery of laminar flow channel wall  modifications that enhance heat transfer

[18] A high-order weighted finite difference scheme with a multi-state  approximate Riemann solver for divergence-free magnetohydrodynamic  simulations

[19] PointSAGE  Mesh-independent superresolution approach to fluid flow  predictions

[20] EPINN-NSE  Enhanced Physics-Informed Neural Networks for Solving  Navier-Stokes Equations

[21] Deep learning based on mixed-variable physics informed neural network  for solving fluid dynamics without simulation data

[22] RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows

[23] Physics-aware deep neural networks for surrogate modeling of turbulent  natural convection

[24] Physics-informed neural networks for blood flow inverse problems

[25] A physics-informed neural network framework for modeling  obstacle-related equations

[26] Multiple Case Physics-Informed Neural Network for Biomedical Tube Flows

[27] Transfer Learning on Multi-Fidelity Data

[28] Towards Real-time Training of Physics-informed Neural Networks   Applications in Ultrafast Ultrasound Blood Flow Imaging

[29] Physics-informed neural networks (PINNs) for fluid mechanics  A review

[30] Physics-Informed Neural Networks for High-Frequency and Multi-Scale  Problems using Transfer Learning

[31] Physics Informed Neural Networks for Modeling of 3D Flow-Thermal  Problems with Sparse Domain Data

[32] Training dynamics in Physics-Informed Neural Networks with feature  mapping

[33] Integration of Quantum Accelerators with High Performance Computing -- A  Review of Quantum Programming Tools

[34] Failure-informed adaptive sampling for PINNs

[35] Transfer Learning with Physics-Informed Neural Networks for Efficient  Simulation of Branched Flows

[36] Hybrid FEM-NN models  Combining artificial neural networks with the  finite element method

[37] The Inverse Problem for Controlled Differential Equations

[38] PINNs-Based Uncertainty Quantification for Transient Stability Analysis

[39] Current density impedance imaging with PINNs

[40] Latent RANSAC

[41] Understanding Automatic Differentiation Pitfalls

[42] GAS  A Gaussian Mixture Distribution-Based Adaptive Sampling Method for  PINNs

[43] Physics-informed neural networks for solving forward and inverse  problems in complex beam systems

[44] Physics-informed neural networks in the recreation of hydrodynamic  simulations from dark matter

[45] Physics Informed Deep Learning (Part II)  Data-driven Discovery of  Nonlinear Partial Differential Equations

[46] On the Stability and Convergence of Physics Informed Neural Networks

[47] Sobolev Training for Physics Informed Neural Networks

[48] MRF-PINN  A Multi-Receptive-Field convolutional physics-informed neural  network for solving partial differential equations

[49] A Novel Adaptive Causal Sampling Method for Physics-Informed Neural  Networks

[50] Stacked Generative Machine Learning Models for Fast Approximations of  Steady-State Navier-Stokes Equations

[51] Neural Operators Learn the Local Physics of Magnetohydrodynamics

[52] Experience report of physics-informed neural networks in fluid  simulations  pitfalls and frustration

[53] Characterizing possible failure modes in physics-informed neural  networks

[54] Multi-Objective Loss Balancing for Physics-Informed Deep Learning

[55] Architectural Strategies for the optimization of Physics-Informed Neural  Networks

[56] An Expert's Guide to Training Physics-informed Neural Networks

[57] Learning in Sinusoidal Spaces with Physics-Informed Neural Networks

[58] The Old and the New  Can Physics-Informed Deep-Learning Replace  Traditional Linear Solvers 

[59] Recipes for when Physics Fails  Recovering Robust Learning of Physics  Informed Neural Networks

[60] Probing optimisation in physics-informed neural networks

[61] Error Estimates and Physics Informed Augmentation of Neural Networks for  Thermally Coupled Incompressible Navier Stokes Equations

[62] Gradient-enhanced physics-informed neural networks for forward and  inverse PDE problems

[63] Critical Investigation of Failure Modes in Physics-informed Neural  Networks

[64] Neural functional a posteriori error estimates

[65] Evaluating Error Bound for Physics-Informed Neural Networks on Linear  Dynamical Systems

[66] Overparameterization of deep ResNet  zero loss and mean-field analysis

[67] Investigating and Mitigating Failure Modes in Physics-informed Neural  Networks (PINNs)

[68] MultiAdam  Parameter-wise Scale-invariant Optimizer for Multiscale  Training of Physics-informed Neural Networks

[69] Loss Landscape Engineering via Data Regulation on PINNs

[70] Finite basis physics-informed neural networks as a Schwarz domain  decomposition method

[71] On the Spectral Bias of Neural Networks

[72] Learning Specialized Activation Functions for Physics-informed Neural  Networks

[73] Domain decomposition-based coupling of physics-informed neural networks  via the Schwarz alternating method

[74] Efficient physics-informed neural networks using hash encoding

[75] Distributed Training of Graph Convolutional Networks

[76] Physics-informed Neural Networks for Elliptic Partial Differential  Equations on 3D Manifolds

[77] Learning Physics-Informed Neural Networks without Stacked  Back-propagation

[78] DeepXDE  A deep learning library for solving differential equations

[79] Modeling, Control and Numerics of Gas Networks

[80] Hybrid Modeling Design Patterns

[81] Enhancing Arterial Blood Flow Simulations through Physics-Informed  Neural Networks

[82] A note on the undercut procedure

[83] DeepCFD  Efficient Steady-State Laminar Flow Approximation with Deep  Convolutional Neural Networks

[84] MPMNet  A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction

[85] Space and Time Continuous Physics Simulation From Partial Observations

[86] Physics-informed neural networks modeling for systems with moving  immersed boundaries  application to an unsteady flow past a plunging foil

[87] PINNsFormer  A Transformer-Based Framework For Physics-Informed Neural  Networks

[88] A Deep Fourier Residual Method for solving PDEs using Neural Networks

[89] Neural Spectral Methods  Self-supervised learning in the spectral domain

[90] Robust Physics Informed Neural Networks

[91] Deep Random Vortex Method for Simulation and Inference of Navier-Stokes  Equations

[92] LatentPINNs  Generative physics-informed neural networks via a latent  representation learning

[93] The Adaptive Sampling Revisited

[94] Paperswithtopic  Topic Identification from Paper Title Only

[95] LSA-PINN  Linear Boundary Connectivity Loss for Solving PDEs on Complex  Geometry

[96] Good Lattice Training  Physics-Informed Neural Networks Accelerated by  Number Theory

[97] RANG  A Residual-based Adaptive Node Generation Method for  Physics-Informed Neural Networks

[98] Inverse-Dirichlet Weighting Enables Reliable Training of Physics  Informed Neural Networks

[99] Adaptive Self-supervision Algorithms for Physics-informed Neural  Networks

[100] Convolutional Neural Operators for robust and accurate learning of PDEs

[101] Challenges in Training PINNs  A Loss Landscape Perspective

[102] Physical Activation Functions (PAFs)  An Approach for More Efficient  Induction of Physics into Physics-Informed Neural Networks (PINNs)

[103] Auto-PINN  Understanding and Optimizing Physics-Informed Neural  Architecture

[104] A reduced-order modeling of pattern formations

[105] Momentum Diminishes the Effect of Spectral Bias in Physics-Informed  Neural Networks

[106] MC-Nonlocal-PINNs  handling nonlocal operators in PINNs via Monte Carlo  sampling

[107] Temporal Consistency Loss for Physics-Informed Neural Networks

[108] Estimates on the generalization error of Physics Informed Neural  Networks (PINNs) for approximating a class of inverse problems for PDEs

[109] An E-PINN assisted practical uncertainty quantification for inverse  problems

[110] On derivations of evolving surface Navier-Stokes equations

[111] Stochastic dynamical modeling of turbulent flows

[112] Visual Exploration of Simulated and Measured Blood Flow

[113] A comprehensive study of non-adaptive and residual-based adaptive  sampling for physics-informed neural networks

[114] Learned Turbulence Modelling with Differentiable Fluid Solvers   Physics-based Loss-functions and Optimisation Horizons

[115] A fully-differentiable compressible high-order computational fluid  dynamics solver

[116] Hybrid quantum physics-informed neural networks for simulating  computational fluid dynamics in complex shapes

[117] Deep Learning for Reduced Order Modelling and Efficient Temporal  Evolution of Fluid Simulations

[118] Multi-fidelity Generative Deep Learning Turbulent Flows

[119] Respecting causality is all you need for training physics-informed  neural networks

[120] Frequency-compensated PINNs for Fluid-dynamic Design Problems

[121] Simulation and Prediction of Countercurrent Spontaneous Imbibition at  Early and Late Times Using Physics-Informed Neural Networks

[122] Understanding the training of PINNs for unsteady flow past a plunging  foil through the lens of input subdomain level loss function gradients

[123] POD-Galerkin reduced order models and physics-informed neural networks  for solving inverse problems for the Navier-Stokes equations

[124] Mitigating Propagation Failures in Physics-informed Neural Networks  using Retain-Resample-Release (R3) Sampling

[125] Sparse Spectral Methods for Solving High-Dimensional and Multiscale  Elliptic PDEs

[126] AdjointNet  Constraining machine learning models with physics-based  codes

[127] RBF-MGN Solving spatiotemporal PDEs with Physics-informed Graph Neural  Network

[128] Structure-Preserving Physics-Informed Neural Networks With Energy or  Lyapunov Structure

[129] RBF-PINN  Non-Fourier Positional Embedding in Physics-Informed Neural  Networks

[130] Physics-informed graph neural Galerkin networks  A unified framework for  solving PDE-governed forward and inverse problems

[131] Fast Neural Network Predictions from Constrained Aerodynamics Datasets

[132] Multiscale Neural Operators for Solving Time-Independent PDEs

[133] One-Shot Transfer Learning of Physics-Informed Neural Networks

[134] SVD-PINNs  Transfer Learning of Physics-Informed Neural Networks via  Singular Value Decomposition

[135] Distributed physics informed neural network for data-efficient solution  to partial differential equations

[136] Spatio-Temporal RBF Neural Networks

[137] PINN for Dynamical Partial Differential Equations is Not Training Deeper  Networks Rather Learning Advection and Time Variance

[138] Improved Training of Physics-Informed Neural Networks with Model  Ensembles

[139] Surrogate-data-enriched Physics-Aware Neural Networks

[140] Bayesian neural networks for weak solution of PDEs with uncertainty  quantification

[141] Neural-PDE  A RNN based neural network for solving time dependent PDEs

[142] Adversarial Adaptive Sampling  Unify PINN and Optimal Transport for the  Approximation of PDEs

[143] Separable PINN  Mitigating the Curse of Dimensionality in  Physics-Informed Neural Networks

[144] Preconditioning for Physics-Informed Neural Networks

[145] Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural  Networks

[146] Scientific Machine Learning through Physics-Informed Neural Networks   Where we are and What's next

[147] Physics-informed neural network solution of thermo-hydro-mechanical  (THM) processes in porous media

[148] A Metalearning Approach for Physics-Informed Neural Networks (PINNs)   Application to Parameterized PDEs

[149] Understanding and mitigating gradient pathologies in physics-informed  neural networks

[150] Training Physics-Informed Neural Networks via Multi-Task Optimization  for Traffic Density Prediction

[151] Meta Learning of Interface Conditions for Multi-Domain Physics-Informed  Neural Networks

[152] Achieving High Accuracy with PINNs via Energy Natural Gradients

[153] Meta-learning PINN loss functions

[154] Investigating Guiding Information for Adaptive Collocation Point  Sampling in PINNs

[155] Adversarial Uncertainty Quantification in Physics-Informed Neural  Networks

[156] TransNet  Transferable Neural Networks for Partial Differential  Equations

[157] PDE-LEARN  Using Deep Learning to Discover Partial Differential  Equations from Noisy, Limited Data

[158] Estimates on the generalization error of Physics Informed Neural  Networks (PINNs) for approximating PDEs

[159] Variational Physics-Informed Neural Networks For Solving Partial  Differential Equations

[160] Brain-Inspired Physics-Informed Neural Networks  Bare-Minimum Neural  Architectures for PDE Solvers

[161] Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural  Networks

[162] A Deep Learning Framework for Solving Hyperbolic Partial Differential  Equations  Part I

[163] Efficient training of physics-informed neural networks via importance  sampling

[164] Multi-resolution partial differential equations preserved learning  framework for spatiotemporal dynamics

[165] Scalable algorithms for physics-informed neural and graph networks

[166] Towards Understanding the Spectral Bias of Deep Learning

[167] Knowledge-Based Convolutional Neural Network for the Simulation and  Prediction of Two-Phase Darcy Flows

[168] Multiscale Neural Operator  Learning Fast and Grid-independent PDE  Solvers

[169] Physics-informed neural networks for solving thermo-mechanics problems  of functionally graded material

[170] Spectrally Adapted Physics-Informed Neural Networks for Solving  Unbounded Domain Problems

[171] PINNs-TF2  Fast and User-Friendly Physics-Informed Neural Networks in  TensorFlow V2

[172] AutoPINN  When AutoML Meets Physics-Informed Neural Networks

[173] CP-PINNs  Data-Driven Changepoints Detection in PDEs Using Online  Optimized Physics-Informed Neural Networks

[174] VarNet  Variational Neural Networks for the Solution of Partial  Differential Equations

[175] On the convergence of physics informed neural networks for linear  second-order elliptic and parabolic type PDEs

[176] An Analysis of Physics-Informed Neural Networks

[177] Efficient Protocols for Distributed Classification and Optimization

[178] Meta-Transfer Learning for Few-Shot Learning

[179] An Easy-to-use Real-world Multi-objective Optimization Problem Suite

[180] Physics-informed ConvNet  Learning Physical Field from a Shallow Neural  Network

[181] Neural Operators for Accelerating Scientific Simulations and Design

[182] Scalable adaptive algorithms for next-generation multiphase flow  simulations

[183] Insights into Multiscale Complexity  from Macroscopic Patterns to  Microscopic Simulations via Deep Learning

[184] A projection method for Navier-Stokes equations with a boundary  condition including the total pressure

[185] Solving multiscale elliptic problems by sparse radial basis function  neural networks

[186] Adversarial Training for Physics-Informed Neural Networks

[187] The Inverse Problems of some Mathematical Programming Problems

[188] Uncertainty Quantification and Deep Ensembles

[189] RANG  Reconstructing reproducible R computational environments

[190] A Unified Framework for the Error Analysis of Physics-Informed Neural  Networks

[191] PINNs and GaLS  A Priori Error Estimates for Shallow Physics Informed  Neural Networks Applied to Elliptic Problems

[192] A certified wavelet-based physics-informed neural network for the  solution of parameterized partial differential equations

[193] Multilayer Perceptron-based Surrogate Models for Finite Element Analysis

[194] Rapid Estimation of Left Ventricular Contractility with a  Physics-Informed Neural Network Inverse Modeling Approach

[195] Provably Correct Physics-Informed Neural Networks

[196] A Model Hierarchy for Predicting the Flow in Stirred Tanks with  Physics-Informed Neural Networks

[197] Physics-Informed Neural Networks with Skip Connections for Modeling and  Control of Gas-Lifted Oil Wells

[198] Artificial Neural Networks in Fluid Dynamics  A Novel Approach to the  Navier-Stokes Equations

[199] Frame-independent vector-cloud neural network for nonlocal constitutive  modeling on arbitrary grids

[200] VW-PINNs  A volume weighting method for PDE residuals in  physics-informed neural networks

[201] Transport Equation based Physics Informed Neural Network to predict the  Yield Strength of Architected Materials

[202] Adaptive quadratures for nonlinear approximation of low-dimensional PDEs  using smooth neural networks

[203] Ensemble learning for Physics Informed Neural Networks  a Gradient  Boosting approach

[204] Reconstructing Rayleigh-Benard flows out of temperature-only  measurements using Physics-Informed Neural Networks

[205] P-Index

[206] Causality for Machine Learning

[207] An Optimized Hybrid Approach for Path Finding

[208] Finite Basis Physics-Informed Neural Networks (FBPINNs)  a scalable  domain decomposition approach for solving differential equations

[209] Solving Partial Differential Equations with Point Source Based on  Physics-Informed Neural Networks

[210] Computer-inspired Quantum Experiments

[211] Uncertainty quantification through Monte Carlo method in a cloud  computing setting

[212] Morse Neural Networks for Uncertainty Quantification

[213] Quantum Amplitude Estimation for Probabilistic Methods in Power Systems

[214] Uncertainty Quantification via Neural Posterior Principal Components

[215] Automated quantum error mitigation based on probabilistic error  reduction

[216] IDRLnet  A Physics-Informed Neural Network Library

[217] How Many Equations of Motion Describe a Moving Human 

[218] Noise sensitivity and stability on groups

[219] PINN surrogate of Li-ion battery models for parameter inference. Part  II  Regularization and application of the pseudo-2D model

[220] Improving Interpretability of Deep Neural Networks with Semantic  Information

[221] Sparse and Continuous Attention Mechanisms

[222] On the scalability of CFD tool for supersonic jet flow configurations

[223] Machine learning and domain decomposition methods -- a survey

[224] On the Convergence of Network Systems

[225] Methodological Approach for the Evaluation of an Adaptive and Assistive  Human-Machine System

[226] Digital Twins

[227] Characterizing and Mitigating the Difficulty in Training  Physics-informed Artificial Neural Networks under Pointwise Constraints

[228] General Covariance Data Augmentation for Neural PDE Solvers

[229] Adversarial Sampling for Active Learning


