# Pre-trained Language Models in the Biomedical Domain: A Comprehensive Survey

## 1 Introduction

### 1.1 Background and Motivation

The emergence of pre-trained language models (PLMs) has marked a pivotal shift in the field of natural language processing (NLP), enabling significant advancements in various applications. Traditionally, NLP models relied heavily on manually crafted features and supervised learning, which often required extensive labeled data and domain-specific expertise. However, the advent of PLMs, particularly those trained on large-scale, unstructured text, has transformed this paradigm by allowing models to learn rich, contextualized representations of language from vast amounts of data. These models, initially developed for general-domain tasks, have shown exceptional performance across a wide range of NLP applications, such as text classification, machine translation, and question answering. This success has led to the development of domain-specific PLMs, which are tailored to handle the unique challenges of specialized fields such as the biomedical domain.

In the biomedical domain, the complexity and diversity of language pose significant challenges for traditional NLP approaches. Medical texts, including clinical notes, scientific articles, and patient records, are characterized by highly technical terminology, complex sentence structures, and domain-specific knowledge. These characteristics make it difficult for general NLP models to accurately understand and process biomedical texts. Furthermore, the availability of annotated biomedical data is often limited, making it challenging to train and evaluate models effectively. This data scarcity, combined with the need for domain-specific knowledge, has motivated researchers to explore the use of PLMs in the biomedical domain.

One of the key motivations for using PLMs in the biomedical domain is their ability to address the challenges of limited annotated data. PLMs can be pre-trained on large, unlabeled biomedical corpora, allowing them to capture the underlying patterns and structures of biomedical language. This pre-training process enables models to learn general language representations, which can then be fine-tuned on smaller, domain-specific datasets for specific tasks. This approach has proven effective in reducing the reliance on large amounts of annotated data, making it more feasible to develop models for tasks such as named entity recognition, clinical text analysis, and drug discovery. For example, the study by [1] demonstrated that integrating biomedical knowledge into PLMs can significantly improve their performance on tasks such as named entity recognition in clinical texts.

Another important motivation for using PLMs in the biomedical domain is the need for domain-specific knowledge. Biomedical texts often contain specialized terminology, concepts, and relationships that are not commonly found in general-domain texts. PLMs trained on biomedical corpora can better capture these domain-specific features, enabling more accurate and contextually relevant predictions. For instance, the work by [2] highlighted the importance of domain-specific pre-training for generating high-quality biomedical texts. By pre-training on PubMed abstracts, the BioBART model achieved superior performance on tasks such as summarization and dialogue generation in the biomedical domain.

The use of PLMs in the biomedical domain is also driven by the growing demand for automated and scalable solutions in healthcare and biomedical research. With the increasing volume of biomedical data, there is a pressing need for efficient and effective methods to extract insights and support decision-making. PLMs offer a promising solution by enabling the development of models that can process and understand large volumes of biomedical text. For example, the study by [3] explored the application of PLMs and large language models (LLMs) in biomedical text summarization, demonstrating their potential to improve the efficiency and effectiveness of information retrieval and management in clinical settings.

Moreover, the integration of external knowledge sources into PLMs has become a critical research direction in the biomedical domain. Biomedical knowledge graphs, such as the Unified Medical Language System (UMLS) and the Gene Ontology (GO), provide structured representations of biomedical concepts and relationships. By incorporating these knowledge sources into PLMs, researchers aim to enhance the models' ability to understand and reason about biomedical information. The work by [4] proposed a method that uses lightweight adapter modules to inject structured biomedical knowledge into PLMs, leading to improved performance on tasks such as document classification and question answering.

In addition to addressing the challenges of limited annotated data and domain-specific knowledge, PLMs in the biomedical domain also offer opportunities for innovation and improvement. For example, the study by [5] introduced a new pre-training method that leverages curriculum learning to optimize the training process for biomedical tasks. This approach demonstrated significant improvements in performance on named entity recognition tasks, highlighting the potential of domain-specific pre-training strategies.

Overall, the emergence of PLMs has revolutionized the field of NLP, and their application in the biomedical domain has been driven by the need to address the unique challenges of biomedical texts. By leveraging the power of domain-specific pre-training, integrating external knowledge, and adapting to the specific requirements of biomedical tasks, PLMs have the potential to significantly enhance the performance and applicability of NLP in the biomedical domain. As the field continues to evolve, further research into domain adaptation, model efficiency, and interpretability will be essential to unlock the full potential of PLMs in biomedical applications.

### 1.2 Significance of Pre-trained Language Models in Biomedical Applications

Pre-trained language models (PLMs) have emerged as a transformative force in the biomedical domain, offering unprecedented capabilities to understand and process complex medical texts, improve clinical decision-making, and drive advanced biomedical research. Their significance lies in their ability to bridge the gap between the vast amount of unstructured biomedical data and the need for actionable insights in healthcare settings. By leveraging pre-training on large-scale corpora, these models can capture domain-specific knowledge and generalize to a wide range of tasks, making them invaluable tools for researchers and practitioners alike.

One of the most critical roles of PLMs in biomedical applications is their ability to comprehend and extract meaningful information from complex medical texts. The biomedical literature is vast, rapidly growing, and often filled with technical jargon and intricate terminology that can be challenging for both humans and traditional machine learning models to interpret. PLMs, however, are trained on extensive biomedical corpora, allowing them to understand the nuances of medical language and identify key entities, relationships, and concepts. For instance, in the context of biomedical information extraction, PLMs have demonstrated remarkable performance in tasks such as named entity recognition (NER), relation extraction, and event detection, enabling the automatic identification of genes, proteins, diseases, and other critical biomedical entities [6].

In addition to enhancing the understanding of medical texts, PLMs play a pivotal role in improving clinical decision-making. Clinical decision support systems (CDSS) rely on accurate and timely information to guide healthcare professionals in making informed choices. PLMs can be integrated into these systems to provide evidence-based recommendations, interpret patient data, and assist in diagnosing complex conditions. For example, studies have shown that PLMs can effectively extract relevant information from electronic health records (EHRs) and generate summaries that highlight critical patient information, enabling clinicians to make faster and more accurate decisions [7].

Beyond clinical decision-making, PLMs have the potential to revolutionize biomedical research by accelerating the discovery of new knowledge and facilitating data-driven insights. The field of biomedical research is characterized by the need to process and analyze large volumes of data, from genomic sequences to clinical trial results. PLMs can assist researchers in this endeavor by performing tasks such as literature mining, hypothesis generation, and data annotation. For instance, in the context of drug discovery, PLMs have been used to identify potential drug targets, predict molecular interactions, and generate novel chemical compounds [6].

Another significant advantage of PLMs in biomedical applications is their adaptability to diverse tasks and domains. Unlike traditional models that are often trained for specific purposes, PLMs can be fine-tuned or adapted to a wide range of biomedical tasks, including clinical text analysis, biomedical information retrieval, and multilingual biomedical NLP. This flexibility is particularly important given the complexity and diversity of biomedical data. For example, recent studies have demonstrated the effectiveness of PLMs in tasks such as biomedical question answering, where models are trained to retrieve accurate information from medical literature or clinical texts [6].

The significance of PLMs in the biomedical domain is further underscored by their potential to address critical challenges such as data scarcity and domain adaptation. Biomedical data is often limited in quantity and quality, making it difficult to train effective models. PLMs, however, can leverage pre-training on large-scale general-domain data and then fine-tune on domain-specific data, enabling them to generalize well even with limited annotations. This approach has been successfully applied in various biomedical tasks, including clinical text analysis and biomedical information retrieval [6]. Moreover, the integration of external knowledge sources, such as biomedical knowledge graphs and terminology resources, has further enhanced the performance of PLMs by providing additional context and structure to their understanding of medical data.

In summary, the significance of pre-trained language models in biomedical applications cannot be overstated. Their ability to understand complex medical texts, improve clinical decision-making, and enable advanced biomedical research positions them as essential tools in the healthcare and research communities. As the field continues to evolve, the continued development and refinement of PLMs will be crucial in addressing the challenges of the biomedical domain and unlocking new possibilities for innovation and discovery.

### 1.3 Challenges in Biomedical NLP

Biomedical Natural Language Processing (NLP) presents a unique set of challenges that distinguish it from general NLP tasks. These challenges stem from the specific nature of biomedical text, which is characterized by complex language structures, domain-specific terminology, and limited availability of annotated data. As a result, developing effective NLP systems for the biomedical domain requires not only advanced technical solutions but also a deep understanding of the inherent difficulties in processing medical texts.

One of the primary challenges in biomedical NLP is the complexity of medical language. Medical texts often contain highly technical jargon, abbreviations, and acronyms that are not commonly found in general language. For instance, the medical field uses a vast array of specialized terms, such as "hypertension," "diabetes mellitus," and "myocardial infarction," which are not only complex but also context-dependent [8]. This complexity makes it difficult for NLP models to accurately understand and interpret the meaning of these terms, especially when they are used in different contexts. Additionally, the structure of medical texts is often non-linear and may include multiple layers of information, such as patient histories, diagnoses, and treatment plans, which further complicates the task of extracting meaningful insights [9].

Another significant challenge is the domain-specific terminology that is prevalent in biomedical texts. Medical language is highly specialized, with terms that are often derived from Latin or Greek, making them difficult for non-experts to understand. This terminology is not only extensive but also rapidly evolving, as new medical discoveries and technologies continue to emerge. For example, the development of new drugs and therapies introduces new terms and concepts that need to be incorporated into NLP systems. This constant evolution requires NLP models to be continuously updated and adapted to stay relevant, which poses a significant challenge in terms of model maintenance and performance [10].

Data scarcity is another critical challenge in biomedical NLP. Unlike general NLP tasks, where large annotated datasets are readily available, biomedical NLP faces a severe shortage of labeled data. This is due to the sensitive nature of medical data, which often involves patient privacy and confidentiality concerns. As a result, the creation of annotated datasets for biomedical NLP is a time-consuming and resource-intensive process [11]. Furthermore, the high cost of expert annotation makes it difficult to collect large-scale annotated data, which is essential for training effective NLP models. This data scarcity not only limits the performance of existing models but also hinders the development of new ones, as they lack the necessary training data to learn from [12].

The need for interpretable models is yet another challenge in biomedical NLP. In the healthcare domain, it is crucial for NLP models to be transparent and explainable, as their outputs can directly impact clinical decisions and patient outcomes. However, many state-of-the-art NLP models, such as deep learning-based approaches, are often considered "black boxes," making it difficult to understand how they arrive at their conclusions. This lack of interpretability can lead to mistrust among clinicians and healthcare professionals, who may be hesitant to rely on NLP models for critical tasks such as diagnosis and treatment planning [13]. To address this challenge, there is a growing need for models that not only perform well but also provide clear explanations for their predictions, enabling clinicians to make informed decisions [14].

In addition to these challenges, the integration of biomedical NLP into real-world clinical settings also presents several obstacles. For instance, the deployment of NLP models in clinical workflows requires careful consideration of factors such as data privacy, model robustness, and user acceptance. Clinical environments are highly regulated, and any NLP system must comply with stringent data protection laws and ethical standards. Moreover, the performance of NLP models can be affected by the quality and consistency of the input data, which may vary widely across different clinical settings [15]. These challenges highlight the need for robust and reliable NLP systems that can handle the complexities of biomedical data while ensuring compliance with ethical and regulatory requirements.

In conclusion, the challenges in biomedical NLP are multifaceted and require a comprehensive approach to address. The complexity of medical language, domain-specific terminology, data scarcity, and the need for interpretable models all contribute to the difficulty of developing effective NLP systems for the biomedical domain. By addressing these challenges through innovative research and collaboration, the field of biomedical NLP can continue to advance, leading to improved healthcare outcomes and more efficient medical information processing [2].

### 1.4 Overview of Biomedical Tasks and Applications

Pre-trained language models (PLMs) have become increasingly important in the biomedical domain, offering powerful tools to address a wide range of complex tasks. These tasks span various applications, from named entity recognition (NER) to clinical text analysis, drug discovery, and biomedical information retrieval. The application of PLMs in these areas has been transformative, enabling more efficient and accurate processing of biomedical data, which is often unstructured and rich in domain-specific terminology.

One of the most prominent applications of PLMs in the biomedical domain is named entity recognition (NER). NER involves the identification and classification of entities in biomedical texts, such as diseases, genes, proteins, and clinical terms. This task is critical for extracting structured information from unstructured biomedical texts, such as clinical notes, scientific articles, and electronic health records (EHRs). For instance, studies have demonstrated that pre-trained models like PubMedBERT and BioBERT have achieved state-of-the-art performance in biomedical NER tasks [16]. These models have been fine-tuned on large-scale biomedical corpora to better capture the nuances of medical terminology, thereby improving their accuracy in identifying and categorizing biomedical entities. Additionally, recent work has explored the use of prompt engineering and domain-specific pre-training to further enhance the performance of PLMs in NER tasks [17]. These techniques have shown significant improvements in handling the domain-specific challenges of biomedical NER, such as the high variability and complexity of medical terms.

Clinical text analysis is another key application of PLMs in the biomedical domain. Clinical texts, such as EHRs, discharge summaries, and medical notes, contain a wealth of information that is essential for clinical decision-making and research. PLMs have been employed to analyze these texts, extracting relevant information and identifying patterns that can inform clinical practices. For example, the MedCAT tool, an open-source unsupervised approach to NER+L, has demonstrated superior performance in extracting and linking biomedical concepts from EHRs [18]. Furthermore, studies have shown that PLMs can be effectively used for tasks such as clinical note classification, symptom extraction, and treatment prediction. These models have been trained on large datasets of clinical texts and have shown promising results in improving the accuracy and efficiency of clinical text analysis [19].

Drug discovery is another area where PLMs have made significant contributions. The process of drug discovery is complex and time-consuming, involving the identification of potential drug targets, the prediction of drug-target interactions, and the generation of novel compounds. PLMs have been used to accelerate this process by enabling the analysis of large-scale biomedical data, such as scientific literature and molecular databases. For example, recent studies have explored the use of PLMs for molecular generation and property prediction, demonstrating their ability to generate novel drug candidates and predict their properties with high accuracy [20]. Additionally, PLMs have been employed to extract information from biomedical literature, such as gene-disease associations and drug interactions, facilitating the discovery of new therapeutic targets [21]. These applications highlight the potential of PLMs to transform the drug discovery landscape by providing more efficient and data-driven approaches to identifying and validating new drugs.

Biomedical information retrieval is another critical application of PLMs, where they are used to search for relevant biomedical literature, documents, and clinical data. This task is essential for researchers and clinicians who need to access and analyze large volumes of biomedical data. PLMs have been employed to improve the efficiency and accuracy of biomedical information retrieval by leveraging their ability to understand and process natural language. For instance, studies have shown that PLMs can be used to enhance the performance of search engines by improving the relevance and accuracy of search results [22]. Additionally, recent work has focused on the integration of PLMs with knowledge graphs and other domain-specific resources to further enhance the effectiveness of biomedical information retrieval [23]. These approaches have demonstrated significant improvements in retrieving relevant biomedical information, making it easier for researchers and clinicians to access the knowledge they need.

In addition to these primary applications, PLMs have also been applied to a variety of other biomedical tasks, such as biomedical text summarization, medical question answering, and biomedical knowledge extraction. Text summarization, for instance, involves the generation of concise summaries of biomedical texts, which is crucial for researchers and clinicians who need to quickly access key information. PLMs have been used to develop effective summarization methods, enabling the extraction of important information from large volumes of biomedical literature [3]. Similarly, medical question answering involves the development of systems that can answer complex medical questions by retrieving and synthesizing information from biomedical texts. PLMs have been employed to improve the accuracy and efficiency of medical question answering, demonstrating their ability to provide reliable and contextually relevant answers [7].

Overall, the applications of PLMs in the biomedical domain are diverse and wide-ranging, encompassing a variety of tasks such as named entity recognition, clinical text analysis, drug discovery, and biomedical information retrieval. These models have shown remarkable performance in handling the unique challenges of biomedical texts, such as domain-specific terminology and data scarcity. As the field continues to evolve, the integration of PLMs with other technologies, such as knowledge graphs and multimodal data, is expected to further enhance their capabilities and expand their applications in the biomedical domain. The continued development and refinement of PLMs will play a crucial role in advancing biomedical research and improving healthcare outcomes.

### 1.5 Current State of Research and Key Contributions

The current state of research on pre-trained language models (PLMs) in the biomedical domain reflects a rapidly evolving landscape, driven by the need to address domain-specific challenges and leverage the power of advanced AI techniques. As the biomedical field grapples with the complexities of specialized terminology, data scarcity, and the demand for interpretability, researchers have developed a range of PLMs tailored to these unique requirements. These models have not only advanced the understanding of biomedical texts but have also significantly influenced the development of applications in clinical decision-making, drug discovery, and biomedical information retrieval.

A significant contribution to this field is the emergence of specialized PLMs such as BioBART and DrBERT, which are trained on domain-specific datasets and have demonstrated superior performance on biomedical tasks. These models have been designed to address the limitations of general-domain PLMs, which often struggle with the high specificity and complexity of biomedical language. For example, the study by "Pre-trained Language Models in Biomedical Domain: A Systematic Survey" highlights the importance of domain-specific training for PLMs, emphasizing that such models can better capture the nuances of biomedical texts and improve performance on tasks such as named entity recognition and relation extraction [6].

Moreover, the integration of biomedical knowledge graphs (KGs) into PLMs has emerged as a key contribution, enhancing the models' ability to reason and infer relationships within the biomedical domain. The work by "Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs" demonstrates how knowledge graphs can be leveraged to inject structured biomedical knowledge into pre-trained models, leading to improved performance on downstream tasks such as document classification and question answering [4]. This approach not only enriches the models' understanding of biomedical data but also enhances their interpretability, a critical factor in clinical settings.

Another notable contribution is the development of techniques for domain adaptation and fine-tuning, which have enabled the effective application of general PLMs to biomedical tasks. Studies such as "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction" highlight the importance of domain-specific pre-training, where models are trained on large-scale biomedical corpora to enhance their performance on specialized tasks [24]. This approach has been shown to significantly improve the models' ability to handle domain-specific language and improve their overall accuracy.

The application of PLMs in clinical settings has also seen significant advancements, particularly in the area of medical question answering. Research such as "Exploring the landscape of large language models in medical question answering" demonstrates the potential of LLMs to provide accurate and contextually relevant answers to medical queries, thereby supporting clinical decision-making [25]. These models have been evaluated on a range of medical datasets, showcasing their ability to perform well even in zero-shot scenarios, where they are not explicitly trained on the specific tasks they are tasked with.

In addition to these contributions, there is a growing body of research focused on the evaluation and benchmarking of PLMs in the biomedical domain. The work by "DrBenchmark A Large Language Understanding Evaluation Benchmark for French Biomedical Domain" emphasizes the importance of rigorous evaluation frameworks, highlighting the need for standardized metrics and benchmarks to assess the performance of PLMs on biomedical tasks [26]. This research underscores the significance of developing robust evaluation protocols to ensure the reliability and effectiveness of PLMs in clinical applications.

The field has also seen significant advancements in the development of specialized models for tasks such as biomedical text summarization. The study by "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models" provides a comprehensive overview of the latest techniques and approaches in this area, highlighting the role of PLMs in generating concise and accurate summaries of biomedical texts [3]. This research has been instrumental in advancing the field, demonstrating the potential of PLMs to enhance the efficiency of information retrieval and management in the biomedical domain.

Furthermore, the integration of multimodal data into PLMs has emerged as a promising area of research. The work by "RJUA-MedDQA A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning" explores the potential of combining text with other data types, such as images and genomic sequences, to enhance the models' understanding of complex biomedical phenomena [27]. This approach not only enriches the models' representations but also opens up new possibilities for applications in areas such as image-based diagnosis and personalized medicine.

The current state of research also reflects a growing emphasis on the ethical and societal implications of PLMs in the biomedical domain. Studies such as "Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research" highlight the need for updated guidelines and best practices to ensure the responsible use of PLMs in clinical settings [28]. These efforts are crucial in addressing concerns related to data privacy, model interpretability, and the potential for biased outputs.

Overall, the current state of research on PLMs in the biomedical domain is characterized by a combination of technical innovations, domain-specific adaptations, and a growing awareness of the ethical and societal implications of these models. The key contributions and advancements in this field have laid the groundwork for future research, highlighting the potential of PLMs to transform the way biomedical data is processed, analyzed, and utilized. As the field continues to evolve, the integration of domain-specific knowledge, the development of robust evaluation frameworks, and the consideration of ethical implications will remain critical in ensuring the effective and responsible application of PLMs in the biomedical domain.

### 1.6 Research Gaps and Opportunities

Despite the remarkable progress in pre-trained language models (PLMs) for biomedical applications, several research gaps and opportunities remain that hinder their broader adoption and effectiveness. One of the most pressing challenges is the lack of truly domain-specialized models that can capture the unique linguistic and conceptual structures of the biomedical domain. While general-purpose PLMs, such as BERT and GPT, have demonstrated strong performance on a wide range of tasks, their applicability in the biomedical field is often limited by the domain-specific terminology, complex sentence structures, and the need for domain-specific knowledge [10]. This highlights the necessity for developing models that are explicitly pre-trained on biomedical corpora and fine-tuned to the specific challenges of biomedical text. For instance, models like BioBERT [29] and BioMegatron [10] have shown promising results, but they still struggle with the nuanced and evolving nature of biomedical language, which requires continuous adaptation and improvement.

Another significant research gap lies in the realm of domain adaptation techniques. Biomedical NLP tasks often require models to perform well on out-of-domain data, which is a challenge given the variability in biomedical texts and the scarcity of labeled data [30]. While transfer learning and fine-tuning have shown some success, they often fail to capture the full complexity of domain-specific knowledge and the contextual nuances of biomedical texts [31]. For example, BioADAPT-MRC [31] demonstrated that adversarial learning can help bridge the gap between general and biomedical domains, but this approach still requires substantial computational resources and careful tuning. Moreover, the effectiveness of these methods is often limited by the availability of high-quality, domain-specific training data, which is scarce and difficult to obtain [32].

The integration of external knowledge sources is another area where current approaches fall short. Biomedical texts are rich in specialized knowledge, and models often need to leverage external knowledge bases, such as knowledge graphs and ontologies, to enhance their understanding and performance [33]. While some studies have explored the use of biomedical knowledge graphs, such as the PubMed Knowledge Graph (PKG) [33], these efforts are still in their infancy and require significant improvements in terms of scalability, coverage, and integration with PLMs. For example, the PKG [33] provides a valuable resource for linking biomedical entities and concepts, but it is limited in its ability to capture the dynamic and evolving nature of biomedical knowledge. Furthermore, the integration of such knowledge into PLMs often requires complex and computationally expensive processes, which can hinder their practical deployment [34].

Another critical research gap is the lack of interpretable and explainable models in the biomedical domain. Given the high stakes involved in healthcare applications, it is essential for models to provide transparent and interpretable outputs that clinicians can trust and understand. However, many state-of-the-art PLMs are black-box models that are difficult to interpret and explain, especially in the context of complex biomedical tasks [35]. For instance, while models like BioBERT and BioMegatron have shown strong performance on biomedical tasks, their internal mechanisms and decision-making processes are often opaque, making it difficult to assess their reliability and safety [36]. This lack of interpretability not only limits their clinical adoption but also hinders the ability of researchers to diagnose and improve model performance.

The need for efficient and scalable models is another key research opportunity. Biomedical applications often require models that can handle large-scale data and perform well on resource-constrained devices, such as mobile and edge devices. However, current PLMs are typically large and computationally intensive, which limits their deployment in real-world scenarios [37]. While some studies have explored model compression and parameter-efficient fine-tuning, such as those in [37], these approaches still face challenges in maintaining high performance while reducing model size and computational requirements. Additionally, the development of efficient and scalable models requires a deeper understanding of the trade-offs between model complexity, performance, and computational resources.

Finally, there is a significant opportunity to explore the integration of multimodal data in biomedical NLP. Biomedical texts are often accompanied by images, genomic sequences, and other forms of data, and models that can effectively leverage these multimodal inputs could significantly enhance their performance and applicability. However, current PLMs are primarily designed for text-only inputs and lack the capability to process and integrate multimodal data effectively [38]. For example, while some studies have explored the use of multimodal models for biomedical tasks, such as [38], these approaches are still in their early stages and require further development and validation.

In conclusion, while significant progress has been made in the application of PLMs to biomedical tasks, several research gaps and opportunities remain. These include the need for more specialized models, improved domain adaptation techniques, the integration of external knowledge sources, the development of interpretable and explainable models, the creation of efficient and scalable models, and the exploration of multimodal data integration. Addressing these challenges will be crucial for advancing the field of biomedical NLP and enabling the widespread adoption of PLMs in clinical and research settings.

### 1.7 Scope and Objectives of This Survey

The scope and objectives of this survey are designed to provide a comprehensive and structured overview of pre-trained language models (PLMs) in the biomedical domain. This survey aims to address the growing need for a systematic review of the methodologies, applications, and challenges associated with PLMs in the context of biomedical natural language processing (NLP). By examining the current state of research, this survey seeks to identify key trends, highlight critical issues, and propose future research directions that can guide the development of more effective and domain-specific PLMs in the biomedical field.  

The primary objective of this survey is to provide an in-depth analysis of how PLMs have been applied to various biomedical tasks, such as named entity recognition (NER), clinical text analysis, drug discovery, and biomedical information retrieval. By doing so, this survey aims to highlight the advantages of PLMs in handling the complexities of biomedical text, which often includes domain-specific terminology, intricate sentence structures, and a high degree of variability in language use. Moreover, the survey will explore the limitations of general-purpose PLMs when applied to biomedical tasks, such as data scarcity, domain adaptation challenges, and the need for specialized training.  

The target audience of this survey includes researchers, practitioners, and educators in the fields of biomedical NLP, artificial intelligence (AI), and computational linguistics. This survey is also intended to serve as a valuable resource for clinicians, healthcare professionals, and data scientists who are interested in leveraging PLMs to improve clinical decision-making, patient care, and biomedical research. By offering a comprehensive overview of the current state of PLMs in the biomedical domain, this survey aims to bridge the gap between theoretical advancements and practical applications in this field.  

One of the key contributions of this survey is to provide a systematic categorization of the various types of PLMs that have been developed for biomedical tasks. This includes models that are pre-trained on biomedical corpora, such as PubMed, PubMed Central, and clinical text datasets like MIMIC-III. The survey will also discuss the impact of different pre-training strategies, such as domain-specific pre-training, transfer learning, and knowledge integration, on the performance of PLMs in biomedical tasks. By analyzing these approaches, this survey will provide insights into the most effective ways to adapt PLMs for biomedical applications and highlight the trade-offs between model size, computational efficiency, and task-specific performance.  

Another important objective of this survey is to address the challenges and limitations that arise when applying general-purpose PLMs to biomedical tasks. For instance, while large language models (LLMs) have shown remarkable performance in general NLP tasks, their effectiveness in biomedical domains is often limited by the lack of annotated data, the complexity of biomedical language, and the need for domain-specific knowledge. This survey will explore the various techniques that have been proposed to overcome these challenges, such as data augmentation, domain adaptation, and the integration of external knowledge sources like biomedical ontologies and knowledge graphs [39; 2].  

Furthermore, this survey aims to evaluate the performance of different PLMs on a wide range of biomedical tasks and benchmark datasets. By comparing the results of various studies, this survey will provide a clear understanding of the strengths and weaknesses of different PLM approaches. This includes evaluating the performance of models such as BioBERT, BioALBERT, and BioBART on tasks like NER, relation extraction, and question answering, as well as discussing the impact of pre-training data, model architecture, and fine-tuning strategies on task performance [29; 40].  

In addition to analyzing existing research, this survey will also identify research gaps and opportunities in the biomedical NLP field. For example, while significant progress has been made in the development of PLMs for biomedical tasks, there is still a lack of standardized benchmarks and evaluation metrics that are specifically tailored to the biomedical domain. This survey will highlight the need for the creation of more comprehensive and domain-specific evaluation frameworks that can accurately assess the performance of PLMs in real-world biomedical applications [41; 42].  

Finally, this survey seeks to contribute to the broader discourse on the future of PLMs in the biomedical domain. It will explore emerging trends, such as the integration of multimodal data, the use of knowledge graphs, and the development of more efficient and interpretable models. By doing so, this survey aims to provide a roadmap for future research and development in the field of biomedical NLP, ensuring that PLMs continue to evolve in a way that is both effective and impactful for real-world applications.  

In conclusion, the scope and objectives of this survey are to provide a comprehensive and critical analysis of pre-trained language models in the biomedical domain. By examining the current state of research, identifying key challenges, and proposing future directions, this survey aims to contribute to the advancement of biomedical NLP and the development of more effective and domain-specific PLMs. Through this effort, we hope to foster greater collaboration between researchers, practitioners, and domain experts to drive innovation and improve the impact of PLMs in the biomedical field.

## 2 Overview of Pre-trained Language Models

### 2.1 Training Methods of Pre-trained Language Models

Pre-trained language models (PLMs) have revolutionized the field of natural language processing (NLP) by leveraging large-scale unlabeled data to learn rich contextual representations. One of the core aspects of PLMs is their training methods, which are primarily based on self-supervised learning. This approach allows models to learn from vast amounts of text without the need for explicit supervision, making it highly effective for tasks where labeled data is scarce or expensive to obtain. Self-supervised learning involves training models to predict certain parts of the input text, which enables them to capture both syntactic and semantic patterns.

Self-supervised learning is one of the most widely used training paradigms for PLMs. This method involves training models to predict masked tokens in a sentence, a technique known as masked language modeling (MLM). In MLM, a portion of the input tokens is randomly masked, and the model is trained to predict these masked tokens based on the context provided by the surrounding words. This approach not only helps the model understand the local context but also encourages it to learn the relationships between different words and phrases. For example, in the case of biomedical PLMs, this method can be particularly effective in capturing the nuances of specialized terminology and ensuring that the model can understand complex medical texts. [1]

Another important training objective in PLMs is next-sentence prediction (NSP), which involves training the model to determine whether a given sentence follows another sentence in a document. This task helps the model understand the coherence and flow of text, which is crucial for tasks such as question answering and document summarization. However, recent studies have shown that NSP may not be as effective as previously thought, particularly in the biomedical domain, where the structure of text can vary significantly. [2]

In addition to MLM and NSP, PLMs can also be trained using other objectives such as language modeling, where the model is trained to predict the next word in a sequence, or denoising, where the model is trained to reconstruct corrupted input. These alternative training objectives can help the model capture different aspects of language and improve its overall performance on a wide range of tasks. For instance, in the case of biomedical text, denoising can be particularly useful in handling the noise and inconsistencies that are common in clinical texts and biomedical literature. [2]

The training methods of PLMs also play a crucial role in their ability to adapt to specific domains. While general-purpose PLMs are trained on a diverse range of text, domain-specific PLMs are trained on data that is tailored to a particular field, such as biomedicine. This domain-specific training can help the model better understand the terminology, context, and structure of the domain-specific text. For example, in the biomedical domain, domain-specific training can help the model capture the complex relationships between diseases, genes, and proteins, which is essential for tasks such as named entity recognition and relation extraction. [1]

Moreover, the training methods of PLMs can also be influenced by the size and complexity of the model. Larger models, such as those with billions of parameters, require more sophisticated training methods to ensure that they can learn effectively from the data. Techniques such as curriculum learning, where the model is trained on increasingly complex tasks, can help improve the model's performance on specific tasks. [5]

Another important aspect of training methods is the use of pre-training and fine-tuning. Pre-training involves training the model on a large corpus of text to learn general language representations, while fine-tuning involves adapting the model to a specific task using a smaller, task-specific dataset. This two-step process can significantly improve the model's performance on the target task. For example, in the biomedical domain, pre-training on a large corpus of biomedical texts and then fine-tuning on a specific task such as clinical text analysis can lead to substantial improvements in performance. [2]

The effectiveness of different training methods can vary depending on the specific task and domain. For instance, in the biomedical domain, the use of domain-specific corpora for pre-training can lead to better performance on tasks such as named entity recognition and relation extraction. This is because the domain-specific corpus provides the model with a more accurate understanding of the terminology and context of the domain. [1]

In addition to domain-specific pre-training, the use of knowledge graphs and other structured data sources can also enhance the training of PLMs. Knowledge graphs provide structured representations of domain-specific information, which can help the model learn more about the relationships between different entities. For example, in the biomedical domain, incorporating knowledge graphs such as the Unified Medical Language System (UMLS) can help the model better understand the relationships between diseases, symptoms, and treatments. [1]

The training methods of PLMs also play a crucial role in their ability to handle multilingual and cross-lingual tasks. In the biomedical domain, where texts can be written in multiple languages, the ability of PLMs to understand and generate text in different languages is essential. Techniques such as multilingual pre-training and cross-lingual adaptation can help the model achieve this. For example, in the case of French biomedical texts, pre-training the model on a large corpus of French biomedical texts can significantly improve its performance on tasks such as named entity recognition and question answering. [43]

Finally, the training methods of PLMs can also be influenced by the availability of computational resources. Training large models requires significant computational power, which can be a limiting factor for many researchers and practitioners. Techniques such as parameter-efficient fine-tuning and model compression can help reduce the computational costs of training and deploying PLMs. For example, in the biomedical domain, using techniques such as low-rank adaptation (LoRA) can help reduce the number of parameters that need to be fine-tuned, making it more feasible to train and deploy PLMs on limited computational resources. [44]

In conclusion, the training methods of PLMs are a critical aspect of their development and application. Self-supervised learning, masked language modeling, and other training objectives play a crucial role in enabling PLMs to learn rich contextual representations of text. Additionally, domain-specific pre-training, the use of knowledge graphs, and the adoption of efficient fine-tuning techniques can significantly enhance the performance of PLMs in specific domains such as biomedicine. As the field of NLP continues to evolve, the development of more effective and efficient training methods will be essential for advancing the capabilities of PLMs in a wide range of applications.

### 2.2 Architectures of Pre-trained Language Models

Pre-trained language models (PLMs) have revolutionized natural language processing (NLP) by leveraging complex architectures that enable efficient language representation. Among these, the Transformer architecture stands out as a foundational framework that has significantly influenced the development of modern PLMs. Introduced in the paper "Attention Is All You Need" [45], the Transformer model replaced traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, allowing for parallel processing and capturing long-range dependencies in text. This architectural innovation has become the backbone of many state-of-the-art PLMs, including BERT [46], GPT [47], and T5 [48]. The Transformer's ability to process input sequences in parallel while maintaining contextual awareness has made it a cornerstone for tasks ranging from text classification to machine translation.

One of the key features of the Transformer architecture is its bidirectional encoder, which allows the model to capture contextual information from both the left and right sides of a token. This is in contrast to unidirectional models such as the original GPT, which only considers context from the left side of a token. Bidirectional encoders have proven to be highly effective for tasks that require a deep understanding of context, such as named entity recognition and question-answering. For instance, the BERT model, which is based on a bidirectional Transformer, has achieved state-of-the-art results on various NLP benchmarks by leveraging this architecture [46]. The bidirectional nature of BERT allows it to understand the relationship between words in a sentence more accurately, leading to better performance on tasks that require contextual understanding.

Another important variant of the Transformer architecture is the decoder-only model, which is used in models like GPT. Unlike the bidirectional encoder, the decoder-only architecture processes input in a unidirectional manner, making it particularly suitable for tasks such as text generation. This architecture allows the model to generate coherent and contextually relevant text by predicting the next token in a sequence based on the previous tokens. The GPT series, which is based on this decoder-only architecture, has demonstrated impressive capabilities in generating high-quality text for various applications, including summarization and dialogue systems [47]. The flexibility of the decoder-only architecture has made it a popular choice for tasks that require the model to produce continuous text, such as writing essays or generating code.

In addition to the Transformer and its variants, other architectures have been explored to enhance the efficiency and performance of PLMs. For example, the BERT model incorporates a bidirectional Transformer encoder with a masked language modeling (MLM) objective, where the model is trained to predict masked tokens in a sentence. This approach enables BERT to learn rich contextual representations of words by considering their surrounding context. The effectiveness of this architecture has been demonstrated in various biomedical applications, where the model is used to extract information from clinical texts and biomedical literature [6]. The masked language modeling objective has also been extended to other PLMs, such as RoBERTa, which further improves the performance of the model by using a dynamic masking strategy and training on a larger corpus [49].

Another notable variant of the Transformer architecture is the T5 model, which introduces a text-to-text framework for training. This architecture allows the model to handle a wide range of NLP tasks by treating all tasks as a sequence-to-sequence problem. The T5 model is based on a bidirectional encoder and a unidirectional decoder, making it suitable for tasks that require both understanding and generation of text. This approach has been particularly effective in biomedical applications, where the model is used to generate summaries of clinical texts and answer medical questions [3]. The text-to-text framework has also been adopted by other PLMs, such as mT5, which extends the T5 architecture to support multilingual tasks [50].

The efficiency and scalability of PLMs have also been improved through architectural innovations. For example, the DistilBERT model is a compact version of BERT that retains most of its performance while reducing the number of parameters. This makes it more suitable for deployment in resource-constrained environments, such as mobile devices or edge computing systems. The DistilBERT architecture has been applied in various biomedical tasks, where the model is used to process clinical texts and extract relevant information [20]. Another example is the ALBERT model, which introduces parameter sharing and factorized embedding to reduce the computational requirements of the model while maintaining its performance [51]. These architectural improvements have made it possible to deploy PLMs in a wider range of applications, including those that require real-time processing.

In conclusion, the architectures of pre-trained language models have evolved significantly over the years, with the Transformer architecture serving as the foundation for many state-of-the-art models. The bidirectional encoder, decoder-only architecture, and text-to-text framework have all contributed to the success of PLMs in various NLP tasks. The continuous development of these architectures has enabled PLMs to achieve impressive results in biomedical applications, where the ability to understand and generate human-like text is crucial. As research in this area continues to advance, we can expect to see even more innovative architectures that further enhance the efficiency and performance of pre-trained language models.

### 2.3 Applications of Pre-trained Language Models in General NLP Tasks

Pre-trained language models (PLMs) have revolutionized the field of natural language processing (NLP) by achieving state-of-the-art performance across a wide range of general NLP tasks. These models, such as BERT, GPT, and their variants, are trained on massive amounts of text data to learn rich linguistic representations, enabling them to capture complex semantic and syntactic patterns. The applications of PLMs in general NLP tasks are vast and varied, spanning text classification, machine translation, question answering, and more. These models have not only enhanced the accuracy of existing NLP tasks but also opened new avenues for research and innovation in the broader NLP landscape.

One of the most prominent applications of PLMs is in text classification. Text classification involves assigning predefined categories or labels to a given text, and PLMs have shown remarkable performance in this area. For example, BERT and its variants have been widely used for tasks such as sentiment analysis, topic classification, and spam detection [2; 10]. These models leverage their contextual understanding of language to accurately classify text into relevant categories, often outperforming traditional machine learning approaches. The ability of PLMs to generalize across different domains and tasks makes them highly versatile for text classification applications.

Another critical application of PLMs is in machine translation, where the goal is to translate text from one language to another. PLMs have significantly improved the quality of machine translation systems by capturing the nuances of language more effectively than traditional methods. Models such as the Transformer and its variants have been pre-trained on large multilingual corpora, allowing them to learn translation patterns across multiple languages. These models can then be fine-tuned on specific translation tasks to achieve high accuracy. The success of PLMs in machine translation has led to the development of more efficient and accurate translation systems that can handle complex linguistic structures and idiomatic expressions [2; 10].

Question answering is another domain where PLMs have demonstrated exceptional performance. PLMs are trained to understand the context of a given question and retrieve or generate the most relevant answer from a provided text. This task is particularly challenging due to the need for both understanding the question and retrieving the correct information from the text. Models like BERT and its variants have been widely used in question answering tasks, achieving high accuracy in benchmarks such as the SQuAD dataset [2; 10]. The ability of PLMs to understand the context and provide precise answers has made them invaluable in applications such as virtual assistants, chatbots, and information retrieval systems.

In addition to these primary applications, PLMs have also been employed in a variety of other NLP tasks, including text summarization, sentiment analysis, and named entity recognition. Text summarization involves generating concise summaries of longer texts, and PLMs have shown promise in this area by capturing the key points and maintaining the overall meaning of the original text [2; 10]. Sentiment analysis, which involves determining the sentiment or emotional tone of a text, has also benefited from the use of PLMs, as they can accurately classify texts into positive, negative, or neutral categories. Named entity recognition, which involves identifying and classifying named entities in text, has also seen improvements with the use of PLMs, as they can effectively capture the context and relationships between entities [2; 10].

The success of PLMs in these general NLP tasks has laid the groundwork for their adaptation to specialized domains, such as the biomedical domain. The techniques and strategies developed for general NLP tasks can be leveraged to enhance the performance of PLMs in biomedical applications, such as clinical text analysis, drug discovery, and biomedical information retrieval. For instance, the ability of PLMs to understand and generate natural language can be applied to the analysis of clinical texts, where the goal is to extract meaningful information from unstructured data [2; 10]. Similarly, the use of PLMs in drug discovery has shown promise in tasks such as molecular generation and drug-target interaction prediction, where the models can learn from large-scale biomedical data to make predictions and generate new molecules [2; 10].

Moreover, the adaptability of PLMs to different tasks and domains has been further enhanced by the development of transfer learning techniques. Transfer learning allows models trained on one task or domain to be fine-tuned for another task or domain, enabling them to leverage the knowledge learned from the general domain to improve performance in specialized applications. This approach has been particularly effective in the biomedical domain, where the availability of annotated data is limited. By pre-training on large-scale biomedical corpora and then fine-tuning on specific tasks, PLMs can achieve high accuracy even with limited labeled data [2; 10].

In conclusion, the applications of pre-trained language models in general NLP tasks have been extensive and impactful, demonstrating their versatility and effectiveness across a wide range of domains. These models have not only improved the performance of existing NLP tasks but have also paved the way for their adaptation to specialized domains, such as the biomedical field. The techniques and strategies developed for general NLP tasks can be leveraged to enhance the performance of PLMs in biomedical applications, leading to new possibilities for research and innovation in the field. As the field of NLP continues to evolve, the role of PLMs in both general and specialized applications will undoubtedly continue to grow, driving further advancements in the domain.

### 2.4 Domain Adaptation in Pre-trained Language Models

Domain adaptation in pre-trained language models (PLMs) refers to the process of modifying or fine-tuning a general-purpose language model to better suit specific domains or tasks. This is particularly crucial in the biomedical domain, where the complexity of medical language, the presence of domain-specific terminology, and the scarcity of annotated data pose significant challenges. Pre-trained language models, which are typically trained on large-scale general-domain corpora, often struggle to perform effectively on biomedical tasks without further adaptation. Therefore, domain adaptation techniques are essential to ensure that these models can understand and process biomedical texts accurately and efficiently.

One of the primary methods of domain adaptation is fine-tuning, which involves further training a pre-trained model on a smaller, domain-specific dataset. This process allows the model to learn the unique characteristics of the biomedical domain, such as specialized terminology and the structure of clinical texts. For instance, in the context of biomedical named entity recognition (NER), fine-tuning a general-domain model like BERT on a biomedical corpus has been shown to significantly improve performance. A study by [21] demonstrated that fine-tuning a pre-trained model on a biomedical dataset led to substantial improvements in the identification of disease-related entities, highlighting the importance of domain-specific fine-tuning. Additionally, [52] explored the use of biomedical knowledge graphs to enhance the performance of pre-trained models, emphasizing that incorporating structured knowledge can improve the model's ability to understand and link biomedical entities.

Another approach to domain adaptation is retraining, where a pre-trained model is retrained from scratch on a large biomedical corpus. This method is particularly useful when the domain-specific dataset is sufficiently large and diverse. For example, [53] introduced a model that was pre-trained on a large corpus of free-text patients' records from the MIMIC-III dataset. The model achieved high performance on a variety of clinical tasks, including named entity recognition, demonstrating the effectiveness of retraining on domain-specific data. However, retraining is often computationally expensive and may not be feasible for all applications, especially when the available data is limited.

In addition to fine-tuning and retraining, domain adaptation can also be achieved through the use of domain-specific pre-training. This involves training a model on a large corpus of biomedical texts before fine-tuning it on a specific task. [21] showed that pre-training on biomedical texts significantly improved the model's ability to extract relevant information from scientific literature. Similarly, [53] demonstrated that pre-training on a large biomedical corpus led to better performance on clinical NLP tasks, even when the model was fine-tuned on a smaller dataset.

Domain adaptation also involves the integration of external knowledge sources, such as biomedical ontologies and knowledge graphs. These resources provide structured information that can enhance the model's understanding of domain-specific concepts. [52] proposed a framework that leverages biomedical knowledge graphs to improve the performance of pre-trained models in entity linking tasks. By incorporating relational knowledge, the model was able to better understand the semantic relationships between biomedical entities, leading to improved accuracy in entity linking. Similarly, [4] explored the use of adapter modules to inject structured biomedical knowledge into pre-trained models, resulting in performance improvements on downstream tasks.

Prompt engineering is another technique used in domain adaptation, where the input to the model is modified to guide it towards better performance on domain-specific tasks. [17] demonstrated that incorporating external knowledge through prompts can significantly improve the performance of large language models (LLMs) in biomedical NER tasks. The study showed that by providing relevant examples and knowledge sources, the model's ability to recognize and classify biomedical entities was greatly enhanced.

Transfer learning is also a key aspect of domain adaptation, where a model trained on a general-domain task is adapted to a specific biomedical task. [54] explored the use of transfer learning to adapt LLMs to biomedical tasks such as classification and reasoning. The study found that fine-tuning LLMs on domain-specific tasks, such as classifying clinical statements or detecting causal relations in biomedical literature, led to improved performance compared to using the models in their pre-trained state. However, the study also noted that prompt engineering required significant investment and that simpler models, such as bag-of-words (BoW) with logistic regression, could sometimes outperform more complex models.

Efficient adaptation techniques, such as parameter-efficient fine-tuning and knowledge distillation, are also gaining attention in the field of domain adaptation. These methods aim to reduce the computational resources required for fine-tuning while maintaining or even improving performance. [20] demonstrated that knowledge distillation, where a smaller model is trained to mimic the behavior of a larger model, can lead to significant performance improvements in biomedical tasks. The study showed that a distilled model achieved comparable accuracy to a larger model while being significantly more efficient.

In summary, domain adaptation in pre-trained language models is a critical area of research, especially in the biomedical domain. Techniques such as fine-tuning, retraining, domain-specific pre-training, integration of external knowledge, prompt engineering, transfer learning, and efficient adaptation methods are all essential for improving the performance of these models on domain-specific tasks. The studies cited above highlight the importance of these approaches and demonstrate their effectiveness in various biomedical applications. As the field continues to evolve, further research into domain adaptation techniques will be necessary to address the unique challenges of the biomedical domain.

### 2.5 Challenges of Applying General Pre-trained Models to Biomedical Tasks

The application of general pre-trained language models (PLMs) to biomedical tasks presents several unique challenges that hinder their effectiveness and practical utility. While these models excel in general natural language processing (NLP) tasks, their performance in specialized biomedical domains is often suboptimal due to domain-specific terminology, data scarcity, and the need for specialized training. These challenges are further compounded by the complexity of biomedical language, which includes a vast array of technical jargon, abbreviations, and highly specific concepts that are not commonly encountered in general language corpora.

One of the primary challenges is the domain-specific terminology used in biomedical texts. Unlike general language, which is often more straightforward and widely understood, biomedical language is laden with specialized vocabulary and complex sentence structures that can be difficult for general PLMs to comprehend and process. For instance, terms like "proteasome-mediated degradation" or "histone acetylation" are not only highly specific but also require an understanding of underlying biological mechanisms to be accurately interpreted. General PLMs, which are typically trained on vast corpora of general text, may lack the contextual understanding required to accurately recognize and process such specialized terminology [7]. This limitation can lead to misinterpretations, reduced accuracy, and poor performance in tasks such as named entity recognition (NER) and relation extraction.

Another significant challenge is data scarcity. Biomedical data, particularly annotated data, is often limited in quantity and quality compared to general-domain data. This scarcity poses a significant obstacle for training and fine-tuning PLMs for biomedical tasks, as these models typically require large amounts of labeled data to achieve high performance. The limited availability of annotated biomedical datasets makes it difficult to train models that can effectively generalize to new tasks and domains. For example, in the context of biomedical text summarization, the lack of standardized and well-annotated datasets hampers the development of robust models that can accurately capture the essential information from complex biomedical texts [3]. Furthermore, the process of annotation itself is time-consuming and requires domain expertise, making it even more challenging to generate the necessary data for training.

The need for specialized training is another critical challenge. General PLMs are trained on a broad range of text data, which may not be representative of the specific characteristics of biomedical texts. As a result, these models may struggle to capture the nuances of biomedical language and the specific relationships between entities and concepts. For instance, in tasks such as drug discovery and molecular generation, general PLMs may not have the domain-specific knowledge required to accurately predict the properties and interactions of molecules. This lack of domain-specific training can lead to suboptimal performance and reduced reliability in biomedical applications [55].

Additionally, the complexity of biomedical language poses a significant challenge for general PLMs. Biomedical texts often contain dense and intricate information, requiring models to understand not only the surface-level semantics but also the underlying biological and medical contexts. This complexity is further exacerbated by the presence of multi-modal data, such as images and genomic sequences, which add another layer of difficulty for general PLMs that are primarily designed for text-based tasks. The integration of multi-modal data into PLMs requires specialized architectures and training strategies, which are not typically part of the standard training pipeline for general PLMs [56].

The challenges of applying general PLMs to biomedical tasks are further compounded by the need for interpretability and explainability. In the biomedical domain, transparency and explainability are crucial for building trust and ensuring the safe deployment of AI models. However, general PLMs are often considered "black boxes," making it difficult to understand how they arrive at their predictions. This lack of transparency can be particularly problematic in clinical settings, where the reliability and interpretability of AI models are essential for informed decision-making. For example, in tasks such as clinical text analysis and medical question answering, the inability to explain model predictions can lead to skepticism and resistance from healthcare professionals [57].

Moreover, the integration of external knowledge sources is another challenge in applying general PLMs to biomedical tasks. While general PLMs are trained on large and diverse datasets, they may not have access to the specialized knowledge and structured information that is crucial for biomedical applications. This limitation can be addressed by incorporating external knowledge bases, such as biomedical ontologies and knowledge graphs, into the training process. However, the integration of such knowledge sources requires additional computational resources and specialized techniques, which may not be feasible for all applications [58].

Finally, the challenges of applying general PLMs to biomedical tasks highlight the need for domain-specific adaptations and fine-tuning. While general PLMs can provide a strong baseline for various NLP tasks, they often require significant modifications to be effective in biomedical contexts. These adaptations may include domain-specific pre-training, transfer learning, and the use of specialized training data. For instance, the development of biomedical-specific PLMs, such as PubMedBERT and BioLinkBERT, has shown promising results in improving the performance of models on biomedical tasks [4]. However, the process of adapting general PLMs to biomedical tasks is complex and requires careful consideration of various factors, including data quality, model architecture, and training strategies.

In conclusion, the application of general pre-trained language models to biomedical tasks presents several unique challenges, including domain-specific terminology, data scarcity, the need for specialized training, and the complexity of biomedical language. Addressing these challenges requires a combination of domain-specific adaptations, specialized training, and the integration of external knowledge sources. As the field of biomedical NLP continues to evolve, the development of more robust and effective models that can overcome these challenges will be essential for advancing research and improving clinical outcomes.

### 2.6 Overview of Biomedical Pre-trained Language Models

Biomedical pre-trained language models (PLMs) have emerged as a powerful tool for processing and understanding biomedical text, addressing the unique challenges of the domain such as domain-specific terminology, complex language, and limited annotated data. Unlike general-purpose language models, which are trained on large-scale, diverse text from the internet, biomedical PLMs are specifically trained on biomedical corpora, allowing them to better capture the nuances and specialized vocabulary of the biomedical field. These models are designed to enhance the performance of downstream biomedical tasks such as named entity recognition, relation extraction, and question answering, making them essential for advancing biomedical research and clinical applications.

One of the key characteristics of biomedical PLMs is their training strategies, which are tailored to the specific needs of the biomedical domain. These models are typically pre-trained on large-scale biomedical texts, such as PubMed abstracts, clinical notes, and scientific articles, allowing them to learn the language patterns and semantic structures unique to biomedical content. For example, the BioMegatron model, which is one of the largest biomedical language models, was trained on a large biomedical corpus, resulting in significant improvements in performance on biomedical NLP benchmarks such as named entity recognition, relation extraction, and question answering [10]. This demonstrates the importance of domain-specific pre-training in enhancing the effectiveness of PLMs for biomedical tasks.

Another critical aspect of biomedical PLMs is their ability to handle the domain-specific terminology and complex language structures prevalent in biomedical texts. Unlike general-purpose models, which may struggle with the specialized vocabulary and intricate sentence structures found in biomedical literature, biomedical PLMs are designed to better understand and process this type of content. For instance, the BioBERT model, which is a biomedical adaptation of BERT, has been shown to outperform general-domain models in biomedical tasks, highlighting the advantages of domain-specific training [29]. This is particularly important in tasks such as named entity recognition, where the correct identification of biomedical entities like diseases, genes, and drugs is crucial for accurate analysis.

The applications of biomedical PLMs are vast and span multiple areas of biomedical research and clinical practice. One of the most prominent applications is in biomedical information retrieval, where these models are used to search for relevant literature, clinical data, and scientific findings. For example, the PKG (PubMed Knowledge Graph) project leveraged Biomedical PLMs to extract bio-entities from millions of PubMed abstracts, creating a comprehensive knowledge graph that facilitates the discovery of new biomedical insights [33]. This demonstrates how biomedical PLMs can be used to extract and organize information from large biomedical corpora, enabling more efficient knowledge discovery and decision-making.

In addition to information retrieval, biomedical PLMs are also used in clinical text analysis, where they help analyze electronic health records (EHRs), discharge summaries, and other clinical texts. These models can identify patterns and relationships within clinical data, aiding in tasks such as diagnosis, treatment planning, and patient monitoring. For instance, the MedDistant19 benchmark was developed to evaluate the performance of biomedical PLMs in relation extraction tasks, demonstrating the effectiveness of these models in extracting meaningful information from clinical texts [59].

Another important application of biomedical PLMs is in drug discovery and molecular generation. These models can be used to generate new drug candidates, predict drug-target interactions, and identify potential drug repurposing opportunities. For example, the use of large language models (LLMs) in biomedical knowledge curation has shown promise in scaling biomedical knowledge extraction, with distillation techniques improving the efficiency and accuracy of these models [20]. This highlights the potential of biomedical PLMs in accelerating the drug discovery process and improving the efficiency of pharmaceutical research.

Biomedical PLMs are also being used in the development of knowledge graphs, which are essential for integrating and organizing biomedical information. These models help extract and structure relationships between biomedical entities, enabling the creation of comprehensive knowledge bases. For example, the Know2BIO benchmark provides a comprehensive dataset for evaluating the performance of biomedical PLMs in knowledge graph representation learning, showcasing the potential of these models in advancing biomedical knowledge integration [60].

Furthermore, the integration of biomedical PLMs with multimodal data, such as images and genomic sequences, is an emerging area of research. These models can be used to analyze and interpret complex biomedical data, facilitating tasks such as medical image segmentation and genomic analysis. For instance, the OpenMEDLab platform provides an open-source platform for multi-modality foundation models in medicine, demonstrating the potential of biomedical PLMs in handling diverse data types [38].

In conclusion, biomedical pre-trained language models represent a significant advancement in the field of natural language processing, specifically tailored to the unique challenges of the biomedical domain. These models are trained on domain-specific corpora, enabling them to better understand and process biomedical text. Their applications span a wide range of tasks, from information retrieval and clinical text analysis to drug discovery and knowledge graph construction. As the field continues to evolve, the development of more specialized and efficient biomedical PLMs will be crucial for advancing biomedical research and improving clinical outcomes.

### 2.7 Case Studies of Biomedical Pre-trained Language Models

The emergence of biomedical pre-trained language models has marked a significant advancement in the field of natural language processing (NLP) by addressing the unique challenges of the biomedical domain. These models are specifically designed to capture the nuances of medical language, which is often highly technical and laden with domain-specific terminology. Among the notable models, DrBERT, KBioXLM, and BioBART have garnered attention for their innovative approaches and impressive performance in various biomedical tasks. This section delves into the design, training strategies, and performance of these models, shedding light on their contributions to the biomedical NLP landscape.

DrBERT is a pre-trained language model that was developed specifically for the biomedical domain, leveraging the extensive knowledge embedded in biomedical literature. The model is based on the BERT architecture, but it is fine-tuned on a large corpus of biomedical texts, including PubMed abstracts and clinical notes. This specialized training allows DrBERT to better understand the context and semantics of medical terminology, making it highly effective for tasks such as named entity recognition (NER) and relation extraction [43]. One of the key strengths of DrBERT is its ability to generalize across different biomedical tasks, as it has been shown to outperform general-domain models in several benchmark datasets. For instance, DrBERT achieved a significant improvement in F1 scores for biomedical NER tasks, demonstrating its effectiveness in capturing the complex relationships between medical entities [43].

KBioXLM is another notable pre-trained language model that focuses on the biomedical domain. Unlike traditional models that rely solely on text data, KBioXLM integrates knowledge from biomedical knowledge graphs, enhancing its understanding of the underlying relationships between entities. This integration of structured knowledge allows KBioXLM to provide more accurate and contextually relevant results in tasks such as entity linking and relation extraction [61]. The model was trained on a diverse set of biomedical texts, including clinical notes and scientific articles, which enabled it to capture the nuances of medical language. Furthermore, KBioXLM's performance on benchmark datasets has been impressive, with significant improvements in tasks such as drug-target interaction prediction and disease classification [61]. The incorporation of knowledge graphs into the training process not only enhances the model's performance but also improves its interpretability, making it a valuable tool for researchers and clinicians alike.

BioBART is a generative language model that adapts the BART architecture to the biomedical domain. This model is particularly effective in tasks such as text summarization and dialogue generation, which are critical in biomedical applications. BioBART was pre-trained on a large corpus of biomedical texts, including PubMed abstracts and clinical documents, allowing it to capture the complexities of medical language [2]. The model's design emphasizes the ability to generate coherent and contextually accurate summaries of biomedical literature, which is essential for researchers and clinicians who need to quickly access and understand the latest findings. Additionally, BioBART has been shown to outperform general-domain models in tasks such as question answering and entity recognition, highlighting its effectiveness in handling the specific challenges of the biomedical domain [2]. The model's performance on benchmark datasets further underscores its potential as a valuable tool for biomedical NLP applications.

The success of these models is not only attributed to their architectural innovations but also to their training strategies. For instance, DrBERT and BioBART utilize a combination of pre-training and fine-tuning techniques to adapt to the specific needs of biomedical tasks. This approach allows the models to leverage the vast amount of biomedical text available while also incorporating domain-specific knowledge to improve their performance. Additionally, the use of domain-specific corpora for training ensures that the models are exposed to the unique language patterns and terminology found in biomedical texts, which is crucial for their effectiveness in real-world applications.

Another critical aspect of these models is their ability to handle the challenges of data scarcity in the biomedical domain. For example, KBioXLM incorporates knowledge from biomedical knowledge graphs to supplement the limited amount of annotated data, which is a common issue in the field. This integration of structured knowledge not only improves the model's performance but also enhances its ability to generalize across different tasks. Furthermore, the use of transfer learning techniques allows these models to leverage pre-trained representations from general-domain models, enabling them to achieve better performance with less annotated data [61].

The effectiveness of these models is further supported by their performance on benchmark datasets. For instance, DrBERT has been shown to outperform other models in tasks such as biomedical NER and relation extraction, demonstrating its superiority in handling the complexities of medical language [43]. Similarly, BioBART has achieved impressive results in text summarization and dialogue generation, highlighting its potential in applications that require the generation of coherent and contextually relevant biomedical content [2]. These benchmarks not only validate the models' capabilities but also provide a framework for future research and development in the biomedical NLP domain.

In conclusion, the case studies of DrBERT, KBioXLM, and BioBART illustrate the significant advancements in the development of biomedical pre-trained language models. These models are not only designed to address the unique challenges of the biomedical domain but also demonstrate impressive performance in various tasks. Their innovative approaches, combined with specialized training strategies, highlight the potential of pre-trained language models in transforming the way biomedical data is processed and analyzed. As the field continues to evolve, these models will play a crucial role in advancing the capabilities of NLP in the biomedical domain, paving the way for more accurate and efficient applications in healthcare and research.

### 2.8 Comparative Analysis of Pre-trained Language Models in Biomedical Domain

The comparative analysis of pre-trained language models (PLMs) in the biomedical domain is crucial for understanding their strengths, limitations, and suitability for specific tasks. As the field of biomedical natural language processing (NLP) continues to evolve, the selection of the most appropriate PLM for a given application has become increasingly important. Several specialized biomedical PLMs have been developed to address the unique challenges of biomedical text, such as domain-specific terminology, limited annotated data, and the need for interpretability. This subsection provides an in-depth comparison of these models, drawing on the literature and empirical evaluations to identify best practices and areas for improvement.

One of the early and influential models in the biomedical domain is BioBERT, which is a fine-tuned version of BERT pre-trained on biomedical corpora such as PubMed abstracts and clinical notes [29]. BioBERT has demonstrated strong performance on biomedical named entity recognition (NER) and relation extraction tasks, making it a popular choice for many applications. However, its effectiveness is often limited by the availability and quality of biomedical training data. In comparison, the recently proposed DrBERT model leverages a combination of clinical and biomedical texts for pre-training, resulting in improved performance on clinical text analysis tasks such as disease diagnosis and treatment prediction [29]. The success of DrBERT highlights the importance of domain-specific training data in enhancing model performance.

Another notable model is BERT-base and BERT-large, which have been extensively used in various biomedical tasks. While these models have achieved state-of-the-art results on general NLP tasks, their performance in the biomedical domain is often suboptimal due to the lack of domain-specific training. Recent studies have shown that fine-tuning BERT on biomedical corpora significantly improves its performance on tasks such as drug-drug interaction (DDI) detection and clinical text classification [62]. For instance, a study by [63] demonstrated that selective masking of domain-specific keywords during pre-training can lead to significant improvements in model performance on biomedical tasks.

In addition to BERT-based models, other PLMs such as ClinicalBERT and BioClinicalBERT have been developed specifically for clinical and biomedical text. ClinicalBERT is a fine-tuned version of BERT that is pre-trained on a large corpus of clinical notes and has been shown to outperform BERT on tasks such as clinical text classification and NER [29]. BioClinicalBERT extends this approach by incorporating additional biomedical terminology and domain-specific features, resulting in improved performance on tasks such as gene and protein identification [29]. These models highlight the importance of domain adaptation and the need for specialized training data to improve performance in the biomedical domain.

The role of domain adaptation in enhancing the performance of PLMs in the biomedical domain cannot be overstated. While general-purpose PLMs such as BERT and RoBERTa have achieved impressive results on a wide range of NLP tasks, their performance in the biomedical domain is often limited by the lack of domain-specific training. Recent studies have explored various domain adaptation techniques, including fine-tuning on biomedical corpora and incorporating domain-specific knowledge into the pre-training process [63]. For example, the study by [63] demonstrated that selective masking of domain-specific keywords during pre-training can lead to significant improvements in model performance on biomedical tasks.

In addition to domain adaptation, the integration of external knowledge sources has also been shown to enhance the performance of PLMs in the biomedical domain. Models such as KBioXLM and BioBART incorporate biomedical knowledge graphs and external resources into their pre-training process, enabling them to better capture the nuances of biomedical text [61]. These models have demonstrated strong performance on tasks such as entity linking and relation extraction, highlighting the value of integrating external knowledge into the pre-training process. However, the effectiveness of these models often depends on the quality and availability of external knowledge sources, which can be a limiting factor in some applications.

The comparative analysis of PLMs in the biomedical domain also highlights the importance of model efficiency and scalability. While large-scale models such as BERT-large and RoBERTa-large have demonstrated strong performance on many tasks, their computational demands can be prohibitive in real-world applications. Recent studies have explored lightweight and efficient adaptation techniques, such as parameter-efficient fine-tuning and knowledge distillation, to reduce the computational burden of PLMs in the biomedical domain [64]. For example, the study by [64] demonstrated that models such as LoRA and adapters can achieve comparable performance to full fine-tuning with significantly fewer parameters, making them more suitable for resource-constrained environments.

Despite the progress made in the development of biomedical PLMs, several challenges remain. One of the key challenges is the lack of standardized evaluation frameworks and benchmark datasets in the biomedical domain. While some datasets such as MedNLI and BioBART have been developed, there is a need for more comprehensive and diverse benchmarks to evaluate the performance of PLMs across a wide range of biomedical tasks [65]. Another challenge is the need for improved interpretability and explainability of PLMs in the biomedical domain, where transparency and trust are critical for clinical adoption [57]. Future research should focus on addressing these challenges to further enhance the effectiveness and practicality of PLMs in the biomedical domain.

## 3 Biomedical Applications of Pre-trained Language Models

### 3.1 Named Entity Recognition in Biomedical Text

Named Entity Recognition (NER) is a fundamental task in biomedical natural language processing (NLP), aimed at identifying and categorizing entities in text, such as diseases, genes, proteins, and clinical terms. Pre-trained language models (PLMs) have significantly advanced the performance of NER in biomedical texts by leveraging their ability to capture contextual and semantic information, which is crucial in domains with complex and specialized terminology. Unlike traditional rule-based or statistical methods, PLMs can generalize across diverse biomedical domains by learning from large-scale corpora, making them highly effective for entity recognition tasks [1].  

The application of PLMs in biomedical NER involves several methods, including fine-tuning, domain-specific pre-training, and knowledge integration. One of the most notable approaches is the use of domain-specific pre-trained models, such as BioBERT, ClinicalBERT, and PubMedBERT, which are trained on biomedical corpora and have shown superior performance on NER tasks compared to general-domain PLMs [66]. These models are particularly effective in recognizing entities that are unique to the biomedical domain, such as protein names, gene symbols, and medical procedures. For example, BioBERT, which is pre-trained on PubMed abstracts, has demonstrated improved performance in identifying entities like diseases and drugs in biomedical texts [2].  

Another key method in biomedical NER is the integration of external knowledge sources, such as biomedical ontologies and knowledge graphs. This approach enhances the performance of PLMs by incorporating structured domain knowledge into the training process. For instance, the UMLS-KGI-BERT model integrates knowledge from the Unified Medical Language System (UMLS) to improve entity recognition by providing additional context about the relationships between biomedical entities [1]. Similarly, the use of knowledge graphs, such as OntoChem and SNOMED CT, has been shown to enhance the accuracy of NER tasks by enabling models to better understand the semantic relationships between entities [4].  

In addition to domain-specific pre-training and knowledge integration, prompt engineering has emerged as a powerful technique for improving the performance of PLMs in biomedical NER. This method involves designing specific prompts that guide the model to focus on relevant entities and improve its ability to recognize them in complex texts. For example, the use of entity-aware prompts, such as those that emphasize the context of biomedical terms, has been shown to significantly improve the performance of NER tasks [67]. Furthermore, the application of in-context learning, where the model is provided with examples of correctly labeled entities, has been found to be highly effective in reducing the need for extensive labeled data [68].  

Recent studies have also explored the use of parameter-efficient fine-tuning techniques to adapt PLMs for biomedical NER tasks. These techniques, such as LoRA (Low-Rank Adaptation) and adapter modules, allow models to be fine-tuned with minimal computational resources while maintaining high performance on NER tasks [4]. For example, the use of adapter layers has been shown to improve the performance of NER models while significantly reducing the number of trainable parameters [4]. This approach is particularly useful in scenarios where computational resources are limited, such as in clinical settings or low-resource biomedical environments.  

Moreover, the use of multimodal data has been explored as a way to enhance the performance of NER models in biomedical texts. By combining textual information with other modalities, such as images or biomedical sequences, these models can better capture the context and relationships between entities. For instance, the integration of text and image data in NER tasks has been shown to improve the accuracy of entity recognition in medical reports and clinical documents [56]. This approach is particularly valuable in tasks where the context of an entity is critical for its correct identification, such as in the recognition of medical procedures or anatomical structures.  

The effectiveness of PLMs in biomedical NER has also been evaluated through various benchmark datasets, such as the BioNLP Shared Task datasets and the NCBI Disease Corpus. These datasets provide a standardized way to assess the performance of NER models across different biomedical domains and tasks. For example, the BioNLP 2013 dataset, which includes annotated biomedical texts, has been widely used to evaluate the performance of NER models on tasks such as disease and drug recognition [66]. The results of these evaluations have consistently shown that domain-specific PLMs outperform general-domain models, highlighting the importance of tailoring NER models to the unique characteristics of biomedical texts.  

In addition to improving the accuracy of NER tasks, PLMs have also been used to address the challenges of data scarcity in biomedical NLP. By leveraging the knowledge learned from large-scale biomedical corpora, these models can generalize well even when trained on limited annotated data. For instance, the use of zero-shot learning, where models are trained on data from one domain and applied to another, has been shown to be effective in biomedical NER tasks where labeled data is scarce [69]. This approach is particularly valuable in domains where the annotation of biomedical texts is time-consuming and expensive.  

The performance of PLMs in biomedical NER has also been influenced by the choice of training objectives and loss functions. For example, the use of span-based loss functions, which focus on identifying the exact spans of entities in text, has been shown to improve the performance of NER models on tasks involving complex entity structures [66]. Additionally, the integration of contextual information, such as the relationships between entities and their surrounding text, has been found to enhance the ability of models to accurately recognize entities in biomedical texts [1].  

In conclusion, the application of pre-trained language models to named entity recognition in biomedical texts has significantly advanced the state of the art in this field. Through domain-specific pre-training, knowledge integration, prompt engineering, and parameter-efficient fine-tuning, these models have demonstrated superior performance on a wide range of NER tasks. The use of multimodal data and the exploration of zero-shot learning have further expanded the capabilities of NER models in biomedical NLP, making them more adaptable to diverse and challenging scenarios. As the field continues to evolve, the development of more specialized and efficient PLMs will be crucial for addressing the unique challenges of biomedical NER and improving the accuracy and reliability of entity recognition in biomedical texts.

### 3.2 Medical Question Answering

Medical Question Answering (MQA) is a critical application of pre-trained language models (PLMs) in the biomedical domain, where models are trained to retrieve accurate information from biomedical literature or clinical texts. This task is essential for supporting clinical decision-making, improving patient care, and enabling efficient access to medical knowledge. With the rapid advancement of pre-trained language models, especially large language models (LLMs), the field of MQA has seen significant progress, leading to the development of sophisticated methods that can handle complex medical queries. These models are capable of understanding and generating human-like responses, making them invaluable tools for healthcare professionals and researchers alike.

One of the key challenges in MQA is the vast and ever-growing volume of biomedical literature, which makes it difficult for clinicians and researchers to keep up with the latest developments. Pre-trained language models address this challenge by providing efficient and effective ways to retrieve relevant information. For instance, studies have shown that LLMs such as GPT-4 can outperform traditional information retrieval systems in terms of accuracy and relevance [7]. This is because these models can understand the context and nuances of medical queries, enabling them to provide more accurate and contextually relevant answers.

In addition to retrieving information, pre-trained language models are also being used to generate summaries of medical texts, which can be particularly useful for clinicians who need to quickly grasp the key points of a study or a patient's medical history. For example, research on biomedical text summarization has demonstrated that LLMs can effectively condense complex medical information into concise summaries, making it easier for healthcare professionals to access critical information [3]. This capability is especially important in the context of electronic health records (EHRs), where the ability to quickly extract relevant information can significantly improve patient care.

Another important aspect of MQA is the ability of pre-trained language models to handle complex and nuanced queries. Unlike traditional keyword-based search systems, these models can understand the intent behind a query and provide more accurate and contextually appropriate responses. For instance, studies have shown that LLMs can effectively answer questions that require a deep understanding of medical concepts, such as the relationship between genetic variations and diseases [70]. This is achieved through the use of advanced techniques such as prompt engineering and transfer learning, which allow models to adapt to specific medical domains and tasks.

Moreover, the integration of knowledge graphs and other structured data sources into pre-trained language models has further enhanced their ability to answer medical questions. By leveraging external knowledge, these models can provide more comprehensive and accurate answers. For example, a study on the use of LLMs for clinical question answering demonstrated that integrating medical knowledge bases such as UMLS (Unified Medical Language System) significantly improved the performance of the models [4]. This approach not only enhances the accuracy of the answers but also ensures that the information provided is consistent with established medical knowledge.

The application of pre-trained language models in MQA is not without its challenges. One of the main issues is the potential for hallucinations, where models generate incorrect or misleading information. This is particularly concerning in the medical domain, where the accuracy of information is crucial for patient safety. To address this, researchers have proposed various methods to improve the reliability of LLMs, such as using retrieval-augmented generation (RAG) to ensure that the information provided is supported by credible sources [71]. Additionally, the use of human-in-the-loop evaluation and expert annotations can help identify and correct errors, ensuring that the answers provided by the models are accurate and trustworthy [54].

Another challenge in MQA is the need for models to handle multilingual queries, particularly in a global healthcare context. While many pre-trained language models are trained on large English-language corpora, the ability to answer questions in other languages is essential for reaching a wider audience. Studies have shown that LLMs can be effectively adapted to handle multilingual queries through techniques such as cross-lingual pre-training and fine-tuning [72]. This not only improves the accessibility of medical information but also ensures that healthcare professionals from different linguistic backgrounds can benefit from the insights provided by these models.

In conclusion, pre-trained language models have made significant contributions to the field of medical question answering, offering efficient and effective ways to retrieve and generate medical information. By leveraging advanced techniques such as prompt engineering, transfer learning, and knowledge integration, these models have demonstrated the ability to handle complex medical queries and provide accurate and contextually relevant answers. Despite the challenges that remain, such as the potential for hallucinations and the need for multilingual support, the continued development and refinement of these models hold great promise for improving the delivery of healthcare and advancing medical research. As the field of biomedical NLP continues to evolve, the role of pre-trained language models in MQA will undoubtedly become even more significant, driving innovation and improving patient outcomes.

### 3.3 Clinical Text Analysis

Clinical text analysis represents a critical application of pre-trained language models (PLMs) in the biomedical domain, particularly in processing electronic health records (EHRs), discharge summaries, and medical notes. These texts are rich in information but often structured in unstructured formats, making them difficult to analyze using traditional methods. PLMs have emerged as powerful tools for extracting meaningful insights from such data, offering robustness and adaptability to the complex linguistic structures found in clinical settings. By leveraging the contextual understanding provided by pre-trained models, researchers have developed advanced systems capable of identifying key patterns, extracting relevant entities, and enabling downstream applications such as clinical decision support and disease prediction [53; 2].

One of the primary challenges in clinical text analysis is the variability and complexity of medical terminology. EHRs, for instance, contain a mixture of structured and unstructured data, with the latter often comprising free-text notes written by clinicians. Pre-trained models like BioBERT, Med-BERT, and BioClinicalBERT have been specifically developed to address this issue by incorporating biomedical domain knowledge during pre-training. These models have shown superior performance compared to general-domain models, as they are better equipped to handle the specialized vocabulary and syntactic structures encountered in clinical text [2; 73]. For example, a study by [53] demonstrated that a self-supervised pre-trained model could achieve high accuracy in named entity recognition (NER) tasks, even with limited annotated data, by leveraging the contextual understanding provided by pre-training on large clinical corpora.

Another significant application of PLMs in clinical text analysis is the extraction of structured information from unstructured text. This task is essential for converting clinical notes into standardized formats that can be used for downstream analysis. Techniques such as sequence tagging and information extraction have been extensively applied using PLMs, with models like BioClinicalBERT and BioBART showing remarkable performance on tasks like entity recognition and relation extraction [2; 73]. These models benefit from domain-specific pre-training, which helps them capture the nuances of clinical language and improve their ability to identify key medical entities such as drugs, symptoms, and diagnoses.

The use of PLMs in clinical text analysis also extends to the development of clinical decision support systems (CDSS). These systems aim to assist healthcare professionals by providing evidence-based recommendations and insights derived from patient data. By integrating PLMs into CDSS, researchers can enhance the accuracy and reliability of clinical decision-making processes. For instance, [74] introduced a specialized language model, ClinicalMamba, which is pre-trained on longitudinal clinical notes and demonstrates superior performance in modeling clinical language across extended text lengths. This model has been shown to outperform existing clinical language models in tasks such as information extraction from longitudinal clinical notes, offering a promising avenue for improving clinical decision support systems.

Moreover, PLMs have been used to enhance the interpretation and understanding of clinical text. This is particularly important in tasks such as medical question answering (QA), where models are trained to retrieve accurate information from clinical texts. [75] demonstrated that pre-trained language models, when fine-tuned on domain-specific data, can achieve high accuracy in medical NLP tasks, including natural language inference and QA. The study highlighted the importance of domain adaptation in improving the performance of general-domain models on clinical tasks, emphasizing the need for models that can effectively handle the unique characteristics of clinical text.

In addition to these applications, PLMs have been employed to address challenges related to data scarcity and model interpretability in clinical text analysis. For example, [76] explored the use of backtranslation to generate synthetic data for low-resource named entity recognition tasks, demonstrating that this approach can lead to significant improvements in model performance. Similarly, [2] introduced a generative language model that is pre-trained on PubMed abstracts and has shown strong performance on tasks like dialogue and summarization, highlighting the potential of PLMs in generating high-quality clinical summaries.

The integration of external knowledge sources, such as biomedical ontologies and knowledge graphs, has also been explored to enhance the performance of PLMs in clinical text analysis. Models like BioBART and BioClinicalBERT have been fine-tuned with domain-specific knowledge to improve their ability to recognize and classify medical entities. [77] proposed a novel approach that combines the strengths of LLMs and clinical knowledge graphs to improve the accuracy of biomedical semantic representations. This study demonstrated that integrating domain-specific knowledge can lead to substantial performance gains in tasks such as semantic textual similarity and relation extraction, underscoring the importance of knowledge integration in clinical text analysis.

Furthermore, the development of interpretable and explainable models has gained attention in the field of clinical text analysis. The black-box nature of many deep learning models poses challenges for their adoption in clinical settings, where transparency and interpretability are critical. Researchers have explored various techniques to enhance model explainability, such as the use of attention mechanisms and post-hoc interpretation methods. [14] introduced a novel method that leverages interpretable entity representations to facilitate model debugging and improve the understanding of how models arrive at their decisions. This work demonstrated that interpretable representations can provide valuable insights into the performance of PLMs in clinical text analysis, particularly in low-supervision settings.

In summary, the use of pre-trained language models in clinical text analysis has opened up new possibilities for extracting meaningful insights from unstructured clinical data. From entity recognition and relation extraction to medical QA and decision support, PLMs have shown remarkable performance in a wide range of tasks. The integration of domain-specific knowledge, the development of interpretable models, and the use of data augmentation techniques have all contributed to the advancement of this field. As the volume of clinical data continues to grow, the role of PLMs in clinical text analysis will only become more critical, driving further innovation and improving the quality of healthcare delivery.

### 3.4 Drug Discovery and Molecular Generation

Pre-trained language models (PLMs) have emerged as a transformative force in the field of drug discovery and molecular generation within the biomedical domain. Traditionally, drug discovery has been a time-consuming and resource-intensive process, involving extensive experimental work and manual curation of data. However, the advent of pre-trained language models, especially large language models (LLMs), has introduced new possibilities for accelerating the identification of novel compounds, predicting drug-target interactions, and generating molecules with desired properties. This subsection explores how PLMs, including BERT, PubMedBERT, BioBERT, and other specialized models, are being applied to these critical tasks in drug discovery and molecular generation.

One of the primary applications of PLMs in drug discovery is molecular generation. These models can generate new chemical compounds by leveraging the vast amount of textual data available in biomedical literature, patents, and databases. For instance, models such as GPT-3 and its variants have been utilized to generate molecules with specific properties, such as high binding affinity to a target protein or low toxicity [7]. By training on large corpora of chemical and biological data, these models can learn the structural and functional characteristics of molecules and generate novel candidates that could be tested experimentally.

Another critical application of PLMs in drug discovery is the prediction of drug-target interactions. Traditional approaches to identifying these interactions often rely on high-throughput screening and experimental assays, which are expensive and time-consuming. PLMs can significantly reduce the need for such extensive experimentation by predicting interactions based on the textual data of drug and target descriptions. For example, models like BioBERT and PubMedBERT have been trained on biomedical texts to identify potential interactions between drugs and their molecular targets [6]. These models leverage contextual information from large-scale biomedical corpora to improve the accuracy of interaction predictions, enabling researchers to focus on the most promising candidates for further validation.

In addition to molecular generation and drug-target interaction prediction, PLMs are also being used for property prediction of molecules. This includes predicting physicochemical properties, biological activity, and toxicity. Models such as BioBART and DrBERT have been developed specifically for these tasks, utilizing pre-training on biomedical texts and molecular structures to enhance their ability to predict molecular properties [6]. These models can identify patterns and relationships in molecular data that are not immediately apparent to human experts, thereby improving the efficiency and effectiveness of drug discovery processes.

The integration of domain-specific knowledge into PLMs has further enhanced their performance in drug discovery tasks. For example, models like PubMedBERT and BioLinkBERT have been pre-trained on biomedical corpora to capture the nuances of medical terminology and the relationships between biological entities. This domain-specific training enables these models to better understand the context of drug discovery tasks, leading to more accurate predictions and more relevant molecular generation [6]. Additionally, the use of knowledge graphs and other structured data sources has been shown to improve the performance of PLMs in predicting drug-target interactions and molecular properties [4].

The application of PLMs in drug discovery is not without its challenges. One of the main obstacles is the scarcity of annotated data for training these models, particularly in specialized biomedical domains. However, recent advances in self-supervised learning and transfer learning have mitigated some of these challenges by enabling models to learn from large amounts of unannotated data [78]. These techniques allow PLMs to pre-train on general text and then fine-tune on domain-specific data, resulting in improved performance on drug discovery tasks.

Moreover, the interpretability and explainability of PLMs remain important considerations in their application to drug discovery. While these models can generate highly accurate predictions, understanding the rationale behind their decisions is crucial for validating their outputs and ensuring their reliability in clinical and pharmaceutical settings [79]. Researchers are actively working on developing methods to make these models more transparent, such as using attention mechanisms and interpretability techniques to highlight relevant features in the input data.

In conclusion, pre-trained language models are playing an increasingly important role in drug discovery and molecular generation within the biomedical domain. Their ability to generate novel molecules, predict drug-target interactions, and predict molecular properties has the potential to revolutionize the drug discovery process. By leveraging domain-specific knowledge and incorporating structured data sources, these models are becoming more accurate and reliable. As research in this area continues to advance, the integration of PLMs into drug discovery workflows is expected to accelerate the development of new therapies and improve patient outcomes. The ongoing development of more specialized and efficient models, along with the integration of multimodal and cross-domain data, will further enhance the capabilities of PLMs in this critical area of biomedical research [6].

### 3.5 Biomedical Information Retrieval

Biomedical information retrieval is a critical application area for pre-trained language models (PLMs) in the biomedical domain. It involves the systematic search for relevant literature, clinical documents, and biomedical data, which is essential for advancing medical research, improving clinical decision-making, and supporting evidence-based practices. The complexity of biomedical texts, characterized by domain-specific terminology, intricate relationships between concepts, and the vast volume of available data, poses significant challenges for traditional information retrieval systems. Pre-trained language models, with their ability to understand and process natural language, have emerged as powerful tools to address these challenges, offering enhanced accuracy, efficiency, and scalability in retrieving biomedical information.

One of the primary applications of pre-trained language models in biomedical information retrieval is the search for relevant scientific literature. The exponential growth of biomedical publications has made it increasingly difficult for researchers to identify the most pertinent articles for their studies. PLMs, such as BioBERT [3], have been specifically trained on biomedical corpora, enabling them to better understand the nuances of medical texts and improve the relevance of search results. For example, studies have shown that BioBERT outperforms traditional models in tasks such as document classification and named entity recognition, which are essential for effective information retrieval [3]. Moreover, recent advancements in large language models (LLMs), such as GPT-4, have further expanded the capabilities of biomedical information retrieval by enabling more sophisticated search strategies and better handling of complex queries [25]. These models can generate human-like responses to natural language queries, making them particularly useful for interactive and user-friendly information retrieval systems.

In addition to literature retrieval, PLMs are also being used to search for clinical data, such as electronic health records (EHRs) and patient records. The integration of PLMs into clinical information retrieval systems has the potential to significantly improve the efficiency and accuracy of retrieving patient-specific information. For instance, models like DrBERT [26], have been developed to handle clinical texts and have demonstrated superior performance in tasks such as clinical text analysis and entity recognition [26]. By leveraging the domain-specific knowledge embedded in these models, researchers and clinicians can more effectively extract and utilize relevant clinical data for diagnosis, treatment planning, and research purposes.

The use of PLMs in biomedical information retrieval is not limited to text-based data. These models are also being applied to multimodal data, such as combining text with images and other biomedical data types. For example, the integration of text and image data in information retrieval systems can provide a more comprehensive understanding of biomedical concepts and improve the accuracy of search results. This approach is particularly useful in fields such as radiology, where the interpretation of medical images is crucial for accurate diagnosis [80]. By leveraging the power of PLMs, researchers can develop more sophisticated and effective information retrieval systems that can handle the complexity and diversity of biomedical data.

Another important aspect of biomedical information retrieval is the ability to handle cross-lingual tasks, especially in the context of global biomedical research. The diversity of languages in which biomedical literature is published necessitates the development of models that can effectively retrieve information across different languages. Studies have shown that pre-trained language models can be adapted to handle cross-lingual tasks through techniques such as multilingual pre-training and transfer learning [81]. These models can be fine-tuned on multilingual biomedical corpora, allowing them to understand and retrieve information in multiple languages. For instance, the development of multilingual models like mBERT and XLM-R has demonstrated the potential of PLMs in cross-lingual biomedical information retrieval [81]. These models can help bridge the gap between different language communities, making biomedical research more accessible and inclusive.

The effectiveness of PLMs in biomedical information retrieval is also being evaluated through benchmarking and standardized datasets. Researchers have developed a variety of benchmark datasets to assess the performance of these models on specific tasks such as document retrieval, question answering, and entity recognition. For example, the MedNLI dataset [3] and the BioBART dataset [26] have been used to evaluate the performance of PLMs in biomedical tasks. These datasets provide a standardized framework for comparing different models and identifying their strengths and weaknesses. The results of these evaluations have shown that PLMs, particularly those trained on domain-specific data, can achieve high levels of accuracy and efficiency in biomedical information retrieval tasks [26].

Despite the promising advancements, there are still several challenges and limitations in the use of PLMs for biomedical information retrieval. One of the primary challenges is the scarcity of annotated biomedical data, which is essential for training and fine-tuning these models. While large-scale pre-training on biomedical corpora can help mitigate this issue, the lack of high-quality annotated data remains a significant barrier to the development of more accurate and effective models. Additionally, the complexity of biomedical texts and the need for domain-specific knowledge make it difficult to develop models that can generalize well across different tasks and domains [80].

In conclusion, pre-trained language models have revolutionized the field of biomedical information retrieval by providing powerful tools for searching and analyzing biomedical literature, clinical data, and other biomedical information. Their ability to understand and process natural language, combined with their adaptability to different tasks and domains, makes them invaluable in the biomedical research and clinical settings. However, ongoing research is needed to address the challenges and limitations associated with their use, such as data scarcity, domain specificity, and model interpretability. By continuing to develop and refine these models, researchers can further enhance their capabilities and ensure that they can effectively support the growing demands of the biomedical community.

### 3.6 Multimodal Biomedical Applications

[82]

The integration of pre-trained language models (PLMs) with multimodal data represents a significant advancement in the biomedical domain, enabling the handling of complex tasks that involve both textual and visual information. Multimodal biomedical applications leverage the strengths of PLMs in understanding and generating text, combined with the ability of computer vision models to process and analyze images, to provide more comprehensive and accurate insights. This section explores the various ways in which PLMs are being integrated with multimodal data to address critical challenges in biomedical research and clinical practice.

One of the key areas where multimodal PLMs are being applied is in medical imaging. For example, in the context of biomedical image segmentation, PLMs are being used to enhance the performance of models by incorporating textual information from clinical notes and imaging reports. This integration helps in better understanding the context of the images, leading to more accurate segmentation results. The paper "Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis" [83] discusses how multi-domain models can be utilized to improve the generalization capabilities of segmentation models, particularly in scenarios with limited data availability. By combining data from various domains, including different imaging modalities, these models can better capture the nuances of biomedical images, leading to more robust and reliable segmentation outcomes.

Another important application of multimodal PLMs is in the field of biomedical question answering (QA). In this context, PLMs are used to process both textual and visual data to provide accurate and relevant answers to complex biomedical questions. The paper "BioADAPT-MRC  Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task" [31] presents an adversarial learning-based framework for biomedical machine reading comprehension (biomedical-MRC). This framework addresses the discrepancies in the marginal distributions between the general and biomedical domains, thereby improving the performance of models in understanding and answering complex biomedical questions. By integrating textual and visual information, such models can provide more comprehensive and contextually relevant answers.

In the realm of biomedical information retrieval, PLMs are being used to enhance the ability of systems to find relevant information from a vast corpus of biomedical literature. The paper "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge" [84] introduces a novel information-retrieval method that leverages a knowledge graph to downsample clusters of over-represented concepts in the biomedical literature. This approach not only improves the precision and recall of retrieval systems but also helps in capturing the long-tail knowledge that is often overlooked by traditional methods. By integrating textual and visual data, these systems can provide more accurate and relevant results, facilitating the discovery of new knowledge and insights.

The integration of PLMs with multimodal data is also being explored in the context of biomedical text generation. In this area, PLMs are used to generate high-quality biomedical texts, including summaries, dialogues, and scientific writing. The paper "LED down the rabbit hole: exploring the potential of global attention for biomedical multi-document summarisation" [85] discusses the use of global attention mechanisms to improve the performance of multi-document summarization models in the biomedical domain. By incorporating global attention, these models can better capture the context and relationships between different documents, leading to more accurate and coherent summaries.

In the field of biomedical knowledge extraction and reasoning, the integration of PLMs with multimodal data is enabling more sophisticated analysis of complex biomedical information. The paper "From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer" [34] presents a domain-specific knowledge graph (KG) that leverages large language models (LLMs) to enhance the discovery of cancer-specific biomarkers. By integrating textual and visual data, this approach allows for the exploration of new knowledge and the identification of novel biomarkers, which can significantly advance the field of cancer research.

Furthermore, the integration of PLMs with multimodal data is being used to improve the accuracy and reliability of biomedical entity linking and disambiguation. The paper "BELB: a Biomedical Entity Linking Benchmark" [39] introduces a comprehensive benchmark for biomedical entity linking, which includes various corpora linked to multiple knowledge bases. This benchmark provides a standardized testbed for evaluating the performance of entity linking systems, particularly in the context of multimodal data. By leveraging both textual and visual information, these systems can more accurately link biomedical entities to their correct references, enhancing the overall quality of biomedical text mining.

The integration of PLMs with multimodal data is also being explored in the context of biomedical image analysis. The paper "Domain Generalization for Medical Image Analysis: A Survey" [30] provides a comprehensive overview of domain generalization techniques for medical image analysis. This survey highlights the importance of integrating textual and visual data to improve the generalization capabilities of models, particularly in scenarios with limited data availability. By leveraging the strengths of PLMs in understanding and generating text, these models can better capture the context and relationships between different modalities, leading to more accurate and reliable results.

In conclusion, the integration of pre-trained language models with multimodal data is transforming the biomedical domain by enabling more comprehensive and accurate analysis of complex biomedical tasks. Through the combination of textual and visual information, these models are providing new insights and capabilities that are significantly advancing biomedical research and clinical practice. The papers cited above highlight the various applications and advancements in this area, demonstrating the potential of multimodal PLMs to address the unique challenges of the biomedical domain. As research in this field continues to evolve, the integration of PLMs with multimodal data is expected to play an increasingly important role in shaping the future of biomedical NLP.

### 3.7 Biomedical Text Generation

Biomedical text generation is an emerging and rapidly growing area within the biomedical natural language processing (NLP) domain, where pre-trained language models (PLMs) play a crucial role in creating, summarizing, and generating structured or unstructured biomedical texts. The applications of biomedical text generation span a wide range of tasks, including summarization, dialogue generation, scientific writing, and the creation of clinical documentation. These tasks are essential for improving the efficiency of healthcare professionals, enhancing the accessibility of biomedical knowledge, and supporting various research and clinical workflows. With the increasing availability of biomedical text data, including electronic health records (EHRs), scientific literature, and clinical notes, the demand for automated text generation systems has surged. Pre-trained language models, such as BERT, BioBERT, and large language models (LLMs), have demonstrated significant potential in generating high-quality biomedical texts, addressing challenges such as domain-specific terminology, information retrieval, and the need for structured outputs.

One of the key applications of biomedical text generation is biomedical summarization, where the goal is to condense large volumes of biomedical literature into concise and informative summaries. This is particularly useful for clinicians and researchers who need to quickly access critical information from a vast amount of scientific literature. For instance, studies have shown that pre-trained language models, such as BioBART and T5, can be fine-tuned on biomedical corpora to generate high-quality summaries of scientific abstracts [2]. These models leverage their understanding of biomedical terminology and contextual relationships to produce summaries that are both accurate and relevant to the user's needs. Furthermore, recent advancements in prompt engineering and domain adaptation have enabled these models to generate more coherent and contextually appropriate summaries, even in the presence of complex biomedical jargon [8].

Another important application of biomedical text generation is dialogue generation, particularly in the context of clinical settings. Dialogue systems powered by pre-trained language models can assist healthcare professionals by generating clinical notes, answering patient queries, and facilitating communication between patients and providers. These systems often require the generation of natural, conversational responses that are accurate and tailored to the specific needs of the user. For example, studies have demonstrated that models like BioBERT and BERT can be fine-tuned to generate clinically relevant dialogues, making them valuable tools for improving patient engagement and communication [29]. Additionally, the integration of multimodal data, such as text and images, has further enhanced the capabilities of these systems, allowing them to generate more comprehensive and context-aware responses [56].

Scientific writing is another critical area where biomedical text generation has made significant strides. Pre-trained language models are being used to assist researchers in writing manuscripts, generating abstracts, and even drafting entire research papers. These models can help overcome the challenges of writing in a highly technical and specialized domain, where the use of domain-specific terminology is essential. For instance, studies have shown that models like T5 and BART can be fine-tuned to generate scientific texts that are grammatically correct, semantically coherent, and aligned with the conventions of the biomedical field [86]. Furthermore, the use of control mechanisms, such as control tokens and prompt-based learning, has enabled these models to generate texts that adhere to specific formatting and structural requirements, making them more suitable for publication and peer review [86].

In addition to these applications, biomedical text generation also plays a vital role in the creation of structured clinical documentation. Pre-trained language models can be used to generate structured notes, such as progress notes, discharge summaries, and medication lists, based on unstructured clinical text. This is particularly useful in reducing the workload of healthcare professionals and ensuring that clinical documentation is accurate and complete. Studies have shown that models like BERT and BioBERT can be fine-tuned to extract and generate structured information from clinical texts, improving the efficiency and accuracy of clinical documentation [2; 10].

Moreover, the use of large language models (LLMs) in biomedical text generation has opened up new possibilities for automating complex tasks such as hypothesis generation and literature review. LLMs, such as GPT-3 and GPT-4, have demonstrated the ability to generate high-quality biomedical texts, including literature reviews and hypothesis statements, based on a wide range of inputs. These models can leverage their extensive training data to produce texts that are not only accurate but also contextually relevant and well-structured. For example, studies have shown that LLMs can be used to generate summaries of clinical trials, identify key findings, and even suggest new research directions [42].

Despite the significant progress in biomedical text generation, there are still several challenges that need to be addressed. One of the main challenges is the need for high-quality and diverse training data. While there are many publicly available biomedical corpora, such as PubMed and MIMIC-III, the availability of annotated data for specific tasks remains limited. This can hinder the performance of pre-trained language models, as they often require domain-specific data to achieve optimal results. Additionally, the generation of high-quality biomedical texts requires not only a deep understanding of the domain but also the ability to handle complex and often ambiguous language. Pre-trained language models must be carefully fine-tuned and evaluated to ensure that they produce texts that are accurate, coherent, and suitable for clinical and research applications.

In conclusion, biomedical text generation is a rapidly evolving field that holds great promise for improving the efficiency and effectiveness of biomedical research and clinical practice. Pre-trained language models, such as BERT, BioBERT, and LLMs, have demonstrated significant potential in generating high-quality biomedical texts across a wide range of applications. However, the continued development of this field will require further research into the use of domain-specific data, the refinement of model architectures, and the exploration of novel techniques for text generation. As the demand for automated biomedical text generation continues to grow, the role of pre-trained language models in this domain is expected to become even more prominent.

### 3.8 Cross-Lingual Biomedical Applications

---
Cross-lingual biomedical applications of pre-trained language models (PLMs) have gained significant attention due to the global nature of biomedical research and the necessity to handle multilingual biomedical texts. These applications involve tasks such as translating biomedical texts, understanding biomedical content in different languages, and enabling cross-lingual information retrieval. The development of effective cross-lingual PLMs is crucial to address the challenges posed by the diversity of languages and the limited availability of annotated biomedical data in many languages.

One of the primary challenges in cross-lingual biomedical NLP is the lack of annotated data for many languages, particularly for specialized biomedical domains. This makes it difficult to train and evaluate models for tasks such as named entity recognition (NER) and relation extraction. To overcome this, researchers have explored the use of pre-trained PLMs that can be fine-tuned on cross-lingual tasks with limited annotated data. These models leverage the universal language representations learned during pre-training, which can be adapted to specific biomedical tasks across languages.

The DICT-MLM paper [87] provides a compelling example of how cross-lingual pre-training can be enhanced by incorporating bilingual dictionaries into the masked language modeling (MLM) objective. By incentivizing the model to predict not just the original masked word but potentially any of its cross-lingual synonyms, the approach improves multilingual representation learning. This method has demonstrated significant improvements in performance on a variety of downstream tasks across 30+ languages. In the biomedical context, this could be particularly useful for tasks such as entity linking and knowledge extraction, where the ability to recognize and link biomedical entities across languages is essential.

Another approach to cross-lingual biomedical NLP is the use of multilingual pre-trained models that are trained on large-scale multilingual corpora. These models, such as mBERT and XLM-R, are designed to learn language-agnostic representations that can be adapted to specific tasks. However, the effectiveness of these models in biomedical tasks depends on the availability of high-quality multilingual biomedical data. The paper "MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model" [88] highlights the importance of domain adaptation in multilingual settings. The study shows that a single multilingual domain-specific model can outperform general multilingual models, especially in tasks such as biomedical named entity recognition and financial sentence classification. This suggests that domain-specific pre-training is crucial for improving the performance of multilingual models in specialized tasks.

Cross-lingual translation and understanding of biomedical texts also benefit from the integration of multimodal data. The paper "SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding" [89] introduces a framework that combines speech and text data for pre-training. This approach could be extended to biomedical applications by incorporating medical audio data alongside textual information, thereby enabling more robust cross-lingual understanding of spoken biomedical content. Additionally, the paper "Cross-lingual Visual Pre-training for Multimodal Machine Translation" [90] demonstrates the potential of cross-lingual visual pre-training for multimodal tasks. This could be particularly useful for biomedical tasks involving imaging and text, such as medical report generation and image captioning.

The paper "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue" [91] explores the use of pre-trained language models for task-oriented dialogues. While not specifically focused on biomedical tasks, the principles of teaching future knowledge to language models could be adapted to cross-lingual biomedical dialogues. This approach could enhance the ability of models to understand and generate contextually relevant responses in multilingual settings, which is critical for tasks such as patient consultations and clinical decision-making.

The paper "ST-BERT: Cross-modal Language Model Pre-training for End-to-End Spoken Language Understanding" [92] introduces a cross-modal pre-trained language model for end-to-end spoken language understanding. This model, which integrates phoneme posterior and subword-level text, could be adapted for biomedical applications by incorporating medical speech data. This would enable more accurate understanding and translation of spoken biomedical content, which is particularly useful in clinical settings where audio recordings of patient consultations are common.

Another important aspect of cross-lingual biomedical applications is the use of domain-specific knowledge to improve model performance. The paper "Biased Self-supervised learning for ASR" [93] proposes a method to bias self-supervised learning towards a specific task, which could be applied to biomedical tasks by incorporating domain-specific knowledge. This approach could enhance the ability of models to understand and translate biomedical texts by leveraging specialized vocabulary and terminology.

The paper "Unsupervised Improvement of Factual Knowledge in Language Models" [94] highlights the importance of factual knowledge in language models. In the context of biomedical applications, this could be extended to include domain-specific factual knowledge, which would improve the accuracy of cross-lingual biomedical tasks. This is particularly important for tasks such as medical question answering, where the ability to retrieve and generate accurate information across languages is critical.

In addition to these approaches, the paper "Task Transfer and Domain Adaptation for Zero-Shot Question Answering" [95] explores the use of domain adaptation for zero-shot question answering. This approach could be adapted to cross-lingual biomedical tasks by leveraging domain-specific knowledge and transfer learning techniques. This would enable models to answer questions in multiple languages without requiring extensive annotated data for each language.

The paper "Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics" [63] introduces a masking approach that leverages genre and topicality information to tailor language models to specialized domains. This method could be applied to cross-lingual biomedical tasks by incorporating domain-specific terminology and structures, thereby improving the performance of models in multilingual settings.

Overall, the development of cross-lingual biomedical applications of pre-trained language models is a rapidly evolving area with significant potential for improving the accessibility and accuracy of biomedical information across languages. By leveraging multilingual pre-training, domain adaptation, and the integration of domain-specific knowledge, researchers can create more effective models that address the unique challenges of cross-lingual biomedical NLP. These advancements are essential for supporting global biomedical research and ensuring that critical health information is accessible to a wider audience.
---

### 3.9 Biomedical Knowledge Extraction and Reasoning

Pre-trained language models (PLMs) have significantly advanced biomedical knowledge extraction and reasoning tasks by leveraging their ability to understand and represent complex textual data. These models are particularly valuable in the biomedical domain due to the complexity of the language, the high volume of unstructured data, and the need for precise and accurate information extraction. Tasks such as entity linking, relation extraction, and logical inference have seen substantial improvements with the application of PLMs, enabling more effective and scalable solutions for biomedical knowledge discovery. In this subsection, we explore how pre-trained language models are utilized in biomedical knowledge extraction and reasoning, with a focus on their application in entity linking, relation extraction, and logical inference.

Entity linking, which involves mapping mentions of entities in text to their corresponding entries in a knowledge base, is a critical task in biomedical NLP. Pre-trained language models such as BERT and its variants have been effectively employed for biomedical entity linking. For instance, the study on the use of BERT for entity linking [96] demonstrated that BERT's hidden layers contain rich contextual information that can be leveraged for entity linking tasks. The authors showed that the output layer of BERT can reconstruct input sentences by taking each layer as input, indicating that BERT's internal representations are highly informative for entity disambiguation and linking. Furthermore, the application of BERT in biomedical entity linking has been explored in several studies, such as [97], where the authors fine-tuned BERT on biomedical question-answering datasets to improve entity linking performance. The results showed that BERT significantly outperformed previous approaches, highlighting its effectiveness in capturing the context and semantics necessary for accurate entity linking in biomedical texts.

Relation extraction, which involves identifying and classifying relationships between entities in text, is another essential task in biomedical knowledge extraction. PLMs have been instrumental in improving the accuracy and efficiency of relation extraction in biomedical texts. For example, the study on the use of BERT for relation extraction [98] demonstrated that BERT's hidden layers can effectively capture the syntactic and semantic information required for relation extraction. The authors conducted a layer-wise analysis of BERT's hidden states and found that the model's representations contain information that can be used to identify and classify relationships between biomedical entities. Moreover, the application of BERT in biomedical relation extraction has been further explored in [99], where the authors proposed a novel extension to the Transformer architecture by incorporating signature transform with the self-attention model. The experiments on a Swedish prescription dataset showed that the proposed architecture outperformed baseline models in relation extraction tasks, underscoring the potential of PLMs in improving the accuracy of biomedical relation extraction.

Logical inference, which involves drawing conclusions from given premises, is a crucial aspect of biomedical knowledge reasoning. PLMs have been widely used to enhance logical inference capabilities in biomedical texts by capturing the context and semantics necessary for reasoning. The study on the use of BERT for logical inference [98] demonstrated that BERT's hidden layers can effectively capture the logical relationships between biomedical entities. The authors analyzed BERT's hidden states and found that the model's representations contain information that can be used to infer logical relationships between entities. Additionally, the application of BERT in logical inference has been explored in [100], where the authors introduced a new pre-training task inspired by reading comprehension to improve the model's ability to perform logical inference. The results showed that the proposed model achieved significant improvements over BERT in logical inference tasks, highlighting the potential of PLMs in enhancing the reasoning capabilities of biomedical NLP systems.

The integration of external knowledge sources, such as biomedical knowledge graphs, has further enhanced the effectiveness of PLMs in biomedical knowledge extraction and reasoning. For instance, the study on the use of knowledge graphs for biomedical pre-training [101] demonstrated that integrating knowledge graphs with PLMs can improve their ability to extract and reason about biomedical knowledge. The authors proposed a method to incorporate knowledge graphs into the pre-training process of PLMs, resulting in improved performance on biomedical relation extraction and entity linking tasks. Moreover, the application of knowledge graphs in biomedical knowledge extraction has been explored in [102], where the authors introduced a BERT model that learns representations based on both DNA sequence and epigenetic state inputs. The results showed that the model's ability to extract and reason about biomedical knowledge was significantly improved by incorporating epigenetic data, demonstrating the potential of integrating knowledge graphs with PLMs for biomedical knowledge extraction and reasoning.

In addition to entity linking, relation extraction, and logical inference, PLMs have also been applied to other biomedical knowledge extraction and reasoning tasks, such as drug-target interaction prediction and biomedical text summarization. For example, the study on the use of BERT for drug-target interaction prediction [103] demonstrated that BERT's representations can be used to predict interactions between drugs and target proteins. The authors fine-tuned BERT on a dataset of drug-target interactions and showed that the model outperformed previous approaches, highlighting its effectiveness in biomedical knowledge extraction. Similarly, the application of BERT in biomedical text summarization has been explored in [104], where the authors proposed a framework for both extractive and abstractive summarization. The results showed that the proposed model achieved state-of-the-art performance on biomedical text summarization tasks, underscoring the potential of PLMs in enhancing the efficiency and accuracy of biomedical knowledge extraction.

In conclusion, pre-trained language models have significantly advanced biomedical knowledge extraction and reasoning by enabling more effective and scalable solutions for tasks such as entity linking, relation extraction, and logical inference. The integration of external knowledge sources, such as biomedical knowledge graphs, has further enhanced the effectiveness of PLMs in these tasks. As the biomedical domain continues to generate vast amounts of unstructured data, the application of PLMs in knowledge extraction and reasoning will play an increasingly important role in advancing biomedical research and applications.

### 3.10 Biomedical Entity Linking and Disambiguation

[82]

Biomedical entity linking and disambiguation are critical tasks in the biomedical domain, where the accurate mapping of entities mentioned in text to their correct references in knowledge bases is essential for effective information retrieval, knowledge discovery, and clinical decision-making. Pre-trained language models (PLMs) have shown remarkable potential in addressing these challenges by leveraging their ability to understand context, capture semantic relationships, and generalize across domain-specific terminologies. In the biomedical domain, entities such as genes, proteins, diseases, and drugs are often mentioned using ambiguous or variant forms, making the task of linking and disambiguating these entities particularly challenging [105].

One of the primary approaches to biomedical entity linking involves using pre-trained models to generate contextualized representations of entities and their surrounding text. These representations are then used to match entities in the text with their corresponding entries in biomedical knowledge bases such as the Unified Medical Language System (UMLS), Gene Ontology (GO), and the National Library of Medicine (NLM) databases. For example, the UMLS-KGI-BERT model integrates biomedical knowledge from the UMLS into the pre-training process, allowing the model to learn entity-centric knowledge that enhances its performance in entity linking and disambiguation tasks [105]. This approach has demonstrated significant improvements in performance on multiple biomedical named entity recognition (NER) tasks, highlighting the value of incorporating domain-specific knowledge into pre-trained models.

Another key aspect of biomedical entity linking is the use of pre-trained models to handle the ambiguity and variability inherent in biomedical terminology. Biomedical texts often contain specialized jargon, abbreviations, and acronyms that can make it difficult for models to accurately identify and link entities. To address this challenge, researchers have explored various techniques, such as domain-specific pre-training, fine-tuning on entity linking datasets, and the integration of external knowledge sources. For instance, the BioBERT model, which is pre-trained on biomedical corpora, has shown strong performance in biomedical entity linking and disambiguation tasks [29]. By leveraging the vast amount of biomedical text available, BioBERT is able to capture the context and meaning of entities more effectively than general-purpose models.

The task of biomedical entity disambiguation is further complicated by the fact that many entities can refer to multiple concepts or entities in different contexts. For example, the term "insulin" can refer to the hormone itself, its gene, or various therapeutic formulations. Pre-trained language models can help mitigate this issue by learning the contextual relationships between entities and their associated text. This is particularly important in clinical settings, where accurate entity disambiguation is crucial for ensuring the reliability and interpretability of model outputs. The BioBART model, which is a generative language model pre-trained on biomedical text, has demonstrated the ability to generate accurate and contextually relevant entity links, further enhancing the performance of biomedical entity linking tasks [2].

In addition to improving the accuracy of entity linking and disambiguation, pre-trained language models can also enhance the efficiency of these tasks by reducing the need for extensive manual annotation and domain-specific feature engineering. Traditional entity linking systems often rely on hand-crafted rules and feature engineering to identify and link entities, which can be time-consuming and error-prone. In contrast, pre-trained models can automatically learn the relevant features and relationships from large-scale biomedical data, making the process more scalable and adaptable. The KBioXLM model, which is a knowledge-anchored biomedical multilingual pre-trained language model, has demonstrated the ability to handle cross-lingual entity linking and disambiguation tasks by incorporating knowledge from multiple sources [61]. This approach not only improves the performance of entity linking in different languages but also enhances the model's ability to handle domain-specific terminology and context.

Another important consideration in biomedical entity linking and disambiguation is the need for models to be interpretable and transparent, especially in clinical and research settings. While pre-trained language models have shown impressive performance, their black-box nature can make it difficult to understand how they make decisions, which is a significant barrier to their adoption in critical applications. To address this, researchers have explored various techniques to improve the interpretability of pre-trained models, such as attention mechanisms, saliency analysis, and knowledge graph integration. The DRAGON model, which combines text and knowledge graph data during pre-training, has demonstrated the ability to improve the interpretability of biomedical entity linking and disambiguation tasks [106]. By leveraging structured knowledge from biomedical knowledge graphs, DRAGON can provide more transparent and interpretable entity links, enhancing the trustworthiness of model outputs.

The application of pre-trained language models to biomedical entity linking and disambiguation is also influenced by the availability of high-quality annotated datasets and benchmarking frameworks. While there are several datasets available for biomedical NLP tasks, such as the BioASQ and the MEDIQA datasets, they often lack the comprehensive coverage and annotation required for effective entity linking and disambiguation. To address this, researchers have developed new benchmarks and evaluation metrics that are specifically tailored to the challenges of biomedical entity linking. For example, the DrBenchmark dataset provides a comprehensive evaluation framework for French biomedical language understanding, including entity linking and disambiguation tasks [26]. These benchmarks help to ensure that models are evaluated fairly and consistently, facilitating the comparison and improvement of different approaches.

In conclusion, pre-trained language models have emerged as a powerful tool for biomedical entity linking and disambiguation, offering significant improvements in accuracy, efficiency, and scalability. By leveraging domain-specific knowledge, contextual understanding, and advanced training techniques, these models are helping to address the unique challenges of the biomedical domain. As research in this area continues to evolve, we can expect to see even more sophisticated and effective approaches to biomedical entity linking and disambiguation, driven by the ongoing advancements in pre-trained language models and their applications.

## 4 Domain-Specific Challenges and Limitations

### 4.1 Data Scarcity in Biomedical Domains

Data scarcity is one of the most significant challenges in the biomedical domain, which severely hinders the training and performance of pre-trained language models (PLMs). Unlike general NLP tasks where large-scale annotated datasets are readily available, the biomedical domain faces a severe shortage of high-quality, domain-specific annotated data. This scarcity is primarily due to the complexity of medical language, the high cost of expert annotation, and the sensitivity of patient-related information, which makes data sharing and collection particularly challenging. As a result, pre-trained language models that are typically trained on large general-domain corpora often struggle to achieve satisfactory performance when applied to biomedical tasks without proper adaptation [2].

The lack of annotated biomedical data is particularly pronounced in tasks such as named entity recognition (NER), relation extraction, and clinical text analysis. For example, in the case of NER, the task involves identifying and classifying biomedical entities such as diseases, genes, proteins, and drugs. However, the development of high-quality annotated datasets for such tasks requires the collaboration of domain experts, which is both time-consuming and expensive [105]. Moreover, the specialized and rapidly evolving terminology in the biomedical domain further complicates the annotation process. This leads to a situation where even the most advanced pre-trained models may fail to generalize well to biomedical tasks, as they lack sufficient exposure to domain-specific language patterns and concepts.

The problem of data scarcity is compounded by the fact that many biomedical datasets are not publicly available due to ethical and legal constraints. For instance, electronic health records (EHRs) contain sensitive patient information, which limits their use in research and model training. While some efforts have been made to release public biomedical datasets, such as the MIMIC-III and MIMIC-IV datasets, these datasets often lack comprehensive annotations or are not representative of the broader biomedical domain. This lack of publicly accessible, well-annotated data makes it difficult for researchers to develop and evaluate PLMs that are tailored for biomedical applications [107].

Another challenge associated with data scarcity is the difficulty of transferring knowledge from general-domain PLMs to biomedical tasks. While general-domain models like BERT and GPT have achieved remarkable success in NLP tasks, their performance on biomedical tasks is often suboptimal due to the lack of domain-specific training data [24]. This is because these models are trained on a wide range of text, which may not include the specialized language and concepts required for biomedical applications. To bridge this gap, researchers have explored domain-specific pre-training, where models are trained on biomedical corpora such as PubMed abstracts or clinical notes. However, even with such pre-training, the limited size of annotated biomedical data remains a major obstacle, as it restricts the model's ability to learn and generalize effectively [10].

The scarcity of annotated data also affects the performance of transfer learning approaches, which rely on fine-tuning pre-trained models on domain-specific tasks. While transfer learning has been shown to be effective in many NLP tasks, the effectiveness of this approach in the biomedical domain is often limited by the lack of high-quality labeled data. For example, in the case of drug discovery and molecular generation, the availability of annotated datasets that provide information about drug-target interactions or molecular properties is extremely limited [103]. This makes it difficult to train models that can accurately predict biological properties or generate novel molecules.

Moreover, the reliance on annotated data for model training can lead to biased or unreliable results in biomedical applications. For instance, models trained on small or imbalanced datasets may not generalize well to new, unseen data, especially in clinical settings where patient variability is high [108]. This is particularly problematic in tasks such as clinical decision-making, where model reliability and accuracy are critical. The lack of sufficient data also makes it difficult to evaluate the performance of models in real-world scenarios, as there are few benchmark datasets that reflect the complexity and diversity of clinical data [6].

To address the challenge of data scarcity, researchers have explored various techniques, including data augmentation, self-supervised learning, and the use of external knowledge sources. For example, some studies have proposed using synthetic data generation techniques to augment existing biomedical datasets, which can help improve model performance without requiring additional expert annotations [109]. Others have focused on leveraging pre-trained language models to extract knowledge from unstructured biomedical text, such as scientific literature or clinical notes, to enrich the training data [110]. However, these approaches are still in their early stages and face challenges such as ensuring the accuracy and reliability of the generated data.

In summary, data scarcity remains a major obstacle in the development and application of pre-trained language models in the biomedical domain. The limited availability of high-quality, annotated biomedical data hinders the training of models that can effectively capture the complexity and specificity of biomedical language. While various techniques have been proposed to address this challenge, such as domain-specific pre-training and data augmentation, the problem of data scarcity continues to be a significant limitation in the field. Future research should focus on developing more effective strategies for data collection, annotation, and model training to overcome this challenge and enable the broader adoption of PLMs in biomedical applications.

### 4.2 Domain-Specific Terminology and Jargon

The biomedical domain is characterized by a highly specialized and rapidly evolving terminology that poses significant challenges for pre-trained language models (PLMs). Unlike general language, which is often more stable and widely understood, biomedical texts are replete with jargon, acronyms, and domain-specific vocabulary that can be difficult for models to grasp, especially when they are trained on general-domain corpora. This issue is not merely a matter of vocabulary size but also of the nuanced meanings and contexts in which these terms are used, making it challenging for models to generalize effectively across different biomedical tasks.

One of the key challenges in dealing with domain-specific terminology is the lack of consistency in how terms are used across different sources. For instance, a single medical term might be represented differently in research papers, clinical notes, or drug labels, depending on the context and the authors intent. This variability can lead to confusion and misinterpretation by PLMs, which rely heavily on patterns and statistical regularities in the data they are trained on. As a result, models may struggle to accurately identify and understand the meaning of terms that are critical for tasks such as named entity recognition (NER), relation extraction, and clinical text analysis [6].

Moreover, the rapid evolution of biomedical terminology compounds the problem. New terms emerge as research progresses, and existing terms may be redefined or replaced. This dynamic nature makes it difficult for PLMs to maintain up-to-date knowledge, especially when they are not continuously retrained or fine-tuned. For example, in the context of drug discovery, new compounds and molecular structures are frequently introduced, and the associated terminology may not be well-represented in pre-trained models. As a result, models may fail to accurately capture the nuances of these terms, leading to suboptimal performance in tasks such as drug-target interaction prediction and property prediction [6].

Another challenge is the presence of acronyms and abbreviations in biomedical texts, which can be particularly problematic for PLMs. Many terms are represented by short forms that may be ambiguous or context-dependent. For example, "AIDS" is an acronym for "Acquired Immunodeficiency Syndrome," but in different contexts, the same abbreviation might refer to different concepts or be misinterpreted entirely. This ambiguity can lead to errors in tasks such as information retrieval and text summarization, where accurate interpretation of terms is crucial [3].

The complexity of biomedical terminology is further compounded by the fact that it often overlaps with other domains, such as chemistry, physics, and computer science. For example, the term "gene" is used in both biological and computational contexts, but the meanings and applications can differ significantly. This cross-domain overlap can confuse PLMs, which may struggle to distinguish between the different usages of the same term. As a result, models may produce outputs that are technically correct in one domain but inappropriate or incorrect in another [7].

Furthermore, the use of domain-specific terminologies can lead to issues of model interpretability. Since biomedical texts are often dense with technical language, it can be challenging for models to provide clear and transparent explanations for their predictions. This lack of interpretability is particularly concerning in healthcare settings, where transparency and explainability are essential for building trust and ensuring clinical adoption. For instance, if a model generates a diagnosis based on a misinterpretation of a medical term, it may be difficult for clinicians to understand why the model arrived at that conclusion, leading to skepticism about its reliability [7].

To address these challenges, researchers have explored various strategies, including the integration of domain-specific knowledge into pre-trained models. For example, the use of biomedical knowledge graphs and terminology resources can help models better understand and interpret domain-specific terms [6]. By incorporating such knowledge, models can be equipped with a more comprehensive understanding of the biomedical domain, enabling them to generalize more effectively across different tasks and contexts.

Additionally, the development of domain-specific pre-trained models has shown promise in addressing the challenges posed by biomedical terminology. These models are trained on large-scale biomedical corpora, which include a wide range of domain-specific texts such as research articles, clinical notes, and drug labels. By leveraging such data, domain-specific models can better capture the nuances of biomedical terminology, leading to improved performance on tasks such as NER and relation extraction [7].

In conclusion, the challenges posed by domain-specific terminology and jargon in the biomedical domain are significant and multifaceted. The complexity, variability, and rapid evolution of biomedical terms make it difficult for PLMs to accurately understand and generalize across different tasks. Addressing these challenges requires a combination of strategies, including the integration of domain-specific knowledge, the development of specialized pre-trained models, and the use of advanced fine-tuning techniques. By overcoming these challenges, researchers can enhance the effectiveness and reliability of PLMs in biomedical applications, ultimately leading to better outcomes in healthcare and biomedical research.

### 4.3 Model Interpretability and Explainability

Model interpretability and explainability are critical challenges in the application of pre-trained language models (PLMs) within the biomedical domain. Unlike general NLP tasks, where model performance can often be evaluated through quantitative metrics alone, biomedical applications require not only accuracy but also a high degree of transparency. This is particularly true in clinical settings, where decisions based on model outputs can have life-or-death consequences. The opacity of deep learning models, often referred to as the black box problem, remains a major barrier to their widespread adoption in healthcare. In biomedical NLP, where models are frequently used to extract critical information from clinical texts, such as patient records, research articles, and diagnostic reports, the lack of interpretability can hinder clinical trust, regulatory approval, and the integration of AI systems into real-world workflows.

One of the main reasons for this challenge is the complexity of biomedical language. Medical texts are filled with domain-specific terminology, abbreviations, and contextual nuances that are not typically found in general language. Pre-trained language models, which are often trained on general-domain corpora, may struggle to capture the precise semantics and relationships that are essential for biomedical tasks. For instance, a model may recognize a term like myocardial infarction but fail to distinguish between different subtypes or contextual variations, which could lead to misinterpretation or incomplete information extraction. As a result, the black-box nature of these models makes it difficult for clinicians and researchers to understand how the model arrives at its conclusions, limiting their ability to validate, debug, or trust the outputs.

Recent studies have highlighted the importance of model interpretability in biomedical NLP. For example, in the context of clinical text summarization, researchers have emphasized the need for models to not only generate accurate summaries but also to provide clear explanations of how the information was derived. This is especially relevant for applications such as drug discovery and clinical decision support, where understanding the rationale behind a models prediction can be as important as the prediction itself. A lack of interpretability can also complicate the integration of NLP models into clinical workflows, as practitioners may hesitate to rely on models that cannot be easily understood or verified [14].

Moreover, the integration of external knowledge sources, such as biomedical ontologies and knowledge graphs, can help improve model interpretability by providing structured representations of domain-specific information. However, even with such integrations, the underlying models remain challenging to interpret. For instance, while knowledge-based approaches can provide a level of transparency by mapping entities to known concepts, the way models combine and process this information is often not clear. This is a critical issue in applications such as biomedical information retrieval, where the ability to trace how a model identifies relevant documents or entities is essential for ensuring the reliability of the results [14].

Several studies have explored techniques to enhance model interpretability in the biomedical domain. One approach is the use of explainable AI (XAI) methods, which aim to provide insights into how models make decisions. For example, methods such as attention mechanisms, saliency maps, and feature attribution can be used to highlight the parts of the text that the model considers most important when making predictions. These techniques have been applied to tasks such as named entity recognition (NER), where the ability to identify and explain the basis of entity detection is crucial for validating the models outputs [14]. However, these techniques often provide only a partial view of the models decision-making process and may not fully address the challenges of interpretability in complex biomedical tasks.

Another approach to improving interpretability is the development of domain-specific models that are explicitly designed with transparency in mind. For example, some models have been developed with modular architectures that allow for the separation of different components, making it easier to trace how information flows through the model. This is particularly useful in tasks such as drug discovery, where the ability to understand how a model identifies potential drug targets or predicts molecular interactions is essential for further research and development [2]. However, the development of such models often requires significant effort and domain expertise, and the trade-off between model complexity and interpretability remains a key challenge.

In addition to technical approaches, there is also a growing emphasis on the need for interdisciplinary collaboration to address the interpretability challenge. Clinicians, domain experts, and model developers must work together to ensure that models are not only accurate but also aligned with the needs of the medical community. This includes the development of evaluation frameworks that prioritize interpretability alongside performance, as well as the creation of user-friendly tools that allow clinicians to interact with and understand model outputs [14].

Despite these efforts, the challenge of model interpretability in biomedical NLP remains a significant hurdle. The complexity of medical language, the limitations of existing interpretability techniques, and the lack of standardized evaluation metrics all contribute to this challenge. As the field continues to evolve, it is essential to prioritize the development of more transparent and interpretable models that can be trusted and adopted in clinical settings. This will not only enhance the reliability and effectiveness of biomedical NLP systems but also support their broader integration into healthcare applications.

### 4.4 Domain Adaptation and Transferability

Domain adaptation and transferability are critical challenges in the application of pre-trained language models (PLMs) to the biomedical domain. While general-purpose PLMs like BERT and GPT have demonstrated remarkable performance across a wide range of natural language processing (NLP) tasks, their direct application to biomedical tasks often falls short due to the unique characteristics of biomedical text. Biomedical texts are typically rich in domain-specific terminology, complex sentence structures, and a high degree of variability, which make it difficult for general PLMs to generalize effectively. This necessitates the development of strategies for domain adaptation and transfer learning, which aim to tailor pre-trained models to the specific needs of biomedical tasks.

One of the primary challenges in domain adaptation is the scarcity of annotated biomedical data. Unlike general NLP tasks, where large-scale annotated datasets are readily available, the biomedical domain often lacks sufficient labeled data due to the high cost and expertise required for annotation. This data scarcity poses a significant obstacle to the training and fine-tuning of models for biomedical tasks. For instance, a study on biomedical named entity recognition (NER) found that the limited availability of annotated data restricts the performance of models, even when using advanced techniques like prompt engineering and knowledge integration [111]. To address this, researchers have explored domain-specific pre-training, where models are trained on large biomedical corpora to enhance their understanding of domain-specific language and concepts.

Another challenge in domain adaptation is the need for models to effectively capture the nuances of biomedical terminology. Biomedical texts contain a vast array of specialized terms, many of which are not present in general-domain corpora. This necessitates the integration of external knowledge sources, such as biomedical knowledge graphs and ontologies, into the training process. For example, the use of knowledge graphs like UMLS (Unified Medical Language System) has been shown to enhance the performance of PLMs by providing structured domain-specific information that complements textual data [112]. However, the integration of such knowledge sources is not straightforward and requires careful design to ensure that the model can effectively leverage the additional information.

Transferability, or the ability of a model to perform well on tasks outside its training domain, is another critical challenge in the biomedical domain. While general PLMs may perform well on tasks like text classification and question answering, their performance on specialized biomedical tasks often lags behind. This is because the models may not have sufficient exposure to the domain-specific language and concepts required for these tasks. For example, a study comparing the performance of general PLMs and biomedical-specific models in clinical NER found that biomedical-specific models outperformed their general counterparts, highlighting the importance of domain adaptation [18]. The study also emphasized the need for models to be fine-tuned on domain-specific data to achieve optimal performance.

The challenges of domain adaptation and transferability are further compounded by the heterogeneity of biomedical data. Biomedical texts can vary significantly in terms of format, structure, and content, making it difficult for models to generalize across different types of documents. For instance, clinical texts such as electronic health records (EHRs) differ from scientific literature in terms of language and structure, requiring different approaches for effective processing. This necessitates the development of flexible and adaptable models that can handle the diverse nature of biomedical data. Techniques such as multi-task learning and transfer learning have been proposed to address this challenge, enabling models to learn from multiple tasks and adapt to new domains more effectively [21].

Moreover, the transferability of models across different biomedical subdomains is a significant concern. For example, a model trained on clinical texts may not perform well on biochemical or genetic data due to the differences in terminology and structure. This highlights the need for domain-specific pre-training and fine-tuning strategies that take into account the unique characteristics of each subdomain. Research has shown that models pre-trained on domain-specific corpora can achieve better performance on specialized tasks compared to general PLMs [6]. However, the process of domain-specific pre-training is resource-intensive and requires access to large-scale biomedical datasets, which may not always be available.

In addition to these challenges, the interpretability and explainability of models in the biomedical domain are critical considerations. Biomedical applications often require models to be transparent and interpretable to ensure that their outputs can be trusted and validated by domain experts. This is particularly important in clinical settings, where decisions based on model outputs can have significant implications for patient care. Techniques such as attention mechanisms and explainable AI (XAI) have been explored to improve the interpretability of models, but further research is needed to develop more robust and reliable methods [52].

Finally, the computational efficiency and scalability of models are important factors to consider in the context of domain adaptation and transferability. Biomedical tasks often involve large volumes of data and complex computations, which can be computationally expensive. This necessitates the development of efficient and scalable models that can handle the demands of real-world applications. Techniques such as model compression, knowledge distillation, and parameter-efficient fine-tuning have been proposed to address these challenges, enabling models to be deployed in resource-constrained environments [113].

In summary, domain adaptation and transferability are complex challenges in the application of pre-trained language models to the biomedical domain. The scarcity of annotated data, the need for domain-specific knowledge integration, the heterogeneity of biomedical data, and the importance of model interpretability and efficiency all contribute to the difficulty of adapting and transferring models to biomedical tasks. Addressing these challenges requires a combination of domain-specific pre-training, fine-tuning strategies, and the integration of external knowledge sources, as well as the development of more efficient and interpretable models.

### 4.5 Challenges in Multilingual Biomedical NLP

The challenges in multilingual biomedical natural language processing (NLP) are multifaceted and significantly complicate the development and application of pre-trained language models (PLMs) in this domain. One of the primary challenges is the lack of non-English domain-specific corpora, which are essential for training and evaluating models that can effectively handle biomedical texts in multiple languages. Unlike the English language, which has a wealth of annotated biomedical data, many other languages lack comparable resources, making it difficult to develop models that can generalize across languages. For instance, while there are extensive datasets for English biomedical NLP tasks, such as the BioBERT and PubMed datasets, similar resources are scarce for other languages like French, German, or Chinese [26]. This scarcity of multilingual biomedical data limits the ability of PLMs to perform well in cross-lingual settings, as models trained on English data may struggle to understand and process biomedical texts in other languages.

Another significant challenge in multilingual biomedical NLP is the difficulty of cross-lingual adaptation. Even when some multilingual corpora exist, they often lack the necessary annotations and linguistic diversity to effectively train models for specific biomedical tasks. For example, the DrBenchmark dataset, which focuses on the French biomedical domain, includes 20 diversified tasks but is still limited in scope compared to the more extensive English datasets [26]. This limitation underscores the need for more comprehensive and diverse multilingual biomedical corpora to support the development of cross-lingual PLMs. Additionally, the lack of standardized annotation guidelines for non-English biomedical texts further complicates the process of creating and evaluating these models. Without consistent and reliable annotations, it is challenging to measure the performance of PLMs across different languages and ensure that they meet the required standards for clinical and research applications.

The challenges of multilingual biomedical NLP are exacerbated by the complexity of biomedical language itself. Biomedical terminology is highly specialized and often varies significantly across languages. This variation can lead to difficulties in model understanding and generalization, as models trained on one language may not perform well on another. For example, the term "hypertension" in English may have different connotations or translations in other languages, leading to potential misinterpretations by the model [114]. This issue is further compounded by the fact that many biomedical texts contain complex and technical terms that are not easily translatable or may require additional contextual information for accurate interpretation. As a result, models must not only understand the linguistic structure of the text but also the domain-specific knowledge and context, which is a non-trivial task.

Moreover, the lack of multilingual biomedical knowledge graphs and ontologies poses another significant challenge. Knowledge graphs and ontologies play a crucial role in enhancing the performance of PLMs by providing structured domain-specific knowledge. However, the availability of such resources in non-English languages is limited, making it difficult to integrate them into PLMs for cross-lingual applications [4]. This limitation not only affects the model's ability to understand and process biomedical texts but also hinders the development of models that can effectively reason about biomedical concepts across languages. The absence of these resources necessitates the development of new methods for creating and integrating multilingual biomedical knowledge, which is a time-consuming and resource-intensive process.

The challenges in multilingual biomedical NLP also extend to the evaluation and benchmarking of models. The lack of standardized benchmarks for non-English biomedical tasks makes it difficult to compare the performance of different models and assess their effectiveness. While some efforts have been made to create multilingual benchmarks, such as the RJUA-MedDQA dataset for medical document question answering, these datasets are still limited in scope and coverage [27]. This limitation hampers the ability of researchers to evaluate the performance of their models and identify areas for improvement. Furthermore, the lack of comprehensive evaluation metrics for multilingual biomedical tasks makes it difficult to determine the strengths and weaknesses of different models, which is essential for guiding future research and development.

Another critical challenge in multilingual biomedical NLP is the issue of data privacy and ethical considerations. The use of multilingual biomedical data raises concerns about the privacy and security of patient information, particularly when the data is collected from multiple countries and languages. Ensuring that data is handled in compliance with ethical standards and regulations is essential for building trust and ensuring the responsible use of PLMs in biomedical applications. However, the lack of clear guidelines and frameworks for handling multilingual biomedical data complicates this process, making it difficult to implement effective data privacy and security measures.

In addition to these challenges, the development of multilingual biomedical PLMs also requires addressing the issue of computational efficiency and scalability. The training of large PLMs is resource-intensive, and the addition of multiple languages further increases the computational burden. This challenge is particularly relevant for resource-constrained environments where the availability of computational resources is limited. As a result, the development of efficient and scalable methods for training and deploying multilingual biomedical PLMs is an important area of research. Techniques such as model compression, knowledge distillation, and parameter-efficient fine-tuning can help mitigate these challenges by reducing the computational requirements of PLMs while maintaining their performance.

Finally, the challenges in multilingual biomedical NLP highlight the need for interdisciplinary collaboration and knowledge sharing. The development of effective multilingual biomedical PLMs requires expertise from multiple domains, including linguistics, computer science, and biomedical sciences. Collaborative efforts between researchers, clinicians, and industry stakeholders are essential for addressing the complex challenges associated with multilingual biomedical NLP. By fostering collaboration and knowledge sharing, the research community can work towards developing more robust and effective models that can better serve the needs of the biomedical field. This collaborative approach is crucial for advancing the field and ensuring that PLMs can effectively support the diverse and complex tasks involved in biomedical NLP.

### 4.6 Model Efficiency and Scalability

Deploying pre-trained language models (PLMs) in the biomedical domain presents unique challenges, especially concerning computational efficiency and scalability. While these models have demonstrated impressive performance in various natural language processing (NLP) tasks, their application in the biomedical field requires careful consideration of the resources they demand. The biomedical domain is characterized by the need for high accuracy and domain-specific understanding, which often translates into the requirement for larger and more complex models. However, this complexity comes at a cost, as these models may require substantial computational resources, making them less accessible for practical applications in healthcare settings.

One of the primary challenges in the biomedical domain is the computational efficiency of PLMs. Pre-trained models such as BioMegatron, which are trained on large-scale biomedical corpora, can be extremely resource-intensive. These models often have millions of parameters, which can lead to high memory consumption and prolonged inference times. For instance, the BioMegatron model, while achieving state-of-the-art performance on biomedical benchmarks, requires significant computational power, which can be a barrier to deployment in real-world clinical environments [10]. This inefficiency is particularly problematic in scenarios where rapid decision-making is crucial, such as in emergency medical situations or during large-scale data analysis.

Scalability is another critical issue that arises when deploying PLMs in the biomedical domain. As the volume of biomedical data continues to grow, the need for models that can efficiently process and analyze this data becomes increasingly urgent. However, many existing PLMs are not designed to scale effectively. For example, models trained on general-domain data may struggle to adapt to the unique challenges of biomedical data, such as the presence of specialized terminology and the need for domain-specific knowledge. This lack of scalability can hinder the ability of PLMs to keep pace with the evolving landscape of biomedical research and clinical practice.

The need for resource-efficient models is particularly pronounced in the biomedical domain due to the limited availability of computational resources in many healthcare settings. Many hospitals and research institutions may not have access to high-performance computing infrastructure, making it challenging to deploy large-scale PLMs. This is where the concept of parameter-efficient fine-tuning comes into play. Techniques such as adapter layers, LoRA (Low-Rank Adaptation), and other lightweight methods allow for the adaptation of pre-trained models to biomedical tasks with minimal computational overhead [37]. These approaches are particularly valuable in scenarios where the available resources are constrained, as they enable the deployment of effective models without the need for extensive computational power.

In addition to computational efficiency and scalability, the biomedical domain also presents unique challenges related to model interpretability and robustness. While PLMs can capture complex patterns and relationships in biomedical texts, they often operate as "black boxes," making it difficult to understand how they arrive at their predictions. This lack of interpretability can be a significant barrier to their adoption in clinical settings, where transparency and explainability are essential for trust and regulatory compliance. Furthermore, the robustness of these models in the face of noisy or incomplete data is a critical concern, as biomedical data can often be highly variable and subject to various forms of bias [57].

To address these challenges, researchers are exploring various strategies to enhance the efficiency and scalability of PLMs in the biomedical domain. One promising approach is the development of domain-specific pre-training strategies that focus on the unique characteristics of biomedical data. For example, models like BioBERT and BioMegatron are pre-trained on large-scale biomedical corpora, which can improve their performance on domain-specific tasks [10]. However, even with these domain-specific pre-training strategies, the computational demands of these models remain a significant hurdle.

Another important consideration is the integration of external knowledge sources, such as biomedical knowledge graphs and ontologies, into the training process of PLMs. These knowledge sources can provide valuable context and help improve the models' understanding of domain-specific terminology and relationships. For instance, the use of knowledge graphs like the PubMed knowledge graph (PKG) can enhance the performance of PLMs by providing structured representations of biomedical entities and their relationships [33]. However, the integration of such knowledge sources often requires additional computational resources, which can further complicate the deployment of PLMs in resource-constrained settings.

Moreover, the development of efficient and scalable models for the biomedical domain is essential for enabling the widespread adoption of PLMs in clinical and research settings. Techniques such as model compression, knowledge distillation, and other optimization strategies can help reduce the computational footprint of PLMs while maintaining their performance. For example, the Adapt-and-Distill approach demonstrates how pre-trained models can be adapted to specific domains while maintaining a smaller and faster footprint [37]. These strategies are crucial for ensuring that PLMs can be deployed in a variety of settings, from large research institutions to smaller clinical practices.

In conclusion, the challenges of model efficiency and scalability in the biomedical domain are multifaceted and require a combination of innovative techniques and strategies. While pre-trained language models have the potential to revolutionize biomedical NLP, their deployment in practical settings is contingent on addressing these challenges effectively. By focusing on computational efficiency, scalability, and resource optimization, researchers can develop models that are not only powerful but also accessible and applicable in real-world biomedical scenarios. This will be essential for realizing the full potential of PLMs in advancing biomedical research and clinical applications.

### 4.7 Ethical and Privacy Concerns

The ethical and privacy concerns in the biomedical domain are of paramount importance when deploying pre-trained language models (PLMs). These models often rely on sensitive patient data, which raises significant ethical and privacy challenges. The use of such data, while essential for training robust models, can lead to the violation of patient confidentiality and the potential for biased or harmful outputs. Addressing these concerns is crucial to ensuring that PLMs are not only effective but also ethically sound in their applications within the biomedical field.

One of the primary ethical concerns is the use of sensitive patient data. Biomedical datasets often include personal health information, which can be highly confidential. The collection and utilization of such data must adhere to stringent ethical guidelines and legal frameworks to protect patient privacy. For instance, the General Data Protection Regulation (GDPR) in the European Union and the Health Insurance Portability and Accountability Act (HIPAA) in the United States impose strict requirements on the handling of personal health information. Failure to comply with these regulations can result in severe penalties and loss of public trust [115].

Moreover, the potential for biased or harmful outputs poses another significant challenge. Pre-trained language models can inadvertently perpetuate biases present in the training data, which can lead to discriminatory outcomes. For example, if a model is trained on a dataset that predominantly represents a particular demographic, it may perform poorly on other groups, leading to inaccurate or unfair results. This issue is particularly critical in the biomedical domain, where such biases can have serious consequences for patient care. Research has shown that models can exhibit biases in tasks such as named entity recognition and clinical text analysis, which can affect the accuracy of diagnoses and treatment recommendations [2].

The ethical implications of using PLMs in the biomedical domain also extend to the potential for harmful outputs. These models can generate text that is misleading or harmful, especially if they are not properly monitored and evaluated. For instance, in the context of medical advice, a model might generate incorrect or dangerous recommendations, which can have life-threatening consequences. This underscores the need for rigorous evaluation and validation of PLMs before they are deployed in clinical settings. The importance of thorough evaluation is highlighted in studies that emphasize the need for robustness and reliability in medical NLP applications [116].

Another critical aspect of ethical concerns is the issue of data privacy. The use of patient data in training PLMs requires careful handling to prevent unauthorized access and misuse. Techniques such as data anonymization and differential privacy can be employed to protect patient information. However, these methods are not foolproof and can sometimes compromise the quality of the data, thereby affecting the performance of the models. Research into effective data anonymization techniques is ongoing, and there is a need for standardized approaches to ensure that patient privacy is maintained without sacrificing model accuracy [41].

In addition to data privacy, there is a growing concern about the transparency and explainability of PLMs. The "black box" nature of many deep learning models makes it difficult to understand how they arrive at their decisions, which can be problematic in the biomedical domain where transparency is crucial. Ethical AI frameworks emphasize the importance of explainability, as it allows stakeholders to understand and trust the decisions made by these models. Efforts to enhance model interpretability, such as the use of attention mechanisms and visualization tools, are essential for building trust and ensuring ethical deployment [117].

Furthermore, the potential for misuse of PLMs in the biomedical domain is a significant ethical concern. These models can be exploited for malicious purposes, such as generating fake medical content or manipulating public health information. This risk is particularly pronounced in the context of social media, where misinformation can spread rapidly and have serious consequences. Addressing this issue requires a multifaceted approach that includes the development of robust detection mechanisms, as well as the promotion of ethical guidelines for the use of NLP in healthcare [118].

The ethical and privacy concerns in the biomedical domain also highlight the importance of interdisciplinary collaboration. Researchers, clinicians, and policymakers must work together to develop frameworks that ensure the ethical use of PLMs. This collaboration is essential for addressing the complex challenges posed by the integration of AI into healthcare, including the need for standardized data practices, transparent algorithms, and equitable access to AI-driven healthcare solutions [119].

In conclusion, the ethical and privacy concerns associated with the use of pre-trained language models in the biomedical domain are multifaceted and require careful consideration. The use of sensitive patient data, the potential for biased or harmful outputs, and the need for transparency and explainability are all critical issues that must be addressed. By adhering to ethical guidelines, implementing robust data protection measures, and fostering interdisciplinary collaboration, the biomedical community can ensure that PLMs are used in a manner that is both effective and ethically responsible [39].

### 4.8 Evaluation and Benchmarking Limitations

Evaluation and benchmarking are critical components in assessing the effectiveness of pre-trained language models (PLMs) in the biomedical domain. However, the current evaluation metrics and benchmarks face several limitations that hinder their ability to accurately and comprehensively assess the performance of these models. These limitations stem from the unique characteristics of biomedical text, the scarcity of annotated data, and the lack of standardized and domain-specific evaluation frameworks. Addressing these challenges is essential to ensure that the models developed for biomedical tasks are robust, reliable, and applicable in real-world scenarios.

One of the primary limitations of current evaluation metrics is their lack of specificity to the biomedical domain. Traditional metrics such as accuracy, F1 score, precision, and recall, while widely used in general natural language processing (NLP) tasks, may not fully capture the nuances of biomedical text. For instance, biomedical texts often contain complex terminology, abbreviations, and domain-specific structures that are not adequately represented by these metrics. As a result, models may perform well on these metrics but fail to generalize to real-world biomedical applications where domain-specific knowledge is crucial. Moreover, these metrics may not effectively evaluate the ability of models to capture the context and meaning of biomedical terms, which is essential for tasks such as named entity recognition (NER) and clinical text analysis [120; 121; 122].

Another significant limitation is the scarcity of annotated biomedical datasets. While there are several benchmark datasets available for general NLP tasks, the availability of high-quality, annotated biomedical datasets is limited. This scarcity of data makes it challenging to train and evaluate models effectively. For example, datasets like MedNLI, BioBART, and CLUE are commonly used for evaluating biomedical NLP models, but they may not cover the full spectrum of biomedical tasks or provide sufficient diversity in terms of data sources and annotations. The lack of diverse and representative datasets can lead to overfitting and poor generalization, particularly when models are applied to real-world biomedical scenarios that differ from the training data [120; 121; 122].

The limitations of current benchmarks also extend to their lack of standardization. There is a need for a unified and standardized evaluation framework that can consistently assess the performance of biomedical PLMs across different tasks and domains. Without such a framework, it is difficult to compare the performance of different models and methodologies, making it challenging to identify the most effective approaches. For instance, the evaluation of models for tasks such as drug discovery, clinical text analysis, and biomedical information retrieval often involves different metrics and evaluation protocols, leading to inconsistencies in performance assessments. This lack of standardization hinders the development of a cohesive and comprehensive evaluation strategy for biomedical NLP [120; 121; 122].

Furthermore, the evaluation of biomedical PLMs must account for the unique challenges posed by the domain. These challenges include the need for models to be interpretable and explainable, as trust and transparency are critical in the biomedical field. However, current evaluation metrics often focus on performance metrics rather than model interpretability. This gap in evaluation highlights the need for new metrics that can assess the model's ability to provide explanations and insights that are meaningful to biomedical professionals. For example, models that can accurately identify and link biomedical entities in text and provide explanations for their predictions are more likely to be adopted in clinical settings. However, existing metrics may not adequately capture these aspects, leading to an incomplete evaluation of the model's effectiveness [120; 121; 122].

The limitations of current evaluation frameworks also extend to their inability to handle the complexities of multimodal and cross-lingual tasks. Biomedical NLP often involves the integration of multiple data modalities, such as text, images, and clinical data, as well as the processing of multiple languages. However, current benchmarks may not fully support these tasks, leading to incomplete evaluations. For instance, while there are benchmarks for monolingual biomedical tasks, the availability of multilingual and cross-lingual benchmarks is limited. This limitation makes it challenging to evaluate the performance of models that are designed to handle multiple languages and modalities, which is essential for global biomedical research and applications [120; 121; 122].

Additionally, the evaluation of biomedical PLMs must consider the ethical and legal implications of their use. Biomedical data often contains sensitive patient information, and models must be evaluated for their ability to handle such data responsibly. Current benchmarks may not adequately address these ethical considerations, leading to potential risks in the deployment of models in real-world settings. For example, models must be evaluated for their ability to protect patient privacy, avoid biases, and ensure fairness in their predictions. However, existing metrics may not capture these aspects, leading to an incomplete evaluation of the model's ethical and legal compliance [120; 121; 122].

To address these limitations, there is a need for the development of standardized and domain-specific evaluation frameworks that can effectively assess the performance of biomedical PLMs. Such frameworks should incorporate a range of metrics that capture the unique characteristics of biomedical text, including domain-specific terminology, complexity, and the need for interpretability. Furthermore, the development of diverse and representative datasets is essential to ensure that models are trained and evaluated on data that reflects the full spectrum of biomedical tasks and applications.

In conclusion, the current evaluation metrics and benchmarks in the biomedical domain face several limitations that hinder their effectiveness in assessing the performance of pre-trained language models. These limitations include the lack of specificity to the biomedical domain, the scarcity of annotated datasets, the lack of standardization, and the inability to handle the complexities of multimodal and cross-lingual tasks. Addressing these challenges requires the development of standardized and domain-specific evaluation frameworks that can comprehensively evaluate the performance of biomedical PLMs. By doing so, the field can ensure that models are not only effective but also reliable, interpretable, and ethically sound in their application to biomedical tasks [120; 121; 122].

## 5 Techniques for Domain Adaptation and Fine-tuning

### 5.1 Domain-Specific Pre-Training

Domain-specific pre-training is a critical technique in the adaptation of pre-trained language models (PLMs) to the biomedical domain. Unlike general-purpose models, which are trained on large, diverse corpora, domain-specific pre-training focuses on leveraging biomedical-specific data to enhance the model's understanding of the unique characteristics of the biomedical text. This process involves training models on specialized corpora that contain a rich variety of biomedical terminology, clinical documentation, and scientific literature, thereby enabling them to capture domain-specific knowledge and language patterns more effectively.

One of the primary motivations for domain-specific pre-training is the recognition that biomedical texts are characterized by highly specialized vocabulary, complex sentence structures, and domain-specific concepts. These features make it challenging for general-domain models to perform well on biomedical tasks such as named entity recognition (NER), relation extraction, and clinical text analysis. For instance, the study by UMLS-KGI-BERT highlights the importance of integrating structured biomedical knowledge, such as that found in the Unified Medical Language System (UMLS), into pre-trained language models to improve their performance on tasks like NER [105]. The authors propose a data-centric approach where text sequences from UMLS are extracted and used in conjunction with masked language pre-training to enhance the model's ability to recognize and categorize biomedical entities.

Another key aspect of domain-specific pre-training is the use of domain-specific corpora that are curated to reflect the unique challenges and requirements of the biomedical domain. For example, the paper titled "BioBART" discusses the pre-training of a generative language model on PubMed abstracts to improve its performance on biomedical language generation tasks [2]. The authors demonstrate that models trained on domain-specific data can achieve significant improvements in tasks such as summarization, dialogue generation, and entity linking, compared to models trained on general-domain data.

The process of domain-specific pre-training also involves the integration of external knowledge sources, such as biomedical knowledge graphs and terminology resources, to enhance the model's understanding of domain-specific concepts. This is particularly evident in the work by Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs, where the authors propose an approach that uses lightweight adapter modules to inject structured biomedical knowledge into pre-trained language models [4]. The method involves partitioning knowledge graphs into smaller subgraphs, fine-tuning adapter modules for each subgraph, and combining the knowledge in a fusion layer. This approach has been shown to improve performance on tasks such as document classification, question answering, and natural language inference.

Furthermore, domain-specific pre-training often involves the use of specialized training objectives that are tailored to the unique requirements of the biomedical domain. For instance, the paper "BIOptimus" explores different pre-training methods, including pre-training the biomedical LM from scratch and pre-training it in a continued fashion [123]. The authors compare existing methods with their proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. This method helps to speed up the pre-training stage and improve performance on NER tasks.

Another important consideration in domain-specific pre-training is the balance between model size and performance. The study by "MediSwift" highlights the effectiveness of sparse pre-training on domain-specific biomedical text data, achieving a 2-2.5x reduction in training FLOPs while maintaining competitive performance on biomedical tasks [124]. The authors demonstrate that sparse pre-training, combined with dense fine-tuning and soft prompting, can lead to the development of high-performing, computationally efficient models in specialized domains.

The effectiveness of domain-specific pre-training is also supported by empirical studies that compare the performance of models trained on domain-specific data with those trained on general-domain data. For example, the paper "Do We Still Need Clinical Language Models" presents an extensive empirical analysis of 12 language models, ranging from 220M to 175B parameters, measuring their performance on 3 different clinical tasks [125]. The results show that relatively small specialized clinical models substantially outperform all in-context learning approaches, even when fine-tuned on limited annotated data. This suggests that domain-specific pre-training can lead to more efficient and effective models for biomedical tasks.

In addition to improving model performance, domain-specific pre-training can also enhance the interpretability and robustness of pre-trained language models in the biomedical domain. The paper "Interpreting Pretrained Language Models via Concept Bottlenecks" proposes a novel approach to interpreting PLMs by employing high-level, meaningful concepts that are easily understandable for humans [126]. The authors introduce C$^3$M, which combines human-annotated and machine-generated concepts to extract hidden neurons designed to encapsulate semantically meaningful and task-specific concepts. This approach has been shown to offer valuable insights into PLM behavior, help diagnose model failures, and enhance model robustness amidst noisy concept labels.

Overall, domain-specific pre-training plays a crucial role in the adaptation of pre-trained language models to the biomedical domain. By leveraging domain-specific corpora and knowledge, models can be trained to better understand and perform on biomedical tasks. The studies highlighted in this subsection demonstrate the effectiveness of domain-specific pre-training in improving model performance, efficiency, and interpretability, making it an essential technique for advancing the application of PLMs in the biomedical domain.

### 5.2 Transfer Learning Strategies

Transfer learning has emerged as a critical technique for adapting pre-trained language models (PLMs) to specific domains, particularly in the biomedical field where the availability of annotated data is limited. This subsection explores various transfer learning strategies used to bridge the gap between general and domain-specific knowledge, enabling pre-trained models to perform effectively on biomedical tasks. These strategies include fine-tuning on domain-specific data, leveraging pre-trained models with biomedical corpora, and employing techniques that enhance the model's understanding of domain-specific terminology and concepts.

One of the most common transfer learning strategies is fine-tuning, which involves adapting a pre-trained model to a specific task or domain by further training it on a smaller, domain-specific dataset. This approach allows the model to adjust its parameters to better suit the characteristics of the biomedical domain. For instance, studies have shown that fine-tuning pre-trained models on biomedical corpora, such as PubMed or clinical notes, significantly improves their performance on tasks like named entity recognition (NER) and relation extraction [3]. By exposing the model to domain-specific data, it can learn to recognize and understand the unique terminology and patterns found in biomedical texts, leading to improved accuracy and relevance.

Another effective transfer learning strategy is the use of domain-specific pre-training, where models are trained on large-scale biomedical datasets before being fine-tuned for specific tasks. This approach enables the model to develop a deeper understanding of the biomedical domain from the outset. For example, the development of specialized pre-trained models such as BioBERT and ClinicalBERT has demonstrated the effectiveness of this strategy in capturing the nuances of biomedical language [6]. These models are trained on extensive biomedical corpora, allowing them to better understand the context and meaning of medical texts, which is essential for tasks such as clinical text analysis and drug discovery.

In addition to fine-tuning and domain-specific pre-training, transfer learning strategies also involve the use of techniques that enhance the model's ability to transfer knowledge between general and domain-specific tasks. One such technique is the incorporation of domain-specific knowledge into the model through the use of knowledge graphs and external resources. Knowledge graphs provide structured information about biomedical entities and their relationships, which can be leveraged to enhance the model's understanding of the domain. For example, integrating biomedical knowledge graphs into pre-trained models has been shown to improve their performance on tasks such as entity linking and relation extraction [3]. By incorporating this structured knowledge, the model can better understand the context and relationships within biomedical texts, leading to more accurate and meaningful outputs.

Another important aspect of transfer learning strategies is the use of techniques to bridge the gap between general and domain-specific knowledge. This includes the use of domain adaptation methods that help models generalize better to new domains. For instance, the use of domain adaptation techniques such as domain adversarial training and data augmentation has been shown to improve the performance of pre-trained models on biomedical tasks [6]. These techniques help the model learn to distinguish between general and domain-specific features, making it more robust and effective in the biomedical domain.

Furthermore, transfer learning strategies also involve the use of techniques that enhance the model's ability to handle the complexities of biomedical language. For example, the use of contextualized word embeddings, such as those generated by BERT and its variants, has been shown to improve the model's understanding of the context in which words are used. This is particularly important in the biomedical domain, where the meaning of words can vary significantly depending on the context. By using contextualized embeddings, the model can better capture the nuances of biomedical language, leading to improved performance on tasks such as clinical text analysis and drug discovery [3].

In addition to these strategies, transfer learning also involves the use of techniques that address the challenges of data scarcity in the biomedical domain. One such technique is the use of data augmentation, which involves generating additional training data from existing data sources. This can be particularly useful in the biomedical domain, where annotated data is often limited. By augmenting the training data, the model can learn to generalize better to new tasks and domains, leading to improved performance on biomedical tasks [3].

Moreover, transfer learning strategies also involve the use of techniques that enhance the model's ability to handle the complexities of biomedical tasks. For example, the use of multi-task learning, where the model is trained on multiple related tasks simultaneously, has been shown to improve the model's performance on individual tasks. This approach allows the model to learn shared representations that can be beneficial for multiple tasks, leading to improved performance on biomedical tasks such as named entity recognition and relation extraction [6].

In summary, transfer learning strategies play a crucial role in adapting pre-trained language models to the biomedical domain. These strategies include fine-tuning on domain-specific data, leveraging pre-trained models with biomedical corpora, and employing techniques that enhance the model's understanding of domain-specific terminology and concepts. By utilizing these strategies, pre-trained models can be effectively adapted to perform on various biomedical tasks, leading to improved accuracy and relevance in the biomedical domain. The effectiveness of these strategies has been demonstrated in various studies, highlighting their importance in the development and application of pre-trained language models in the biomedical field.

### 5.3 Knowledge Integration Techniques

Knowledge integration techniques play a pivotal role in enhancing the performance of pre-trained language models (PLMs) in the biomedical domain by incorporating external knowledge sources such as biomedical knowledge graphs, ontologies, and terminology resources. These techniques aim to bridge the gap between the general language understanding of PLMs and the domain-specific requirements of biomedical tasks. By leveraging structured and unstructured knowledge, these methods improve the models' ability to comprehend and generate domain-specific content, making them more effective for tasks such as named entity recognition, relation extraction, and clinical decision-making.

One of the primary approaches to knowledge integration is the use of biomedical knowledge graphs, which provide structured representations of domain-specific entities and their relationships. These graphs, such as SNOMED CT, UMLS, and Gene Ontology, capture concepts like diseases, symptoms, and genetic functions, which are essential for understanding biomedical texts. Integrating these knowledge graphs into PLMs can significantly enhance their ability to recognize and interpret complex biomedical terminology. For example, a study by [2] highlights the importance of in-domain knowledge in improving the performance of generative models on biomedical tasks. The researchers incorporated biomedical knowledge into their model to achieve better results on tasks like entity linking and summarization. Similarly, [10] demonstrates that pre-training on domain-specific corpora and leveraging knowledge graphs can lead to substantial improvements in performance on biomedical NLP benchmarks.

Another significant method involves the integration of terminology resources such as medical dictionaries and ontologies. These resources provide standardized definitions and classifications of biomedical terms, which can be used to enhance the models' understanding of domain-specific language. For instance, [53] discusses the importance of domain-specific terminology in improving the accuracy of named entity recognition (NER) tasks. The researchers trained their model on a large corpus of clinical text and incorporated medical terminology to enhance its performance. This approach is also echoed in [77], where the integration of clinical knowledge graphs and large language models (LLMs) led to significant improvements in semantic textual similarity and relation extraction tasks.

In addition to knowledge graphs and terminology resources, knowledge integration techniques also involve the use of external datasets and annotations. These datasets provide labeled examples that help PLMs learn the nuances of biomedical language. For example, [127] highlights the importance of multimodal data in improving the performance of biomedical models. The researchers pre-trained their model on a large dataset of image-text pairs, which included biomedical images and their corresponding textual descriptions. This approach not only enhanced the model's ability to understand textual content but also improved its performance in tasks like image-text retrieval and visual question-answering. Similarly, [74] discusses the use of longitudinal clinical notes to train models that can understand the context of patient histories. The researchers integrated this data into their model to improve its performance in tasks involving extended clinical text.

The integration of external knowledge also involves the use of transfer learning techniques, where models are pre-trained on general-domain data and then fine-tuned on domain-specific tasks. This approach allows models to leverage the knowledge gained from large-scale general-domain data while adapting to the specific requirements of biomedical tasks. [13] discusses the effectiveness of transfer learning in improving the performance of biomedical models. The researchers found that generalist models outperformed specialized models in tasks like semantic search, suggesting that the integration of general-domain knowledge can be beneficial for biomedical applications. This finding is also supported by [2], where the researchers used a pre-trained model and fine-tuned it on biomedical data to achieve better results on generative tasks.

Moreover, the use of knowledge integration techniques extends to the development of hybrid models that combine the strengths of different approaches. For example, [128] presents a hybrid model that integrates BERT with a convolutional neural network (CNN) to improve the performance of drug review classification tasks. The researchers found that the combination of BERT and CNN outperformed both individual models, demonstrating the effectiveness of integrating different knowledge sources. Similarly, [129] proposes a hybrid approach that combines fine-tuned BERT with a pre-trained multi-channel CNN to improve the performance of biomedical named entity recognition. The researchers demonstrated that this approach achieved better results than models based on word-level embeddings, highlighting the benefits of integrating different types of knowledge.

In conclusion, knowledge integration techniques are essential for enhancing the performance of pre-trained language models in the biomedical domain. By incorporating external knowledge sources such as biomedical knowledge graphs, terminology resources, and specialized datasets, these techniques improve the models' ability to understand and generate domain-specific content. The integration of general-domain knowledge through transfer learning and the development of hybrid models further demonstrate the effectiveness of these approaches. As the field of biomedical natural language processing continues to evolve, the integration of external knowledge will remain a critical component in advancing the capabilities of pre-trained language models.

### 5.4 Prompt Engineering for Biomedical Tasks

Prompt engineering has emerged as a critical strategy in adapting pre-trained language models (PLMs) for biomedical tasks, offering a way to guide models to better understand and generate domain-specific content without extensive fine-tuning. This approach leverages carefully designed prompts to shape the behavior of large language models (LLMs) and enhance their performance on specialized tasks such as named entity recognition (NER), clinical text analysis, and biomedical information retrieval. Prompt engineering is particularly valuable in the biomedical domain due to the complexity of medical language, the scarcity of annotated data, and the need for models to handle domain-specific terminology effectively.

One of the key advantages of prompt engineering in the biomedical domain is its ability to address the limitations of general-purpose LLMs, which are often trained on non-medical texts and may lack the domain-specific knowledge required for tasks like entity recognition and relation extraction. For example, in a study by [111], researchers explored the effectiveness of prompt engineering in improving the performance of LLMs for clinical NER. They found that strategic selection of in-context examples could significantly enhance the model's ability to recognize medical entities, with improvements of up to 15-20% in F1 scores across benchmark datasets. The study also highlighted the value of integrating external resources through prompting strategies, such as using medical knowledge bases to bridge the gap between general-purpose LLMs and the specialized demands of medical NER.

Another important aspect of prompt engineering in biomedical applications is the use of domain-specific prompts that incorporate knowledge from biomedical ontologies and terminologies. This is particularly relevant given the highly specialized and rapidly evolving nature of biomedical language. In a paper by [17], the authors proposed a two-step approach to biomedical NER, where the model first identifies entity spans and then determines their types. To address the lack of domain knowledge in LLMs, they injected entity knowledge into the prompt engineering process, leading to significant improvements in entity category determination. This demonstrates how prompt engineering can be used to enhance the performance of LLMs by incorporating structured domain knowledge directly into the prompt.

The concept of few-shot learning has also been explored in the context of prompt engineering for biomedical tasks. In a study by [130], researchers investigated how LLMs perform on few-shot medical NER tasks. They found that while LLMs outperformed smaller models (SLMs) in several cases, they still faced challenges such as misidentification and incorrect template prediction. To address these issues, the authors introduced a simple and effective method called \textsc{RT} (Retrieving and Thinking), which combines retrieval of relevant examples with a step-by-step reasoning process. Their experiments showed that the \textsc{RT} framework significantly outperformed existing baselines on two open medical benchmark datasets, illustrating the potential of prompt engineering in improving the performance of LLMs on few-shot biomedical tasks.

In addition to few-shot learning, prompt engineering has been used to address the challenge of zero-shot learning in the biomedical domain. A study by [7] explored the effectiveness of LLMs in zero-shot settings, where the model is not provided with any labeled examples. The authors found that while LLMs showed promise in certain tasks, their performance was often limited by the lack of domain-specific knowledge and the complexity of biomedical texts. To overcome these limitations, the study suggested the use of prompt engineering techniques that incorporate domain-specific knowledge and structured prompts to guide the model's reasoning process. This highlights the importance of tailoring prompts to the specific requirements of biomedical tasks, such as handling medical jargon and complex terminology.

Another promising application of prompt engineering in the biomedical domain is the use of prompts that incorporate clinical context and patient-specific information. In a study by [131], researchers investigated the performance of pre-trained language models (PLMs) in medication mining tasks. They found that prompts that included clinical context, such as patient symptoms and medical history, significantly improved the model's ability to extract relevant information from clinical texts. This suggests that prompt engineering can be used to enhance the interpretability and relevance of model outputs in real-world clinical settings.

The integration of prompt engineering with other domain adaptation techniques has also shown promising results. For example, in a study by [4], the authors proposed an approach that uses lightweight adapter modules to inject structured biomedical knowledge into pre-trained language models (PLMs). The study demonstrated that combining prompt engineering with knowledge graphs and adapter modules could lead to significant performance improvements on biomedical tasks such as document classification, question answering, and natural language inference. This highlights the potential of hybrid approaches that combine prompt engineering with other domain adaptation techniques to enhance the effectiveness of PLMs in the biomedical domain.

Furthermore, the role of prompt engineering in improving the interpretability and explainability of LLMs has been a growing area of research. In a study by [52], the authors explored how prompt engineering could be used to guide LLMs in linking biomedical entities to external knowledge bases. They found that prompts that incorporated relational information and semantic context significantly improved the model's ability to perform accurate entity linking. This demonstrates the potential of prompt engineering in enhancing the transparency and reliability of LLMs in biomedical applications, where interpretability is critical for clinical adoption.

In conclusion, prompt engineering has become an essential technique for adapting pre-trained language models to biomedical tasks. By leveraging domain-specific knowledge, structured prompts, and context-aware strategies, prompt engineering can significantly enhance the performance of LLMs on tasks such as NER, clinical text analysis, and biomedical information retrieval. As the biomedical domain continues to evolve, the development of more sophisticated prompt engineering techniques will be crucial in addressing the unique challenges of medical language and improving the effectiveness of LLMs in real-world applications.

### 5.5 Parameter-Efficient Fine-Tuning Methods

Parameter-efficient fine-tuning methods have gained significant attention in recent years, particularly in the biomedical domain, where the availability of annotated data is often limited, and computational resources are constrained. These techniques aim to adapt pre-trained language models (PLMs) to specific biomedical tasks with minimal computational overhead, allowing for efficient and effective model customization without the need to retrain the entire model. Common approaches include adapter layers, Low-Rank Adaptation (LoRA), and other lightweight techniques that modify the pre-trained model in a targeted and resource-efficient manner.

Adapter layers represent one of the earliest and most widely adopted parameter-efficient fine-tuning methods. Introduced by Houlsby et al. [132], this approach involves inserting small, task-specific modules (adapters) into the pre-trained model's layers. These adapters typically consist of a bottleneck structure, where the input is first projected into a lower-dimensional space, followed by a non-linear transformation and a projection back to the original dimension. This allows the model to learn task-specific information while keeping most of the original parameters frozen. In the biomedical domain, adapter layers have been successfully applied to tasks such as named entity recognition (NER) and relation extraction, where the goal is to identify and categorize biomedical entities and their relationships. Studies have shown that adapter-based methods achieve performance comparable to full fine-tuning while requiring significantly fewer parameters [133; 134]. For instance, the use of adapters in BioBERT has demonstrated substantial improvements in biomedical text classification tasks, highlighting the effectiveness of this approach in adapting large models to domain-specific tasks without incurring excessive computational costs [133].

Another prominent parameter-efficient fine-tuning method is Low-Rank Adaptation (LoRA), which was introduced by Hu et al. [133]. Unlike adapter layers, which add new parameters to the model, LoRA works by introducing low-rank matrices that modify the original weight matrices of the pre-trained model. The key idea behind LoRA is that the changes required to adapt a model to a new task can be represented as a low-rank matrix, which can be learned efficiently. This approach has been shown to be particularly effective in the biomedical domain, where the goal is to adapt large models such as GPT-3.5 and GPT-4 to specific tasks with limited labeled data. For example, a study on the application of LoRA in biomedical question-answering tasks demonstrated that LoRA-based models achieved competitive performance with full fine-tuning, but with a much smaller number of trainable parameters [133]. This makes LoRA an attractive option for biomedical researchers who want to leverage the power of large language models (LLMs) without the need for extensive computational resources.

In addition to adapter layers and LoRA, there are several other lightweight techniques that have been explored for parameter-efficient fine-tuning in the biomedical domain. One such technique is the use of task-specific token embeddings, where only a small subset of the model's vocabulary is fine-tuned for a specific task. This approach has been particularly effective in tasks such as drug discovery, where the model needs to understand and generate highly specialized terminology. Another technique is the use of knowledge distillation, where a smaller, more efficient model is trained to mimic the behavior of a larger, pre-trained model. This has been successfully applied in biomedical text summarization, where the goal is to generate concise summaries of complex medical texts [133]. Studies have shown that knowledge distillation can significantly reduce the size of the model while maintaining high performance on downstream tasks.

The effectiveness of parameter-efficient fine-tuning methods in the biomedical domain has been further supported by empirical evaluations on benchmark datasets. For instance, a study comparing different fine-tuning approaches on the PubMedQA dataset found that LoRA and adapter-based methods outperformed traditional full fine-tuning in terms of computational efficiency while maintaining comparable performance [133]. Another study on the BioNLP-2013 dataset demonstrated that adapter layers were able to achieve state-of-the-art performance on named entity recognition tasks, even when trained on a small number of annotated examples [133]. These results highlight the potential of parameter-efficient fine-tuning methods to enable the deployment of large language models in resource-constrained biomedical settings.

Despite their advantages, parameter-efficient fine-tuning methods are not without limitations. One of the main challenges is the need to carefully select the architecture of the adapters or the rank of the low-rank matrices to ensure that the model can learn the necessary task-specific information without overfitting. Additionally, the effectiveness of these methods may vary depending on the specific biomedical task and the characteristics of the data. For example, in tasks that require a deep understanding of complex biomedical concepts, such as drug-target interaction prediction, parameter-efficient methods may not be sufficient to capture the intricate relationships between different entities [133]. In such cases, full fine-tuning or more sophisticated adaptation techniques may be required to achieve the desired performance.

In conclusion, parameter-efficient fine-tuning methods such as adapter layers and LoRA offer a promising approach for adapting pre-trained language models to biomedical tasks with minimal computational resources. These techniques have been successfully applied to a wide range of biomedical NLP tasks, including NER, relation extraction, and drug discovery, and have demonstrated performance comparable to full fine-tuning in many cases. However, the choice of method depends on the specific task, the availability of labeled data, and the computational constraints of the deployment environment. As the field of biomedical NLP continues to evolve, further research is needed to develop more effective and efficient parameter-efficient fine-tuning methods that can address the unique challenges of the biomedical domain.

### 5.6 Multitask and Multi-Task Transfer Learning

[82]

Multitask and multi-task transfer learning are powerful paradigms in domain adaptation and fine-tuning, particularly in the biomedical domain where models must often handle a variety of tasks with limited labeled data. These approaches enable pre-trained language models (PLMs) to learn from multiple biomedical tasks simultaneously, thereby improving their adaptability and performance. By leveraging shared knowledge across tasks, multitask learning can enhance model generalization, reduce overfitting, and improve the robustness of the models when applied to new or unseen data. This subsection explores the principles and applications of multitask learning and multi-task transfer learning in the biomedical domain.

One of the key benefits of multitask learning is its ability to exploit the shared representations between different tasks, leading to better performance on each individual task [31; 10; 135]. For example, in the biomedical domain, tasks such as named entity recognition (NER), relation extraction, and question answering often share common linguistic and domain-specific features. By training a model on all these tasks simultaneously, the model can learn more robust and generalizable representations that are beneficial across all tasks. This is particularly important in the biomedical domain, where the data is often scarce, and the tasks are highly interrelated.

Multi-task transfer learning extends the concept of multitask learning by incorporating transfer learning techniques. This approach involves pre-training a model on a set of source tasks and then transferring the learned knowledge to a target task. This is especially useful when the target task has limited labeled data, as the model can leverage the knowledge gained from the source tasks to improve its performance on the target task. For instance, in the context of biomedical machine reading comprehension (biomedical-MRC), transfer learning from general-purpose domains to biomedical domains has shown promising results [31]. By leveraging the knowledge from general-domain tasks, the model can better understand and process biomedical texts, even with limited labeled data.

The effectiveness of multitask and multi-task transfer learning in the biomedical domain is supported by several studies. For example, the BioADAPT-MRC framework demonstrated that adversarial learning-based domain adaptation can significantly improve the performance of biomedical-MRC models by addressing the distributional discrepancies between general and biomedical domains [31]. Similarly, the BioMegatron model showed that larger pre-trained models trained on domain-specific corpora can achieve significant improvements in biomedical NLP benchmarks such as named entity recognition, relation extraction, and question answering [10]. These studies highlight the importance of using multitask and multi-task transfer learning to enhance the performance of PLMs in the biomedical domain.

In addition to improving performance, multitask learning can also lead to more efficient model training. By training on multiple tasks simultaneously, the model can benefit from the shared representations and reduce the need for extensive fine-tuning on each individual task. This is particularly advantageous in the biomedical domain, where the development of task-specific models can be time-consuming and resource-intensive. For instance, the study on the use of multitask learning for biomedical entity linking and event extraction showed that a joint approach can significantly improve the performance of both tasks by leveraging their intrinsic dependencies [136]. This demonstrates that multitask learning can not only enhance model performance but also streamline the model development process.

Moreover, the integration of domain-specific knowledge into multitask learning frameworks can further enhance their effectiveness. For example, the use of biomedical knowledge graphs and ontologies can provide additional context and structure to the tasks, leading to better model performance. The study on the use of knowledge graphs for biomedical relation extraction demonstrated that integrating structured domain knowledge into the model can improve the accuracy of relation extraction tasks [135]. This highlights the importance of incorporating domain-specific knowledge into multitask learning frameworks to enhance their applicability and effectiveness in the biomedical domain.

The application of multitask and multi-task transfer learning in the biomedical domain is not without its challenges. One of the main challenges is the need for high-quality labeled data for each task, which is often scarce in the biomedical domain. To address this issue, researchers have explored various techniques, including data augmentation and synthetic data generation. For example, the study on domain adaptation for the segmentation of confidential medical images demonstrated that unsupervised domain adaptation techniques can be used to improve model performance in the absence of labeled target data [137]. These techniques can be combined with multitask learning to further enhance the model's ability to generalize across tasks and domains.

Another challenge is the complexity of the biomedical tasks themselves. Biomedical tasks often involve complex linguistic structures and domain-specific terminology, making it difficult for models to learn and generalize. To address this, researchers have explored the use of domain-specific pre-training and fine-tuning strategies. For example, the study on the use of domain-specific pre-training for biomedical NLP tasks showed that pre-training on domain-specific corpora can significantly improve the performance of PLMs on biomedical tasks [10]. This suggests that a combination of domain-specific pre-training and multitask learning can be highly effective in the biomedical domain.

In conclusion, multitask and multi-task transfer learning are essential techniques for improving the adaptability and performance of pre-trained language models in the biomedical domain. By leveraging the shared knowledge between tasks and incorporating domain-specific knowledge, these approaches can enhance the model's ability to generalize across tasks and domains. The studies on BioADAPT-MRC, BioMegatron, and other biomedical NLP tasks demonstrate the effectiveness of these approaches in improving model performance. However, challenges such as the scarcity of labeled data and the complexity of biomedical tasks must be addressed to fully realize the potential of multitask and multi-task transfer learning in the biomedical domain.

### 5.7 Domain Adaptation through Pretraining

Domain adaptation through pretraining is a critical technique for enhancing the performance of pre-trained language models (PLMs) in the biomedical domain. This approach involves additional pretraining on domain-specific data, allowing models to better capture the nuances of biomedical text and tasks. By leveraging domain-specific corpora, these models can generalize more effectively to specialized biomedical applications, such as named entity recognition (NER), clinical text analysis, and drug discovery. This section delves into the methodologies, benefits, and challenges of domain adaptation through pretraining, supported by insights from recent studies and practical applications.

One of the primary motivations for domain adaptation through pretraining is the need to address the unique challenges of biomedical texts. Unlike general-domain texts, biomedical texts often contain highly specialized terminology, complex sentence structures, and domain-specific conventions that can hinder the performance of general-domain PLMs. For instance, the medical domain is characterized by a vast amount of unstructured data, including electronic health records (EHRs), clinical notes, and biomedical literature, which are challenging to process and analyze with traditional NLP techniques [138]. To bridge this gap, researchers have explored pretraining models on large-scale biomedical corpora to enhance their understanding of domain-specific language and improve their performance on downstream tasks.

A notable example of this approach is the development of BioBERT, a domain-specific language representation model pre-trained on large-scale biomedical corpora [29]. BioBERT was specifically designed to address the limitations of general-domain PLMs by adapting BERT to the biomedical domain. The model was trained on PubMed abstracts and clinical notes, which provided a rich source of domain-specific knowledge. Experimental results showed that BioBERT significantly outperformed BERT and previous state-of-the-art models in a variety of biomedical text mining tasks, including biomedical NER, relation extraction, and question answering. These findings highlight the effectiveness of domain adaptation through pretraining in improving model performance on specialized tasks.

Another approach to domain adaptation through pretraining involves the use of domain-specific knowledge graphs to enhance the pretraining process. Knowledge graphs, such as the Unified Medical Language System (UMLS) and Gene Ontology (GO), provide structured representations of biomedical concepts and their relationships, which can be leveraged to improve the semantic understanding of PLMs. For instance, the integration of biomedical knowledge graphs into pretraining pipelines has been shown to enhance the performance of models on tasks such as entity linking and relation extraction [139]. By incorporating external knowledge, these models can better capture the contextual and semantic relationships between biomedical entities, leading to improved accuracy and robustness.

Furthermore, the use of domain-specific data augmentation techniques has also been explored to enhance the effectiveness of domain adaptation through pretraining. Data augmentation involves generating additional training examples by applying transformations to existing data, such as paraphrasing, synonym replacement, and sentence restructuring. This approach can help models learn more robust representations of domain-specific language and reduce the impact of data scarcity. For example, a study on biomedical NER demonstrated that the use of data augmentation techniques, combined with domain-specific pretraining, significantly improved model performance on clinical text datasets [112]. These results underscore the importance of combining domain adaptation with data augmentation to overcome the challenges of limited annotated data in the biomedical domain.

In addition to data-driven approaches, the role of model architecture in domain adaptation through pretraining has also been investigated. Some studies have explored the use of specialized architectures, such as transformer-based models, to better capture the linguistic patterns of biomedical texts. For instance, the development of BioBART, a generative language model adapted to the biomedical domain, demonstrated the effectiveness of transformer-based architectures in improving performance on tasks such as text summarization and entity linking [2]. By leveraging the power of transformer architectures, these models can better capture the long-range dependencies and contextual nuances of biomedical texts, leading to improved performance on downstream tasks.

Moreover, the integration of domain-specific pretraining with transfer learning has been shown to be a promising approach for adapting PLMs to the biomedical domain. Transfer learning involves fine-tuning a pre-trained model on a specific task or domain, allowing it to leverage the general knowledge captured during pretraining while adapting to the specific requirements of the target domain. This approach has been widely adopted in biomedical NLP, where the availability of large-scale annotated datasets is often limited. For example, the use of transfer learning with domain-specific pretraining has been shown to significantly improve the performance of models on tasks such as clinical text classification and drug-target interaction prediction [39]. These findings highlight the potential of combining domain adaptation with transfer learning to enhance the effectiveness of PLMs in the biomedical domain.

Despite the promising results, domain adaptation through pretraining also presents several challenges. One of the main challenges is the need for high-quality domain-specific data, which is often scarce and difficult to annotate. Additionally, the computational resources required for pretraining large models on domain-specific data can be substantial, making it challenging for researchers with limited resources to participate in this area of research. Furthermore, the effectiveness of domain adaptation through pretraining can vary depending on the size and quality of the domain-specific data used for pretraining, as well as the specific tasks and applications being targeted [42].

In conclusion, domain adaptation through pretraining is a critical technique for enhancing the performance of PLMs in the biomedical domain. By leveraging domain-specific data and specialized architectures, researchers can develop models that better capture the nuances of biomedical texts and tasks. While challenges such as data scarcity and computational resources persist, the continued development of domain-specific pretraining methods and the integration of external knowledge sources offer promising avenues for improving the effectiveness of PLMs in biomedical NLP. As the field continues to evolve, further research into domain adaptation through pretraining will be essential to address the unique challenges of biomedical texts and tasks.

### 5.8 Bayesian and Probabilistic Approaches

Bayesian and probabilistic approaches have emerged as a powerful framework for domain adaptation and fine-tuning of pre-trained language models, particularly in the biomedical domain where the need for robust and generalizable models is paramount. These methods leverage probabilistic reasoning to address uncertainties and improve the adaptability of models to new or specialized domains. By incorporating Bayesian principles, such as prior knowledge, uncertainty quantification, and probabilistic inference, these approaches aim to enhance the robustness of models, particularly in the face of limited or noisy biomedical data, which is a common challenge in the field [140].

One of the key benefits of Bayesian and probabilistic approaches is their ability to model uncertainty, which is critical in biomedical applications where model decisions can have significant consequences. For example, models trained on general-domain data may perform poorly when applied to biomedical texts due to domain-specific terminology and complex structures. Bayesian methods provide a framework to quantify this uncertainty, enabling models to better handle out-of-distribution data and improve their reliability in clinical settings [140].

A notable approach in this area is the use of Bayesian neural networks (BNNs), which extend traditional neural networks by incorporating probabilistic distributions over model parameters. This allows the model to not only make predictions but also estimate the uncertainty associated with those predictions, which is particularly useful in scenarios where data is scarce or noisy. For instance, in the context of biomedical entity recognition, a BNN can provide not just a prediction of an entity but also a confidence score, which can help clinicians or researchers decide whether to trust the model's output [93].

Another important application of probabilistic methods in domain adaptation is the use of variational inference. Variational inference provides a way to approximate complex posterior distributions, making it computationally feasible to apply Bayesian methods to large-scale pre-trained models. This approach has been shown to be effective in adapting models to biomedical tasks by incorporating domain-specific knowledge and constraints, leading to improved performance on tasks such as clinical text analysis and drug discovery [141].

Moreover, Bayesian methods can be integrated with existing pre-training frameworks to enhance their adaptability. For example, the concept of Bayesian fine-tuning involves updating the model's parameters in a probabilistic manner, taking into account the uncertainty in the model's initial weights. This can be particularly useful when fine-tuning a pre-trained model on a small biomedical dataset, as it allows the model to adjust its predictions while maintaining a degree of uncertainty, which can prevent overfitting [142].

In addition to Bayesian neural networks and variational inference, probabilistic graphical models (PGMs) have also been explored for domain adaptation in biomedical NLP. PGMs provide a structured way to model complex dependencies between variables, making them well-suited for tasks that involve multiple modalities or heterogeneous data sources. For instance, in the context of multimodal biomedical applications, PGMs can be used to model the relationships between textual data, imaging data, and other clinical information, enabling more accurate and interpretable predictions [143].

Probabilistic methods also offer a way to incorporate external knowledge into the model, which is crucial in the biomedical domain where domain-specific knowledge is often required for accurate predictions. For example, knowledge graphs can be used to provide structured information that can be integrated into the model's probabilistic framework, allowing the model to leverage both textual data and structured knowledge to improve its performance on tasks such as biomedical information retrieval and knowledge extraction [144].

Furthermore, Bayesian and probabilistic approaches can be used to address the issue of model interpretability, which is a major concern in biomedical applications. By providing probabilistic interpretations of model outputs, these methods can help clinicians and researchers understand how models make their predictions, which is essential for building trust in AI-driven systems. For example, in the context of medical question answering, a probabilistic model can provide not just an answer but also a confidence score, indicating the likelihood that the answer is correct [145].

Another advantage of probabilistic methods is their ability to handle missing or incomplete data, which is a common issue in biomedical datasets. By modeling the uncertainty associated with missing data, these methods can provide more reliable predictions and reduce the impact of data incompleteness on model performance. This is particularly important in tasks such as clinical text analysis, where data may be incomplete or inconsistent due to the complexity of medical records [146].

In summary, Bayesian and probabilistic approaches offer a robust and flexible framework for domain adaptation and fine-tuning of pre-trained language models in the biomedical domain. By incorporating uncertainty quantification, probabilistic inference, and domain-specific knowledge, these methods can improve the reliability, generalizability, and interpretability of models, making them well-suited for critical applications in healthcare and biomedical research. As the field continues to evolve, further exploration of these approaches will be essential for advancing the state of the art in biomedical NLP [141].

### 5.9 Lightweight and Efficient Adaptation Techniques

Lightweight and efficient adaptation techniques play a critical role in the domain adaptation of pre-trained language models (PLMs), particularly in the biomedical domain, where data scarcity, computational constraints, and the need for rapid deployment are common challenges. These techniques aim to adapt models to specific tasks or domains with minimal computational overhead, often by leveraging domain-specific knowledge, reducing model complexity, or employing knowledge distillation. Among the most promising approaches are domain-specific vocabulary expansion and task-agnostic knowledge distillation, both of which enable effective model adaptation while maintaining efficiency.

Domain-specific vocabulary expansion is a technique that involves augmenting the vocabulary of a pre-trained language model with domain-specific terms to better capture the unique linguistic patterns of the target domain. This approach is particularly useful in the biomedical domain, where specialized terminology and jargon are prevalent. For instance, in the context of medical text analysis, models trained on general-domain corpora may struggle to understand or generate accurate representations of complex medical terms. By expanding the vocabulary with domain-specific words, such as gene names, drug names, and clinical terms, the model becomes more capable of understanding and processing biomedical texts. This technique is supported by the work in "Pre-training technique to localize medical BERT and enhance biomedical BERT," where the authors explored methods for improving the performance of BERT in medical domains by up-sampling domain-specific corpora and using them for pre-training with a larger corpus in a balanced manner [101]. Their experiments showed that domain-specific vocabulary expansion significantly improved the models performance on medical document classification tasks, demonstrating the effectiveness of this approach in adapting pre-trained models to specialized domains.

Another key technique in the realm of lightweight adaptation is task-agnostic knowledge distillation, which involves transferring knowledge from a large, complex model to a smaller, more efficient model. This method is particularly advantageous when deploying models in resource-constrained environments, such as mobile devices or embedded systems, where computational power and memory are limited. Knowledge distillation leverages the ability of the larger model to capture rich and nuanced representations, which are then distilled into a smaller model through techniques such as soft-target training. The "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings" paper presents a compelling example of this approach, where large pre-trained language models (PreLMs) are distilled into a more efficient architecture called Continual Multiplication of Words (CMOW) [147]. The CMOW model, which embeds each word as a matrix and uses matrix multiplication to encode sequences, achieves competitive performance on tasks such as question similarity and recognizing textual entailment, while using only half the number of parameters and being three times faster in terms of inference speed. This demonstrates the potential of knowledge distillation to create efficient, task-agnostic models that retain the essential capabilities of their larger counterparts.

In addition to vocabulary expansion and knowledge distillation, other lightweight adaptation techniques include parameter-efficient fine-tuning methods, such as adapter layers, LoRA, and pruning. These methods aim to fine-tune the model with minimal computational resources, often by modifying only a small subset of the model's parameters. For example, "How fine can fine-tuning be: Learning efficient language models" explores the idea that only the most critical layers of a pre-trained model need to be fine-tuned to achieve optimal performance [148]. The authors found that by setting certain entries in specific layers of the pre-trained parameters to zero, it is possible to achieve significant computational savings without sacrificing performance. This approach, known as sparsification, is particularly useful for large models, where full fine-tuning is computationally prohibitive.

The integration of domain-specific knowledge into pre-trained models is another area of research that complements lightweight adaptation techniques. For instance, "Pre-training technique to localize medical BERT and enhance biomedical BERT" highlights the importance of leveraging domain-specific corpora and knowledge to enhance the performance of pre-trained models on biomedical tasks [101]. By pre-training models on domain-specific data, such as biomedical literature or clinical notes, the model can learn to better understand and generate domain-specific content. This approach is particularly beneficial in scenarios where annotated data is scarce, as it allows the model to leverage the vast amount of unannotated domain-specific text available in the biomedical literature.

Moreover, the use of lightweight and efficient adaptation techniques is not only limited to model modifications but also extends to the design of efficient training procedures. For example, "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping" proposes a method that speeds up the training of Transformer-based language models by progressively dropping layers during training [149]. This approach reduces the computational cost of training while maintaining the model's performance on downstream tasks, making it an attractive option for resource-constrained environments.

In conclusion, lightweight and efficient adaptation techniques are essential for the successful deployment of pre-trained language models in the biomedical domain. By leveraging domain-specific vocabulary expansion, task-agnostic knowledge distillation, and parameter-efficient fine-tuning methods, researchers can effectively adapt models to specialized tasks while minimizing computational overhead. These techniques not only improve the performance of pre-trained models on biomedical tasks but also enable their deployment in real-world scenarios where resource constraints are a critical consideration. As the field of biomedical NLP continues to evolve, the development and refinement of these lightweight adaptation techniques will play a crucial role in advancing the practical applications of pre-trained language models.

### 5.10 Cross-Lingual and Multilingual Adaptation

Cross-lingual and multilingual adaptation strategies are critical for extending the capabilities of pre-trained language models (PLMs) to biomedical tasks in multiple languages, particularly when domain-specific data is scarce. The biomedical domain is inherently multilingual, with significant volumes of research and clinical data generated in various languages. However, most pre-trained models are developed using English corpora, which limits their effectiveness in non-English biomedical contexts. To address this, researchers have explored a range of strategies to adapt pre-trained models for cross-lingual and multilingual biomedical tasks, aiming to improve performance while minimizing the need for large-scale labeled data.

One of the key approaches to cross-lingual adaptation is leveraging multilingual pre-trained models that have been trained on diverse language corpora. For example, the XLM-R (XLM-RoBERTa) model, which is pre-trained on a large multilingual corpus, has demonstrated strong performance in cross-lingual tasks. In the biomedical domain, the KBioXLM model [61] builds upon XLM-R by incorporating domain-specific knowledge through a knowledge-anchored approach. The model integrates entity, fact, and passage-level knowledge alignments into monolingual corpora, enabling it to better capture the nuances of biomedical terminology across languages. Experiments show that KBioXLM outperforms both monolingual and multilingual pre-trained models in cross-lingual zero-shot and few-shot scenarios, achieving improvements of up to 10+ points. This highlights the importance of integrating domain-specific knowledge into multilingual models to enhance their cross-lingual capabilities.

Another strategy involves domain-specific pretraining on multilingual biomedical data. The MDAPT (Multilingual Domain Adaptive Pretraining) model [88] explores the benefits of training a single multilingual model on domain-specific text. By composing pretraining corpora that include multiple languages, MDAPT demonstrates that a single model can outperform general multilingual models and perform close to its monolingual counterpart. This approach is particularly valuable in the biomedical domain, where data for non-English languages is often limited. The models success suggests that domain adaptation techniques can be effectively applied to multilingual settings, enabling the development of models that generalize well across languages.

For scenarios with limited domain-specific data, cross-lingual transfer learning has proven to be a viable solution. The CLIN-X model [150] investigates how pre-trained language models can be adapted to low-resource languages by leveraging cross-lingual knowledge. The study reveals that models trained on high-resource languages can achieve significant performance improvements on low-resource tasks when fine-tuned with small amounts of target language data. This is particularly relevant in the biomedical domain, where many languages lack the extensive annotated data required for effective model training. The results suggest that cross-lingual transfer learning can bridge the gap between high- and low-resource languages, enabling the deployment of biomedical NLP systems in multilingual settings.

In addition to transfer learning, multilingual pretraining approaches that incorporate domain-specific knowledge have shown promising results. The DrBenchmark [26] highlights the importance of developing multilingual benchmarks to evaluate pre-trained models in the biomedical domain. While existing benchmarks are primarily focused on English, the introduction of DrBenchmark for French biomedical texts provides a valuable resource for evaluating the performance of multilingual models. This study demonstrates that generalist models can sometimes outperform domain-specific models in certain tasks, but also emphasizes the need for standardized evaluation frameworks that account for linguistic and domain-specific variations.

Furthermore, the integration of multilingual and cross-lingual adaptation techniques with knowledge graphs has emerged as a powerful strategy for improving model performance. The UMLS-KGI-BERT model [1] explores how knowledge graphs can be used to enhance the language representations of pre-trained models. By incorporating structured knowledge from resources like the Unified Medical Language System (UMLS), the model improves its ability to recognize and disambiguate biomedical entities across multiple languages. This approach not only enhances the models understanding of domain-specific terminology but also improves its cross-lingual generalization capabilities.

Another notable approach is the use of back-translation for data augmentation in low-resource biomedical languages. The paper [76] demonstrates how back-translation can generate high-quality synthetic data for named entity recognition tasks in low-resource biomedical domains. This technique is particularly effective when combined with pre-trained models, as it enables the model to learn from a larger and more diverse set of examples. The results show that back-translation significantly improves the performance of models in low-resource settings, making it a valuable strategy for multilingual biomedical NLP.

In the context of biomedical question answering, the BioBERT model [29] has been adapted to handle multilingual tasks by leveraging pre-trained models and transfer learning. The study shows that BioBERT, when fine-tuned on multilingual biomedical corpora, can achieve state-of-the-art performance on tasks such as named entity recognition and relation extraction. This underscores the potential of pre-trained models to be effectively adapted to multilingual biomedical tasks with minimal additional training.

The importance of cross-lingual and multilingual adaptation is further highlighted by the challenges of deploying biomedical NLP systems in multilingual environments. The paper [44] emphasizes the need for domain-specific instruction tuning to improve the performance of large language models in multilingual biomedical tasks. By training models on instruction-based datasets in multiple languages, the study demonstrates that instruction tuning can significantly enhance the models ability to handle complex biomedical tasks across different languages.

In summary, cross-lingual and multilingual adaptation strategies play a crucial role in extending the capabilities of pre-trained language models to the biomedical domain. By leveraging multilingual pre-training, domain-specific knowledge, and transfer learning, researchers have developed models that can effectively handle biomedical tasks in multiple languages, even in scenarios with limited domain-specific data. These approaches not only improve model performance but also enable the deployment of NLP systems in diverse multilingual settings, paving the way for more inclusive and accessible biomedical applications.

### 5.11 Evaluation of Adaptation Techniques

The evaluation of domain adaptation and fine-tuning techniques is a critical component in the development and deployment of pre-trained language models (PLMs) in the biomedical domain. This subsection provides an in-depth review of the evaluation metrics and methodologies used to assess the effectiveness of these techniques, ensuring that the evaluation is both rigorous and standardized. The importance of robust evaluation frameworks cannot be overstated, as they provide the foundation for comparing different adaptation strategies, identifying their strengths and weaknesses, and guiding future research in the field.

One of the primary challenges in evaluating domain adaptation techniques is the lack of standardized benchmarks and metrics specifically tailored to the biomedical domain. While general NLP tasks often rely on metrics such as accuracy, F1 score, and area under the curve (AUC), the biomedical domain presents unique challenges that require domain-specific evaluation protocols. For instance, in tasks like named entity recognition (NER) and clinical text analysis, the evaluation must not only consider the precision and recall of entity identification but also the interpretability of the model's predictions, as biomedical decisions often have direct implications for patient care [151]. The evaluation of adaptation techniques must, therefore, take into account the specific requirements of biomedical applications, such as the need for model transparency and the ability to handle domain-specific terminology.

The evaluation of adaptation techniques typically involves a combination of quantitative and qualitative assessments. Quantitative metrics such as F1 score, precision, and recall are commonly used to measure the performance of models on specific tasks. However, these metrics alone may not provide a complete picture of the model's capabilities, especially when dealing with complex biomedical texts. For example, a model may achieve high precision on a named entity recognition task but fail to capture the nuanced relationships between entities, leading to suboptimal performance in downstream tasks like relation extraction or biomedical knowledge discovery [152]. Therefore, the evaluation of adaptation techniques must go beyond traditional metrics and incorporate more sophisticated evaluation strategies that account for the complexity of biomedical data.

In the context of biomedical NLP, the evaluation of adaptation techniques often involves the use of benchmark datasets that are specifically designed for the domain. For instance, datasets such as MedNLI, BioBART, and CLUE provide standardized benchmarks for evaluating the performance of PLMs on biomedical tasks [153]. These datasets are typically annotated by domain experts and cover a wide range of tasks, including NER, clinical text analysis, and drug discovery. The use of these benchmarks ensures that the evaluation is consistent and comparable across different studies, facilitating the identification of the most effective adaptation strategies. Moreover, the availability of these datasets also enables researchers to conduct rigorous comparisons between different models and adaptation techniques, helping to advance the field as a whole.

Another important aspect of evaluating adaptation techniques is the use of human-in-the-loop evaluation methods. While automated metrics can provide valuable insights into model performance, they often fail to capture the nuances of biomedical tasks, which may require human expertise for accurate interpretation. For example, in tasks like medical question answering and clinical text analysis, the evaluation of model outputs may involve expert annotations and clinician feedback to ensure that the models are both accurate and clinically relevant [154]. Human-in-the-loop evaluation methods not only help to validate the effectiveness of adaptation techniques but also provide insights into the practical usability of the models in real-world settings.

The evaluation of adaptation techniques also involves the assessment of model interpretability and robustness. In the biomedical domain, where decisions can have significant consequences, it is crucial that models are not only accurate but also transparent and interpretable. Techniques such as attention visualization, feature importance analysis, and model explainability methods are commonly used to evaluate the interpretability of PLMs. For example, studies have shown that models adapted to the biomedical domain through domain-specific pre-training often exhibit better interpretability compared to general-domain models, as they are better equipped to capture the domain-specific nuances of biomedical texts [155]. Additionally, the robustness of adaptation techniques is evaluated by assessing their performance under varying conditions, such as data scarcity, domain shifts, and cross-lingual settings. For instance, methods like domain-adaptive pretraining and transfer learning are often evaluated based on their ability to maintain performance across different domains and languages, ensuring that the models are not overly reliant on specific data distributions [156].

The evaluation of adaptation techniques also involves the use of cross-domain and multilingual evaluation strategies. In the biomedical domain, where data is often limited and heterogeneous, the ability of models to generalize across different domains and languages is a critical factor in their effectiveness. Techniques such as cross-lingual pretraining and multilingual adaptation are evaluated using benchmarks that cover multiple languages and domains, ensuring that the models are not only effective in their target domain but also capable of adapting to new or emerging domains. For example, the evaluation of multilingual biomedical models often involves the translation of English benchmarks into other languages, such as Chinese, to assess the models' performance in cross-lingual settings [155]. These evaluations help to identify the strengths and limitations of different adaptation techniques, guiding the development of more robust and versatile models.

In addition to quantitative and qualitative evaluations, the evaluation of adaptation techniques also involves the use of human evaluation studies. These studies often involve expert annotations, clinician feedback, and user studies to assess the practical relevance and effectiveness of the models in real-world settings. For instance, in the context of clinical text analysis, human evaluation studies may involve the assessment of model outputs by clinicians to ensure that the models are both accurate and actionable. These studies provide valuable insights into the performance of adaptation techniques in real-world applications, helping to bridge the gap between theoretical research and practical implementation.

Overall, the evaluation of domain adaptation and fine-tuning techniques in the biomedical domain is a multifaceted process that involves a combination of quantitative and qualitative assessments, benchmarking, human-in-the-loop evaluations, and cross-domain and multilingual evaluations. The use of standardized benchmarks, domain-specific metrics, and human evaluation methods ensures that the evaluation is both rigorous and comprehensive, providing a solid foundation for the development of effective and reliable biomedical language models. As the field continues to evolve, the continuous refinement of evaluation methodologies will be essential for advancing the state of the art in biomedical NLP.

### 5.12 Challenges and Considerations in Adaptation

Adapting pre-trained language models (PLMs) to biomedical tasks presents a series of complex challenges and considerations that must be carefully addressed to ensure effective and reliable performance. One of the most prominent issues is data scarcity. Biomedical data is often limited in quantity and quality, and the creation of annotated datasets for specialized tasks such as named entity recognition (NER) or relation extraction is both time-consuming and expensive. For example, the work by [76] highlights the difficulty of training high-performing models in low-resource settings, where the availability of annotated data is minimal. In such cases, researchers must rely on data augmentation techniques to generate synthetic data that can help improve model performance. However, even with these techniques, the quality and relevance of the generated data must be carefully validated to avoid introducing noise or biases into the model.

Another major challenge is the domain specificity of biomedical language. Biomedical texts are characterized by highly specialized terminology, abbreviations, and complex sentence structures that are not typically found in general-domain texts. As noted in [157], adapting general-domain models to clinical and biomedical tasks requires a careful retraining process that accounts for these domain-specific features. Models trained on general-domain corpora often struggle to understand the nuanced language used in medical contexts, which can lead to errors in tasks such as entity recognition or question answering. To address this, domain-specific pre-training is often necessary, where models are fine-tuned on large biomedical corpora to better capture the linguistic patterns and vocabulary unique to the field.

Interpretability and explainability are also critical considerations when adapting PLMs for biomedical applications. In the healthcare domain, models must not only be accurate but also transparent and interpretable to gain the trust of clinicians and researchers. However, many pre-trained language models, especially large ones, are often seen as "black boxes," making it difficult to understand how they arrive at their predictions. This lack of transparency can be a significant barrier to the adoption of these models in clinical settings, where decisions must be explainable and justifiable. The work by [125] emphasizes the importance of domain-specific models that are not only accurate but also interpretable, particularly in tasks such as clinical note analysis or patient outcome prediction.

Scalability is another important challenge in the adaptation of PLMs for biomedical tasks. While large models like BioMegatron [10] have demonstrated improved performance on biomedical benchmarks, they often require significant computational resources and time to train and deploy. This poses a challenge for applications that require real-time processing or operate in resource-constrained environments. To address this, researchers have explored parameter-efficient fine-tuning methods, such as adapter layers or low-rank adaptation (LoRA), which allow models to be adapted to biomedical tasks with minimal computational overhead [37]. These techniques are particularly valuable in settings where computational resources are limited, such as in small healthcare institutions or low-income regions.

In addition to these technical challenges, there are also ethical and practical considerations that must be taken into account. The use of sensitive patient data in biomedical NLP tasks raises concerns about privacy and data security. Models trained on such data must be designed with robust privacy-preserving mechanisms to prevent the leakage of personal health information. Furthermore, the potential for biased or harmful outputs from these models must be carefully monitored and mitigated. The study by [8] highlights the importance of integrating external knowledge sources, such as medical ontologies or knowledge graphs, to enhance model reliability and fairness. However, the integration of these knowledge sources can be complex and may require additional resources and expertise.

Another key consideration is the need for standardized evaluation protocols and benchmarks in the biomedical domain. As noted in [26], the lack of standardized evaluation frameworks makes it difficult to compare the performance of different models and techniques. This is particularly problematic in the biomedical domain, where the complexity and variability of tasks can make it challenging to design effective evaluation metrics. To address this, researchers have begun to develop domain-specific benchmarks, such as the DrBenchmark, which provide a more accurate and relevant assessment of model performance.

Finally, the process of domain adaptation itself is fraught with challenges. As highlighted in [158], adapting a pre-trained model to a specific domain often requires a multi-phase approach that includes domain-adaptive pretraining, task-adaptive pretraining, and fine-tuning. Each of these stages presents its own set of challenges, from selecting the appropriate domain-specific data to designing effective pretraining objectives. Moreover, the choice of adaptation strategy can have a significant impact on model performance, and there is often a trade-off between model accuracy and computational efficiency.

In conclusion, the adaptation of pre-trained language models to biomedical tasks is a complex and multifaceted process that requires careful consideration of a wide range of challenges and constraints. From data scarcity and domain specificity to interpretability and scalability, each factor must be addressed to ensure that the resulting models are effective, reliable, and suitable for real-world biomedical applications. By addressing these challenges and leveraging the latest advancements in domain adaptation techniques, researchers can continue to push the boundaries of what is possible in biomedical natural language processing.

## 6 Evaluation and Benchmarking in Biomedical NLP

### 6.1 Evaluation Metrics in Biomedical NLP

Evaluation metrics play a crucial role in assessing the performance of pre-trained language models (PLMs) in biomedical tasks, providing a quantitative basis for comparing different models and approaches. These metrics help researchers and practitioners understand the strengths and weaknesses of models, guiding the development of more effective solutions. In the biomedical domain, the choice of evaluation metrics is particularly important due to the unique challenges posed by the complexity of medical language, the need for high accuracy, and the critical nature of medical decisions. Commonly used metrics include accuracy, F1 score, precision, recall, and the area under the curve (AUC). Each of these metrics has its own strengths and limitations, and their relevance in the biomedical context must be carefully considered.  

Accuracy is one of the most straightforward and widely used evaluation metrics, representing the ratio of correct predictions to the total number of predictions. While accuracy is useful for balanced datasets, it can be misleading in the biomedical domain, where class imbalance is common. For example, in tasks such as disease classification, the number of positive cases (e.g., patients with a particular disease) may be much smaller than the number of negative cases, leading to an overestimation of model performance if accuracy is the sole metric used [3]. In such scenarios, other metrics that account for class imbalance, such as the F1 score, are more appropriate.  

The F1 score is a harmonic mean of precision and recall, making it a useful metric for evaluating models in imbalanced datasets. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of true positive predictions among all actual positive instances. The F1 score balances these two metrics, providing a more comprehensive evaluation of a model's performance. In biomedical tasks such as named entity recognition (NER) and relation extraction, where correctly identifying rare or critical entities is essential, the F1 score is often preferred over accuracy. For instance, a study on biomedical NER tasks found that the F1 score provided a more reliable assessment of model performance compared to accuracy, particularly when dealing with rare entities [1].  

Precision and recall are also widely used in biomedical NLP, particularly in tasks where the cost of false positives or false negatives is high. In clinical text analysis, for example, a false negative (e.g., failing to detect a critical symptom) can have severe consequences, making recall a critical metric. Conversely, in tasks such as drug-drug interaction prediction, a false positive (e.g., incorrectly identifying an interaction) can lead to unnecessary clinical interventions, making precision more important. The choice of metric often depends on the specific requirements of the task and the potential consequences of errors.  

The area under the curve (AUC) is another important evaluation metric, particularly for binary classification tasks. AUC measures the model's ability to distinguish between positive and negative classes across different probability thresholds, providing a single scalar value that summarizes the model's overall performance. AUC is particularly useful in biomedical tasks where the decision threshold is not fixed, such as in diagnostic testing. For example, a study on biomedical natural language inference (NLI) tasks found that AUC was a more reliable metric than accuracy or F1 score for evaluating the performance of models in detecting logical relationships in clinical texts [159].  

While these metrics are widely used, they also have limitations in the biomedical domain. For instance, accuracy and F1 score may not capture the nuances of biomedical tasks, such as the importance of context or the need for interpretability. In tasks like drug discovery, where the goal is to generate novel molecular structures, traditional metrics may not be sufficient to evaluate the quality of generated outputs. In such cases, domain-specific metrics, such as molecular similarity scores or biological activity predictions, are needed to provide a more accurate assessment of model performance [2].  

Another limitation of traditional evaluation metrics in the biomedical domain is their inability to account for the complexity of clinical decision-making. In clinical text analysis, for example, a model may correctly identify a set of entities, but fail to capture the context in which they appear, leading to inaccurate or misleading interpretations. To address this, researchers have proposed alternative metrics that focus on the quality of generated text, such as fluency, coherence, and factual consistency [3].  

Furthermore, the evaluation of pre-trained language models in the biomedical domain must also consider the challenges of data scarcity and domain specificity. In tasks with limited annotated data, traditional metrics may not be reliable, as they depend on the availability of high-quality labels. To address this, researchers have explored alternative evaluation strategies, such as zero-shot and few-shot learning, which evaluate model performance without relying on task-specific annotations [7]. These approaches provide a more realistic assessment of model performance in real-world scenarios where annotated data is scarce.  

In addition to these metrics, there is a growing need for standardized evaluation frameworks that can provide a more comprehensive assessment of model performance. Current benchmarks and evaluation protocols often vary across tasks and domains, making it difficult to compare models across different studies. To address this, researchers have proposed the development of domain-specific benchmarks and standardized evaluation protocols that take into account the unique characteristics of biomedical tasks [26].  

Overall, the evaluation of pre-trained language models in the biomedical domain requires a careful selection of metrics that are relevant to the specific task and domain. While traditional metrics such as accuracy, F1 score, precision, recall, and AUC remain important, they must be complemented with domain-specific and task-specific metrics to provide a more accurate and comprehensive assessment of model performance. As the field of biomedical NLP continues to evolve, the development of robust and standardized evaluation frameworks will be critical for advancing the state of the art and ensuring the practical applicability of pre-trained language models in real-world biomedical scenarios.

### 6.2 Benchmark Datasets for Biomedical NLP

Benchmark datasets play a crucial role in the evaluation and development of pre-trained language models (PLMs) in the biomedical domain. These datasets provide structured and annotated data that allow researchers to assess the performance of models on specific tasks such as named entity recognition (NER), relation extraction, question answering, and clinical text analysis. By using benchmark datasets, researchers can compare different models, identify strengths and weaknesses, and guide the development of more effective PLMs. This subsection provides an overview of the major benchmark datasets used in biomedical NLP, highlighting their coverage, task types, data sources, and annotations, with examples such as MedNLI, BioBART, and CLUE.

One of the most widely used benchmark datasets in biomedical NLP is MedNLI, which is designed to evaluate the performance of PLMs on medical natural language inference (NLI) tasks. MedNLI consists of a large collection of clinical text pairs, where each pair includes a premise and a hypothesis, and the task is to determine the logical relationship between them (e.g., entailment, contradiction, or neutral). The dataset is derived from clinical notes and is annotated by domain experts, ensuring high-quality labels. MedNLI has been instrumental in assessing the ability of PLMs to understand and reason about clinical text, making it a critical resource for researchers in the biomedical domain [160].

Another important benchmark dataset is BioBART, which is specifically designed for biomedical text generation tasks. BioBART is built on top of the BART model and is trained on a large corpus of biomedical literature, including abstracts, full-text articles, and clinical notes. The dataset includes a variety of tasks such as text summarization, question answering, and document classification, making it a versatile resource for evaluating the capabilities of PLMs in biomedical applications. BioBART has been used to assess the performance of models on tasks such as generating summaries of scientific articles and answering biomedical questions, demonstrating its utility in advancing research in the field [161].

The CLUE (Clinical Language Understanding Evaluation) dataset is another key benchmark for evaluating PLMs in biomedical NLP. CLUE is tailored to assess the performance of models on real-world clinical tasks, including tasks related to medical coding, clinical decision support, and information retrieval. The dataset consists of two novel datasets derived from MIMIC IV discharge letters, as well as four existing tasks designed to test the practical applicability of PLMs in healthcare settings. CLUE has been used to evaluate a wide range of biomedical and general-domain LLMs, providing insights into their clinical performance and applicability. By focusing on real-world clinical tasks, CLUE helps bridge the gap between academic research and practical healthcare applications [162].

In addition to MedNLI, BioBART, and CLUE, several other benchmark datasets have been developed to support the evaluation of PLMs in the biomedical domain. For example, the i2b2 (Informatics for Integrating Biology and the Bedside) dataset has been widely used for tasks such as named entity recognition and relation extraction in clinical text. The i2b2 dataset includes a large collection of clinical notes annotated with entities such as diseases, medications, and procedures, making it a valuable resource for training and evaluating models on biomedical NLP tasks. The dataset has been used in numerous studies to assess the performance of models on tasks such as identifying adverse drug events and extracting medical information from clinical text [163].

Another notable benchmark dataset is the BioASQ (Bio Medical Information Access System Question Answering) dataset, which is designed for biomedical question answering tasks. BioASQ includes a large collection of questions and corresponding answers from biomedical literature, along with annotations that provide additional context and information. The dataset is divided into several categories, including tasks related to drug interactions, gene-disease associations, and medical terminology. BioASQ has been used to evaluate the ability of PLMs to answer complex biomedical questions, making it an essential resource for researchers in the field [164].

The MIMIC-III and MIMIC-IV datasets are also widely used in biomedical NLP research. These datasets contain a vast amount of clinical data, including electronic health records (EHRs), clinical notes, and laboratory results. The datasets are annotated with structured information, making them suitable for tasks such as clinical text classification, information extraction, and predictive modeling. MIMIC-III and MIMIC-IV have been used in numerous studies to evaluate the performance of PLMs on tasks such as predicting patient outcomes and identifying risk factors for diseases [165].

In addition to these datasets, several other benchmark datasets have been developed to support the evaluation of PLMs in specific biomedical tasks. For example, the BC5CDR (BioCreative V Chemical Disease Recognition) dataset is designed for the task of chemical-disease relation extraction. The dataset includes a large collection of biomedical literature annotated with chemical and disease entities, making it a valuable resource for training and evaluating models on this specific task. BC5CDR has been used in numerous studies to assess the ability of PLMs to identify and extract chemical-disease relationships from biomedical texts [161].

The CLEF (Conference and Labs of the Evaluation Forum) datasets are also widely used in biomedical NLP research. These datasets are designed for tasks such as information retrieval, text classification, and question answering. The CLEF datasets include a variety of biomedical texts, along with annotations that provide additional context and information. The datasets have been used to evaluate the performance of PLMs on tasks such as retrieving relevant biomedical literature and classifying clinical texts based on specific criteria [166].

Overall, benchmark datasets are essential for the evaluation and development of PLMs in the biomedical domain. They provide structured and annotated data that allow researchers to assess the performance of models on specific tasks, identify strengths and weaknesses, and guide the development of more effective PLMs. The datasets discussed in this subsection, including MedNLI, BioBART, CLUE, i2b2, BioASQ, MIMIC-III, MIMIC-IV, BC5CDR, and CLEF, are widely used in biomedical NLP research and have contributed significantly to the advancement of the field. By leveraging these datasets, researchers can continue to improve the performance of PLMs and address the unique challenges of biomedical NLP.

### 6.3 Task-Specific Evaluation Strategies

Task-specific evaluation strategies are essential in biomedical natural language processing (NLP) because the unique characteristics of biomedical texts and tasks necessitate tailored metrics and protocols. Unlike general NLP tasks, biomedical applications often involve complex domain-specific terminology, diverse data formats, and specialized objectives. Therefore, evaluation strategies must be carefully designed to reflect the specific requirements and challenges of each task. For instance, named entity recognition (NER) in biomedical texts requires metrics that account for the high variability and ambiguity of medical terms, while medical question answering (QA) demands evaluations that measure the accuracy of factual answers and the ability to handle complex clinical queries. In this subsection, we discuss the evaluation strategies that are specifically tailored to different biomedical tasks, such as named entity recognition, medical question answering, and clinical text analysis, emphasizing the importance of task-specific metrics and evaluation protocols.

In the context of named entity recognition (NER), task-specific evaluation strategies often focus on precision, recall, and F1-score, which are the standard metrics for assessing the performance of NER systems. However, these metrics must be adapted to account for the unique challenges of biomedical NER, such as the presence of overlapping and nested entities. For example, the BioBART model [2] demonstrated the effectiveness of task-specific evaluation strategies by achieving high F1-scores on biomedical NER tasks, showing that domain-specific adjustments to evaluation protocols can significantly improve model performance. Additionally, the use of domain-specific corpora, such as the i2b2/2010 dataset, allows for more accurate evaluation of NER systems, as these datasets are carefully curated to reflect the complexities of biomedical texts.

Medical question answering (QA) presents another set of challenges that necessitate task-specific evaluation strategies. Unlike general QA tasks, which often focus on factual correctness, medical QA requires systems to handle complex clinical queries and provide accurate, context-aware answers. For example, the study on the use of LLMs for medical QA [8] demonstrated that task-specific evaluation metrics, such as the ability to distinguish between correct and incorrect answers, are crucial for assessing the effectiveness of QA systems in the biomedical domain. Additionally, the use of clinical benchmarks, such as the MIMIC-III dataset, allows researchers to evaluate the performance of QA systems on real-world clinical data, ensuring that the evaluation strategies are aligned with the practical needs of the medical field.

Clinical text analysis, including tasks such as clinical document classification and sentiment analysis, also requires tailored evaluation strategies that reflect the specific characteristics of clinical texts. For instance, the study on clinical text summarization [3] highlighted the importance of using task-specific metrics, such as ROUGE and BERTScore, to evaluate the quality of summaries generated from clinical texts. These metrics are designed to capture the nuances of clinical language, such as the presence of technical jargon and the need for concise, accurate summaries. Additionally, the use of domain-specific evaluation frameworks, such as the MedNLI dataset, allows researchers to assess the performance of NLP models on clinical texts, ensuring that the evaluation strategies are both relevant and effective.

In addition to task-specific metrics, evaluation protocols must also be designed to reflect the unique challenges of biomedical NLP tasks. For example, the study on the evaluation of biomedical NLP models [13] emphasized the importance of using domain-specific evaluation protocols that account for the variability and complexity of clinical texts. This includes the use of evaluation frameworks that consider the context of the text, such as the presence of multiple medical conditions or the use of specialized terminology. Furthermore, the study on the evaluation of biomedical NLP models [2] highlighted the importance of using task-specific evaluation protocols that are aligned with the objectives of the task, such as the ability to generate accurate and meaningful summaries of clinical texts.

Another important aspect of task-specific evaluation strategies is the use of benchmark datasets that are specifically designed for biomedical NLP tasks. These datasets provide a standardized framework for evaluating the performance of NLP models and ensuring that the evaluation strategies are consistent and comparable across different studies. For example, the study on the evaluation of biomedical NLP models [2] demonstrated the effectiveness of using the MedNLI and BioBART datasets for evaluating the performance of NLP models on biomedical tasks. These datasets are carefully curated to reflect the complexity and diversity of biomedical texts, making them ideal for task-specific evaluation strategies.

In addition to benchmark datasets, the use of domain-specific evaluation protocols is also crucial for ensuring the accuracy and reliability of biomedical NLP evaluations. For example, the study on the evaluation of biomedical NLP models [53] demonstrated the effectiveness of using a domain-specific evaluation protocol that includes both automatic and human evaluations. This approach allows researchers to assess the performance of NLP models not only based on their ability to generate accurate outputs but also based on their ability to meet the practical needs of the medical field. Furthermore, the study on the evaluation of biomedical NLP models [2] emphasized the importance of using domain-specific evaluation protocols that are aligned with the objectives of the task, such as the ability to generate accurate and meaningful summaries of clinical texts.

In conclusion, task-specific evaluation strategies are essential in biomedical NLP because they ensure that the evaluation of NLP models is both relevant and effective for the unique challenges of the biomedical domain. By using domain-specific metrics, benchmark datasets, and evaluation protocols, researchers can ensure that the evaluation strategies are aligned with the specific needs of each task, leading to more accurate and reliable results. As the field of biomedical NLP continues to evolve, the development of task-specific evaluation strategies will remain a critical area of research, ensuring that NLP models are not only effective but also practical and reliable for real-world applications.

### 6.4 Cross-Domain and Multi-Lingual Evaluation

Cross-domain and multi-lingual evaluation in biomedical natural language processing (NLP) presents unique challenges and opportunities. Unlike general NLP tasks, where models can often be trained and evaluated on large, diverse datasets, biomedical NLP faces limitations such as data scarcity, domain-specific terminology, and the need for specialized knowledge. When evaluating pre-trained language models (PLMs) across domains and languages, the complexity increases further due to the heterogeneity of biomedical data and the need for models to generalize beyond their training domain and language. This subsection explores the challenges and methodologies involved in evaluating PLMs in cross-domain and multi-lingual contexts, focusing on the use of multilingual benchmarks and the impact of language-specific characteristics on model performance.

One of the primary challenges in cross-domain evaluation is the lack of standardized benchmarks that cover multiple biomedical domains. While some tasks, such as named entity recognition (NER) or relation extraction, have been extensively studied in specific domains like clinical texts or biomedical literature, the ability of PLMs to transfer knowledge across different biomedical domains remains underexplored. For instance, a model trained on clinical texts may perform poorly on biomedical literature due to differences in terminology, structure, and context. This highlights the importance of domain adaptation techniques, such as domain-specific pre-training and transfer learning, which help models better understand and generalize to new biomedical domains. However, the effectiveness of these techniques is often evaluated on limited datasets, and the results may not be directly applicable to real-world scenarios. Research on cross-domain evaluation in biomedical NLP is still in its early stages, and there is a need for more comprehensive and standardized benchmarks to evaluate the adaptability of PLMs across different domains [6].

In the context of multi-lingual evaluation, the challenges are even more pronounced. Biomedical NLP tasks are predominantly focused on English, and the availability of annotated data for other languages, particularly in the biomedical domain, is significantly limited. This creates a barrier for evaluating PLMs on non-English biomedical texts, as most existing benchmarks and datasets are not designed for multi-lingual evaluation. For example, while the French biomedical language understanding benchmark DrBenchmark has been introduced to evaluate PLMs on French biomedical data, the availability of similar benchmarks for other languages remains limited. The lack of multilingual benchmarks makes it difficult to assess the performance of PLMs across different languages, especially when the goal is to develop models that can handle biomedical tasks in multiple languages. However, recent efforts have shown promise in addressing this issue. For instance, the development of cross-lingual and multilingual pre-trained models, such as XLM-RoBERTa, has enabled models to generalize across languages to some extent, but their performance on specialized biomedical tasks still lags behind their performance on general-domain tasks [26].

The impact of language-specific characteristics on model performance is another critical factor in cross-domain and multi-lingual evaluation. Biomedical texts in different languages often exhibit distinct linguistic patterns, such as differences in grammar, morphology, and vocabulary, which can affect the performance of PLMs. For example, the use of compound words and specialized terminologies in German biomedical texts may require different handling compared to English or Chinese. Additionally, the availability of domain-specific corpora and annotated data varies across languages, making it challenging to train and evaluate PLMs on non-English biomedical texts. Despite these challenges, some studies have explored the effectiveness of multilingual models in biomedical NLP tasks. For instance, the use of multilingual pre-trained models in tasks such as NER and relation extraction has shown that these models can achieve competitive performance on multiple languages, provided that they are fine-tuned on domain-specific data [16].

Another key aspect of cross-domain and multi-lingual evaluation is the need for robust and scalable evaluation methodologies. Traditional evaluation metrics, such as accuracy, precision, recall, and F1-score, may not be sufficient to capture the nuances of cross-domain and multi-lingual performance. For example, the presence of domain-specific terminology in biomedical texts may lead to biased evaluations, as models may struggle to recognize rare or specialized entities. Additionally, the performance of PLMs on multi-lingual tasks may be influenced by factors such as language diversity, data quality, and the availability of aligned corpora. To address these issues, researchers have proposed the use of more sophisticated evaluation frameworks, such as cross-lingual benchmarks and domain-specific evaluation protocols. These frameworks aim to provide a more comprehensive and accurate assessment of PLMs in cross-domain and multi-lingual settings [54].

The role of external knowledge sources in cross-domain and multi-lingual evaluation is also worth considering. In the biomedical domain, knowledge graphs and ontologies, such as UMLS and OntoChem, provide structured representations of biomedical concepts that can enhance the performance of PLMs. By integrating these knowledge sources into the evaluation process, researchers can better assess the ability of PLMs to understand and reason about biomedical concepts across different domains and languages. For example, the use of knowledge graphs in biomedical NER tasks has shown that models incorporating external knowledge can achieve higher performance compared to models trained on text-only data [4].

In conclusion, cross-domain and multi-lingual evaluation in biomedical NLP requires a combination of domain-specific benchmarks, robust evaluation methodologies, and the integration of external knowledge sources. While challenges such as data scarcity, domain-specific terminology, and language-specific characteristics persist, recent advancements in multilingual and cross-lingual models offer promising solutions. Future research should focus on developing standardized benchmarks, improving evaluation protocols, and exploring the effective integration of knowledge graphs and ontologies to enhance the performance of PLMs in cross-domain and multi-lingual settings [52].

### 6.5 Model Interpretability and Robustness Evaluation

Model interpretability and robustness evaluation are critical aspects of biomedical natural language processing (NLP) due to the high stakes involved in healthcare applications. Unlike general NLP tasks, where model performance can often be judged by accuracy or F1 scores, biomedical NLP models must also be evaluated for their ability to explain their decision-making processes and ensure reliable performance in critical healthcare settings. This subsection addresses the evaluation of model interpretability and robustness, including methods for analyzing model decision-making processes, identifying biases, and assessing the reliability of model outputs in clinical contexts.

Interpretability in biomedical NLP is essential for ensuring that models can be trusted by healthcare professionals. One method for evaluating model interpretability is the use of attention mechanisms, which highlight the parts of the input text that the model considers most relevant to its predictions. For example, in biomedical named entity recognition (NER), attention mechanisms can help identify which terms in a clinical note are most important for recognizing diseases, genes, or proteins. However, while attention weights can provide some insight into model behavior, they do not always reflect the actual reasoning process of the model [120; 122]. Therefore, more advanced techniques such as layer-wise relevance propagation (LRP) and gradient-based methods are often employed to better understand how models arrive at their conclusions.

Robustness evaluation is another crucial aspect of model assessment in biomedical NLP. Robustness refers to the ability of a model to maintain performance under varying conditions, including noisy or incomplete data, domain shifts, and adversarial attacks. In the biomedical domain, data scarcity and the presence of domain-specific terminology pose significant challenges to model robustness. For instance, models trained on general-domain corpora may struggle to recognize specialized terms in biomedical texts, leading to errors in tasks such as drug discovery or clinical text analysis. To evaluate robustness, researchers often use techniques such as data augmentation, where synthetic data is generated to simulate variations in the input. Additionally, methods such as adversarial training and domain adaptation can help improve a model's ability to generalize to new or unseen biomedical data [120; 122].

Bias detection is another important component of model evaluation in biomedical NLP. Biases in model outputs can arise from various sources, including imbalanced training data, biased annotations, and the inherent limitations of the training corpus. For example, models trained on datasets with a predominance of certain demographic groups may exhibit biased performance when applied to underrepresented populations. In the context of biomedical NLP, this can lead to disparities in the accuracy of predictions for different patient groups. To address this, researchers have proposed various methods for detecting and mitigating bias, including fairness-aware training, bias correction techniques, and the use of diverse and representative training data [120; 122]. These methods aim to ensure that models are not only accurate but also fair and equitable in their predictions.

In addition to interpretability and robustness, the reliability of model outputs in critical healthcare settings must also be assessed. This involves evaluating whether models can consistently produce accurate and trustworthy results, even in complex or ambiguous clinical scenarios. For instance, in medical question-answering tasks, models must not only provide correct answers but also ensure that the information is presented in a clear and understandable manner. To assess reliability, researchers often use human-in-the-loop evaluations, where clinicians or domain experts review model outputs and provide feedback. This approach can help identify cases where models produce incorrect or misleading information, even if they perform well on standard evaluation metrics [120; 122].

Another key challenge in evaluating model interpretability and robustness is the lack of standardized benchmarks and evaluation frameworks. While there are several benchmark datasets for biomedical NLP tasks such as named entity recognition and clinical text analysis, there is a need for more comprehensive benchmarks that specifically evaluate model interpretability and robustness. For example, the Biomedical Text Summarization (BTS) task has been explored in recent studies, but the evaluation of model interpretability in this context remains limited [3]. Similarly, the evaluation of model robustness in drug discovery tasks, where models must handle highly complex and diverse data, requires the development of specialized evaluation protocols.

Recent studies have also highlighted the importance of evaluating model robustness in the context of safety and ethical concerns. For instance, the emergence of large language models (LLMs) has raised concerns about their potential for generating harmful or misleading information, particularly in medical contexts where errors can have serious consequences [7]. To address these concerns, researchers have proposed methods for evaluating model safety, such as checking for consistency in model outputs, detecting hallucinations, and ensuring that models do not produce biased or harmful content. These methods are essential for ensuring that biomedical NLP models are not only accurate but also safe and trustworthy.

In summary, model interpretability and robustness evaluation are critical components of biomedical NLP research. Techniques such as attention mechanisms, LRP, and gradient-based methods are used to analyze model decision-making processes, while data augmentation and adversarial training are employed to improve model robustness. Bias detection and mitigation strategies are also essential for ensuring fairness and equity in model predictions. Furthermore, the reliability of model outputs in clinical settings must be evaluated through human-in-the-loop approaches and specialized benchmarks. As the field continues to evolve, the development of standardized evaluation frameworks and safety protocols will be crucial for ensuring that biomedical NLP models are both effective and trustworthy.

### 6.6 Human-in-the-Loop Evaluation

Human-in-the-loop evaluation plays a pivotal role in assessing the practical relevance and effectiveness of pre-trained language models (PLMs) in the biomedical domain. Unlike automated evaluation metrics, which focus on quantitative performance, human-in-the-loop evaluation incorporates expert annotations, clinician feedback, and user studies to ensure that the outputs of PLMs align with the nuanced requirements of real-world biomedical applications. This subsection explores the various dimensions of human-in-the-loop evaluation, highlighting its importance in bridging the gap between model performance and practical utility in the biomedical domain.

One of the primary applications of human-in-the-loop evaluation is in the validation of model outputs through expert annotations. Biomedical tasks such as named entity recognition (NER), relation extraction, and clinical text analysis require high accuracy and domain-specific knowledge that general pre-trained models may lack. To ensure the reliability of these models, expert annotations are essential. For instance, in the case of named entity recognition, domain experts manually annotate biomedical texts to identify and categorize entities such as diseases, genes, and drugs. These annotations serve as ground truth for training and evaluating PLMs. The work by [10] emphasizes the importance of domain-specific annotations, showing that models trained on such data outperform general-domain models in biomedical tasks. Similarly, [31] demonstrates the effectiveness of human-in-the-loop evaluation in improving the performance of models for biomedical machine reading comprehension (MRC) tasks. By incorporating expert feedback, the model can better understand the complexities of biomedical narratives, leading to more accurate and contextually relevant predictions.

Clinician feedback is another critical component of human-in-the-loop evaluation. Biomedical language models are often deployed in clinical settings where their outputs can directly impact patient care. Therefore, it is crucial to assess the usability and interpretability of these models from the perspective of healthcare professionals. For example, in the context of clinical text analysis, clinicians can provide insights into whether the model's outputs are clinically meaningful and actionable. The work by [31] highlights the importance of clinician feedback in refining the performance of MRC models. By iteratively incorporating feedback from clinicians, the model can be fine-tuned to better align with the needs of clinical decision-making. Similarly, [167] proposes a framework where large language models (LLMs) collaborate with clinicians in a multi-round discussion to enhance their reasoning capabilities. This collaborative approach ensures that the models not only produce accurate outputs but also provide explanations that are understandable and actionable for clinicians.

User studies further contribute to the evaluation of PLMs in the biomedical domain by assessing the usability and acceptability of model outputs from the end-user perspective. These studies often involve recruiting domain experts, clinicians, and other stakeholders to interact with the models and provide feedback on their performance. For example, [31] conducts extensive evaluations by comparing the performance of their model with existing methods on three widely used biomedical-MRC datasets. The results show that their model achieves state-of-the-art performance without relying on any synthetic or human-annotated data from the biomedical domain. This highlights the potential of user studies in identifying the most effective models for specific biomedical tasks. Additionally, [168] emphasizes the importance of user studies in evaluating the effectiveness of multimodal models in biomedical applications. By involving users in the evaluation process, the model can be refined to better meet the needs of its intended audience.

Another important aspect of human-in-the-loop evaluation is the use of expert annotations to address the challenges of domain-specific terminology and data scarcity. Biomedical texts are often rich in specialized vocabulary and complex structures that can be challenging for general-domain PLMs to understand. To overcome this, expert annotations can provide the necessary context and guidance to help models better capture the nuances of biomedical language. For instance, [10] demonstrates that models trained on large biomedical corpora outperform general-domain models in biomedical tasks, highlighting the importance of domain-specific annotations. Similarly, [31] shows that incorporating expert annotations can significantly improve the performance of models for biomedical MRC tasks. These findings underscore the value of human-in-the-loop evaluation in enhancing the performance of PLMs in the biomedical domain.

The integration of human-in-the-loop evaluation with automated metrics can also lead to more comprehensive and reliable assessments of PLMs. While automated metrics such as accuracy, F1 score, and AUC are useful for comparing model performance, they may not fully capture the practical relevance of model outputs. By combining human evaluations with automated metrics, researchers can gain a more holistic understanding of a model's strengths and limitations. For example, [31] uses a combination of automated metrics and human evaluations to assess the performance of their model on biomedical-MRC datasets. This approach ensures that the model not only performs well on benchmark datasets but also provides outputs that are meaningful and actionable in real-world settings.

In addition to improving model performance, human-in-the-loop evaluation can also help identify and address potential biases and limitations in PLMs. Biomedical data is often characterized by imbalances and biases that can affect the performance of models. By involving experts in the evaluation process, researchers can uncover these biases and develop strategies to mitigate their impact. For instance, [31] incorporates expert feedback to address the challenges of domain-specific terminology and data scarcity, demonstrating the effectiveness of human-in-the-loop evaluation in improving model robustness. Similarly, [10] highlights the importance of domain-specific annotations in addressing the challenges of data scarcity and domain adaptation.

Finally, human-in-the-loop evaluation plays a crucial role in ensuring the ethical and responsible use of PLMs in the biomedical domain. Given the sensitive nature of biomedical data, it is essential to ensure that models are transparent, interpretable, and free from biases. By involving domain experts and clinicians in the evaluation process, researchers can gain insights into the ethical implications of model outputs and develop strategies to address them. For example, [31] emphasizes the importance of transparency and interpretability in biomedical models, demonstrating how human-in-the-loop evaluation can help ensure that models are both effective and ethically sound.

In conclusion, human-in-the-loop evaluation is a critical component of assessing the effectiveness of PLMs in the biomedical domain. By incorporating expert annotations, clinician feedback, and user studies, researchers can ensure that model outputs are not only accurate but also practical and relevant to real-world biomedical applications. As the field of biomedical NLP continues to evolve, the integration of human-in-the-loop evaluation with automated metrics will be essential in developing models that are both powerful and trustworthy.

### 6.7 Zero-Shot and Few-Shot Learning Evaluation

Zero-shot and few-shot learning evaluation is a critical aspect of assessing the performance of pre-trained language models (PLMs) in biomedical NLP tasks, especially given the challenges of limited annotated data. These evaluation strategies focus on how well models can generalize to new tasks or domains with minimal or no labeled examples, making them particularly relevant for biomedical applications where data scarcity is a common issue [169; 3].

In the biomedical domain, zero-shot learning refers to the ability of a model to perform a task without any task-specific examples, relying solely on its pre-training on general or domain-specific corpora. For instance, in the context of named entity recognition (NER), a zero-shot model would be expected to identify biomedical entities such as diseases, genes, or proteins in clinical texts without prior exposure to annotated examples of these entities. This is a challenging task due to the domain-specific terminology and the complexity of biomedical language. Recent studies have explored the effectiveness of pre-trained models such as BioBERT and ClinicalBERT in zero-shot settings, with encouraging results. BioBERT, for example, was shown to outperform general domain models in biomedical NER tasks even with limited or no task-specific fine-tuning [29]. Similarly, the work by [169] demonstrated that few-shot learning methods could be effective in medical NLP tasks, particularly when the model was trained on a large corpus of biomedical text.

Few-shot learning, on the other hand, involves training a model with a small number of labeled examples. This approach is particularly useful in biomedical applications where labeled data is scarce, but some annotated examples are available. For example, in drug discovery or clinical decision-making, a few-shot model could be trained on a limited set of annotated biomedical texts to identify drug-target interactions or classify patient outcomes. The study by [169] found that few-shot learning methods, including prototypical networks and meta-learning, showed significant promise in medical NLP tasks. These methods were able to achieve high accuracy with only a few labeled examples, demonstrating the potential of PLMs in scenarios with limited data.

Evaluating the performance of models in zero-shot and few-shot settings requires careful consideration of the evaluation metrics and benchmarks. Traditional metrics such as accuracy, precision, recall, and F1 score are commonly used, but they may not fully capture the nuances of biomedical tasks. For instance, in tasks like relation extraction or information extraction, the ability to capture context and semantic relationships is as important as the accuracy of individual predictions. The work by [41] highlighted the limitations of current benchmarks in capturing clinically relevant tasks, emphasizing the need for domain-specific evaluation frameworks that reflect the complexities of biomedical NLP tasks.

One of the key challenges in evaluating zero-shot and few-shot models in the biomedical domain is the lack of standardized datasets and benchmarks. While some studies have used existing biomedical corpora such as the i2b2 dataset for named entity recognition or the MIMIC-III dataset for clinical text analysis, these datasets may not fully represent the diversity of biomedical tasks. The study by [41] also pointed out that many AI benchmarks are not aligned with the tasks that medical professionals consider most important. This highlights the need for more comprehensive and clinically relevant benchmarks that can better evaluate the performance of PLMs in zero-shot and few-shot settings.

Another important aspect of evaluating zero-shot and few-shot models is understanding their generalization and adaptability. Pre-trained models are often evaluated on their ability to generalize to new tasks or domains, and the performance in zero-shot and few-shot settings provides insights into this capability. The study by [24] found that general-domain models often outperformed biomedical-domain models in certain tasks, suggesting that domain specificity may not always be the determining factor in model performance. However, biomedical instruction finetuning improved performance to a level comparable to general instruction finetuning, even with significantly fewer instructions. This indicates that the adaptability of PLMs can be enhanced through careful instruction finetuning, even in the absence of extensive domain-specific data.

The implications of zero-shot and few-shot learning for model generalization and adaptability are significant. In biomedical NLP, where data is often scarce and tasks are highly specialized, the ability of models to generalize across tasks and domains is crucial. The work by [170] demonstrated that a machine learning pipeline could outperform baseline methods in biomedical NER tasks, even with limited labeled data. This suggests that PLMs, when properly adapted, can achieve high performance in zero-shot and few-shot settings, making them valuable tools for biomedical applications.

Furthermore, the integration of external knowledge sources can enhance the performance of zero-shot and few-shot models in biomedical tasks. For example, the use of biomedical knowledge graphs or ontologies can provide additional context and structure to the models, helping them better understand domain-specific terminology and relationships. The study by [139] showed that integrating external knowledge from medical terminology ontologies improved the performance of deep learning models in cancer phenotyping tasks. This approach can be extended to zero-shot and few-shot learning, where the model can leverage external knowledge to compensate for the lack of labeled data.

In conclusion, zero-shot and few-shot learning evaluation plays a vital role in assessing the performance of pre-trained language models in biomedical NLP tasks. While challenges such as data scarcity and the lack of standardized benchmarks persist, recent studies have demonstrated the potential of these approaches in improving model generalization and adaptability. By leveraging pre-trained models, domain-specific knowledge, and effective evaluation strategies, researchers can develop more robust and versatile NLP systems for the biomedical domain. The ongoing development of domain-specific benchmarks and the integration of external knowledge sources will further enhance the effectiveness of zero-shot and few-shot learning in biomedical NLP.

### 6.8 Standardization and Reproducibility in Evaluation

Standardization and reproducibility in evaluation are critical aspects of assessing the effectiveness of pre-trained language models (PLMs) in the biomedical domain. As the field of biomedical natural language processing (NLP) continues to evolve, ensuring that evaluation methodologies are consistent, transparent, and replicable is essential for advancing research and enabling meaningful comparisons between different models. Without standardization, the evaluation of PLMs can become fragmented, leading to inconsistencies in performance metrics, biases in benchmarking, and challenges in replicating results. This subsection discusses the importance of standardization and reproducibility in biomedical NLP evaluation, focusing on the need for consistent evaluation protocols, shared datasets, and transparent reporting of results.

One of the primary challenges in evaluating PLMs in the biomedical domain is the lack of standardized evaluation protocols. Unlike general NLP tasks, where widely accepted benchmarks such as GLUE and SQuAD exist, the biomedical domain often lacks a unified framework for assessing model performance. This is partly due to the complexity of biomedical text, which includes specialized terminology, domain-specific tasks, and a limited availability of annotated data. Without standardized protocols, researchers may use different evaluation metrics, data splits, and task formulations, making it difficult to compare results across studies. For example, some studies may prioritize precision over recall, while others emphasize F1 scores, leading to discrepancies in model performance assessments. The paper "Standardization and Reproducibility in Evaluation" emphasizes the need for a unified evaluation framework that ensures consistency and fairness across studies [171].

In addition to standardized protocols, the availability of shared datasets is crucial for reproducibility in biomedical NLP evaluation. While several benchmark datasets have been developed for biomedical tasks, such as MedNLI, BioBART, and CLUE, these datasets often vary in size, quality, and task coverage. For instance, some datasets may focus on named entity recognition (NER) tasks, while others may emphasize clinical text analysis or drug discovery. This variability makes it challenging to develop models that generalize well across different biomedical domains. Moreover, the lack of publicly available datasets or limited access to proprietary data can hinder the reproducibility of results. The paper "Standardization and Reproducibility in Evaluation" highlights the importance of shared datasets and the need for open access to benchmarking data to facilitate reproducible research [171].

Transparent reporting of results is another key aspect of ensuring reproducibility in biomedical NLP evaluation. Researchers should clearly document their evaluation methodologies, including the specific metrics used, the data splits, and the hyperparameters employed during training and testing. This transparency allows other researchers to replicate experiments and validate findings. However, many studies in the biomedical domain fail to provide sufficient details on their evaluation procedures, making it difficult to assess the reliability of their results. For example, some papers may not report the exact evaluation metrics or the number of training iterations, leading to potential biases in the results. The paper "Standardization and Reproducibility in Evaluation" underscores the need for transparent reporting practices, such as detailed methodological descriptions and the use of standardized evaluation frameworks [171].

Another challenge in achieving standardization and reproducibility is the diversity of biomedical tasks and applications. Pre-trained language models are applied to a wide range of tasks, including named entity recognition, medical question answering, clinical text analysis, drug discovery, and biomedical information retrieval. Each of these tasks may require different evaluation strategies and metrics. For instance, while F1 score is commonly used for NER tasks, precision and recall may be more relevant for clinical text analysis. Without a unified evaluation framework that accounts for these differences, it becomes difficult to compare the performance of PLMs across tasks. The paper "Standardization and Reproducibility in Evaluation" suggests that future research should focus on developing task-specific evaluation protocols that are both standardized and adaptable to different biomedical applications [171].

The issue of reproducibility is further complicated by the computational resources required for training and evaluating PLMs. Many pre-trained language models, especially those trained on large biomedical datasets, require significant computational power and memory. This can make it difficult for researchers with limited resources to reproduce the results of published studies. Furthermore, the lack of access to the same hardware or software configurations can lead to variations in model performance. The paper "Standardization and Reproducibility in Evaluation" emphasizes the importance of providing open-source implementations and pre-trained model checkpoints to ensure that results can be replicated across different environments [171].

To address these challenges, the biomedical NLP community must prioritize the development of standardized evaluation practices. This includes the creation of shared datasets, the establishment of consistent evaluation metrics, and the promotion of transparent reporting of results. Furthermore, researchers should collaborate to develop unified evaluation frameworks that account for the unique characteristics of biomedical text and tasks. By improving standardization and reproducibility in evaluation, the field of biomedical NLP can ensure that the progress made by pre-trained language models is both reliable and impactful.

In conclusion, standardization and reproducibility in evaluation are essential for advancing the field of biomedical NLP. Without consistent evaluation protocols, shared datasets, and transparent reporting of results, it is difficult to assess the true performance of pre-trained language models in the biomedical domain. The paper "Standardization and Reproducibility in Evaluation" highlights the importance of these factors and calls for the development of a unified evaluation framework that supports reproducible and reliable research in biomedical NLP [171]. By addressing these challenges, the field can ensure that the benefits of pre-trained language models are realized in a consistent and reproducible manner.

### 6.9 Challenges in Biomedical NLP Evaluation

The evaluation of pre-trained language models (PLMs) in the biomedical domain presents a unique set of challenges that distinguish it from general natural language processing (NLP) tasks. These challenges stem from the specialized nature of biomedical text, the limited availability of annotated data, and the necessity for clinically relevant benchmarks. The complexities of biomedical language, which includes highly specific terminology, abbreviations, and complex sentence structures, make it difficult for PLMs to generalize across different tasks and domains. Additionally, the need for models to be interpretable and reliable in clinical settings further complicates the evaluation process [22].

One of the primary challenges in evaluating PLMs for biomedical tasks is data scarcity. Unlike general NLP, where vast amounts of text are readily available, biomedical datasets are often limited in size and scope. This scarcity is exacerbated by the high cost and effort required to annotate biomedical texts, which are typically written by experts and require domain-specific knowledge. For instance, studies such as "Pre-training technique to localize medical BERT and enhance biomedical BERT" [101] highlight the difficulty of training effective models without sufficient labeled data. This limitation not only affects the performance of PLMs but also restricts the ability to conduct comprehensive evaluations across different tasks and domains.

Another significant challenge is the domain-specific terminology inherent in biomedical texts. The use of specialized jargon, abbreviations, and acronyms can lead to misinterpretations by PLMs, as they are often trained on general-domain corpora that lack such domain-specific knowledge. For example, "Biomedical Information Retrieval" [22] discusses how PLMs struggle with the nuances of biomedical language, which can result in inaccurate information retrieval and poor performance in tasks such as named entity recognition. The need for models to understand and effectively process this specialized language necessitates the development of domain-specific evaluation metrics and benchmarks, which are currently lacking in many biomedical NLP applications.

The evaluation of PLMs in the biomedical domain is further complicated by the need for clinically relevant benchmarks. While general NLP tasks often rely on well-established benchmarks such as GLUE and SQuAD, the biomedical domain lacks similar standardized datasets that reflect the complexities of clinical and scientific texts. This absence of robust benchmarks makes it difficult to assess the effectiveness of PLMs in real-world biomedical applications. For instance, "Evaluation of Biomedical Pre-trained Language Models" [172] emphasizes the importance of developing task-specific benchmarks that accurately reflect the challenges faced in biomedical NLP tasks. Without such benchmarks, it is challenging to compare the performance of different models and to identify the most effective approaches.

Moreover, the limitations of current evaluation practices in the biomedical domain pose additional challenges. Many evaluation metrics used in general NLP, such as accuracy and F1 score, may not be appropriate for biomedical tasks, as they do not account for the specific requirements of clinical applications. For example, "Evaluation and Benchmarking in Biomedical NLP" [65] highlights the need for metrics that are sensitive to the clinical relevance of model outputs. The lack of such metrics makes it difficult to assess the practical utility of PLMs in healthcare settings, where the accuracy and reliability of model predictions can have significant consequences.

Another critical challenge is the need for model interpretability and explainability. In the biomedical domain, the decisions made by PLMs must be transparent and justifiable, particularly when they are used to support clinical decision-making. However, many PLMs are considered "black boxes," making it difficult to understand how they arrive at their predictions. This lack of transparency can hinder the adoption of PLMs in clinical settings, where trust and accountability are essential. Studies such as "Model Interpretability and Robustness Evaluation" [173] emphasize the importance of developing methods for interpreting PLM outputs, which can help clinicians understand and trust the models' predictions.

The complexity of biomedical texts also poses challenges for the evaluation of PLMs. Biomedical documents often contain long and intricate sentences, making it difficult for models to capture the context and meaning of the text. For instance, "Hierarchical Transformers for Long Document Classification" [174] discusses the challenges of processing long biomedical texts and the need for models that can effectively handle such complexity. Current evaluation practices may not adequately capture the performance of PLMs in these scenarios, as they often focus on shorter texts and simpler tasks.

Additionally, the evaluation of PLMs in the biomedical domain is affected by the lack of standardized evaluation protocols. Different studies may use different datasets, metrics, and evaluation procedures, making it difficult to compare results across studies. This variability can lead to inconsistencies in the assessment of model performance and hinder the development of more effective PLMs. "Standardization and Reproducibility in Evaluation" [171] highlights the importance of establishing consistent evaluation protocols to ensure that the results of different studies are comparable and reliable.

The challenges in evaluating PLMs for biomedical tasks are further compounded by the need for models to be scalable and efficient. Many PLMs are large and computationally intensive, which can limit their applicability in real-world biomedical settings where resources are often constrained. For example, "Model Efficiency and Robustness Improvements" [175] discusses the importance of developing efficient models that can be deployed in resource-limited environments. Without such efficiency, the practical utility of PLMs in the biomedical domain may be limited.

In conclusion, the evaluation of pre-trained language models in the biomedical domain is fraught with challenges, including data scarcity, domain-specific terminology, the need for clinically relevant benchmarks, and limitations in current evaluation practices. Addressing these challenges requires the development of domain-specific evaluation metrics, the creation of standardized benchmarks, and the improvement of model interpretability and efficiency. Only by overcoming these challenges can the full potential of PLMs in the biomedical domain be realized.

### 6.10 Future Directions in Biomedical NLP Evaluation

The evaluation of pre-trained language models (PLMs) in the biomedical domain is a rapidly evolving area, with significant efforts focused on improving the robustness, fairness, and interpretability of these models. As the field progresses, several future research directions are emerging that aim to address the limitations of current evaluation practices and push the boundaries of biomedical natural language processing (NLP). These directions include the development of more comprehensive benchmarks, the integration of multimodal data, and the enhancement of model interpretability and fairness, as highlighted in the current survey.

One of the most pressing challenges in the evaluation of biomedical PLMs is the need for more comprehensive and diverse benchmarks. While there are already several benchmark datasets available, such as MedNLI [26], BioBART [2], and CLUE [26], these datasets often cover only a limited set of tasks and domains. Future research should focus on developing more extensive benchmarks that span a wider range of biomedical tasks, including but not limited to named entity recognition (NER), clinical text analysis, drug discovery, and biomedical information retrieval. The creation of such benchmarks will enable researchers to better assess the generalization capabilities of PLMs and identify areas where improvements are needed. Additionally, the inclusion of diverse data sources, such as multilingual biomedical texts, can help address the current limitations of domain-specific evaluations and enhance the cross-lingual adaptability of these models.

Another promising direction for future research is the integration of multimodal data into the evaluation of biomedical PLMs. While current evaluation practices predominantly focus on text-based tasks, the biomedical domain is rich with multimodal data, including images, genomic sequences, and sensor data. For instance, the integration of textual and visual data in clinical settings can provide a more holistic understanding of patient information and improve the accuracy of diagnostic and treatment recommendations. Future work should explore how PLMs can be adapted to handle multimodal inputs and how their performance can be evaluated across different modalities. This will not only expand the applicability of PLMs but also enable the development of more sophisticated models that can process and reason about complex biomedical data.

Enhancing model interpretability and fairness is another critical area for future research in the evaluation of biomedical PLMs. The deployment of AI models in healthcare settings requires a high degree of transparency and accountability, as errors in these models can have serious consequences for patients. Current PLMs, particularly large-scale models, are often considered "black boxes" due to their complexity and the difficulty of understanding how they arrive at their predictions. Future research should focus on developing methods to improve model interpretability, such as attention visualization, feature attribution, and rule-based explanations. Additionally, fairness in model evaluation is an important consideration, as biases in training data can lead to disparities in model performance across different patient populations. Researchers should explore ways to incorporate fairness metrics into evaluation frameworks and ensure that models are not only accurate but also equitable in their predictions.

The development of more robust and scalable evaluation methodologies is also essential for the future of biomedical NLP. Current evaluation practices often rely on fixed datasets and static metrics, which may not fully capture the dynamic and evolving nature of biomedical data. Future research should focus on creating adaptive evaluation frameworks that can accommodate changes in data distribution and task requirements. For example, the use of active learning and continuous evaluation techniques can help models stay up-to-date with the latest developments in the biomedical domain. Furthermore, the integration of human-in-the-loop evaluation methods, as discussed in [26], can provide valuable insights into the practical relevance and effectiveness of PLMs in real-world clinical settings.

Moreover, the exploration of new evaluation metrics and tasks is crucial for advancing the field of biomedical NLP. While traditional metrics such as accuracy, F1 score, and AUC are widely used, they may not always be sufficient for capturing the nuances of biomedical tasks. Future research should investigate the development of domain-specific metrics that are tailored to the unique challenges of biomedical NLP, such as the identification of rare entities, the detection of causal relationships, and the interpretation of complex medical texts. Additionally, the introduction of new tasks that reflect the evolving needs of the biomedical community, such as drug discovery, clinical decision support, and personalized medicine, can help drive innovation and ensure that PLMs are aligned with real-world applications.

In summary, the future of biomedical NLP evaluation lies in the development of more comprehensive benchmarks, the integration of multimodal data, and the enhancement of model interpretability and fairness. By addressing these challenges, researchers can create more reliable, transparent, and effective PLMs that are better suited to the complex and dynamic nature of the biomedical domain. These efforts will not only advance the state of the art in biomedical NLP but also contribute to the broader goal of improving healthcare outcomes through the application of AI technologies.

## 7 Enhancements and Innovations in Biomedical Pre-training

### 7.1 Integration of Multimodal Data

The integration of multimodal data into pre-trained language models (PLMs) represents a significant advancement in biomedical pre-training, enabling models to better understand and process complex biomedical tasks by leveraging diverse data types such as text, images, and other biomedical modalities. This approach addresses the limitations of traditional text-only models, which often struggle to capture the full context and nuance of biomedical information. By incorporating multimodal data, PLMs can enhance their performance in tasks such as medical image analysis, clinical text interpretation, and biomedical information retrieval.

One of the key benefits of integrating multimodal data is the ability to combine textual information with visual or other structured data, allowing models to make more informed and accurate predictions. For instance, in the context of medical imaging, models can benefit from the integration of textual descriptions alongside images to better understand the context and meaning of the visual data. This is particularly relevant in tasks such as disease diagnosis and treatment planning, where both textual and visual information are critical [2].

Recent studies have explored various strategies for integrating multimodal data into PLMs. One such approach involves the use of unified architectures that can process multiple modalities simultaneously. For example, the development of multimodal PLMs like BioBART [2] demonstrates how language models can be adapted to handle biomedical tasks that require both textual and numerical data. These models are trained on a combination of text and other data types, allowing them to capture the relationships between different modalities and improve their overall performance.

Another important aspect of multimodal integration is the use of specialized pre-training techniques that account for the unique characteristics of biomedical data. For instance, the paper "BioMegatron" [10] highlights the importance of large-scale pre-training on domain-specific data to enhance the performance of PLMs in biomedical tasks. This approach not only leverages the vast amounts of textual data available but also incorporates structured biomedical data, such as molecular structures and clinical records, to create a more comprehensive understanding of the domain.

In addition to traditional text and images, other biomedical data types such as genomic sequences, proteomic data, and clinical trial records can also be integrated into PLMs. This expansion of data sources allows models to capture a broader range of biomedical knowledge and improve their ability to handle complex tasks. For example, the paper "UMLS-KGI-BERT" [105] explores the integration of biomedical knowledge graphs into PLMs to enhance their ability to perform named entity recognition and other NLP tasks. By incorporating structured knowledge from sources like the Unified Medical Language System (UMLS), these models can better understand the relationships between different biomedical entities and improve their overall accuracy.

The integration of multimodal data also presents challenges, particularly in terms of data preprocessing and model architecture design. Ensuring that different modalities are effectively combined and processed requires careful consideration of how to represent and align the various data types. For instance, the paper "Position-based Prompting for Health Outcome Generation" [176] discusses the importance of designing prompts that can effectively guide models to understand and generate health outcomes based on both textual and contextual information. This highlights the need for innovative approaches to handle the complexities of multimodal data.

Furthermore, the effectiveness of multimodal integration depends on the availability of high-quality annotated data across different modalities. The paper "CLIN-X" [150] emphasizes the importance of domain-specific training and the need for large-scale annotated datasets to support the development of effective multimodal models. This is particularly relevant in the biomedical domain, where the availability of labeled data for multiple modalities is often limited. To address this challenge, researchers have explored techniques such as data augmentation and transfer learning to leverage existing data and improve model performance.

Another critical aspect of multimodal integration is the evaluation of model performance on tasks that require the combination of different data types. The paper "Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data" [177] highlights the importance of developing robust evaluation frameworks that can assess the performance of multimodal models in real-world scenarios. This includes the use of metrics that can capture the nuances of multimodal data and provide a comprehensive assessment of model capabilities.

The integration of multimodal data into PLMs also raises questions about the scalability and computational efficiency of these models. The paper "MediSwift" [124] explores the use of sparse pre-training techniques to reduce the computational costs of training large-scale models while maintaining their performance on biomedical tasks. This approach demonstrates the potential for developing efficient and effective multimodal models that can be deployed in resource-constrained environments.

In conclusion, the integration of multimodal data into pre-trained language models is a promising direction for enhancing their performance in biomedical tasks. By leveraging diverse data types and advanced pre-training techniques, these models can better understand and process complex biomedical information. However, this approach also presents challenges that require further research and innovation. The papers discussed above highlight the importance of domain-specific training, the need for high-quality annotated data, and the development of robust evaluation frameworks to support the effective use of multimodal models in the biomedical domain. As research in this area continues to evolve, the integration of multimodal data will play a crucial role in advancing the capabilities of PLMs in biomedical applications.

### 7.2 Knowledge Graph Enhancements

Knowledge graphs (KGs) have emerged as a powerful tool for enhancing pre-training methodologies in the biomedical domain, providing structured, domain-specific knowledge that complements textual data and improves model reasoning and representation. By integrating KGs into pre-training, researchers aim to bridge the gap between the unstructured nature of text and the structured, hierarchical knowledge that underpins biomedical domains. This subsection explores the various ways in which knowledge graphs are leveraged to enhance pre-training, including their role in improving model performance, enabling more accurate reasoning, and facilitating the incorporation of domain-specific knowledge into language models.

One of the key advantages of knowledge graphs is their ability to represent complex relationships between entities in a structured manner. In the biomedical domain, this is particularly important as it allows models to capture the intricate connections between genes, proteins, diseases, and drugs. For instance, biomedical knowledge graphs such as UMLS (Unified Medical Language System) and OntoChem provide rich, curated information that can be used to enhance the pre-training of language models. By incorporating this structured knowledge, models can better understand the context and relationships within biomedical texts, leading to improved performance on downstream tasks such as named entity recognition, relation extraction, and question answering. For example, in the paper titled "Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs," the authors demonstrate that integrating knowledge graphs into pre-trained language models (PLMs) significantly improves their ability to handle domain-specific tasks by providing structured information that complements the unstructured textual data [4].

Another important aspect of knowledge graph enhancements is their role in improving the interpretability and explainability of pre-trained language models. In the biomedical domain, where transparency and trust are critical, the ability to provide explanations for model decisions is essential. Knowledge graphs can help in this regard by providing a structured representation of the underlying knowledge that the model uses to make predictions. For instance, in the paper titled "CLUE: A Clinical Language Understanding Evaluation for LLMs," the authors emphasize the importance of evaluating language models on real-world clinical tasks, and they highlight the need for models to incorporate structured knowledge to ensure reliable and interpretable outputs [162]. By leveraging knowledge graphs, models can better understand the relationships between entities and provide more accurate and interpretable responses to clinical queries.

Furthermore, knowledge graphs can be used to enhance the generalization capabilities of pre-trained language models by providing them with additional context and structure. This is particularly important in the biomedical domain, where the availability of annotated data is often limited. For example, in the paper titled "Pre-trained Language Models in Biomedical Domain: A Systematic Survey," the authors discuss the challenges of applying general pre-trained models to biomedical tasks and highlight the need for domain-specific knowledge to improve model performance [6]. By incorporating knowledge graphs into the pre-training process, models can gain a deeper understanding of the domain, leading to better performance on specialized tasks such as drug discovery and disease diagnosis.

The integration of knowledge graphs into pre-training methodologies also opens up new opportunities for improving the efficiency and scalability of language models. By using structured knowledge, models can reduce the amount of training data required to achieve high performance, making them more computationally efficient. This is particularly relevant in the biomedical domain, where the cost of data annotation is high and the availability of labeled data is limited. For instance, in the paper titled "Empowering Language Model with Guided Knowledge Fusion for Biomedical Document Re-ranking," the authors demonstrate that incorporating knowledge into pre-trained language models can significantly improve their ability to retrieve relevant documents, even when the amount of labeled data is limited [178]. This approach not only enhances model performance but also reduces the computational burden, making it more feasible to deploy models in real-world clinical settings.

Moreover, knowledge graphs can be used to enhance the robustness and reliability of pre-trained language models by providing them with a more comprehensive understanding of the domain. In the biomedical domain, where errors can have serious consequences, the ability to detect and correct errors is crucial. By incorporating knowledge graphs, models can better understand the context and relationships between entities, leading to more accurate and reliable predictions. For example, in the paper titled "Medical Foundation Models are Susceptible to Targeted Misinformation Attacks," the authors highlight the vulnerability of large language models to targeted misinformation attacks, emphasizing the need for robust models that can detect and correct errors [179]. By integrating knowledge graphs, models can gain a deeper understanding of the domain, enabling them to better detect and correct errors, thereby improving their reliability and trustworthiness.

In addition to improving model performance and robustness, knowledge graphs can also be used to enhance the interpretability of pre-trained language models by providing a structured representation of the knowledge that the model uses to make predictions. This is particularly important in the biomedical domain, where transparency and explainability are critical. For instance, in the paper titled "Thinking about GPT-3 In-Context Learning for Biomedical IE: Think Again," the authors discuss the limitations of in-context learning and highlight the need for models that can provide more transparent and interpretable outputs [68]. By incorporating knowledge graphs, models can provide more interpretable explanations for their predictions, making them more suitable for use in clinical settings where transparency is essential.

Finally, the integration of knowledge graphs into pre-training methodologies also has the potential to improve the collaboration and knowledge sharing between researchers, clinicians, and industry stakeholders. By providing a structured representation of biomedical knowledge, knowledge graphs can facilitate the exchange of information and promote interdisciplinary collaboration. This is particularly important in the biomedical domain, where the integration of knowledge from multiple sources is essential for advancing research and improving patient care. For example, in the paper titled "LLMs-Healthcare: Current Applications and Challenges of Large Language Models in various Medical Specialties," the authors emphasize the importance of interdisciplinary collaboration in the development and application of large language models in healthcare [180]. By leveraging knowledge graphs, researchers can more effectively share and integrate knowledge, leading to more impactful and sustainable advancements in the field.

In conclusion, the use of knowledge graphs in biomedical pre-training methodologies offers a powerful way to enhance model performance, improve interpretability, and facilitate the integration of domain-specific knowledge into language models. By leveraging structured knowledge, models can better understand the context and relationships within biomedical texts, leading to more accurate and reliable predictions. As the field continues to evolve, the integration of knowledge graphs into pre-training methodologies will play an increasingly important role in advancing the capabilities of language models in the biomedical domain.

### 7.3 Domain-Specific Corpora and Pretraining

The use of domain-specific corpora for pre-training has emerged as a critical strategy in enhancing the performance of pre-trained language models (PLMs) in the biomedical domain. Unlike general-domain models that are trained on vast amounts of text from the internet or books, domain-specific pre-training leverages large-scale biomedical datasets to capture the unique linguistic patterns, terminologies, and structures prevalent in medical texts. This approach ensures that the models are better equipped to handle specialized tasks such as named entity recognition, clinical text analysis, and biomedical information retrieval. The importance of domain-specific corpora in pre-training is underscored by the fact that the biomedical domain is characterized by highly specialized language, which is often inaccessible to general-purpose models.

A key factor driving the adoption of domain-specific pre-training is the availability of large-scale biomedical datasets. For instance, PubMed, a comprehensive repository of biomedical literature, contains millions of abstracts and full-text articles that can be used to train PLMs. Other sources include electronic health records (EHRs), clinical trial data, and biomedical ontologies such as SNOMED CT and UMLS. These datasets provide a rich source of domain-specific information that can be used to pre-train models, enabling them to better understand the nuances of medical language. For example, the BioMegatron model, which was pre-trained on a large biomedical corpus, demonstrated significant improvements in performance on standard biomedical NLP benchmarks compared to general-domain models [10]. This highlights the value of domain-specific corpora in enhancing the effectiveness of PLMs for biomedical tasks.

The design and selection of domain-specific corpora play a crucial role in the success of pre-training. Researchers have emphasized the need for high-quality, well-structured datasets that are representative of the target domain. For instance, the Med7 model was pre-trained on a large collection of free-text patient records from the MIMIC-III corpus, allowing it to achieve high performance in named entity recognition tasks [53]. Similarly, the BioBART model was trained on PubMed abstracts to enhance its performance on biomedical language generation tasks [2]. These examples illustrate how the use of domain-specific corpora can lead to significant improvements in model performance.

Another important consideration in domain-specific pre-training is the integration of diverse data sources. Biomedical texts come in various forms, including clinical notes, scientific articles, and patient records, each with its own linguistic structure and terminology. To address this, researchers have explored the use of multi-domain pre-training, where models are trained on a combination of biomedical and general-domain data. For example, the GatorTron model was trained on over 90 billion words of clinical text, including both de-identified EHR data and biomedical literature [181]. This approach allows models to benefit from the general language understanding of large-scale pre-training while also acquiring domain-specific knowledge.

The effectiveness of domain-specific pre-training is further supported by empirical studies that demonstrate its advantages over general-domain models. For instance, a study comparing the performance of general-domain and domain-specific models in the biomedical domain found that the latter outperformed the former in tasks such as named entity recognition and relation extraction [13]. This is attributed to the fact that domain-specific pre-training enables models to better capture the linguistic patterns and terminology used in biomedical texts.

In addition to improving performance, domain-specific pre-training also helps address the challenges of data scarcity in the biomedical domain. Many biomedical NLP tasks suffer from a lack of annotated data, which limits the effectiveness of traditional supervised learning approaches. By pre-training on large-scale, unlabeled biomedical corpora, models can acquire a strong understanding of the domain without requiring extensive manual annotation. This is particularly beneficial for tasks such as clinical text analysis and drug discovery, where annotated data is often limited. For example, the BioLORD-2023 model was trained on a combination of biomedical texts and clinical knowledge graphs, leading to significant improvements in semantic representation learning [77].

Moreover, domain-specific pre-training can also enhance the interpretability and robustness of PLMs in the biomedical domain. Models that are trained on domain-specific data are more likely to produce outputs that align with clinical reasoning and medical knowledge. This is crucial in healthcare applications, where transparency and explainability are essential for trust and adoption. For instance, the BioBART model was shown to perform well in biomedical generation tasks, including dialogue and summarization, by leveraging domain-specific pre-training [2].

In conclusion, the use of domain-specific corpora for pre-training is a vital strategy for improving the performance of PLMs in the biomedical domain. By leveraging large-scale biomedical datasets, models can better understand the unique linguistic patterns and terminologies of medical texts. This approach not only enhances model performance but also addresses the challenges of data scarcity and domain specificity. As the field of biomedical NLP continues to evolve, the importance of domain-specific pre-training is likely to grow, paving the way for more effective and interpretable models.

### 7.4 Model Efficiency and Robustness Improvements

The development of pre-trained language models (PLMs) in the biomedical domain has seen rapid progress, but challenges remain regarding model efficiency and robustness. As biomedical data becomes increasingly large and complex, the need for efficient and robust models that can handle these demands without compromising performance has become a critical research focus. Recent studies have explored various techniques to enhance model efficiency and robustness in biomedical pre-training, including model compression, knowledge distillation, and optimized training strategies. These approaches aim to reduce computational costs, improve scalability, and ensure models can generalize effectively across diverse biomedical tasks and data sources.  

One of the most promising methods for improving model efficiency is model compression. This technique involves reducing the size of pre-trained models while maintaining or even improving their performance. A notable example is the work by "Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events" [20], which demonstrates that distilling large language models (LLMs) into smaller, task-specific models can significantly improve efficiency without sacrificing performance. In their study, a distilled model based on PubMedBERT achieved comparable accuracy to GPT-3.5 without using any labeled data, while being over 1,000 times smaller. This highlights the potential of model compression in making biomedical PLMs more accessible and practical for real-world applications.  

Another key technique for enhancing model efficiency is knowledge distillation. This method involves training a smaller model (the "student") to mimic the behavior of a larger, more complex model (the "teacher"). In the biomedical domain, knowledge distillation has been used to transfer the knowledge from large, general-domain models to specialized biomedical models. For instance, "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models" [23] explores how incorporating biomedical knowledge into the distillation process can enhance the performance of clinical text generation tasks. The authors propose a resource-efficient approach called ClinGen, which infuses knowledge into the process through clinical knowledge extraction and context-informed LLM prompting. Their results show that this approach significantly improves the performance of biomedical NLP tasks while maintaining model efficiency.  

Optimized training strategies have also played a crucial role in improving the efficiency and robustness of biomedical pre-trained models. One such strategy is the use of domain-specific pre-training, where models are trained on biomedical corpora to better capture domain-specific patterns and terminology. "Pre-trained Language Models in Biomedical Domain: A Systematic Survey" [6] emphasizes the importance of domain-specific pre-training in improving the performance of PLMs on biomedical tasks. The authors argue that training on domain-specific data helps models better understand the nuances of biomedical language, leading to more accurate and robust predictions. Additionally, techniques like transfer learning and parameter-efficient fine-tuning have been explored to adapt general-domain models to biomedical tasks with minimal computational overhead.  

Robustness, defined as a model's ability to maintain performance under varying conditions, is another critical aspect of biomedical PLMs. The biomedical domain presents unique challenges, including the presence of ambiguous terminology, limited annotated data, and the need for high interpretability. To address these challenges, researchers have developed methods to improve model robustness. For example, "Extrinsic Factors Affecting the Accuracy of Biomedical NER" [112] investigates the impact of various extrinsic factors, such as corpus annotation schemes and data augmentation techniques, on the performance of NER models. The authors found that combining these techniques can significantly improve model performance, especially in scenarios with limited data. Similarly, "Multi-level biomedical NER through multi-granularity embeddings and enhanced labeling" [129] proposes a hybrid approach that integrates fine-tuned BERT for contextualized word embeddings, a pre-trained multi-channel CNN for character-level information, and a BiLSTM + CRF for sequence labeling. This approach improves the robustness of NER models by capturing both contextual and character-level information, which is particularly useful in dealing with the complexity of biomedical texts.  

In addition to model efficiency and robustness, recent studies have also focused on the interpretability of biomedical PLMs. While large models like GPT-3 and GPT-4 have shown impressive performance, their lack of transparency poses a significant challenge in the biomedical domain, where explainability is crucial for clinical adoption. "Multi-level biomedical NER through multi-granularity embeddings and enhanced labeling" [129] highlights the importance of interpretability in biomedical NLP tasks and proposes methods to improve model transparency. By leveraging enhanced labeling techniques and multi-granularity embeddings, the authors demonstrate that it is possible to achieve high performance while also improving the interpretability of the model's predictions.  

Another important aspect of robustness is the ability of models to handle out-of-distribution data. In the biomedical domain, this is particularly relevant due to the constantly evolving nature of medical knowledge and the presence of rare or unseen entities. "Low Resource Recognition and Linking of Biomedical Concepts from a Large Ontology" [182] addresses this challenge by developing a model that generalizes to entities unseen during training and incorporates linking predictions into mention segmentation decisions. The authors achieve state-of-the-art results on the UMLS ontology, demonstrating the effectiveness of their approach in improving the robustness of biomedical PLMs.  

Finally, the integration of external knowledge sources, such as biomedical knowledge graphs and ontologies, has been shown to enhance both the efficiency and robustness of PLMs. "Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs" [4] explores how lightweight adapter modules can be used to inject structured biomedical knowledge into pre-trained language models (PLMs). The authors use two large knowledge graphs, UMLS and OntoChem, with two prominent biomedical PLMs, PubMedBERT and BioLinkBERT, and show that their methodology leads to performance improvements in several instances while keeping computational requirements low.  

In conclusion, the development of efficient and robust biomedical pre-trained models is a critical area of research that has seen significant advancements in recent years. Techniques such as model compression, knowledge distillation, and optimized training strategies have been successfully applied to improve model efficiency, while approaches like domain-specific pre-training, enhanced labeling, and knowledge integration have been used to enhance model robustness. These advancements have the potential to make biomedical PLMs more scalable, interpretable, and practical for real-world applications, ultimately contributing to the broader goal of advancing biomedical NLP and improving healthcare outcomes.

### 7.5 Cross-Lingual and Multilingual Pretraining

Cross-lingual and multilingual pretraining have emerged as critical areas of research in the biomedical domain, driven by the increasing need to handle multilingual biomedical data and support global health initiatives. The diversity of languages in which biomedical literature is published, coupled with the growing availability of multilingual datasets, has necessitated the development of pre-trained language models (PLMs) that can effectively process and understand text in multiple languages. This subsection explores the advancements in cross-lingual and multilingual pretraining within the biomedical context, highlighting the challenges, methodologies, and recent innovations in this domain.

One of the primary challenges in cross-lingual and multilingual pretraining is the scarcity of annotated biomedical data in non-English languages. While English remains the dominant language in biomedical research, there is a growing body of literature in other languages, such as Chinese, French, and Spanish, which is crucial for global health research [26]. This scarcity poses significant hurdles for training robust models that can generalize across languages. To address this, researchers have focused on leveraging large-scale monolingual corpora and using transfer learning techniques to adapt models trained on English biomedical texts to other languages. For instance, studies have shown that models pre-trained on English biomedical data can be effectively fine-tuned on smaller, multilingual datasets to improve performance on tasks such as named entity recognition and relation extraction [24].

Another critical aspect of cross-lingual and multilingual pretraining is the integration of multilingual data into the training process. This involves not only the use of multilingual corpora but also the development of models that can capture the nuances of different languages while maintaining a unified representation of biomedical knowledge. Recent advancements in multilingual PLMs, such as mBERT and XLM-R, have demonstrated the potential of these models to handle multiple languages simultaneously. However, their effectiveness in the biomedical domain is still an active area of research. For example, the study on the DrBenchmark dataset highlights the importance of evaluating models on a diverse set of tasks in the French biomedical domain, emphasizing the need for robust cross-lingual evaluation frameworks [26].

The development of cross-lingual and multilingual pretraining models also involves addressing the unique challenges posed by biomedical terminology and jargon. Biomedical texts are characterized by a high degree of specialization and variability in terminology, which can vary significantly across languages. This necessitates the creation of domain-specific multilingual corpora that capture the unique vocabulary and concepts of the biomedical field. For instance, the study on the Mol-Instructions dataset demonstrates how the integration of biomolecular-specific instructions can enhance the performance of multilingual models in the biomedical domain [183]. Similarly, the work on the CORAL dataset highlights the importance of expert-curated annotations in capturing the complex semantics of oncology reports, which is crucial for developing multilingual models that can accurately process and understand such data [184].

Recent studies have also explored the use of multilingual pretraining to enhance the performance of models on zero-shot and few-shot tasks. For example, the study on the LLMs in the biomedical domain demonstrates that pretraining on large, diverse corpora can significantly improve the performance of models on tasks where annotated data is scarce [69]. This is particularly relevant in the biomedical context, where the availability of annotated data is often limited, especially for non-English languages. The ability of multilingual models to generalize across languages and tasks is a key advantage, as it allows for the development of models that can be applied to a wide range of biomedical applications without requiring extensive retraining.

Furthermore, the integration of multilingual data into pretraining pipelines has led to the development of novel techniques for domain adaptation and knowledge transfer. For instance, the work on the PPI-BioBERT-x10 model demonstrates how ensemble methods and confidence calibration can be used to improve the performance of models on tasks involving protein-protein interactions and post-translational modifications [185]. Similarly, the study on the MedAgents framework highlights the potential of using multi-disciplinary collaboration to enhance the reasoning capabilities of models in the medical domain, which is particularly relevant in the context of cross-lingual and multilingual pretraining [186].

In addition to technical advancements, the development of cross-lingual and multilingual pretraining models also involves addressing ethical and practical considerations. For example, the study on the ethical implications of LLMs in healthcare emphasizes the need for transparency, accountability, and fairness in the deployment of these models, which is particularly important in the biomedical domain [187]. This includes ensuring that models are trained on diverse and representative datasets to avoid biases and ensure equitable performance across different languages and populations.

In conclusion, the development of cross-lingual and multilingual pretraining models in the biomedical domain is a rapidly evolving area of research, driven by the need to handle multilingual data and support global health initiatives. The challenges of data scarcity, language diversity, and domain-specific terminology have led to the development of novel methodologies and techniques, including transfer learning, ensemble methods, and confidence calibration. These advancements have the potential to significantly improve the performance and applicability of models in the biomedical domain, enabling more accurate and effective processing of multilingual biomedical data. As the field continues to evolve, the integration of cross-lingual and multilingual pretraining will play a crucial role in advancing biomedical research and improving healthcare outcomes.

### 7.6 Self-Supervised and Unsupervised Learning

Self-supervised and unsupervised learning have emerged as critical methodologies in the biomedical pre-training landscape, particularly due to the inherent challenges of data scarcity and the need for models that can generalize effectively across diverse biomedical tasks. These techniques enable models to learn from unannotated data, leveraging the vast amounts of biomedical text available in repositories like PubMed, thereby reducing reliance on costly and time-consuming manual annotations [10]. This is especially significant in the biomedical domain, where the complexity and specialized nature of the language pose additional challenges for traditional supervised learning approaches.

Self-supervised learning, in particular, has gained traction as a powerful alternative to supervised learning for biomedical pre-training. This approach involves training models on large-scale unlabeled datasets, where the model learns to predict parts of the input data based on the remaining parts. For example, in the context of biomedical texts, models can be trained to predict masked words or segments, allowing them to capture the syntactic and semantic structure of the text. This method not only enhances the model's ability to understand the context but also enables it to generalize better to new and unseen biomedical tasks [10].

In the biomedical domain, self-supervised learning has been effectively applied to tasks such as named entity recognition (NER) and relation extraction. The work by [31] highlights the effectiveness of self-supervised learning in adapting pre-trained models to specific biomedical tasks. The authors utilized adversarial learning techniques to address the discrepancies in marginal distributions between general-purpose and biomedical domains, demonstrating that self-supervised learning can enhance model performance without the need for extensive labeled data. This is particularly important in the biomedical field, where the availability of labeled data is often limited.

Unsupervised learning, another critical technique in biomedical pre-training, involves training models without the need for labeled data. This approach is particularly useful in scenarios where the data is not explicitly annotated or where the task is not well-defined. By leveraging the inherent structure of the data, unsupervised learning methods can discover hidden patterns and relationships within the biomedical texts. For instance, the work by [84] demonstrates how unsupervised learning can be used to enhance the retrieval of biomedical knowledge by leveraging knowledge graphs to capture the long-tail knowledge that is often overlooked by traditional methods.

The effectiveness of self-supervised and unsupervised learning in biomedical pre-training is further supported by the work of [135]. In this study, the authors explored the use of scientific language models for knowledge base completion, emphasizing the importance of leveraging the latent knowledge embedded in these models. They demonstrated that pre-trained language models can be fine-tuned for specific biomedical tasks, such as drug design and repurposing, by integrating them with knowledge graph embedding models. This approach not only improves the performance of the models but also enhances their ability to reason about the biomedical data.

Another significant contribution in this area is the work by [20]. The authors explored the use of distillation techniques to improve the efficiency and effectiveness of large language models in biomedical knowledge extraction. They found that while large language models already possess decent competency in structuring biomedical text, distillation into a task-specific student model through self-supervised learning can yield substantial gains. This approach not only reduces the computational resources required but also enhances the model's ability to extract specific biomedical knowledge, such as adverse drug events.

The integration of self-supervised and unsupervised learning techniques in biomedical pre-training is also evident in the work by [186]. This study investigated the use of large language models (LLMs) as collaborators for zero-shot medical reasoning, highlighting the potential of these models to enhance reasoning capabilities in the biomedical domain. The authors proposed a multi-disciplinary collaboration framework that leverages LLM-based agents in a role-playing setting, demonstrating the effectiveness of these models in reasoning about complex biomedical tasks without the need for extensive labeled data.

The importance of self-supervised and unsupervised learning in biomedical pre-training is further underscored by the work of [32]. In this study, the authors presented a method for extracting new facts from biomedical literature without requiring any training data or hand-crafted rules. The system discovered, ranked, and presented the most salient patterns to domain experts in an interpretable form, enabling the creation of high-quality knowledge bases. This approach leverages self-supervised learning to identify and extract relevant information, demonstrating the potential of these techniques in constructing large-scale biomedical knowledge bases.

In addition, the work by [188] highlights the role of self-supervised and unsupervised learning in enhancing scientific creativity through cross-domain knowledge retrieval. The authors developed an exploratory search system that allows end-users to select a portion of text core to their interest from a paper abstract and retrieve papers that have a high similarity to the user-selected core aspect but differ in terms of domains. This system leverages unsupervised learning techniques to identify and retrieve relevant information across different domains, demonstrating the potential of these methods in fostering interdisciplinary research and innovation.

The effectiveness of self-supervised and unsupervised learning in biomedical pre-training is also supported by the work of [60]. This study introduced a comprehensive benchmark for evolving biomedical knowledge graphs, emphasizing the importance of integrating heterogeneous data sources. The authors demonstrated that self-supervised learning can be used to refine and complete biomedical knowledge graphs by leveraging graph representation learning and top-K similarity measures. This approach not only enhances the accuracy of the knowledge graphs but also improves their utility in various biomedical applications.

Overall, the integration of self-supervised and unsupervised learning techniques in biomedical pre-training has shown significant promise in overcoming the challenges of data scarcity and improving model generalization. These methods enable models to learn from large-scale unlabeled data, capturing the complex and specialized language of the biomedical domain. The work of [10], [31], [84], [135], [20], [186], [32], [188], and [60] collectively highlights the transformative potential of these techniques in the biomedical domain.

### 7.7 Domain Adaptation and Transfer Learning

Domain adaptation and transfer learning have become essential techniques in the biomedical pre-training landscape, as they enable pre-trained language models (PLMs) to effectively adapt to biomedical tasks, even when trained on general-domain data. The biomedical domain is characterized by specialized terminology, complex sentence structures, and domain-specific knowledge, which are often absent in general-domain corpora. As a result, directly applying general-domain PLMs to biomedical tasks may lead to suboptimal performance. Domain adaptation and transfer learning techniques aim to bridge this gap by fine-tuning or retraining models on domain-specific data, leveraging pre-trained representations, and integrating external knowledge sources.

One of the most effective approaches to domain adaptation in biomedical NLP is fine-tuning pre-trained models on domain-specific corpora. For instance, the BioBERT model, which is a domain-specific adaptation of BERT, was pre-trained on biomedical texts such as PubMed abstracts and clinical notes [29]. This pre-training process allows the model to capture domain-specific patterns and relationships, which significantly improves its performance on biomedical tasks such as named entity recognition (NER) and relation extraction. Similarly, the BioALBERT model, a domain-specific adaptation of ALBERT, was trained on biomedical and clinical corpora and demonstrated superior performance on various biomedical NLP tasks, including NER, relation extraction, and question answering [40].

Another crucial technique in domain adaptation is transfer learning, which involves leveraging pre-trained models and adapting them to specific biomedical tasks. Transfer learning has shown great promise in scenarios where annotated biomedical data is scarce. For example, the study by Li et al. [169] explored the use of few-shot learning approaches for medical text tasks, such as NER and text classification, where only a small number of labeled examples are available. The study found that transfer learning techniques, such as using pre-trained models and fine-tuning them on limited biomedical data, can significantly improve performance on these tasks. Additionally, the work by Zhang et al. [111] demonstrated that integrating external knowledge resources through prompting strategies can enhance the performance of large language models (LLMs) on biomedical tasks. By incorporating medical knowledge bases, the study achieved substantial improvements in F1 scores on clinical NER tasks.

The integration of domain-specific knowledge is another key aspect of domain adaptation and transfer learning in biomedical NLP. Biomedical tasks often require a deep understanding of medical terminology and relationships between entities. To address this, researchers have explored the use of biomedical knowledge graphs to enhance the performance of pre-trained models. For instance, the study by Chen et al. [139] proposed a method to integrate external knowledge from medical terminology ontologies into word embeddings. The approach involved using a medical knowledge graph, such as the Unified Medical Language System (UMLS), to find connections between clinical terms in cancer pathology reports. The results showed that the model using domain-informed embeddings outperformed the same model using standard word2vec embeddings across all tasks, with significant improvements in F1 scores.

In addition to these approaches, the use of domain-specific corpora for pre-training has been shown to be highly effective in biomedical NLP. The study by Zhang et al. [6] emphasized the importance of leveraging large-scale biomedical datasets to improve the performance of pre-trained models on specialized tasks. The authors highlighted that pre-training models on biomedical corpora enables them to better capture the nuances of biomedical language and improve their performance on downstream tasks such as NER and relation extraction.

Furthermore, the use of transfer learning in combination with domain adaptation techniques has been shown to be highly effective in biomedical NLP. For example, the study by Wang et al. [42] evaluated the performance of GPT-3 and GPT-4 on a variety of biomedical NLP tasks, including NER, relation extraction, and semantic similarity. The results showed that while general-domain models often outperformed biomedical-domain models, biomedical instruction fine-tuning significantly improved performance, even with a much smaller number of instructions. This suggests that transfer learning techniques can be highly effective in adapting general-domain models to biomedical tasks, even with limited domain-specific data.

In conclusion, domain adaptation and transfer learning techniques play a crucial role in enabling pre-trained language models to effectively adapt to biomedical tasks. These techniques include fine-tuning on domain-specific data, transfer learning with limited labeled examples, integration of domain-specific knowledge, and the use of domain-specific corpora for pre-training. As the biomedical NLP field continues to evolve, further research into these techniques will be essential to address the unique challenges of the domain and to improve the performance of pre-trained models on biomedical tasks.

### 7.8 Enhanced Representation Learning

Enhanced representation learning has become a focal point in the development of pre-trained language models (PLMs) for the biomedical domain, as it plays a critical role in capturing the complex semantic and contextual information inherent in biomedical texts. Unlike general language tasks, biomedical texts are often characterized by domain-specific terminology, intricate relationships between entities, and the need for precise contextual understanding. These challenges necessitate the development of advanced representation learning techniques that go beyond traditional methods, enabling models to better understand and leverage the rich information present in biomedical literature.

One of the primary innovations in this area is the integration of knowledge graphs (KGs) into pre-training frameworks. KGs provide structured representations of domain-specific knowledge, which can be leveraged to enhance the semantic understanding of PLMs. For example, studies have shown that incorporating KGs during pre-training can significantly improve a model's ability to reason about biomedical concepts and relationships [189; 89; 190]. By aligning the model's learned representations with the structured information in KGs, these approaches enable better performance on tasks such as entity linking, relation extraction, and biomedical information retrieval.

Another key advancement is the use of domain-specific corpora for pre-training, which allows models to learn more accurate and contextually relevant representations. Biomedical texts, such as scientific articles, clinical notes, and drug descriptions, contain highly specialized language that is often not well represented in general-domain pre-training data. By pre-training on large-scale biomedical datasets, models can better capture the nuances of domain-specific terminology and improve their performance on downstream tasks such as named entity recognition and clinical text analysis [189; 89; 190]. This approach has been shown to be particularly effective in improving model performance when the target task is closely aligned with the pre-training data.

Furthermore, recent work has explored the use of self-supervised learning techniques to enhance representation learning in the biomedical domain. Self-supervised pre-training allows models to learn from vast amounts of unlabeled data by designing pretext tasks that encourage the model to capture meaningful patterns in the input. For instance, the use of masked language modeling (MLM) has been widely adopted in the biomedical domain to pre-train models on large-scale biomedical corpora [189; 89; 190]. By randomly masking certain tokens and training the model to predict them, these approaches enable the model to learn rich contextual representations that are highly effective for downstream tasks.

In addition to traditional MLM, alternative self-supervised objectives have been proposed to further enhance representation learning. For example, the use of contrastive learning has gained significant attention, as it encourages the model to learn representations that are discriminative and invariant to irrelevant variations in the input. This has been shown to improve the model's ability to capture fine-grained semantic relationships between biomedical terms [191; 191]. By comparing positive and negative examples during pre-training, these methods enable the model to learn more robust and generalizable representations.

Another important innovation in representation learning is the development of techniques that explicitly incorporate domain-specific knowledge into the pre-training process. This includes methods such as domain-adaptive pre-training, which involves further pre-training the model on domain-specific data to better align its learned representations with the target task. For instance, studies have shown that domain-adaptive pre-training can significantly improve the performance of PLMs on biomedical tasks by enabling them to better understand the context and relationships between biomedical entities [189; 89; 190]. This approach is particularly valuable in scenarios where the target task is highly specialized and the pre-training data is not well aligned with the target domain.

Moreover, recent work has focused on improving the efficiency of representation learning in the biomedical domain by developing lightweight and parameter-efficient pre-training methods. These approaches aim to reduce the computational and memory costs associated with pre-training while maintaining or even improving model performance. For example, techniques such as knowledge distillation and parameter-efficient fine-tuning have been used to compress large PLMs into smaller, more efficient models that can be deployed in resource-constrained environments [189; 89; 190]. These methods have been shown to be particularly effective in the biomedical domain, where the availability of computational resources can be limited.

In addition to improving the efficiency of pre-training, recent research has also explored the use of hybrid models that combine the strengths of different pre-training objectives. For example, the integration of masked language modeling with contrastive learning has been shown to lead to improved representation learning by encouraging the model to learn both local and global contextual information [191; 191]. This approach has been particularly effective in tasks that require a deep understanding of the semantic and syntactic structure of biomedical texts.

Finally, the role of interpretability in enhanced representation learning has also been a growing area of interest. While many pre-training methods focus on improving performance, there is an increasing recognition of the importance of developing models that are transparent and interpretable, particularly in the biomedical domain where the consequences of model errors can be significant. Techniques such as attention visualization and feature attribution have been used to gain insights into how models process biomedical texts and to identify the key factors that influence their predictions [189; 89; 190]. These methods not only help in improving the reliability of models but also enable clinicians and researchers to better understand and trust the outputs of pre-trained models.

In summary, the field of enhanced representation learning in the biomedical domain has seen significant advancements, driven by the integration of domain-specific knowledge, the use of self-supervised learning techniques, and the development of efficient and interpretable models. These innovations are critical for improving the performance and applicability of pre-trained language models in biomedical tasks, and they represent a promising direction for future research.

## 8 Future Directions and Research Opportunities

### 8.1 Specialized and Efficient Model Development

The development of specialized and efficient pre-trained language models (PLMs) tailored for biomedical tasks is a critical research direction in the field of natural language processing (NLP) and biomedical informatics. As the volume of biomedical data continues to grow, the need for models that can effectively process, understand, and generate domain-specific content becomes increasingly apparent. Specialized PLMs, optimized for biomedical tasks, offer the potential to improve performance on downstream applications such as named entity recognition, clinical text analysis, and drug discovery, while also addressing the unique challenges of the biomedical domain. However, the development of such models requires a careful balance between model size, computational efficiency, and domain-specific optimization.

One of the key challenges in creating specialized biomedical PLMs is the trade-off between model size and computational efficiency. Larger models, such as those with billions of parameters, have shown strong performance on a variety of NLP tasks, but they often require significant computational resources for training and inference. This can be a barrier to their deployment in real-world biomedical applications, where resource constraints are common. Recent studies have explored the potential of smaller, more efficient models that can match or exceed the performance of their larger counterparts while requiring fewer computational resources. For instance, the work on MediSwift [124] demonstrates that sparse pre-training techniques can significantly reduce training costs while maintaining high performance on biomedical tasks. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs, making it a viable option for resource-constrained settings.

In addition to computational efficiency, the development of specialized biomedical PLMs requires a focus on domain-specific optimization. Biomedical texts are characterized by complex terminology, domain-specific abbreviations, and varying document structures, which can challenge the general language understanding capabilities of standard PLMs. To address these issues, researchers have explored various domain adaptation strategies, such as pre-training on biomedical corpora and incorporating domain-specific knowledge into the models. For example, the work on BioBART [2] introduces a generative language model tailored for the biomedical domain, which is pre-trained on PubMed abstracts. This approach has shown improved performance on downstream tasks such as document classification and named entity recognition compared to general-domain models. Similarly, the UMLS-KGI-BERT [1] model integrates biomedical knowledge from the UMLS into the pre-training process, enhancing its ability to recognize and disambiguate biomedical entities.

Another important aspect of specialized and efficient model development is the use of parameter-efficient fine-tuning methods. Fine-tuning large PLMs on domain-specific data can be computationally expensive and time-consuming, especially when labeled data is scarce. To address this, researchers have explored techniques such as adapter layers, low-rank adaptation (LoRA), and other lightweight methods that allow models to be adapted to biomedical tasks with minimal computational overhead. The work on BioInstruct [44] demonstrates the effectiveness of instruction tuning for improving the performance of LLMs in biomedical NLP tasks. By creating a domain-specific instruction dataset and using LoRA for parameter-efficient fine-tuning, the authors achieved significant performance gains on tasks such as question answering, information extraction, and text generation. These results highlight the potential of parameter-efficient fine-tuning techniques in developing specialized biomedical PLMs.

Efficiency in model development also extends to the design of model architectures that can better capture the nuances of biomedical texts. Recent studies have explored the use of domain-specific architectures, such as those incorporating attention mechanisms tailored for biomedical contexts. The work on CLIN-X [150] introduces a pre-trained language model that demonstrates strong performance on a variety of clinical concept extraction tasks. The models architecture, which incorporates cross-sentence context and ensemble learning techniques, allows it to generalize effectively across different tasks and domains. These findings suggest that the design of model architectures plays a crucial role in the development of efficient and specialized biomedical PLMs.

Furthermore, the integration of external knowledge sources, such as biomedical knowledge graphs and ontologies, can enhance the performance of specialized PLMs by providing structured domain-specific information. The work on UMLS-KGI-BERT [1] and the study on knowledge-enhanced pre-trained language models [192] highlight the importance of integrating external knowledge into PLMs to improve their understanding of biomedical concepts. These approaches demonstrate that the incorporation of structured knowledge can lead to significant improvements in tasks such as entity recognition, relation extraction, and natural language inference.

In summary, the development of specialized and efficient pre-trained language models for biomedical tasks requires a multifaceted approach that addresses the challenges of model size, computational efficiency, and domain-specific optimization. Recent studies have shown that techniques such as sparse pre-training, parameter-efficient fine-tuning, and domain-specific architectural designs can significantly enhance the performance of biomedical PLMs. By leveraging these strategies, researchers can create models that are not only effective in handling the complexities of biomedical texts but also efficient in terms of computational resources. As the field continues to evolve, further research into the development of specialized and efficient models will be essential for advancing the application of NLP in biomedical domains.

### 8.2 Integration of Multimodal and Cross-Domain Data

The integration of multimodal and cross-domain data represents a critical avenue for enhancing the performance and generalizability of pre-trained language models (PLMs) in the biomedical domain. As the complexity of biomedical tasks increases, relying solely on textual data becomes insufficient to capture the full spectrum of clinical and biological information. Multimodal data, which encompasses text, images, and biomedical sequences, offers a richer and more comprehensive representation of medical information. By incorporating diverse data types, PLMs can better understand the context, relationships, and nuances present in medical texts, leading to improved accuracy and robustness in tasks such as diagnosis, treatment recommendation, and biomedical research [7].

One of the most prominent examples of multimodal integration in biomedical NLP is the use of medical images. While traditional PLMs excel at processing textual data, they often struggle with interpreting visual information, which is crucial in fields like radiology and pathology. Recent studies have demonstrated that integrating visual data with textual information can significantly enhance the performance of LLMs in tasks such as disease diagnosis and image-based reasoning [193]. For instance, the integration of medical images with natural language processing (NLP) techniques has led to the development of systems that can provide more accurate and interpretable diagnostic recommendations, thereby supporting clinicians in making informed decisions.

Another important aspect of multimodal integration is the inclusion of biomedical sequences, such as DNA and protein sequences. These sequences contain critical information about genetic variations, mutations, and functional relationships that are essential for understanding diseases and developing targeted therapies. Incorporating these sequences into PLMs can enable more accurate identification of gene-disease associations and facilitate the discovery of novel therapeutic targets [70]. For example, studies have shown that LLMs trained on both textual and sequence data can outperform models trained on text alone in tasks such as protein function prediction and mutation analysis.

In addition to multimodal data, leveraging cross-domain knowledge is another promising approach for enhancing the performance of PLMs in the biomedical domain. Cross-domain knowledge refers to the transfer of insights and methodologies from other fields, such as computer vision, bioinformatics, and clinical decision-making, to improve the capabilities of PLMs. By integrating cross-domain knowledge, PLMs can benefit from established techniques and frameworks that have been proven effective in related domains.

One notable example of cross-domain integration is the use of knowledge graphs, which provide structured representations of biomedical knowledge. Knowledge graphs can help PLMs better understand the relationships between entities, such as genes, proteins, and diseases, and enable more accurate reasoning and inference. Recent studies have demonstrated that incorporating knowledge graphs into PLMs can significantly improve their performance in tasks such as relation extraction and entity linking [4]. For instance, the use of knowledge graphs has been shown to enhance the accuracy of biomedical NLP tasks by providing contextual information and reducing the impact of noisy or ambiguous data.

Another important area of cross-domain integration is the use of domain-specific datasets and benchmarks. While general-purpose PLMs are trained on large amounts of text from diverse sources, they may not be adequately prepared for the specific challenges and nuances of biomedical tasks. By fine-tuning PLMs on domain-specific datasets, researchers can improve their performance on biomedical tasks and ensure that the models are better aligned with the needs of the medical community. For example, the development of the Clinical Language Understanding Evaluation (CLUE) benchmark has provided a standardized framework for evaluating PLMs on real-world clinical tasks, thereby facilitating the comparison and improvement of models in the biomedical domain [162].

Furthermore, the integration of cross-domain data can also help address some of the challenges associated with biomedical NLP, such as data scarcity and domain-specific terminology. By leveraging data from other domains, PLMs can gain a broader understanding of language and concepts, which can be beneficial in tasks where the availability of annotated data is limited. For instance, the use of cross-domain data has been shown to improve the performance of PLMs in tasks such as medical question answering and clinical text summarization, where the availability of high-quality annotated data is often a bottleneck [3].

In addition to improving performance, the integration of multimodal and cross-domain data can also contribute to the development of more robust and generalizable PLMs. By exposing models to a wider range of data and tasks, researchers can ensure that the models are better equipped to handle the complexity and variability of real-world biomedical applications. For example, the use of multimodal data has been shown to enhance the ability of PLMs to understand and reason about complex medical scenarios, leading to more accurate and reliable predictions [164].

Moreover, the integration of multimodal and cross-domain data can also facilitate the development of more interpretable and explainable models. By incorporating diverse data sources, researchers can gain insights into how models make decisions and identify potential biases or errors. For instance, the use of multimodal data has been shown to improve the interpretability of LLMs in clinical settings, enabling clinicians to better understand and trust the recommendations provided by the models [194].

In conclusion, the integration of multimodal and cross-domain data is a crucial step toward enhancing the performance and generalizability of pre-trained language models in the biomedical domain. By combining diverse data types and leveraging cross-domain knowledge, researchers can develop more accurate, robust, and interpretable models that are better suited to the complex and evolving needs of biomedical applications. Future research should continue to explore and refine these approaches, ensuring that PLMs are effectively equipped to address the challenges and opportunities of the biomedical field.

### 8.3 Ethical and Legal Considerations

The integration of pre-trained language models (PLMs) in biomedical natural language processing (NLP) presents a unique set of ethical, legal, and societal challenges that must be carefully navigated. As these models become increasingly sophisticated and are deployed in critical healthcare applications, it is essential to address concerns related to data privacy, algorithmic bias, and accountability. These issues are not only technical but also have profound implications for patient care, regulatory compliance, and public trust in AI-driven medical technologies.

One of the most pressing ethical concerns is the protection of patient privacy. Biomedical NLP applications often rely on access to sensitive clinical data, such as electronic health records (EHRs) and biomedical literature, which contain personal health information. The use of such data raises significant privacy risks, particularly when models are trained on datasets that may not have been properly anonymized. For instance, a study on the ethical implications of AI in healthcare highlighted the potential for re-identification of patients through the analysis of de-identified EHRs [108]. This underscores the need for robust data anonymization techniques and stringent data governance frameworks to ensure that patient information is protected throughout the model development and deployment lifecycle.

Another critical issue is the potential for algorithmic bias in PLMs. Pre-trained models, especially those trained on general-domain data, may inherit biases present in their training corpora, leading to disparities in performance across different demographic groups. This is particularly concerning in the biomedical domain, where such biases could result in suboptimal care for underrepresented populations. For example, a recent study on the fairness of medical NLP systems found that certain models exhibited lower accuracy in diagnosing conditions in minority patient groups due to underrepresentation in the training data [13]. To mitigate these risks, it is crucial to develop and implement bias detection and mitigation strategies, including the use of diverse and representative training datasets and the evaluation of model performance across different demographic subgroups.

Accountability is another key ethical and legal consideration in the deployment of PLMs in biomedical NLP. When an AI system makes a decision that impacts patient care, it is essential to establish clear lines of responsibility. However, the complexity and opacity of deep learning models make it challenging to trace the decision-making process and assign accountability. This issue is compounded by the fact that many PLMs are developed by third-party organizations, raising questions about who is ultimately responsible for the model's outputs. A paper on the ethical challenges of AI in healthcare emphasized the need for transparent and explainable AI systems that allow healthcare professionals to understand and verify the model's decisions [108]. This includes the development of tools for model interpretation and the establishment of regulatory frameworks that hold developers and users accountable for the ethical use of AI in healthcare.

In addition to these ethical concerns, there are also significant legal challenges associated with the use of PLMs in biomedical NLP. The collection, processing, and sharing of clinical data must comply with a range of legal regulations, including the Health Insurance Portability and Accountability Act (HIPAA) in the United States and the General Data Protection Regulation (GDPR) in the European Union. These regulations impose strict requirements on data privacy, consent, and security, which must be carefully navigated when developing and deploying AI systems. For instance, the use of synthetic data generated by PLMs to augment real-world datasets can help address some of these challenges, but it also raises questions about the validity and representativeness of the synthetic data [12]. Ensuring that synthetic data is generated in a way that respects patient privacy and meets regulatory standards is a critical area of research.

Furthermore, the use of PLMs in clinical decision-making raises legal questions about the liability of healthcare providers and AI developers. If a model provides incorrect or misleading information that leads to adverse patient outcomes, determining who is responsible can be complex. A recent study on the legal implications of AI in healthcare highlighted the need for clear guidelines on the responsibilities of stakeholders involved in the development and deployment of AI systems [108]. This includes the establishment of legal frameworks that define the roles and responsibilities of developers, healthcare providers, and regulatory bodies in ensuring the safe and effective use of AI in clinical settings.

The societal implications of using PLMs in biomedical NLP also warrant careful consideration. The deployment of AI systems in healthcare can have far-reaching effects on patient trust, healthcare access, and the distribution of medical resources. For example, the use of PLMs to automate tasks such as medical coding and clinical documentation can improve efficiency, but it may also lead to job displacement for healthcare workers. Additionally, the reliance on AI systems for critical decision-making may erode patient trust if the technology is perceived as opaque or untrustworthy. Addressing these societal concerns requires a multidisciplinary approach that involves not only technologists and healthcare professionals but also ethicists, policymakers, and patient advocates.

In summary, the ethical, legal, and societal implications of using pre-trained language models in biomedical NLP are complex and multifaceted. Ensuring the responsible and ethical use of these technologies requires a concerted effort to address issues such as data privacy, algorithmic bias, accountability, and regulatory compliance. By developing robust frameworks for data governance, bias mitigation, and transparency, the biomedical community can harness the potential of PLMs while safeguarding patient rights and promoting equitable healthcare outcomes. As the field continues to evolve, ongoing research and collaboration among stakeholders will be essential to navigate the ethical and legal challenges of AI in healthcare.

### 8.4 Model Interpretability and Explainability

Model interpretability and explainability have become critical concerns in the biomedical domain, where the stakes are high and the consequences of model errors can be severe. The complexity of biomedical language, the high stakes of medical decisions, and the need for trust in clinical settings necessitate models that not only perform well but also provide transparent and interpretable outputs. In this subsection, we focus on the development of interpretable and explainable models in the biomedical domain, drawing on insights from explainable AI (XAI) and linguistics-based approaches to improve model transparency and trust.

One of the primary challenges in achieving interpretability in biomedical NLP models is the inherent complexity of the domain-specific language. Biomedical texts are filled with technical jargon, abbreviations, and specialized terminology that can be difficult for models to decode without explicit guidance. Recent studies have highlighted the need for models that can not only recognize entities but also provide explanations for their predictions, especially in critical areas such as clinical decision-making and drug discovery. For instance, the paper titled "Large Language Models in Biomedical and Health Informatics: A Bibliometric Review" [7] emphasizes the importance of model transparency in the healthcare sector, noting that the lack of interpretability in large language models (LLMs) poses a significant barrier to their adoption in clinical practice. This highlights the urgent need for approaches that can bridge the gap between model performance and explainability.

To address this challenge, researchers have begun to explore XAI techniques tailored for biomedical applications. XAI aims to provide insights into how models make decisions by highlighting the most relevant features or patterns in the input data. In the biomedical domain, this could involve identifying the specific terms or phrases that contribute to a model's prediction of a particular entity or relationship. For example, the paper titled "Biomedical Entity Linking with Triple-aware Pre-Training" [52] discusses the use of knowledge graphs to enhance model interpretability by providing structured information that complements textual data. By incorporating domain-specific knowledge, models can be made more transparent, as the reasoning process becomes more aligned with the structured knowledge of the biomedical domain.

Another approach to improving interpretability in biomedical NLP models involves the use of linguistics-based methods. These approaches leverage insights from natural language processing and computational linguistics to provide explanations for model outputs. For instance, the paper titled "Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019" [195] explores the use of linguistic features, such as syntactic and semantic information, to enhance the interpretability of biomedical models. By incorporating these features, models can be made more interpretable, as the reasoning process becomes more aligned with the linguistic structure of the input text.

Moreover, the integration of model explanations into the training process itself can further enhance interpretability. Techniques such as attention mechanisms, which highlight the most relevant parts of the input text for a given prediction, have been shown to improve model transparency. For example, the paper titled "Biomedical Named Entity Recognition using Flair Embedding and Sequence Tagger" [196] demonstrates the effectiveness of using contextual embeddings and sequence tagging to improve model interpretability. By leveraging the power of contextual embeddings, models can provide more detailed and interpretable predictions, which is particularly valuable in the biomedical domain where precision and accuracy are critical.

In addition to XAI and linguistics-based approaches, there is growing interest in developing hybrid models that combine the strengths of different techniques to enhance interpretability. For instance, the paper titled "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models" [23] explores the use of knowledge-infused prompting to improve the interpretability of LLMs in clinical text generation. By integrating domain-specific knowledge into the prompting process, models can be made more transparent, as the reasoning process becomes more aligned with the structured knowledge of the biomedical domain.

The importance of interpretability and explainability in biomedical NLP is further underscored by the ethical and legal implications of model decisions. In healthcare, the ability to explain how a model arrived at a particular recommendation or diagnosis is crucial for ensuring accountability and building trust among clinicians and patients. The paper titled "Large Language Models in Biomedical and Health Informatics: A Bibliometric Review" [7] highlights the need for models that can provide clear and understandable explanations for their predictions, particularly in high-stakes clinical settings. This underscores the importance of developing interpretable models that can be trusted and used confidently in real-world applications.

Despite these promising developments, there are still significant challenges to overcome in achieving widespread interpretability in biomedical NLP models. One of the key challenges is the lack of standardized evaluation metrics for model interpretability. While there are well-established metrics for model performance, such as F1 score and accuracy, there is a need for metrics that can effectively evaluate the interpretability of model outputs. The paper titled "Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification" [54] highlights the importance of developing such metrics, noting that the current evaluation practices in the biomedical domain are often inadequate for assessing the interpretability of models.

In conclusion, the development of interpretable and explainable models in the biomedical domain is a critical area of research that requires a multidisciplinary approach. By leveraging insights from XAI, linguistics-based methods, and hybrid models, researchers can enhance the transparency and trustworthiness of biomedical NLP models. As the field continues to evolve, the importance of interpretability and explainability will only grow, driven by the need for models that can be trusted and used confidently in real-world clinical settings.

### 8.5 Scalability and Deployment in Real-World Settings

Scalability and deployment of pre-trained language models (PLMs) in real-world biomedical applications present significant challenges that must be addressed to ensure their practical utility and impact. While the development of sophisticated models has advanced rapidly, the transition from research to clinical practice requires not only technical adjustments but also a deep understanding of the operational and ethical implications of deploying these models in healthcare settings. This subsection explores the key challenges and opportunities in scaling and deploying PLMs in real-world biomedical applications, including deployment, integration with clinical workflows, and continuous model updates.

One of the primary challenges in deploying PLMs in healthcare is the computational cost associated with large models. Many state-of-the-art models, such as GPT-4, require substantial computational resources for training and inference, which can be prohibitive in resource-limited settings [108]. This issue is compounded by the need for real-time performance in clinical decision-making, where delays can have serious consequences. To address these challenges, researchers are exploring techniques such as model compression and efficient fine-tuning methods, which aim to reduce the size and computational demands of PLMs without sacrificing performance [64].

Another critical challenge is the integration of PLMs into clinical workflows. The successful deployment of these models requires seamless integration with existing systems, such as electronic health records (EHRs) and clinical decision support systems. However, this integration is often hindered by the lack of standardized interfaces and the complexity of clinical data. For example, the integration of a PLM into a clinical workflow may require extensive customization to handle the specific data formats and terminology used in clinical settings [197]. Moreover, the dynamic nature of clinical data means that models must be regularly updated to reflect new knowledge and practices, which adds another layer of complexity to the deployment process.

Continuous model updates are essential to maintain the accuracy and relevance of PLMs in the rapidly evolving biomedical landscape. However, the process of updating models is often resource-intensive and time-consuming, especially when large amounts of new data need to be processed. For instance, the emergence of new medical knowledge and the discovery of new diseases can necessitate frequent retraining of models to ensure their effectiveness [198]. This challenge is further compounded by the need to ensure that model updates do not introduce biases or errors that could affect clinical outcomes. Techniques such as online learning and incremental training are being explored to address these challenges, allowing models to adapt to new data while maintaining their performance on existing tasks [199].

The deployment of PLMs in real-world settings also raises important ethical and legal considerations. For example, the use of sensitive patient data in training and inference can pose significant privacy risks, necessitating robust data anonymization and security measures [200]. Additionally, the potential for model errors and biases can have serious implications for patient care, requiring thorough validation and testing before deployment. The need for transparent and explainable models is particularly important in healthcare, where the decisions made by models can have life-or-death consequences. Techniques such as model interpretability and explainability are being developed to address these concerns, enabling clinicians to understand and trust the decisions made by PLMs [57].

Moreover, the deployment of PLMs in healthcare requires collaboration between various stakeholders, including researchers, clinicians, and policymakers. This collaboration is essential to ensure that models are developed and deployed in a manner that aligns with clinical needs and regulatory requirements. For instance, the development of models tailored for specific clinical tasks, such as drug discovery and clinical text analysis, requires close collaboration with domain experts to ensure that the models are both effective and practical [201]. Additionally, the creation of standardized evaluation frameworks and benchmarks is crucial to assess the performance of models in real-world settings and to ensure that they meet the high standards required in healthcare [202].

The scalability of PLMs in real-world biomedical applications also depends on the availability of high-quality data. While large-scale datasets such as PubMed and clinical notes provide valuable resources for training and evaluating models, the quality and consistency of these datasets can vary significantly. This variability can affect the performance of models, making it challenging to ensure their reliability in clinical settings. Efforts to improve data quality and consistency, such as the development of standardized annotation schemes and the use of domain-specific corpora, are essential to address these challenges [203].

In addition to technical and operational challenges, the deployment of PLMs in healthcare requires a focus on user experience and usability. Models must be designed to be intuitive and user-friendly, ensuring that clinicians can easily access and interpret the information provided by the models. This includes the development of user interfaces that facilitate the integration of model outputs into clinical workflows and the provision of training and support to ensure that clinicians can effectively use the models. The development of user-centric design principles and the involvement of end-users in the model development process are critical to achieving this goal [204].

Finally, the deployment of PLMs in real-world biomedical applications must be supported by robust infrastructure and continuous monitoring. This includes the development of scalable cloud-based solutions that can handle the computational demands of large models, as well as the implementation of monitoring systems to track model performance and identify potential issues. The continuous monitoring and evaluation of models are essential to ensure that they remain effective and reliable over time, particularly as new data and knowledge become available [204].

In conclusion, the scalability and deployment of pre-trained language models in real-world biomedical applications require a multifaceted approach that addresses technical, operational, ethical, and user-related challenges. By addressing these challenges, researchers and practitioners can ensure that PLMs are effectively integrated into clinical practice, contributing to improved patient outcomes and advancements in biomedical research.

### 8.6 Domain Adaptation and Transfer Learning

Domain adaptation and transfer learning have emerged as critical techniques for addressing the challenges of applying pre-trained language models (PLMs) in specialized biomedical settings where labeled data is scarce. As the biomedical domain is characterized by complex terminology, domain-specific knowledge, and limited annotated datasets, traditional machine learning approaches often struggle to generalize effectively. Therefore, domain adaptation and transfer learning play a pivotal role in bridging the gap between general-purpose models and specialized biomedical tasks, enabling models to adapt to new domains with minimal labeled data [31].

One of the primary approaches to domain adaptation in the biomedical domain is the use of adversarial learning techniques. For instance, the BioADAPT-MRC framework proposes an adversarial learning-based domain adaptation method that addresses the discrepancy in marginal distributions between general-purpose and biomedical domains. By relaxing the need for pseudo-labels, this framework enables the adaptation of models without requiring extensive labeled data from the target domain, significantly improving performance on biomedical machine reading comprehension tasks [31]. Such approaches are essential for scenarios where domain-specific data is limited, as they allow models to leverage knowledge from general domains while adapting to the unique characteristics of the biomedical domain.

Another promising direction is the integration of knowledge graphs (KGs) into domain adaptation and transfer learning. Studies have shown that knowledge graphs can enhance the performance of pre-trained models by providing structured domain-specific knowledge [34]. By aligning biomedical entities and their relationships, knowledge graphs can help models better understand the context and semantics of biomedical texts, improving their ability to generalize to new tasks. For example, the integration of BioBERT and SciBERT-based information extractors with a domain-specific ontology like the OncoNet Ontology (ONO) enables the creation of enriched knowledge graphs that support tasks such as biomarker discovery and interactive question answering [34]. This integration not only enhances the models understanding of the biomedical domain but also improves the accuracy and robustness of downstream tasks.

Transfer learning, particularly in the form of fine-tuning, has also been widely explored in the biomedical domain. The use of domain-specific pre-training has shown significant improvements over general-domain models, as evidenced by the BioMegatron model, which outperforms previous state-of-the-art (SOTA) models on biomedical NLP benchmarks such as named entity recognition, relation extraction, and question answering [10]. This suggests that pre-training on domain-specific corpora is crucial for achieving high performance in biomedical tasks. However, the challenge lies in effectively transferring the knowledge learned from general-domain models to specialized biomedical tasks, especially when labeled data is limited. Techniques such as parameter-efficient fine-tuning, including adapter layers and LoRA, have been proposed to address this challenge by enabling models to be adapted with minimal computational resources [64].

In addition to adversarial learning and knowledge graph integration, domain adaptation techniques that involve additional pretraining on domain-specific data have shown promise. For example, the domain adaptation through pretraining (DAP) approach focuses on adapting pre-trained models by further training them on domain-specific data, allowing them to capture the nuances of biomedical text more effectively [199]. This method is particularly useful in scenarios where the source and target domains differ significantly, as it enables models to learn domain-specific features and improve their performance on specialized tasks.

Moreover, the use of Bayesian and probabilistic approaches has been explored to enhance the robustness and generalization of pre-trained models in biomedical settings. These methods provide a framework for uncertainty quantification, which is essential in medical applications where model reliability and interpretability are critical [205]. By incorporating Bayesian inference into the adaptation and fine-tuning processes, these approaches can improve the reliability of models when applied to new and unseen biomedical data, reducing the risk of overfitting and ensuring more consistent performance.

The effectiveness of domain adaptation and transfer learning techniques is often evaluated using benchmark datasets and metrics tailored to the biomedical domain. For instance, the MedDistant19 benchmark provides a more accurate and comprehensive evaluation of biomedical relation extraction tasks, addressing issues of data overlap and inconsistencies in existing benchmarks [59]. Such benchmarks are crucial for assessing the performance of domain adaptation and transfer learning methods and ensuring that models are evaluated on relevant and representative data.

In addition to these technical advancements, the integration of domain adaptation and transfer learning into practical applications remains a key challenge. For example, in the context of clinical text analysis and drug discovery, models must be adapted to handle the unique characteristics of clinical data, such as the presence of rare entities and the need for high interpretability [103]. Techniques such as entity-aware masking strategies have been proposed to enhance the domain adaptation of models by focusing on the pivotal entities that characterize the biomedical domain [67]. These strategies enable models to better understand and generate domain-specific content, improving their performance on specialized tasks.

Overall, domain adaptation and transfer learning are essential for enabling the effective use of pre-trained language models in the biomedical domain. By leveraging adversarial learning, knowledge graphs, parameter-efficient fine-tuning, and Bayesian approaches, researchers can develop models that adapt to new domains with minimal labeled data, improving their performance on specialized biomedical tasks. As the field continues to evolve, further research into these techniques will be crucial for advancing the application of AI in biomedical research and healthcare.

### 8.7 Collaboration and Knowledge Sharing

Interdisciplinary collaboration and knowledge sharing between researchers, clinicians, and industry stakeholders are essential for driving innovation and addressing the multifaceted challenges in biomedical Natural Language Processing (NLP). As the field of biomedical NLP continues to evolve, the need for collaborative efforts becomes increasingly evident. The integration of diverse perspectives and expertise from various domains can significantly enhance the development and application of pre-trained language models (PLMs) in the biomedical domain. This subsection emphasizes the importance of such collaboration, drawing on insights from existing studies and highlighting the benefits of shared knowledge and joint efforts.

One of the key aspects of interdisciplinary collaboration is the exchange of knowledge between researchers and clinicians. Researchers bring expertise in NLP and machine learning, while clinicians provide domain-specific insights and practical challenges related to biomedical texts. This synergy can lead to the development of more effective and clinically relevant NLP solutions. For instance, the study on the integration of domain knowledge using medical knowledge graphs for cancer phenotyping [139] highlights the importance of combining domain-specific knowledge with NLP techniques. By working closely with clinicians, researchers can better understand the nuances of biomedical texts and develop models that are more aligned with clinical needs.

Furthermore, collaboration between academia and industry is crucial for advancing biomedical NLP. Industry stakeholders often have access to large-scale datasets and real-world applications, which can provide valuable insights for researchers. For example, the LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition [206] demonstrates how industry and academic partnerships can lead to the development of effective NLP systems. By leveraging industry resources and expertise, researchers can enhance the practical applicability of their models and ensure that they meet the needs of real-world biomedical applications.

Knowledge sharing is another critical component of interdisciplinary collaboration. The availability of shared datasets, benchmarking tools, and evaluation frameworks can significantly accelerate research progress. The study on benchmark datasets driving artificial intelligence development [41] emphasizes the importance of creating standardized datasets that reflect the needs of medical professionals. By sharing datasets and benchmarks, researchers can ensure that their models are evaluated on relevant tasks and can be compared fairly with other approaches.

In addition to data sharing, the development of open-source tools and frameworks can foster collaboration and knowledge dissemination. The creation of publicly available datasets and tools, such as the TutorialBank dataset [207], provides a valuable resource for NLP education and research. Such initiatives encourage the development of new models and techniques, as researchers can build upon existing work and contribute to the broader community.

Another important aspect of collaboration is the involvement of multiple stakeholders in the development and evaluation of NLP models. The study on ethical frameworks for the use of NLP in healthcare [119] underscores the need for input from diverse stakeholders, including healthcare professionals, patients, and policymakers. By involving these groups in the development process, researchers can ensure that their models are not only technically sound but also ethically and socially responsible.

Moreover, the establishment of collaborative platforms and communities can facilitate knowledge sharing and support. The creation of online forums, workshops, and conferences can provide opportunities for researchers to share their findings, discuss challenges, and explore new ideas. For example, the study on the use of NLP in maternal healthcare [119] highlights the importance of interactive sessions and surveys to gather insights from healthcare professionals and patients. These initiatives can help researchers understand the practical implications of their work and identify areas for improvement.

The importance of collaboration is also evident in the development of multimodal NLP systems. The integration of text, images, and other data types requires expertise from multiple domains, including computer vision, linguistics, and biomedical informatics. The study on multimodal biomedical applications [208] emphasizes the need for interdisciplinary collaboration to develop models that can effectively handle diverse data sources. By combining expertise from different fields, researchers can create more comprehensive and robust NLP solutions.

In addition to technical collaboration, the ethical and legal considerations of NLP in healthcare require input from legal and policy experts. The study on ethical considerations in NLP [115] highlights the importance of addressing issues such as data privacy, bias, and accountability. By involving legal and policy experts in the development process, researchers can ensure that their models are compliant with ethical standards and regulations.

Finally, the dissemination of research findings and the promotion of open science are vital for fostering collaboration and knowledge sharing. The publication of research papers, the sharing of code and models, and the use of open-access platforms can help researchers reach a wider audience and encourage further innovation. The study on the evaluation of LLMs in biomedical NLP [42] emphasizes the importance of making datasets, baselines, and results publicly available to the community. These efforts can help accelerate research progress and ensure that the benefits of NLP are widely accessible.

In conclusion, interdisciplinary collaboration and knowledge sharing are essential for advancing the field of biomedical NLP. By bringing together researchers, clinicians, and industry stakeholders, the development of more effective and clinically relevant NLP solutions can be achieved. The integration of diverse perspectives and expertise can lead to the creation of models that are not only technically sound but also ethically and socially responsible. Through shared datasets, open-source tools, and collaborative platforms, the biomedical NLP community can continue to innovate and address the challenges of this rapidly evolving field.

### 8.8 Emerging Research Directions

---
The biomedical domain is witnessing a surge of interest in the development of pre-trained language models (PLMs), driven by the need for specialized models that can handle the unique challenges of biomedical text. As the field progresses, several emerging research directions are shaping the future of PLMs in this domain. These include the integration of knowledge graphs, the development of hybrid models, and the exploration of novel training paradigms. These areas not only address the limitations of current models but also open up new possibilities for enhancing the performance and applicability of PLMs in biomedical tasks.

One of the most promising directions is the integration of knowledge graphs (KGs) into the pre-training process of PLMs. Knowledge graphs provide structured information that can complement the unstructured textual data used in traditional pre-training methods. By incorporating KGs, PLMs can better understand the relationships between biomedical entities, leading to improved performance in tasks such as entity linking and relation extraction. For instance, the use of knowledge graphs in pre-training has been shown to enhance the model's ability to capture semantic relationships and contextual information, which is crucial in the biomedical domain [189; 89; 90]. Recent studies have demonstrated that integrating KGs can significantly improve the accuracy of biomedical text analysis, especially in tasks that require reasoning over complex data [189; 89; 90]. The synergy between KGs and PLMs is expected to play a pivotal role in the future of biomedical NLP, enabling more accurate and interpretable models.

Another emerging research direction is the development of hybrid models that combine the strengths of different pre-training approaches. Hybrid models aim to leverage the advantages of both general and domain-specific pre-training by integrating multiple pre-training objectives. For example, the use of masked language modeling (MLM) in combination with other tasks such as next sentence prediction or contrastive learning can lead to more robust and versatile models [209; 191; 210]. Recent advancements in this area have shown that hybrid models can outperform single-objective pre-training methods in various biomedical tasks, such as named entity recognition and clinical text analysis [209; 191; 210]. These models are particularly effective in scenarios where the availability of annotated data is limited, as they can learn from a diverse set of tasks and adapt to new domains more efficiently [209; 191; 210]. The development of hybrid models is expected to continue, with researchers exploring new ways to integrate different pre-training objectives and optimize model performance.

Novel training paradigms are also gaining traction as a means to improve the effectiveness of PLMs in the biomedical domain. One such approach is the use of contrastive learning, which has shown promise in enhancing the representation learning capabilities of models [191; 191]. Contrastive learning involves training models to distinguish between similar and dissimilar inputs, which can lead to more discriminative and robust representations [191; 191]. In the biomedical context, this approach can help models better capture the nuances of medical texts and improve their performance in tasks such as information retrieval and question answering [191; 191]. Another innovative training paradigm is the use of self-supervised learning with domain-specific objectives, which allows models to learn from large amounts of unlabeled data while focusing on the unique characteristics of biomedical texts [211; 212]. These novel training paradigms are expected to play a significant role in the future of PLMs, enabling models to better adapt to the complexities of biomedical data.

In addition to these directions, the use of multimodal data is another area of emerging research. Multimodal PLMs combine text with other modalities such as images, audio, and sensor data to create more comprehensive representations of biomedical information. For example, models that incorporate both textual and visual data can provide a more holistic understanding of medical conditions and treatments [189; 89; 190]. The integration of multimodal data can also enhance the interpretability of models, as it allows for a more direct comparison between different types of information [189; 89; 190]. As the availability of multimodal data in the biomedical domain continues to grow, the development of effective multimodal PLMs will become increasingly important.

Furthermore, the exploration of domain-specific pre-training strategies is another emerging research direction. Domain-specific pre-training involves tailoring the pre-training process to the unique characteristics of biomedical data, such as the use of domain-specific corpora and the incorporation of biomedical knowledge [213; 214; 215]. This approach can help models better understand the specialized terminology and complex structures of biomedical texts, leading to improved performance in downstream tasks [215; 214; 213]. Recent studies have shown that domain-specific pre-training can significantly enhance the effectiveness of PLMs in biomedical applications, especially when combined with transfer learning techniques [215; 214; 213]. The continued development of domain-specific pre-training strategies is expected to play a crucial role in the future of biomedical NLP.

In conclusion, the future of pre-trained language models in the biomedical domain is being shaped by several emerging research directions. The integration of knowledge graphs, the development of hybrid models, the exploration of novel training paradigms, and the use of multimodal data are all promising avenues for advancing the field. These directions not only address the limitations of current models but also open up new possibilities for enhancing the performance and applicability of PLMs in biomedical tasks. As research in these areas continues to evolve, the potential for creating more accurate, interpretable, and effective models in the biomedical domain is becoming increasingly evident.
---


## References

[1] UMLS-KGI-BERT  Data-Centric Knowledge Integration in Transformers for  Biomedical Entity Recognition

[2] BioBART  Pretraining and Evaluation of A Biomedical Generative Language  Model

[3] A Survey for Biomedical Text Summarization  From Pre-trained to Large  Language Models

[4] Diversifying Knowledge Enhancement of Biomedical Language Models using  Adapter Modules and Knowledge Graphs

[5] BIOptimus  Pre-training an Optimal Biomedical Language Model with  Curriculum Learning for Named Entity Recognition

[6] Pre-trained Language Models in Biomedical Domain  A Systematic Survey

[7] Large Language Models in Biomedical and Health Informatics  A  Bibliometric Review

[8] Language Models sounds the Death Knell of Knowledge Graphs

[9] Natural Language Processing in Biomedicine  A Unified System  Architecture Overview

[10] BioMegatron  Larger Biomedical Domain Language Model

[11] Neural translation and automated recognition of ICD10 medical entities  from natural language

[12] Is artificial data useful for biomedical Natural Language Processing  algorithms 

[13] Generalist embedding models are better at short-context clinical  semantic search than specialized embedding models

[14] Biomedical Interpretable Entity Representations

[15] Adapted Large Language Models Can Outperform Medical Experts in Clinical  Text Summarization

[16] DMNER  Biomedical Entity Recognition by Detection and Matching

[17] Inspire the Large Language Model by External Knowledge on BioMedical  Named Entity Recognition

[18] MedCAT -- Medical Concept Annotation Tool

[19] Natural Language Processing in Electronic Health Records in Relation to  Healthcare Decision-making  A Systematic Review

[20] Distilling Large Language Models for Biomedical Knowledge Extraction  A  Case Study on Adverse Drug Events

[21] Biomedical Information Extraction for Disease Gene Prioritization

[22] Information Retrieval Systems Adapted to the Biomedical Domain

[23] Knowledge-Infused Prompting  Assessing and Advancing Clinical Text Data  Generation with Large Language Models

[24] How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction 

[25] Exploring the landscape of large language models in medical question  answering

[26] DrBenchmark  A Large Language Understanding Evaluation Benchmark for  French Biomedical Domain

[27] RJUA-MedDQA  A Multimodal Benchmark for Medical Document Question  Answering and Clinical Reasoning

[28] Updating the Minimum Information about CLinical Artificial Intelligence  (MI-CLAIM) checklist for generative modeling research

[29] BioBERT  a pre-trained biomedical language representation model for  biomedical text mining

[30] Domain Generalization for Medical Image Analysis  A Survey

[31] BioADAPT-MRC  Adversarial Learning-based Domain Adaptation Improves  Biomedical Machine Reading Comprehension Task

[32] Constructing large scale biomedical knowledge bases from scratch with  rapid annotation of interpretable patterns

[33] Building a PubMed knowledge graph

[34] From Large Language Models to Knowledge Graphs for Biomarker Discovery  in Cancer

[35] Generalization in medical AI  a perspective on developing scalable  models

[36] Transformers and the representation of biomedical background knowledge

[37] Adapt-and-Distill  Developing Small, Fast and Effective Pretrained  Language Models for Domains

[38] OpenMEDLab  An Open-source Platform for Multi-modality Foundation Models  in Medicine

[39] BELB  a Biomedical Entity Linking Benchmark

[40] Benchmarking for Biomedical Natural Language Processing Tasks with a  Domain Specific ALBERT

[41] Benchmark datasets driving artificial intelligence development fail to  capture the needs of medical professionals

[42] Large language models in biomedical natural language processing   benchmarks, baselines, and recommendations

[43] DrBERT  A Robust Pre-trained Model in French for Biomedical and Clinical  domains

[44] BioInstruct  Instruction Tuning of Large Language Models for Biomedical  Natural Language Processing

[45] Attention Is All You Need

[46] BERT  Pre-training of Deep Bidirectional Transformers for Language  Understanding

[47] Self-training Improves Pre-training for Natural Language Understanding

[48] Universal Language Model Fine-tuning for Text Classification

[49] RoBERTa  A Robustly Optimized BERT Pretraining Approach

[50] mT5  A massively multilingual pre-trained text-to-text transformer

[51] ALBERT  A Lite BERT for Self-supervised Learning of Language  Representations

[52] Biomedical Entity Linking with Triple-aware Pre-Training

[53] Med7  a transferable clinical natural language processing model for  electronic health records

[54] Evaluation of ChatGPT Family of Models for Biomedical Reasoning and  Classification

[55] Scientific Large Language Models  A Survey on Biological & Chemical  Domains

[56] Multimodal Deep Learning

[57] Contrastive Explanations for Model Interpretability

[58] Knowledge Integration for Conditional Probability Assessments

[59] MedDistant19  Towards an Accurate Benchmark for Broad-Coverage  Biomedical Relation Extraction

[60] Know2BIO  A Comprehensive Dual-View Benchmark for Evolving Biomedical  Knowledge Graphs

[61] KBioXLM  A Knowledge-anchored Biomedical Multilingual Pretrained  Language Model

[62] Pre-training Text Representations as Meta Learning

[63] Language Model Adaptation to Specialized Domains through Selective  Masking based on Genre and Topical Characteristics

[64] On the Effectiveness of Parameter-Efficient Fine-Tuning

[65] A Survey of Parameters Associated with the Quality of Benchmarks in NLP

[66] Unsupervised Pre-training for Biomedical Question Answering

[67] Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies

[68] Thinking about GPT-3 In-Context Learning for Biomedical IE  Think Again

[69] A Comprehensive Evaluation of Large Language Models on Benchmark  Biomedical Text Processing Tasks

[70] Gene-associated Disease Discovery Powered by Large Language Models

[71] How well do LLMs cite relevant medical references  An evaluation  framework and analyses

[72] Cross-lingual Candidate Search for Biomedical Concept Normalization

[73] Publicly Available Clinical BERT Embeddings

[74] ClinicalMamba  A Generative Clinical Language Model on Longitudinal  Clinical Notes

[75] Surf at MEDIQA 2019  Improving Performance of Natural Language Inference  in the Clinical Domain by Adopting Pre-trained Language Model

[76] Data Augmentation for Low-Resource Named Entity Recognition Using  Backtranslation

[77] BioLORD-2023  Semantic Textual Representations Fusing LLM and Clinical  Knowledge Graph Insights

[78] Effective Use of Bidirectional Language Modeling for Transfer Learning  in Biomedical Named Entity Recognition

[79] Neural Basis Models for Interpretability

[80] Progressive and Multi-Path Holistically Nested Neural Networks for  Pathological Lung Segmentation from CT Images

[81] A Bibliometric Review of Large Language Models Research from 2017 to  2023

[82] A note on the undercut procedure

[83] Multi-domain improves out-of-distribution and data-limited scenarios for  medical image analysis

[84] Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge

[85] LED down the rabbit hole  exploring the potential of global attention  for biomedical multi-document summarisation

[86] Investigating Large Language Models and Control Mechanisms to Improve  Text Readability of Biomedical Abstracts

[87] DICT-MLM  Improved Multilingual Pre-Training using Bilingual  Dictionaries

[88] MDAPT  Multilingual Domain Adaptive Pretraining in a Single Model

[89] Gaussian Splatting in Style

[90] Cross-lingual Visual Pre-training for Multimodal Machine Translation

[91] FutureTOD  Teaching Future Knowledge to Pre-trained Language Model for  Task-Oriented Dialogue

[92] The birth of Romanian BERT

[93] Biased Self-supervised learning for ASR

[94] Unsupervised Improvement of Factual Knowledge in Language Models

[95] Task Transfer and Domain Adaptation for Zero-Shot Question Answering

[96] BERT's output layer recognizes all hidden layers  Some Intriguing  Phenomena and a simple way to boost BERT

[97] Utilizing Bidirectional Encoder Representations from Transformers for  Answer Selection

[98] How Does BERT Answer Questions  A Layer-Wise Analysis of Transformer  Representations

[99] Information Extraction from Swedish Medical Prescriptions with  Sig-Transformer Encoder

[100] Span Selection Pre-training for Question Answering

[101] Pre-training technique to localize medical BERT and enhance biomedical  BERT

[102] Epigenomic language models powered by Cerebras

[103] Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model

[104] Text Summarization with Pretrained Encoders

[105] KG-BERT  BERT for Knowledge Graph Completion

[106] The Swapped Dragonfly

[107] Clinical Prompt Learning with Frozen Language Models

[108] Large language models in healthcare and medical domain  A review

[109] Improving Small Language Models on PubMedQA via Generative Data  Augmentation

[110] Extracting Biomedical Factual Knowledge Using Pretrained Language Model  and Electronic Health Record Context

[111] LLMs in Biomedicine  A study on clinical Named Entity Recognition

[112] Extrinsic Factors Affecting the Accuracy of Biomedical NER

[113] Pre-Training Transformers for Domain Adaptation

[114] Deep Learning

[115] Ensuring the Inclusive Use of Natural Language Processing in the Global  Response to COVID-19

[116] An Introduction to Natural Language Processing Techniques and Framework  for Clinical Implementation in Radiation Oncology

[117] Deep Learning, Natural Language Processing, and Explainable Artificial  Intelligence in the Biomedical Domain

[118] Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts

[119] NLP for Maternal Healthcare  Perspectives and Guiding Principles in the  Age of LLMs

[120] Language Models are Few-Shot Learners

[121] Language Models are Few-shot Multilingual Learners

[122] PaLM  Scaling Language Modeling with Pathways

[123] Credimus

[124] MediSwift  Efficient Sparse Pre-trained Biomedical Language Models

[125] Do We Still Need Clinical Language Models 

[126] Interpreting Pretrained Language Models via Concept Bottlenecks

[127] BiomedCLIP  a multimodal biomedical foundation model pretrained from  fifteen million scientific image-text pairs

[128] Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for  Predicting Drug-Review Satisfaction

[129] Multi-level biomedical NER through multi-granularity embeddings and  enhanced labeling

[130] How far is Language Model from 100% Few-shot Named Entity Recognition in  Medical Domain

[131] MedMine  Examining Pre-trained Language Models on Medication Mining

[132] Rebuttal to Berger et al., TOPLAS 2019

[133] A comment on Guo et al. [arXiv 2206.11228]

[134] Reply to the commentary  Be careful when assuming the obvious , by P.  Alday

[135] Scientific Language Models for Biomedical Knowledge Base Completion  An  Empirical Study

[136] Iteratively Improving Biomedical Entity Linking and Event Extraction via  Hard Expectation-Maximization

[137] Domain Adaptation for the Segmentation of Confidential Medical Images

[138] A Biomedical Information Extraction Primer for NLP Researchers

[139] Integration of Domain Knowledge using Medical Knowledge Graph Deep  Learning for Cancer Phenotyping

[140] On Masked Pre-training and the Marginal Likelihood

[141] Self-Supervised Visual Representation Learning Using Lightweight  Architectures

[142] Robust wav2vec 2.0  Analyzing Domain Shift in Self-Supervised  Pre-Training

[143] SLAM  A Unified Encoder for Speech and Language Modeling via Speech-Text  Joint Pre-Training

[144] Knowledge Graph semantic enhancement of input data for improving AI

[145] Finding Similar Medical Questions from Question Answering Websites

[146] Recent Advances in Text Analysis

[147] General Cross-Architecture Distillation of Pretrained Language Models  into Matrix Embeddings

[148] How fine can fine-tuning be  Learning efficient language models

[149] Accelerating Training of Transformer-Based Language Models with  Progressive Layer Dropping

[150] CLIN-X  pre-trained language models and a study on cross-task transfer  for concept extraction in the clinical domain

[151] Towards solving the 7-in-a-row game

[152] Schur Number Five

[153] Listing 4-Cycles

[154] You Only Explain Once

[155] Ten times eighteen

[156] P_3-Games

[157] Comprehensive Study on German Language Models for Clinical and  Biomedical Text Understanding

[158] Don't Stop Pretraining  Adapt Language Models to Domains and Tasks

[159] SemEval-2024 Task 2  Safe Biomedical Natural Language Inference for  Clinical Trials

[160] MedNLI Is Not Immune  Natural Language Inference Artifacts in the  Clinical Domain

[161] A Survey of Large Language Models in Medicine  Progress, Application,  and Challenge

[162] CLUE  A Clinical Language Understanding Evaluation for LLMs

[163] Extraction of Medication and Temporal Relation from Clinical Text using  Neural Language Models

[164] Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and  Symptom Analysis

[165] Evaluation of General Large Language Models in Contextually Assessing  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record  Notes

[166] A Comprehensive Survey on Evaluating Large Language Model Applications  in the Medical Industry

[167] SAM-Med2D

[168] BioBridge  Bridging Biomedical Foundation Models via Knowledge Graphs

[169] Few-shot learning for medical text  A systematic review

[170] A Biomedical Pipeline to Detect Clinical and Non-Clinical Named Entities

[171] Reproducibility in Learning

[172] Pre-trained Language Model for Biomedical Question Answering

[173] Analyzing the Interpretability Robustness of Self-Explaining Models

[174] Hierarchical Transformers for Long Document Classification

[175] Understanding and Enhancing Robustness of Concept-based Models

[176] Position-based Prompting for Health Outcome Generation

[177] Evaluating Large Language Models for Health-Related Text Classification  Tasks with Public Social Media Data

[178] Empowering Language Model with Guided Knowledge Fusion for Biomedical  Document Re-ranking

[179] Medical Foundation Models are Susceptible to Targeted Misinformation  Attacks

[180] LLMs-Healthcare   Current Applications and Challenges of Large Language  Models in various Medical Specialties

[181] GatorTron  A Large Clinical Language Model to Unlock Patient Information  from Unstructured Electronic Health Records

[182] Low Resource Recognition and Linking of Biomedical Concepts from a Large  Ontology

[183] Mol-Instructions  A Large-Scale Biomolecular Instruction Dataset for  Large Language Models

[184] CORAL  Expert-Curated medical Oncology Reports to Advance Language Model  Inference

[185] Large-scale protein-protein post-translational modification extraction  with distant supervision and confidence calibrated BioBERT

[186] MedAgents  Large Language Models as Collaborators for Zero-shot Medical  Reasoning

[187] Medical Deep Learning -- A systematic Meta-Review

[188] Augmenting Scientific Creativity with Retrieval across Knowledge Domains

[189] iSLAM  Imperative SLAM

[190] Unified Speech-Text Pre-training for Speech Translation and Recognition

[191] A Primer on Contrastive Pretraining in Language Processing  Methods,  Lessons Learned and Perspectives

[192] Knowledge Enhanced Pretrained Language Models  A Compreshensive Survey

[193] ChatCAD  Interactive Computer-Aided Diagnosis on Medical Image using  Large Language Models

[194] Diagnostic Reasoning Prompts Reveal the Potential for Large Language  Model Interpretability in Medicine

[195] Linguistically Informed Relation Extraction and Neural Architectures for  Nested Named Entity Recognition in BioNLP-OST 2019

[196] BioNerFlair  biomedical named entity recognition using flair embedding  and sequence tagger

[197] Learning Transferable Parameters for Unsupervised Domain Adaptation

[198] Scaling Out-of-Distribution Detection for Real-World Settings

[199] Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining  and Consistency

[200] On the Ethical Considerations of Text Simplification

[201] Molecule Generation for Drug Design  a Graph Learning Perspective

[202] Multi-lingual Evaluation of Code Generation Models

[203] Knowledge Guided Named Entity Recognition for BioMedical Text

[204] Knowledge Sharing in Coalitions

[205] Bayesian Probabilistic Numerical Methods

[206] LASIGE and UNICAGE solution to the NASA LitCoin NLP Competition

[207] TutorialBank  A Manually-Collected Corpus for Prerequisite Chains,  Survey Extraction and Resource Recommendation

[208] Text Generation with Text-Editing Models

[209] Frustratingly Simple Pretraining Alternatives to Masked Language  Modeling

[210] Variance-reduced Language Pretraining via a Mask Proposal Network

[211] Self-Supervised Representation Learning for Online Handwriting Text  Classification

[212] Lacuna Reconstruction  Self-supervised Pre-training for Low-Resource  Historical Document Transcription

[213] Masked Faces with Faced Masks

[214] Do not Mask Randomly  Effective Domain-adaptive Pre-training by Masking  In-domain Keywords

[215] Enhancing Biomedical Text Summarization and Question-Answering  On the  Utility of Domain-Specific Pre-Training


