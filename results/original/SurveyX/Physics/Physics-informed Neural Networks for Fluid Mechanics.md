# Deep Learning in Partial Differential Equations and Fluid Mechanics: A Survey

www.surveyx.cn

# Abstract

This survey paper systematically examines the integration of deep learning with partial differential equations (PDEs) in computational fluid dynamics (CFD) and physics-based modeling, focusing on Physics-Informed Neural Networks (PINNs) as a transformative paradigm. We present a comprehensive analysis of PINN architectures, training methodologies, and their applications in solving Navier-Stokes equations, turbulence modeling, and multiscale fluid phenomena. The paper critically evaluates hybrid approaches that combine neural networks with traditional numerical methods, highlighting their advantages in accuracy and computational efficiency while maintaining physical consistency. Key challenges in scalability, generalization, and uncertainty quantification are discussed, along with emerging solutions such as domain decomposition techniques and adaptive activation functions. The survey also explores cutting-edge applications in geophysical flows, medical imaging, and multiphysics systems, identifying future research directions in high-dimensional PDE solving, interpretable AI, and multi-physics coupling. By synthesizing recent advancements and persistent limitations, this work provides a roadmap for advancing deep learning in computational physics, emphasizing the need for theoretically-grounded approaches that bridge machine learning with fundamental physical principles.

# 1 Introduction

The intersection of deep learning and traditional computational methods represents a significant frontier in the field of applied mathematics and engineering. Among the various domains where this integration has shown promise, computational fluid dynamics (CFD) stands out due to its complex nature and the critical role that partial differential equations (PDEs) play in modeling fluid behavior. The rapid advancements in deep learning techniques have opened new avenues for solving PDEs more efficiently and accurately, thereby enhancing simulation capabilities. This review paper aims to provide an exhaustive survey of the current state of research at this intersection, focusing on the methodologies, applications, and challenges faced in integrating deep learning with PDEs. By systematically exploring these themes, this paper seeks to illuminate the potential for future developments in this dynamic area of study.

# 1.1 Structure of the Survey

This survey systematically explores the integration of deep learning with partial differential equations (PDEs) in computational fluid dynamics (CFD) and physics-based modeling. The paper begins with foundational concepts in Section 2, covering deep learning fundamentals, PDE theory, and traditional CFD methods. This foundational knowledge is essential for understanding how deep learning can be effectively applied to solve complex fluid dynamics problems. By establishing a clear connection between traditional methods and modern machine learning techniques, the survey sets the stage for a more in-depth discussion of the specific applications and innovations that follow. Section 3 introduces Physics-Informed Neural Networks (PINNs), detailing their architecture and training methodologies [1]. The significance of PINNs lies in their ability to incorporate physical laws directly into the learning process, thereby enhancing the accuracy and reliability of the models developed.

![](images/16e778382650adbfbfe573ef578fc5eaecd0b3f3ae6d2cbfddb487dc0af496ed.jpg)  
Figure 1: chapter structure

Section 4 examines deep learning applications in CFD, including neural network architectures for fluid simulations and solutions to Navier-Stokes equations. This section highlights the transformative impact of deep learning on traditional CFD approaches, showcasing how neural networks can outperform conventional numerical methods in both speed and accuracy. Furthermore, it discusses various architectures that have been proposed, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), emphasizing their specific advantages in handling the complexities inherent in fluid dynamics. Section 5 analyzes machine learning approaches for PDEs in fluid mechanics, comparing data-driven methods with hybrid modeling techniques. This comparative analysis not only sheds light on the strengths and weaknesses of each approach but also identifies gaps in the current literature that warrant further exploration.

The survey concludes with Section 6’s critical discussion of current challenges and future directions, addressing scalability, generalization, and emerging applications in this interdisciplinary field. The challenges highlighted are crucial for advancing the integration of deep learning into CFD, as they encompass both theoretical and practical aspects that need to be addressed for widespread adoption. By identifying these challenges and proposing potential solutions, the paper aims to pave the way for future research that can enhance the robustness and applicability of deep learning techniques in solving PDEs. This structured approach not only facilitates a comprehensive understanding of the current landscape but also encourages ongoing dialogue and exploration within the research community.The following sections are organized as shown in Figure 1.

# 2 Background and Preliminaries

# 2.1 Fundamentals of Deep Learning and Neural Networks

Deep learning, particularly through artificial neural networks (ANNs), underpins the computational approach to solving partial differential equations (PDEs) in fluid mechanics. ANNs, composed of interconnected neuron layers optimized via gradient-based methods, excel in learning complex input-output mappings, essential for modeling intricate physical systems [2]. Three frameworks integrate physical knowledge into neural networks: Physics-Guided Neural Networks $( \mathrm { P g N N s } )$ impose physics constraints on data-driven models; Physics-Informed Neural Networks (PINNs) embed governing equations in loss functions [3]; and Physics-Encoded Neural Networks (PeNNs) hard-code physical laws into the architecture [4]. Each framework enhances neural network performance where physical fidelity is crucial.

Training neural networks for physical systems involves challenges in convergence and efficiency. Traditional stochastic gradient descent methods often require adaptations for complex loss landscapes in PDE solutions [5]. Innovations like locally adaptive activation functions and Moderate Adaptive Linear Units (MoLU) improve convergence and accuracy by addressing slow learning dynamics [6, 7]. Hybrid approaches integrating supervised machine learning with PINNs show promise in applications such as temperature distribution prediction, illustrating their versatility [8].

Specialized architectures cater to specific computational physics applications. Transformer networks with physics-informed learning enhance orientation estimation in sequential data [9], while WarpPINN applies PINNs for image registration in medical imaging [10]. Physics-informed Gated Recurrent Networks (PiGRN) predict joint torques from sEMG signals, adhering to physical laws [11]. These architectures improve predictive capabilities while grounding models in physical realities.

The integration of neural networks with traditional numerical methods shows promise in fluid mechanics. Differentiable finite element solvers facilitate seamless integration with machine learning frameworks [12], and multiscale architectures like MscalePINNs capture phenomena across scales [13]. Uncertainty quantification techniques, such as neural network representations of probability distributions in stochastic systems, further enhance modeling capabilities [14].

# 2.2 Partial Differential Equations (PDEs) in Physical Modeling

Partial Differential Equations (PDEs) form the mathematical backbone for modeling continuumscale physical phenomena across engineering and scientific fields, especially in fluid mechanics where they embody conservation laws. The Navier-Stokes equations, governing viscous fluid flow through nonlinear coupled PDEs, exemplify this role. Advances in computational methods, including Physics-Informed Neural Networks (PINNs), have improved the approximation of solutions to these equations, addressing complex behaviors like nonlinear dynamics and shock wave formation. Attention mechanisms in neural networks overcome traditional limitations of numerical discretization, enabling accurate modeling of intricate systems [15, 16, 17].

The Navier-Stokes equation $\begin{array} { r } { \rho \left( \frac { \partial \mathbf { v } } { \partial t } + \mathbf { v } \cdot \nabla \mathbf { v } \right) = - \nabla p + \mu \nabla ^ { 2 } \mathbf { v } + \mathbf { f } } \end{array}$ describes fluid motion, where $\rho$ is fluid density, v the velocity vector, $p$ pressure, $\mu$ dynamic viscosity, and f external forces. This equation balances inertial forces with pressure gradients and viscous forces, providing a framework for fluid dynamics modeling [18, 16, 19, 20]. The versatility of these equations underscores their central role in fluid dynamics, necessitating effective computational methods for their solution.

Traditional numerical methods face challenges in accuracy and efficiency [21]. Physics-informed deep learning approaches excel in incompressible laminar flows without extensive data [22], proving advantageous in data-scarce scenarios. PDE-based modeling spans diverse systems, from nonlinear wave phenomena to interfacial dynamics in multilayer materials and convection-diffusion equations [23, 24, 25]. These applications illustrate the breadth of PDEs in modeling and the ongoing refinement of computational techniques for their solution.

Recent advancements address limitations across domains, integrating quantum computing with AI in quantum chemistry, generative AI in geoscience, and multiobjective optimization in machine learning [26, 27, 28, 29, 30]. Generative AI and large language models in geoscience enable innovative applications, enhancing data-driven modeling and uncertainty quantification while addressing challenges like interpretability and misuse [31, 26].

# 2.3 Computational Fluid Dynamics (CFD) and Traditional Numerical Methods

Traditional numerical methods in computational fluid dynamics (CFD), such as the Finite Difference Method (FDM), Finite Element Method (FEM), and Finite Volume Method (FVM), have been foundational in solving fluid flow PDEs [32]. Despite their success, these methods face challenges like long simulation times, high computational demands, and energy consumption [33]. Their computational intensity limits real-time application, prompting exploration of machine learning-enhanced alternatives.

Traditional CFD methods struggle with nonlinear PDEs and complex boundary conditions, constrained by slow generalization rates and high computational costs for fractional derivatives [34, 35]. Complex geometries further demand significant computational resources [36]. The complexity of modern engineering problems necessitates efficient computational strategies accommodating intricate fluid dynamics.

Comparative analyses reveal trade-offs in traditional CFD methods concerning accuracy, efficiency, and complex flow phenomena handling. While finite-difference methods achieve high accuracy quickly, physics-informed neural networks (PINNs) solve Navier-Stokes equations without extensive data, though they struggle with dynamic flows like vortex shedding [37, 38, 39]. Data assimilation with turbulence models enhances mean flow reconstruction, outperforming traditional methods by reducing errors in steep gradient regions.

Generative Artificial Intelligence (GAI) and large language models in geoscience foster innovative applications, enhancing data-driven modeling and uncertainty quantification while addressing interpretability and misuse challenges [31, 26].

The challenges of traditional CFD methods have led to hybrid approaches combining numerical methods with machine learning, overcoming computational bottlenecks while maintaining physical consistency in simulations. This trend reflects a broader movement towards integrating advanced computational techniques to enhance simulation efficiency and accuracy, enabling researchers to tackle increasingly complex physical systems effectively.

# 3 Physics-Informed Neural Networks (PINNs)

<html><body><table><tr><td>Category</td><td>Feature</td><td>Method</td></tr><tr><td>Architecture and Formulation of PINNs</td><td>Fourier Analysis</td><td>k-PINN[40]</td></tr><tr><td rowspan="3">Training Methodologies and Optimization</td><td>Error Correction Techniques</td><td>SR-PINN[41]</td></tr><tr><td> Adversarial ndProbabilitig -pgroaches</td><td>WGAPN-PNNs[42]</td></tr><tr><td>Optimization with Physical Constraints</td><td>nPINNs[44]</td></tr></table></body></html>

Table 1: This table provides a comprehensive overview of various methodologies for enhancing Physics-Informed Neural Networks (PINNs). It categorizes advancements in architecture formulation and training optimization, detailing specific techniques like Fourier Analysis, Error Correction, and Hierarchical Learning Strategies, which contribute to improved accuracy and efficiency in solving partial differential equations.

Table 3 offers a comprehensive comparison of various methodologies applied within the framework of Physics-Informed Neural Networks (PINNs), emphasizing their unique architectural and optimization strategies as well as their application domains. Physics-Informed Neural Networks (PINNs) offer a significant advance by integrating known physical laws into the architecture of neural networks to solve partial differential equations (PDEs) with high accuracy while depending less on large datasets. Table 1 presents a detailed summary of the diverse methods employed in the architecture and optimization of Physics-Informed Neural Networks (PINNs), highlighting their significance in advancing computational efficiency and accuracy. The PINN framework bridges the gap between physics and machine learning, ensuring that models adhere to physical laws, which improves the predictive capability of complex systems.

# 3.1 Architecture and Formulation of PINNs

PINNs utilize deep learning architectures to encode physical constraints comprehensively, offering a novel approach for solving PDEs. The architecture employs a composite loss function incorporating terms corresponding to PDEs, boundary conditions, and initial conditions. Enhancements like the self-adaptive loss balancing scheme, ReLoBRaLo, optimize the training process by fine-tuning the balance between loss terms, boosting accuracy with minimal computational burden across benchmark equations [45, 46].

The core mathematical formulation of PINNs involves minimizing an objective function that quantifies residual errors at collocation points. The loss function $\mathcal { L }$ comprises $\mathcal { L } _ { P D E }$ , $\mathcal { L } _ { B C }$ , $\mathcal { L } _ { I C }$ , and $\mathcal { L } _ { D a t a }$ , each weighted by coefficients $\lambda$ , designing a learning model consistent with both empirical data and governing PDEs [30, 3, 34]. This formulation strengthens the model’s capability to predict physical behaviors across multifaceted domains, such as biology and engineering, even with limited data [47, 16, 20, 18].

Innovative architectural developments expand PINN applicability. Variational PINNs (hp-VPINNs) merge variational methods with neural networks for differential equations [43], while Nonlocal PINNs (nPINNs) address nonlocal models, allowing parameter inference from scant observational data [44]. For vibrational analysis, k-space PINNs employ Fourier functions to enhance dynamic response learning [40].

Integrating generative AI with PINNs, such as through GANs and transformer-based architectures, tackles fundamental training challenges in geoscience, enhancing model reliability and convergence [27, 26]. These improvements bolster PINNs’ role in predictive modeling within climate science and urban planning, addressing concerns of interpretability and misuse [31, 29].

Theoretical analyses affirm PINNs’ convergence properties, particularly for linear second-order elliptic and parabolic PDEs, resulting in strong convergence to true PDE solutions as data expands [48, 47]. This integration of physical laws in training distinguishes PINNs from traditional models, proving crucial in data-lean settings and positioning them as pivotal in scientific machine learning advancements across various disciplines.

# 3.2 Training Methodologies and Optimization

<html><body><table><tr><td>Method Name</td><td>Optimization Strategies</td><td>Integration of Domain Knowledge</td><td>Training Efficiency</td></tr><tr><td>SR-PINN[41]</td><td>Vanishing Viscosity Technique</td><td>Partial Differential Equations</td><td>Domain Decomposition</td></tr><tr><td>WGAN-PINNs[42]</td><td>Groupsort Activation Functions</td><td>Physical Constraints Integration</td><td></td></tr><tr><td>hp-VPINNs[43]</td><td>Gradient Descent Methods</td><td>Localized Learning</td><td>Domain Decomposition</td></tr><tr><td>nPINNs[44]</td><td>Implicit Regularization Terms</td><td>Governing Equations</td><td>Limited Observational Data</td></tr></table></body></html>

Table 2: Summary of training methodologies and optimization strategies utilized in various PhysicsInformed Neural Network (PINN) architectures. This table details the integration of domain knowledge, optimization strategies employed, and the effect on training efficiency for each method. The enhancements are targeted at improving accuracy and computational efficiency in solving complex partial differential equations.

Specialized optimization strategies enhance the training of PINNs, harmonizing physical accuracy with computational efficiency. Innovations like the vanishing viscosity technique and stacked residual PINN architectures address challenges in solution discontinuities [41]. The WGAN-PINNs framework infuses a probabilistic perspective into PINN training, factoring physical constraints into adversarial processes to bolster uncertainty quantification [42]. Moreover, hp-VPINNs leverage global-local learning and hierarchical basis functions to improve approximation efficacy [43].

Key optimizations focus on collocation point strategy, domain knowledge integration from PDEs, and advanced weighting mechanisms utilizing residuals for accelerated training [30, 27, 49]. These developments, alongside the integration of generative AI, advance applications like data augmentation and super-resolution in fields such as climate and atmospheric science, albeit with challenges in interpretability and model trustworthiness remaining [31, 29, 26]. Table 2 provides an overview of different PINN methodologies, focusing on optimization strategies, domain knowledge integration, and training efficiency to improve the computational performance and accuracy of solutions to complex system equations.

As depicted in Figure 2, this figure illustrates the hierarchical structure of training methodologies for Physics-Informed Neural Networks (PINNs), highlighting optimization strategies, collocation point strategies, and implicit regularization techniques. Each category encompasses specific advancements that enhance the accuracy, efficiency, and applicability of PINNs in solving complex partial differential equations. Implicit regularization employing nonlocal PINNs during training directs optimization toward physically viable solutions, even with limited data [44]. Additionally, multi-objective strategies like ReLoBRaLo adaptively adjust loss contributions, resolving gradient conflicts and outperforming traditional methods concerning accuracy and overhead [28, 46].

Domain decomposition techniques, coupled with parallel computing, also significantly boost training efficiency for large-scale PINNs [50, 43, 51]. Adaptive strategies like neuron-wise activation functions enhance neural network performance in multiscale problems by optimizing activation parameters through modified algorithms [5, 6]. These advancements elevate physics-informed machine learning, fulfilling rigor, consistency, and computational demands simultaneously.

![](images/28785adf8948d0e81dd298755c1217b18f33cab0a93d0d994f223b1b3b03e7cf.jpg)  
Figure 2: This figure illustrates the hierarchical structure of training methodologies for PhysicsInformed Neural Networks (PINNs), highlighting optimization strategies, collocation point strategies, and implicit regularization techniques. Each category encompasses specific advancements that enhance the accuracy, efficiency, and applicability of PINNs in solving complex partial differential equations.

<html><body><table><tr><td>Feature</td><td>Physics-Informed Neural Networks (PINNs)Variational PINNs (hp-VPINNs)Nonlocal PINNs (nPINNs)</td><td></td><td></td></tr><tr><td>Architecture Type</td><td>Composite LossFunction</td><td>Variational Methods</td><td>Nonlocal Models</td></tr><tr><td>Optimization Strategy</td><td>Self-adaptive Balancing</td><td>Global-local Learning</td><td>Implicit Regularization</td></tr><tr><td>Application Domain</td><td>Pdes,Biology,Engineering</td><td>Differential Equations</td><td>Parameter Inference</td></tr></table></body></html>

Table 3: This table provides a comparative analysis of three advanced methodologies in the realm of Physics-Informed Neural Networks (PINNs): standard PINNs, Variational PINNs (hp-VPINNs), and Nonlocal PINNs (nPINNs). It highlights the distinctive features of each approach, focusing on their architecture types, optimization strategies, and application domains, thereby elucidating their respective contributions to enhancing computational efficiency and accuracy in solving differential equations.

# 4 Deep Learning in Computational Fluid Dynamics

The integration of deep learning into computational fluid dynamics (CFD) has significantly transformed traditional methodologies, enhancing predictive capabilities and computational efficiency. This section examines neural network architectures tailored for CFD, focusing on their effectiveness in addressing challenges such as turbulence modeling, flow dynamics, and the resolution of the Navier-Stokes equations. Figure 3 illustrates the hierarchical categorization of deep learning applications in CFD, highlighting specialized neural network architectures and innovative methodologies for solving the Navier-Stokes equations. This structured visualization underscores the advancements in turbulence modeling and high-fidelity simulations, demonstrating how physics-informed approaches and neural network innovations collectively enhance predictive accuracy and computational efficiency in fluid dynamics. These advancements illustrate how neural networks improve computational efficiency while adhering to the physical principles governing fluid behavior, potentially revolutionizing fluid dynamics problem-solving by offering accurate and computationally feasible solutions.

# 4.1 Neural Network Architectures for CFD

Specialized neural network architectures have been developed to address critical challenges in CFD, such as multiscale phenomena and turbulent flow modeling. Recurrent Neural Networks (RNNs) effectively model pollution dispersion by integrating outputs from multiple subdomains, capturing complex transport dynamics across varying spatial scales [52]. This shift towards data-driven learning combined with physical constraints exemplifies accurate predictions with computational efficiency. RNNs’ ability to manage temporal dependencies makes them suitable for various fluid dynamics applications, particularly time-varying flow fields.

Recent architectural innovations in CFD focus on integrating physics-informed neural networks (PINNs) to address limitations of traditional methods. Although PINNs are complementary to conventional CFD solvers, their potential to solve the Navier-Stokes equations without given data is noteworthy. Experiments on flow problems like the 2D Taylor-Green vortex and 2D cylinder flow show mixed results, highlighting the need for refinement in PINN methodologies [38, 39]. Moving boundary-enabled PINNs (MB-PINNs) accurately reconstruct velocity and recover pressure in unsteady flows, emphasizing spatial zones’ importance in training effectiveness.

Asymptotic-Preserving Neural Networks (APNNs) maintain accuracy across different flow regimes by incorporating specialized loss functions that preserve physical scaling laws [53]. Macroscopic Auxiliary Asymptotic-Preserving Neural Networks (MA-APNNs) extend this capability to radiative transfer problems through auxiliary equation constraints [54]. Deep Finite Volume Methods (DFVMs) hybridize neural networks with traditional discretization schemes, enhancing accuracy for discontinuous solutions [55]. Fourier Neural Operators (FNOs) leverage spectral representations to efficiently model complex fluid-structure interactions [56]. These advancements illustrate the breadth of neural network architectures tailored for CFD, each contributing to improved performance and reliability.

Integrating physical constraints into neural architectures proves valuable for inverse problems and parameter estimation. Physics-informed architectures learn probability density functions and governing equations from limited data, addressing challenges in stochastic fluid mechanics [57]. Explicit Physics-Informed Neural Networks (XPINNs) enforce mass conservation constraints while capturing multiscale features in porous media flows [58]. This synergy enhances predictive power and ensures solutions remain grounded in fluid dynamics principles.

Training methodologies for CFD-specific architectures have evolved to address optimization challenges. Moderate Adaptive Linear Units (MoLU) accelerate convergence in neural ordinary differential equation solvers through balanced activation functions [7], while pre-trained convolutional architectures provide robust initialization for inverse problems in seismic imaging [59]. These advancements showcase the maturation of neural network applications in CFD, transitioning from generic architectures to purpose-built frameworks addressing domain-specific challenges while maintaining physical consistency and computational efficiency.

# 4.2 Solving Navier-Stokes Equations with Deep Learning

Deep learning approaches address the computational challenges of the Navier-Stokes equations through innovative architectures and physics-informed methodologies. Physics-Informed Neural Networks (PINNs) effectively solve these equations by handling nonlinearities and complex boundary conditions [60]. Pressure-velocity coupling strategies within PINN architectures significantly improve prediction accuracy for incompressible flows while automatically satisfying continuity constraints [22].

Architectural innovations in computational methods, particularly PINNs, demonstrate their ability to solve Reynolds-averaged Navier-Stokes (RANS) equations for incompressible turbulent flows without predefined turbulence models or extensive data, using only boundary information. Initial applications succeed in simulating laminar flows with high accuracy, but challenges remain in capturing complex phenomena like vortex shedding in turbulent flows [39, 61].

Emerging methodologies, especially PINNs, tackle challenges associated with solving the NavierStokes equations, including data needs in turbulent flow scenarios and generalization across geometries. While promising in simulating laminar flows with minimal error, PINNs struggle with complex turbulent flows and require significant training time compared to traditional CFD methods. Advancements in error estimation and convergence analysis are critical for enhancing PINNs’ reliability, demonstrating their evolving role as complementary tools to conventional solvers in fluid dynamics [62, 63, 39, 61].

The mathematical formulation integrates the incompressible Navier-Stokes equations with advanced neural network architectures, such as PINNs and graph neural networks, to solve complex fluid dynamics problems. PINNs leverage physics-based loss functions to approximate solutions for laminar and turbulent flows, while graph neural networks enhance adaptability to various geometries and boundary conditions, facilitating application across diverse scenarios in CFD [63, 62, 61, 32].

The equation presented utilizes PINNs, a contemporary deep learning technique designed to solve differential equations by integrating physical laws into the training process, enhancing accuracy and efficiency in computational physics [64, 16, 30, 3]. The Navier-Stokes equations, represented by

$$
\frac { \partial { \bf u } } { \partial t } + ( { \bf u } \cdot \nabla ) { \bf u } = - \nabla p + { \nu } { \nabla ^ { 2 } } { \bf u } + { \bf f } ,
$$

describe fluid motion, where $\mathbf { u }$ is the fluid velocity vector, $t$ is time, $p$ is the pressure field, $\nu$ represents kinematic viscosity, and f accounts for external forces. This formulation is fundamental in fluid dynamics, applicable in simulations using methods like PINNs, which integrate physical laws into machine learning frameworks for solving PDEs [20, 18, 65, 19, 66].

The command ’(0)’ in LaTeX signifies the conclusion of a mathematical equation environment, essential for formatting and presenting equations in scientific documents, particularly in studies involving advanced mathematical concepts like those explored in PINNs and differential equations [3, 67, 20, 68, 30].

The dynamics of the averaged velocity field $\overline { { u _ { i } } }$ in a fluid system are represented by

$$
\frac { \partial \overline { { u _ { i } } } } { \partial t } + \overline { { u _ { j } } } \frac { \partial \overline { { u _ { i } } } } { \partial x _ { j } } = - \frac { 1 } { \rho } \frac { \partial \overline { { p } } } { \partial x _ { i } } + \nu \frac { \partial ^ { 2 } \overline { { u _ { i } } } } { \partial x _ { j } \partial x _ { j } } - \frac { \partial \tau _ { i j } } { \partial x _ { j } } ,
$$

where $\overline { { u _ { i } } }$ is the resolved velocity field, $\overline { { p } }$ the resolved pressure, and $\tau _ { i j }$ the subgrid-scale stress tensor. Deep learning approaches tackle turbulence modeling’s closure problem by learning the stress tensor from data or enhancing existing models through neural network corrections incorporating physical laws, improving predictive accuracy with minimal data requirements [22, 61]. This methodology is effective for complex flows where traditional models struggle, such as separated flows, transitional boundary layers, and compressible turbulence, demonstrating deep learning’s transformative potential in turbulence modeling.

Recent breakthroughs in high-dimensional PDE solving demonstrate deep learning approaches’ scalability, with Stochastic Dimension Gradient Descent (SDGD) enabling efficient training of PINNs on Navier-Stokes equations up to 100,000 dimensions [69]. For unsteady flows with moving boundaries, specialized training strategies address velocity field reconstruction and pressure distribution recovery in the presence of strong gradients [38]. The methodology effectively extends to complex nonlinear wave phenomena, where improved backward-compatible PINN formulations handle various initial and boundary condition configurations [23].

Transforming numerical approximations through neural network correction shows promise for convection-diffusion problems in fluid mechanics, where traditional methods struggle with steep boundary layers [25]. Near-wall blood flow modeling exemplifies the approach’s capability to leverage known physics for accurate predictions under data sparsity and uncertainty [70]. These advancements demonstrate how deep learning techniques overcome traditional limitations in solving the Navier-Stokes equations, handling nonlinearities, complex geometries, and multi-scale phenomena inherent in fluid flow problems. Integrating physical constraints with neural network architectures ensures solutions remain consistent with conservation laws while benefiting from deep learning models’ representational power.

# 4.3 Turbulence Modeling and High-Fidelity Simulations

Deep learning has revolutionized turbulence modeling and high-fidelity simulations by addressing numerical accuracy, computational efficiency, and physical consistency challenges. The integration of turbulence models with PINNs shows promise, as demonstrated by the Spalart-Allmaras turbulence model-augmented PINN (PINN-DA-SA), which significantly improves flow reconstruction accuracy while maintaining governing equations’ physical constraints [37]. This hybrid approach combines traditional turbulence modeling strengths with neural networks’ flexibility, overcoming conventional Reynolds-averaged Navier-Stokes (RANS) methods’ limitations.

Recent advancements in numerical integration techniques enhance physics-informed approaches’ training and performance for turbulence simulation. Improved numerical integration methods provide higher accuracy in evaluating loss functions, crucial for capturing turbulent flows’ multiscale nature [71]. These developments complement physics-informed enhanced super-resolution generative adversarial networks (PIESRGAN), which reconstruct fully-resolved turbulence fields from filtered data while preserving underlying physics [72]. The methodology bridges the gap between large-eddy simulation (LES) and direct numerical simulation (DNS) resolutions.

Key innovations in deep learning for turbulence modeling focus on integrating PINNs to enhance model training with minimal data while adhering to physical laws, developing frameworks that extend deep learning techniques’ applicability beyond traditional domains through domaindecomposition methods, and applying deep learning for high-resolution reconstruction of flow-field data from sparse and noisy measurements, ensuring predictions comply with fundamental physical principles like mass and momentum conservation [73, 22, 52].

Recent advancements in high-dimensional PDE solving demonstrate deep learning approaches’ scalability, with SDGD enabling efficient PINN training on Navier-Stokes equations up to 100,000 dimensions [69]. For unsteady flows with moving boundaries, specialized training strategies address velocity field reconstruction and pressure distribution recovery in the presence of strong gradients [38]. The methodology extends to complex nonlinear wave phenomena, where improved backwardcompatible PINN formulations handle various initial and boundary condition configurations [23].

The mathematical formulation of turbulence modeling using deep learning integrates governing equations, like RANS equations, with statistical closure models, enabling effective PINN application to simulate laminar and turbulent flows. This approach allows accurate solutions without relying on specific turbulence models and facilitates flow state reconstruction and parameter inference from limited and potentially noisy observational data, enhancing traditional fluid dynamics methods’ predictive capabilities [74, 61].

Integrating deep learning into turbulence modeling represents a significant step toward practical applications in engineering and environmental sciences, where accurate turbulent flow modeling is essential for predicting phenomena like weather patterns, pollutant dispersion, and aerodynamic performance. The ongoing development of these methodologies continues to push fluid dynamics simulations’ boundaries, promising more efficient and reliable tools for researchers and practitioners.

As illustrated in Figure 4, the hierarchical categorization of deep learning methodologies in turbulence modeling highlights key turbulence models, numerical techniques, and high-fidelity simulation approaches, further emphasizing the multifaceted nature of this field.

# 5 Machine Learning for PDEs in Fluid Mechanics

The integration of machine learning with partial differential equations (PDEs) in fluid mechanics is revolutionizing traditional solution methods, enhancing both accuracy and efficiency. This section examines methodologies that leverage data-driven approaches to solve PDEs, focusing on maintaining physical consistency while utilizing advanced computational techniques.

# 5.1 Data-Driven Methods for PDE Solutions

Data-driven methods in fluid mechanics utilize deep learning to overcome limitations of traditional numerical approaches, ensuring physical consistency. The Deep Learning Consistency (DLC) framework exemplifies this by enforcing domain consistency in air pollution modeling [52]. PhysicsInformed Neural Networks (PINNs) further enhance performance by embedding PDEs into loss functions, effectively handling complex geometries and sparse data [43].

Innovations such as Generative Artificial Intelligence (GAI), quantum computing, and PINNs are addressing challenges across various domains. GAI aids in data generation and simulation in geoscience and medicine, while quantum computing explores applications in quantum chemistry. PINNs optimize collocation point sampling for PDE accuracy, demonstrating the interplay of innovative sampling and computational efficiency [30, 29, 26].

The integration of GAI and large language models transforms geoscience by advancing data generation and predictive modeling. Machine learning advancements, including GANs and PINNs, improve tasks like data augmentation and environmental monitoring, though challenges in interpretability and trustworthiness persist. PINNs have enhanced predictions in industrial settings by combining classical mathematics with deep learning [31, 26].

Key research findings include the use of Physics-Informed Enhanced Super-Resolution GANs (PIESRGAN) for turbulence reconstruction [72], Wasserstein GANs (WGAN-PINNs) for uncertainty quantification [42], and $\mathbf { k }$ -space PINNs for structural dynamics [40]. Physics-Informed Gated Recurrent Networks (PiGRN) demonstrate consistency in musculoskeletal dynamics predictions [11].

The hp-Variational PINN (hp-VPINN) method leverages hierarchical basis functions and domain decomposition for accuracy and reduced computational cost [43]. Standardized benchmarking frameworks enable rigorous evaluation across PDE problems [75]. In medical imaging, specialized architectures capture critical local tissue deformations [10].

Comparative analysis highlights trade-offs between traditional numerical algorithms and machine learning techniques like PINNs and the Deep Ritz Method (DRM), focusing on optimization efficiency and generalization [34, 16]. Mathematical formulations address nonlinearities and boundary conditions, with PINNs utilizing attention mechanisms and the Theory of Functional Connections (TFC) for efficient training [15, 63, 76]. While promising, challenges in solution uniqueness and scalability remain, driving innovation in hybrid approaches combining neural networks with numerical analysis for problems requiring conservation properties.

# 5.2 Hybrid Models Integrating Machine Learning and Traditional Methods

Hybrid modeling that combines machine learning with traditional numerical methods presents a transformative approach for solving PDEs in fluid mechanics. These models leverage neural networks’ approximation capabilities and the rigor of numerical analysis, optimizing PINN training for complex problems [77].

Architectural innovations in hybrid modeling, including differentiable finite element solvers and Physics-informed PointNet, enhance structural optimization and predictions across geometries. Stochastic Dimension Gradient Descent (SDGD) addresses high-dimensional PDE challenges, improving efficiency and accuracy in modeling and uncertainty quantification [12, 26, 69, 78, 32].

Hybrid approaches consistently outperform purely data-driven or traditional methods, excelling in nonlinear problems with solution discontinuities or multiscale phenomena. Future research aims to expand hybrid methods to more intricate PDE classes, exploring advanced sampling strategies for optimization and leveraging neural basis functions for time-dependent PDEs [79, 34, 16]. These advancements highlight the transformative potential of hybrid modeling in computational fluid mechanics, merging data-driven and physics-based approaches.

# 5.3 Advanced Neural Network Architectures for Fluid Mechanics

Advanced neural network architectures in fluid mechanics address domain-specific challenges in accuracy, efficiency, and physical consistency. Sequential Meta-Transfer Learning (SMTL) introduces adaptive temporal segmentation, allowing PINNs to learn from simpler subproblems for unsteady flows [80].

The Continuous-Time Echo State Network (CTESN) architecture uses continuous-time reservoir computing to handle stiffness in nonlinear systems, capturing rapid flow transitions [81]. Adversarial multi-task learning frameworks enhance generalization by optimizing related fluid mechanics tasks while maintaining physical consistency [82].

Multilevel neural networks integrate solver concepts with deep learning for high-dimensional parametric PDEs, achieving precision comparable to traditional methods. This architecture approximates multigrid V-cycles, improving performance in scenarios like high-dimensional random conductivity problems [47, 43, 83, 20, 84].

These architectures enhance the efficiency of solving complex flow problems, integrate physical laws for accuracy with limited data, and adapt to real-time geometries in biomedical applications [73, 85, 22, 39, 38].

Mathematical formulations enable handling nonlinearities and boundary conditions in fluid mechanics. Integration of physical constraints with innovations like hard constraints in hPINNs ensures solutions adhere to conservation laws, reducing errors in momentum conservation and simplifying complex inverse design problems [60, 86, 87]. Future research focuses on architectures for extreme-scale problems and multiphysics coupling, bridging data-driven methods and traditional computational fluid dynamics.

# 6 Challenges and Future Directions

The integration of deep learning with partial differential equations (PDEs) in fluid mechanics presents a complex landscape of challenges and opportunities. Addressing scalability and computational efficiency is crucial for the effective application of Physics-Informed Neural Networks (PINNs) in practical scenarios. The following subsection outlines key aspects of scalability and computational efficiency, highlighting inherent difficulties and proposing methodologies to overcome these limitations.

# 6.1 Scalability and Computational Efficiency

Integrating deep learning with PDEs in fluid mechanics faces scalability and computational efficiency challenges, particularly with high-dimensional parameter spaces and complex systems. PINNs encounter computational overhead as problem dimensionality increases, with training costs rising for large-scale simulations due to factors like loss term scale disparities, optimization complexity, and memory constraints, especially in modeling multi-dimensional heat transfer [88]. The curse of dimensionality exacerbates these issues, necessitating approaches like Stochastic Dimension Gradient Descent (SDGD) and FastVPINNs to enhance computational efficiency [26, 69, 89, 29, 90].

Training physics-constrained networks poses optimization challenges, with methods often struggling with loss function ill-conditioning, leading to slow convergence and inaccuracies [77]. The scorebased SDE solver effectively addresses the curse of dimensionality, ensuring stable performance in high-dimensional settings [91]. Traditional numerical techniques struggle with moving flow fronts, a challenge addressed by residual-based adaptive methods [92]. TE-PINN architectures effectively handle dynamic environments by addressing non-linearities and sensor noise [9].

Hard constraints in PINNs demand substantial computational resources for large-scale problems. Improved backward-compatible PINN formulations reduce error accumulation in complex wave phenomena [23]. Generative artificial intelligence (GAI) in geoscience highlights transformative potential, particularly in synthetic data creation and challenges like climate change and Earth system dynamics. Integrating GAI models, including GANs, PINNs, and GPT-based structures, advances tasks like data generation, super-resolution, and land surface change detection, though challenges in physical interpretability and trustworthiness persist [93, 26].

# 6.2 Generalization and Robustness

Generalization and robustness of deep learning models in fluid mechanics are critical, impacting reliability across diverse scenarios and boundary conditions. PINNs show varying generalization capabilities, with evolutionary optimization approaches maintaining accuracy on unseen tasks without extensive retraining [94]. However, these methods remain sensitive to data quality and distribution, particularly with non-smooth parameter fields or complex conductivity distributions [95].

Model generalization limitations stem from challenges in generative model interpretation in geoscience, multiobjective optimization complexities, and spectral bias in neural networks, affecting effectiveness in high-resolution data applications [28, 93, 26]. Neural networks struggle with highfrequency behaviors, challenging fluid flow predictions in turbulent regimes [18]. FastLRNR’s effectiveness reduces when initial coefficient guesses approach optimal values, limiting robustness [96]. Graph grammar-based approaches require extensive hyperparameter tuning, introducing computational overhead and potentially compromising generalization [97].

Integrating GAI and quantum computing methodologies offers advancement opportunities across fields like geoscience, quantum chemistry, and material science, enabling synthetic data generation, complex system modeling, and PDE solving accuracy improvements while addressing data privacy and heterogeneity challenges. Leveraging these technologies enhances safety, efficiency, and sustainability, paving the way for transformative changes in scientific research and industrial practices [30, 19, 29, 26].

# 6.3 Interpretability and Uncertainty Quantification

Interpretability of deep learning models for PDEs presents challenges compared to traditional solvers, especially in risk-sensitive applications where understanding model decisions is crucial [98]. PINNs exhibit limitations, as complex architectures and nonlinear optimization obscure input-output relationships. Attention-enhanced Quantum PINNs (AQ-PINNs) improve interpretability through quantum-inspired architectures, though challenges remain in sub-grid process parameterization and extreme event prediction [99].

Uncertainty quantification (UQ) in PDE solutions faces challenges from input data randomness, DNN architecture limitations in PINNs, and complexities in implementing UQ methods within large-scale applications. These challenges hinder seamless UQ integration despite theoretical advantages and growing interest in using machine learning models for scientific computing [100, 101]. Recent UQ advancements improve uncertainty assessment and management, particularly in SciML and PINNs, with comprehensive libraries like NeuralUQ facilitating UQ method implementation for machine learning models in scientific computing. Innovative approaches like ensemble PINNs (E-PINNs) effectively quantify uncertainties in inverse problems, enhancing prediction reliability in engineering and industrial contexts. These advancements address data randomness and deep learning architecture limitations, providing robust frameworks for integrating uncertainty assessments into complex modeling scenarios, promoting broader adoption in research and industry [102, 100, 103, 101].

# 6.4 Emerging Applications and Future Directions

The integration of deep learning with PDEs in fluid mechanics expands into novel domains, presenting promising avenues for methodological advancement. PINNs demonstrate significant potential in fields like geophysical flows, cardiac modeling, and solar physics, with emerging applications requiring enhanced boundary condition handling and optimized network architectures. The WGANPINNs method shows promise for uncertainty quantification in high-dimensional spaces, improving accuracy over existing methods [42]. Future research should refine optimization processes and extend methods to more complex, higher-dimensional problems [43].

Emerging computational frameworks, such as GAI, quantum computing, SciML, and PINNs, offer advancement opportunities across diverse fields, including geoscience, quantum chemistry, and material science. These frameworks enable synthetic data generation, complex system modeling, and PDE solving accuracy improvements while addressing data privacy and heterogeneity challenges. Leveraging these technologies enhances safety, efficiency, and sustainability, paving the way for transformative changes in scientific research and industrial practices [30, 19, 29, 26].

# 7 Conclusion

This survey has explored the integration of deep learning with partial differential equations (PDEs) in computational fluid dynamics (CFD) and physics-based modeling, highlighting substantial progress across various facets. Physics-Informed Neural Networks (PINNs) stand out as a pivotal advancement, merging data-driven techniques with core physical principles through novel architectures and training methods. The advent of fully differentiable model discovery has marked a significant leap forward, surpassing traditional non-differentiable methods in accurately recovering equations under diverse datasets and noise levels. By merging deep learning with conventional techniques, there is an enhancement in predictive capabilities, unlocking solutions to previously unsolvable complex problems.

The survey underscores several key developments that collectively propel computational methods forward:

• Tailored neural architectures, such as hp-VPINNs, nPINNs, and MA-APNNs, have been crafted to address core challenges in multiscale modeling, nonlocal phenomena, and transitions in flow regimes. These designs harness the intrinsic attributes of PDEs, boosting simulation accuracy and efficiency, and thereby enabling more dependable predictions in intricate fluid dynamics scenarios.

• Hybrid approaches that integrate deep learning with traditional numerical solvers have achieved notable improvements in both accuracy and computational efficiency. This synergy capitalizes on the strengths of data-driven models while retaining the robustness of established numerical methods, resulting in more effective solutions for practical engineering challenges.   
• Innovations in training methods, including evolutionary optimization and adaptive activation functions, have enhanced convergence rates and generalization across complex fluid systems. These advancements enable models to better adapt to varying conditions, broadening their applicability across diverse scenarios.   
• Emerging applications in fields such as geoscience, medical imaging, and turbulence modeling highlight the transformative potential of generative AI approaches. However, they also bring attention to ongoing challenges related to data quality and model trustworthiness, which are essential to address to fully leverage these advanced methodologies.

Despite these advancements, challenges persist in the scalability, generalization, and interpretability of deep learning models for PDE solutions. The field continues to confront issues related to the computational demands of high-dimensional problems, model robustness under varying boundary conditions, and the need for theoretically sound uncertainty quantification frameworks. Tackling these challenges is vital for the wider adoption of deep learning methodologies in practical applications. Future research should focus on developing memory-efficient architectures that manage large datasets without compromising performance, enhancing physical consistency through constraints that ensure adherence to governing equations, and advancing multi-physics coupling strategies that effectively integrate various physical phenomena.

The incorporation of explainable AI techniques with physics-informed learning offers a promising avenue to enhance model interpretability in risk-sensitive applications. By promoting transparency in model predictions and decision-making processes, researchers can build user trust and facilitate the adoption of these techniques in critical domains.

The interdisciplinary nature of deep learning for PDEs in fluid mechanics presents both challenges and opportunities for future innovation. As the field evolves, the collaboration between computational physics, numerical analysis, and machine learning will continue to drive transformative advancements in simulation, optimization, and discovery across engineering and scientific disciplines. The methodologies discussed in this work collectively demonstrate the potential to revolutionize computational approaches to fluid mechanics problems while maintaining strict adherence to fundamental physical principles. This ongoing evolution not only promises enhanced computational capabilities but also fosters a deeper understanding of the underlying physical processes, ultimately leading to more effective solutions in real-world applications.

References [1] Gaetan Raynaud, Sebastien Houde, and Frederick P. Gosselin. Modalpinn: an extension of physics-informed neural networks with enforced truncated fourier decomposition for periodic flow reconstruction using a limited number of imperfect sensors, 2022. [2] Xuhui Meng, Liu Yang, Zhiping Mao, Jose del Aguila Ferrandis, and George Em Karniadakis. Learning functional priors and posteriors from data and physics, 2021. [3] Luis Medrano Navarro, Luis Martín Moreno, and Sergio G Rodrigo. Solving differential equations with deep learning: a beginner’s guide, 2023. [4] Salah A Faroughi, Nikhil Pawar, Celio Fernandes, Maziar Raissi, Subasish Das, Nima K. Kalantari, and Seyed Kourosh Mahjour. Physics-guided, physics-informed, and physicsencoded neural networks in scientific computing, 2023. [5] Steffen Dereich, Arnulf Jentzen, and Adrian Riekert. Learning rate adaptive stochastic gradient descent optimization methods: numerical simulations for deep learning methods for partial differential equations and convergence analyses, 2024. [6] Ameya D. Jagtap, Kenji Kawaguchi, and George Em Karniadakis. Locally adaptive activation functions with slope recovery term for deep and physics-informed neural networks, 2020. [7] Hankyul Koh, Joon hyuk Ko, and Wonho Jhe. Moderate adaptive linear units (molu), 2025. [8] Akshansh Mishra. Supervised machine learning and physics based machine learning approach for prediction of peak temperature distribution in additive friction stir deposition of aluminium alloy, 2023. [9] Arman Asgharpoor Golroudbari. Te-pinn: Quaternion-based orientation estimation using transformer-enhanced physics-informed neural networks, 2024.   
[10] Pablo Arratia López, Hernán Mella, Sergio Uribe, Daniel E. Hurtado, and Francisco Sahli Costabal. Warppinn: Cine-mr image registration with physics-informed neural networks, 2022.   
[11] Rajnish Kumar, Anand Gupta, Suriya Prakash Muthukrishnan, Lalan Kumar, and Sitikantha Roy. semg-driven physics-informed gated recurrent networks for modeling upper limb multijoint movement dynamics, 2025.   
[12] Gaoyuan Wu. Jax-sso: Differentiable finite element analysis solver for structural optimization and seamless integration with neural networks, 2024.   
[13] Roberto Riganti, Yilin Zhu, Wei Cai, Salvatore Torquato, and Luca Dal Negro. Multiscale physics-informed neural networks for the inverse design of hyperuniform optical materials, 2024.   
[14] Wayne Isaac Tan Uy and Mircea Grigoriu. Neural network representation of the probability density function of diffusion processes, 2020.   
[15] Ruben Rodriguez-Torrado, Pablo Ruiz, Luis Cueto-Felgueroso, Michael Cerny Green, Tyler Friesen, Sebastien Matringe, and Julian Togelius. Physics-informed attention-based neural network for solving non-linear partial differential equations, 2021.   
[16] Lukas Gonon, Arnulf Jentzen, Benno Kuckuck, Siyu Liang, Adrian Riekert, and Philippe von Wurstemberger. An overview on machine learning methods for partial differential equations: from physics informed neural networks to deep operator learning, 2024.   
[17] D. A. Kaltsas, L. Magafas, P. Papadopoulou, and G. N. Throumoulopoulos. Multi-soliton solutions and data-driven discovery of higher-order burgers’ hierarchy equations with physics informed neural networks, 2025.   
[18] M. H. Saadat, B. Gjorgiev, L. Das, and G. Sansavini. Neural tangent kernel analysis of pinn for advection-diffusion equation, 2022.   
[19] Handi Zhang, Langchen Liu, and Lu Lu. Federated scientific machine learning for approximating functions and solving differential equations with data heterogeneity, 2024.   
[20] Wenrui Hao, Xinliang Liu, and Yahong Yang. Newton informed neural operator for computing multiple solutions of nonlinear partials differential equations, 2024.   
[21] Junjun Yan, Xinhai Chen, Zhichao Wang, Enqiang Zhoui, and Jie Liu. St-pinn: A self-training physics-informed neural network for partial differential equations, 2023.   
[22] Chengping Rao, Hao Sun, and Yang Liu. Physics-informed deep learning for incompressible laminar flows, 2020.   
[23] Shuning Lin and Yong Chen. The improved backward compatible physics-informed neural networks for reducing error accumulation and applications in data-driven higher-order rogue waves, 2023.   
[24] Jiaxin Zhang, Congjie Wei, and Chenglin Wu. Thermodynamic consistent neural networks for learning material interfacial mechanics, 2020.   
[25] Jiajing Guan and Howard Elman. Transformed physics-informed neural networks for the convection-diffusion equation, 2024.   
[26] Abdenour Hadid, Tanujit Chakraborty, and Daniel Busby. When geoscience meets generative ai and large language models: Foundations, trends, and future challenges, 2024.   
[27] Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, and George Em Karniadakis. Residual-based attention and connection to information bottleneck theory in pinns, 2023.   
[28] Junaid Akhter, Paul David Fährmann, Konstantin Sonntag, Sebastian Peitz, and Daniel Schwietert. Common pitfalls to avoid while using multiobjective optimization in machine learning, 2025.   
[29] Pratibha Raghupati Hegde, Oleksandr Kyriienko, Hermanni Heimonen, Panagiotis Tolias, Gilbert Netzer, Panagiotis Barkoutsos, Ricardo Vinuesa, Ivy Peng, and Stefano Markidis. Beyond the buzz: Strategic paths for enabling useful nisq applications, 2024.   
[30] Jose Florido, He Wang, Amirul Khan, and Peter K. Jimack. Investigating guiding information for adaptive collocation point sampling in pinns, 2024.   
[31] Mahdi Nasiri, Sahel Iqbal, and Simo Särkkä. Physics-informed machine learning for grade prediction in froth flotation, 2024.   
[32] Tianyu Li, Yiye Zou, Shufan Zou, Xinghua Chang, Laiping Zhang, and Xiaogang Deng. A fully differentiable gnn-based pde solver: With applications to poisson and navier-stokes equations, 2024.   
[33] Kun Qian and Mohamed Kheir. Investigating kan-based physics-informed neural networks for emi/emc simulations, 2024.   
[34] Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, and Jose Blanchet. Machine learning for elliptic pdes: Fast rate generalization bound, neural scaling law and minimax optimality, 2021.   
[35] Ling Guo, Hao Wu, Xiaochen Yu, and Tao Zhou. Monte carlo pinns: deep learning approach for forward and inverse problems involving high dimensional fractional partial differential equations, 2022.   
[36] Han Gao, Luning Sun, and Jian-Xun Wang. Phygeonet: Physics-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state pdes on irregular domain, 2020.   
[37] Yusuf Patel, Vincent Mons, Olivier Marquet, and Georgios Rigas. Turbulence model augmented physics informed neural networks for mean flow reconstruction, 2024.   
[38] Rahul Sundar, Didier Lucor, and Sunetra Sarkar. Understanding the training of pinns for unsteady flow past a plunging foil through the lens of input subdomain level loss function gradients, 2024.   
[39] Pi-Yueh Chuang and Lorena A. Barba. Experience report of physics-informed neural networks in fluid simulations: pitfalls and frustration, 2022.   
[40] Saeid Hedayatrasa, Olga Fink, Wim Van Paepegem, and Mathias Kersemans. k-space physicsinformed neural network (k-pinn) for compressed spectral mapping and efficient inversion of vibrations in thin composite laminates, 2024.   
[41] Katayoun Eshkofti and Matthieu Barreau. Vanishing stacked-residual pinn for state reconstruction of hyperbolic systems, 2025.   
[42] Yihang Gao and Michael K. Ng. Wasserstein generative adversarial uncertainty quantification in physics-informed neural networks, 2022.   
[43] Ehsan Kharazmi, Zhongqiang Zhang, and George Em Karniadakis. hp-vpinns: Variational physics-informed neural networks with domain decomposition, 2020.   
[44] Guofei Pang, Marta D’Elia, Michael Parks, and George E. Karniadakis. npinns: nonlocal physics-informed neural networks for a parametrized nonlocal universal laplacian operator. algorithms and applications, 2020.   
[45] Nima Hosseini Dashtbayaz, Ghazal Farhani, Boyu Wang, and Charles X. Ling. Physicsinformed neural networks: Minimizing residual loss with wide networks and effective activations, 2024.   
[46] Rafael Bischof and Michael Kraus. Multi-objective loss balancing for physics-informed deep learning, 2022.   
[47] Yeonjong Shin, Jerome Darbon, and George Em Karniadakis. On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type pdes, 2020.   
[48] Zhikang Dong and Pawel Polak. Cp-pinns: Data-driven changepoints detection in pdes using online optimized physics-informed neural networks, 2024.   
[49] Dat Phan-Trong, Hung The Tran, Alistair Shilton, and Sunil Gupta. Pinn-bo: A black-box optimization algorithm using physics-informed neural networks, 2024.   
[50] John Mango and Ronald Katende. Adaptive error-bounded hierarchical matrices for efficient neural network compression, 2024.   
[51] Xinquan Huang and Tariq Alkhalifah. Efficient physics-informed neural networks using hash encoding, 2023.   
[52] Philipp Haehnel, Jakub Marecek, Julien Monteil, and Fearghal O’Donncha. Using deep learning to extend the range of air-pollution monitoring and forecasting, 2020.   
[53] Giulia Bertaglia. Asymptotic-preserving neural networks for hyperbolic systems with diffusive scaling, 2022.   
[54] Hongyan Li, Song Jiang, Wenjun Sun, Liwei Xu, and Guanyu Zhou. Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations, 2024.   
[55] Jianhuan Cen and Qingsong Zou. Deep finite volume method for partial differential equations, 2024.   
[56] Fanny Lehmann, Filippo Gatti, Michaël Bertin, and Didier Clouteau. Fourier neural operator surrogate model to predict 3d seismic waves propagation, 2023.   
[57] Xiaoli Chen, Liu Yang, Jinqiao Duan, and George Em Karniadakis. Solving inverse stochastic problems from discrete particle observations using the fokker-planck equation and physicsinformed neural networks, 2020.   
[58] Ehsan Taghizadeh, Helen M. Byrne, and Brian D. Wood. Explicit physics-informed neural networks for non-linear upscaling closure: the case of transport in tissues, 2021.   
[59] Deep-pretrained-fwi: combining supervised learning with physics-informed neural network.   
[60] Mahyar Jahaninasab and Mohamad Ali Bijarchi. Enhancing convergence speed with featureenforcing physics-informed neural networks: Utilizing boundary conditions as prior knowledge for faster convergence, 2024.   
[61] Hamidreza Eivazi, Mojtaba Tahani, Philipp Schlatter, and Ricardo Vinuesa. Physics-informed neural networks for solving reynolds-averaged navierx2013stokes equations, 2021.   
[62] Viktor Grimm, Alexander Heinlein, and Axel Klawonn. Learning the solution operator of two-dimensional incompressible navier-stokes equations using physics-aware convolutional neural networks, 2023.   
[63] Shoaib Goraya, Nahil Sobh, and Arif Masud. Error estimates and physics informed augmentation of neural networks for thermally coupled incompressible navier stokes equations, 2022.   
[64] Hubert Baty and Leo Baty. Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests, 2023.   
[65] Jassem Abbasi and Pål Østebø Andersen. Physical activation functions (pafs): An approach for more efficient induction of physics into physics-informed neural networks (pinns), 2023.   
[66] Shupeng Wang and George Em Karniadakis. Gmc-pinns: A new general monte carlo pinns method for solving fractional partial differential equations on irregular domains, 2024.   
[67] Enrico Schiassi, Carl Leake, Mario De Florio, Hunter Johnston, Roberto Furfaro, and Daniele Mortari. Extreme theory of functional connections: A physics-informed neural network method for solving parametric differential equations, 2020.   
[68] Minhui Tan, Qing Xu, Hairong Yuan, Man Xu, Ke Liu, Aifang Qu, and Xiaoda Xu. Mathematical analysis and numerical computation of string vibration equations with elastic supports for bridge cable force evaluation, 2024.   
[69] Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, and Kenji Kawaguchi. Tackling the curse of dimensionality with physics-informed neural networks, 2024.   
[70] Amirhossein Arzani, Jian-Xun Wang, and Roshan M. D’Souza. Uncovering near-wall blood flow from sparse data with physics-informed neural networks, 2021.   
[71] Josef Dank and Jan Pospíil. Challenges in numerical integration in physics-informed neural networks modelling, 2025.   
[72] Mathis Bode, Michael Gauding, Zeyu Lian, Dominik Denker, Marco Davidovic, Konstantin Kleinheinz, Jenia Jitsev, and Heinz Pitsch. Using physics-informed super-resolution generative adversarial networks for subgrid modeling in turbulent reactive flows, 2019.   
[73] Hamidreza Eivazi, Yuning Wang, and Ricardo Vinuesa. Physics-informed deep-learning applications to experimental fluid mechanics, 2024.   
[74] Vladimir Parfenyev, Mark Blumenau, and Ilia Nikitin. Inferring parameters and reconstruction of two-dimensional turbulent flows with physics-informed neural networks, 2024.   
[75] Hugo Gangloff and Nicolas Jouvin. jinns: a jax library for physics-informed neural networks, 2024.   
[76] Abhiram Anand Thiruthummal, Sergiy Shelyag, and Eun jin Kim. Extremization to fine tune physics informed neural networks for solving boundary value problems, 2024.   
[77] Jorge F. Urbán, Petros Stefanou, and José A. Pons. Unveiling the optimization process of physics informed neural networks: How accurate and competitive can pinns be?, 2024.   
[78] Ali Kashefi, Leonidas J. Guibas, and Tapan Mukerji. Physics-informed pointnet: On how many irregular geometries can it solve an inverse problem simultaneously? application to linear elasticity, 2023.   
[79] Chinmay Datar, Taniya Kapoor, Abhishek Chandra, Qing Sun, Iryna Burak, Erik Lien Bolager, Anna Veselovska, Massimo Fornasier, and Felix Dietrich. Solving partial differential equations with sampled neural networks, 2024.   
[80] Milad Ramezankhani and Abbas S. Milani. A sequential meta-transfer (smt) learning to combat complexities of physics-informed neural networks: Application to composites autoclave processing, 2023.   
[81] Ranjan Anantharaman, Yingbo Ma, Shashi Gowda, Chris Laughman, Viral Shah, Alan Edelman, and Chris Rackauckas. Accelerating simulation of stiff nonlinear systems using continuous-time echo state networks, 2021.   
[82] Pongpisit Thanasutives, Masayuki Numao, and Ken ichi Fukui. Adversarial multi-task learning enhanced physics-informed neural networks for solving partial differential equations, 2021.   
[83] Cosmas HeiSS, Ingo Gühring, and Martin Eigel. Multilevel cnns for parametric pdes, 2023.   
[84] Ziad Aldirany, Régis Cottereau, Marc Laforest, and Serge Prudhomme. Multi-level neural networks for accurate solutions of boundary-value problems, 2023.   
[85] Hong Shen Wong, Wei Xuan Chan, Bing Huan Li, and Choon Hwai Yap. Multiple case physics-informed neural network for biomedical tube flows, 2023.   
[86] Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G. Johnson. Physics-informed neural networks with hard constraints for inverse design, 2021.   
[87] Anthony Baez, Wang Zhang, Ziwen Ma, Subhro Das, Lam M. Nguyen, and Luca Daniel. Guaranteeing conservation laws with projection in physics-informed neural networks, 2024.   
[88] Ibai Ramirez, Joel Pino, David Pardo, Mikel Sanz, Luis del Rio, Alvaro Ortiz, Kateryna Morozovska, and Jose I. Aizpurua. Residual-based attention physics-informed neural networks for spatio-temporal ageing assessment of transformers operated in renewable power plants, 2024.   
[89] Michael Penwarden, Houman Owhadi, and Robert M. Kirby. Kolmogorov n-widths for multitask physics-informed machine learning (piml) methods: Towards robust metrics, 2024.   
[90] Thivin Anandh, Divij Ghose, Himanshu Jain, and Sashikumaar Ganesan. Fastvpinns: Tensordriven acceleration of vpinns for complex geometries, 2024.   
[91] Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, and Kenji Kawaguchi. Scorebased physics-informed neural networks for high-dimensional fokker-planck equations, 2024.   
[92] John M. Hanna, Jose V. Aguado, Sebastien Comas-Cardona, Ramzi Askri, and Domenico Borzacchiello. Residual-based adaptivity for two-phase flow simulation in porous media using physics-informed neural networks, 2022.   
[93] Kaumudi Joshi, Vukka Snigdha, and Arya Kumar Bhattacharya. Constructing extreme learning machines with zero spectral bias, 2023.   
[94] Jian Cheng Wong, Chin Chun Ooi, Abhishek Gupta, Pao-Hsiung Chiu, Joshua Shao Zheng Low, My Ha Dao, and Yew-Soon Ong. Evolutionary optimization of physics-informed neural networks: Advancing generalizability by the baldwin effect, 2025.   
[95] Xuanxuan Yang, Yangming Zhang, Haofeng Chen, Gang Ma, and Xiaojie Wang. A two-stage imaging framework combining cnn and physics-informed neural networks for full-inverse tomography: A case study in electrical impedance tomography (eit), 2025.   
[96] Woojin Cho, Kookjin Lee, Noseong Park, Donsub Rim, and Gerrit Welper. Fastlrnr and sparse physics informed backpropagation, 2024.   
[97] Maciej Sikora, Albert Oliver-Serra, Leszek Siwik, Natalia Leszczyska, Tomasz Maciej Ciesielski, Eirik Valseth, Jacek Leszczyski, Anna Paszyska, and Maciej Paszyski. Graph grammars and physics informed neural networks for simulating of pollution propagation on spitzbergen, 2024.   
[98] Zhongkai Hao, Chengyang Ying, Hang Su, Jun Zhu, Jian Song, and Ze Cheng. Bi-level physics-informed neural networks for pde constrained optimization using broyden’s hypergradients, 2023.   
[99] Siddhant Dutta, Nouhaila Innan, Sadok Ben Yahia, and Muhammad Shafique. Aq-pinns: Attention-enhanced quantum physics-informed neural networks for carbon-efficient climate modeling, 2024.   
[100] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-informed neural networks for solving forward and inverse stochastic problems, 2018.   
[101] Zongren Zou, Xuhui Meng, Apostolos F Psaros, and George Em Karniadakis. Neuraluq: A comprehensive library for uncertainty quantification in neural differential equations and operators, 2022.   
[102] Ryoichiro Agata. Quantification of uncertainty and its propagation in seismic velocity structure and earthquake source inversion, 2025.   
[103] Xinchao Jiang, Xin Wanga, Ziming Wena, Enying Li, and Hu Wang. An e-pinn assisted practical uncertainty quantification for inverse problems, 2022.

# Disclaimer:

SurveyX is an AI-powered system designed to automate the generation of surveys. While it aims to produce high-quality, coherent, and comprehensive surveys with accurate citations, the final output is derived from the AI’s synthesis of pre-processed materials, which may contain limitations or inaccuracies. As such, the generated content should not be used for academic publication or formal submissions and must be independently reviewed and verified. The developers of SurveyX do not assume responsibility for any errors or consequences arising from the use of the generated surveys.

![](images/852a18885198019e2b34ce670c1b3e4efee29b125657372be2fba20ed8d044e7.jpg)  
Figure 3: This figure illustrates the hierarchical categorization of deep learning applications in computational fluid dynamics (CFD), highlighting specialized neural network architectures, innovative methodologies for solving Navier-Stokes equations, and advancements in turbulence modeling and high-fidelity simulations. The structured visualization underscores the integration of physicsinformed approaches and neural network innovations to enhance predictive accuracy and computational efficiency in fluid dynamics.

![](images/be8d4d5cc6f06a442562d03298f36d56d0d2428ff37a42c25de0c4b8d98ba46f.jpg)  
Figure 4: This figure illustrates the hierarchical categorization of deep learning methodologies in turbulence modeling, highlighting key turbulence models, numerical techniques, and high-fidelity simulation approaches.