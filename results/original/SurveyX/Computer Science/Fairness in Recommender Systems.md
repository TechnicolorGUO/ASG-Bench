# Algorithmic Fairness in Recommender Systems: A Survey

www.surveyx.cn

# Abstract

This survey paper provides a comprehensive examination of algorithmic fairness in recommender systems, addressing its principles, challenges, and mitigation strategies to ensure equitable and transparent personalized recommendations. We systematically analyze the sources of biasdata-driven, algorithmic, user interaction, and systemichighlighting their impact on diverse stakeholders. The paper presents a taxonomy of fairness notions, distinguishing between consumer-side and producer-side perspectives, and evaluates established and emerging fairness metrics. A critical review of bias mitigation techniques categorizes interventions into pre-processing, in-processing, and post-processing methods, alongside hybrid approaches that integrate multiple strategies. Ethical and regulatory considerations are discussed, emphasizing global guidelines and the tension between compliance and practical implementation. Key challenges, including fairnessperformance trade-offs, scalability, and interdisciplinary collaboration, are examined, with future research directions proposed to advance dynamic fairness mechanisms, intersectional analysis, and stakeholder-aware frameworks. This survey synthesizes state-of-the-art research to guide the development of recommender systems that balance accuracy, diversity, and equity while addressing societal and ethical imperatives.

# 1 Introduction

The rapid advancement of technology has significantly transformed the landscape of recommender systems, which are pivotal in shaping user experiences across various platforms, from e-commerce to social media. As these systems increasingly influence consumer choices and behavior, the importance of algorithmic fairness has come to the forefront of research and practice. Algorithmic fairness refers to the equitable treatment of individuals in algorithmic decision-making processes, ensuring that no particular group is unjustly disadvantaged or favored. This review aims to provide a comprehensive examination of the current state of algorithmic fairness in recommender systems, elucidating the foundational concepts, identifying sources of bias, and exploring mitigation techniques alongside ethical considerations. The significance of this topic lies not only in its theoretical implications but also in its practical ramifications for developers, policymakers, and users alike, making it essential to address these issues in a structured manner.

# 1.1 Structure of the Survey

This survey systematically examines algorithmic fairness in recommender systems through a structured organization that addresses foundational concepts, sources of bias, mitigation techniques, and ethical considerations. The survey begins with an introduction to the importance of fairness in recommender systems, establishing a critical framework for understanding the implications of biased algorithms. Following this introductory section, a detailed exploration of core concepts is presented in Section 2, which encompasses the foundations of recommender systems, key fairness notions, and the ethical implications associated with algorithmic decisions. This foundational knowledge is crucial for readers to appreciate the complexities of fairness in algorithmic contexts.

![](images/7cbd2f060f043e2121e9d0f7ae74a19c003c297307fb4c1a7b6923b344fce2b8.jpg)  
Figure 1: chapter structure

In Section 3, the survey categorizes and analyzes various sources of bias, including data-driven, algorithmic, and systemic biases. Each type of bias presents unique challenges and requires tailored approaches for identification and mitigation. By dissecting these biases, the survey highlights the multifaceted nature of fairness issues in recommender systems, emphasizing that biases can arise from multiple sources, including the data used for training algorithms and the algorithms themselves. This analysis not only clarifies the origins of bias but also sets the stage for understanding the importance of developing effective mitigation strategies.

Section 4 reviews fairness metrics and evaluation frameworks, distinguishing between consumerside and producer-side perspectives. This distinction is vital as it allows stakeholders to evaluate fairness from different vantage points, ensuring that both users and developers have a clear understanding of fairness implications. The metrics discussed provide a quantitative basis for assessing fairness outcomes, which is essential for guiding improvements in algorithmic design. Following this, Section 5 presents a taxonomy of bias mitigation techniques, including pre-processing, inprocessing, and post-processing methods, as well as emerging hybrid approaches. Each of these techniques plays a critical role in addressing identified biases, and their comparative effectiveness is explored to provide a nuanced understanding of how best to achieve fairness in practice.

Ethical and regulatory considerations are discussed in Section 6, highlighting global guidelines and stakeholder responsibilities. This section underscores the necessity for ethical frameworks that govern the development and deployment of recommender systems. As algorithms operate within societal contexts, the ethical implications of their use must be carefully considered to foster trust and accountability among users. Finally, Section 7 identifies open challenges, such as trade-offs between fairness and performance, and proposes future research directions, including dynamic fairness and interdisciplinary collaboration. This forward-looking perspective not only identifies gaps in the current research but also encourages ongoing dialogue among researchers, practitioners, and policymakers to address the evolving landscape of algorithmic fairness. This structured approach ensures a comprehensive and coherent narrative that guides readers through the multifaceted landscape of algorithmic fairness in recommender systems.The following sections are organized as shown in Figure 1.

# 2 Background and Core Concepts

# 2.1 Foundations of Recommender Systems and Algorithmic Fairness

Recommender systems are critical predictive algorithms that drive personalized suggestions across platforms such as e-commerce and social media, employing methodologies like collaborative filtering, content-based filtering, and hybrid approaches [1]. These techniques rely on modeling user preferences accurately but face challenges due to diverse user behaviors, similar to issues in stance detection where multiple valid interpretations coexist [2, 3]. The field of algorithmic fairness has evolved from prioritizing accuracy to incorporating diversity, serendipity, and equitable outcomes, with fairness notions classified into group, individual, and hybrid categories [4, 5]. These approaches vary between static constraints and dynamic mechanisms responsive to contextual changes in user interactions [6].

Fairness evaluation distinguishes consumer-side fairness, focusing on users, from producer-side fairness, emphasizing content visibility [7]. Advancements consider intersectional two-sided fairness, addressing both user and item dimensions [8, 9]. Two-sided marketplaces often suffer from biases due to disproportionate exposure of active users and popular items, impacting recommendation integrity [10]. Balancing accuracy with diversity metrics while considering stakeholder interests requires addressing trade-offs between system performance and fairness for minority groups, as seen in music recommendation systems [11, 12, 13, 14].

Emerging research underscores the importance of both user and item fairness in two-sided markets [15]. The complex trade-offs between these dimensions necessitate approaches that maintain recommendation quality and relevance [16]. Current frameworks address transparency, cognitive models, privacy-aware approaches, and mechanisms targeting popularity bias to ensure equitable representation [17]. Future research must develop adaptive fairness metrics that respond to user contexts and societal values while tackling the interplay between privacy and fairness in recommendation scenarios.

# 2.2 Key Fairness Concepts and Stakeholder Perspectives

Fairness metrics in recommender systems are diverse, including Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value, each presenting unique implementation challenges [13]. These metrics often conflict, particularly concerning stakeholder needs, as seen with content providers facing visibility inequalities categorized by popularity tiers [14]. Stakeholder analysis highlights three critical dimensions: user fairness, item fairness, and diversity, creating tradeoffs when optimizing these metrics [18]. Balancing fairness for users and items while maintaining system quality presents acute challenges in two-sided marketplaces [16].

Operationalizing fairness principles involves addressing power asymmetries in recommendation ecosystems, with consumers prioritizing accuracy and producers demanding equitable exposure [12, 19]. Platform operators must reconcile competing demands while adhering to ethical standards and regulatory requirements, necessitating frameworks that dynamically adjust fairness constraints based on contextual factors [20]. Developing adaptive mechanisms that account for fairness as a compositional system property will be crucial for managing these interactions within complex recommendation pipelines and ensuring practical alignment with ethical guidelines.

# 2.3 Ethical Implications and Societal Impact of Biased Recommendations

Biased recommendations have ethical ramifications affecting democratic discourse, economic equity, and cultural diversity, often conflating fairness with equality without contextual consideration [21]. Existing bias mitigation methods frequently introduce new patterns while failing to eliminate discrimination [22]. Algorithmic inability to handle uncertainty exacerbates inequities for marginalized groups [23]. Ethical challenges include engagement with adverse impacts, implementation guidelines, and tension between quality and fairness objectives, with healthcare systems exemplifying these dilemmas [24, 13].

The societal impact of biased recommendations perpetuates inequality, with content producers facing visibility barriers and consumers limited by non-diverse suggestions [25]. Addressing algorithmic bias is essential for equitable outcomes, requiring responsible AI research that considers societal implications and ethical responsibilities [26]. Implementing adaptive frameworks that integrate diverse ethical principles like transparency, justice, and fairness, alongside practical deployment considerations, is crucial for aligning AI practices with societal expectations and ensuring algorithmic accountability [20, 19]. Collaboration across disciplines will aid in crafting systems that meet technical and societal standards, maintaining validity and reliability of AI assessments.

# 3 Sources and Types of Bias in Recommender Systems

The multifaceted biases within recommender systems stem from various sources, influencing the fairness and efficacy of algorithmic suggestions. This section delineates four core bias categories: data-driven, algorithmic, user interaction, and systemic biases, each profoundly impacting recommendation algorithms. Understanding these biases illuminates complexities in ensuring equitable recommendations. As illustrated in Figure 2, this figure organizes the sources and types of biases within recommender systems into distinct categories, detailing the specific impacts and solutions relevant to each type. It highlights the complexities related to fairness and discrimination challenges, illustrating the relationships between data-driven, algorithmic, user interaction, and systemic biases. Data-driven biases arise from training datasets, where inherent disparities influence algorithmic fairness, significantly affecting diverse user groups [27]. The latency-fairness trade-off illustrates this challenge, as optimizing user fairness and average delay remains problematic [28].

![](images/b30d78873c7fb625ed512fb6acd4ca0e78e403fc37ab98452d82365c17587d2e.jpg)  
Figure 2: This figure organizes the sources and types of biases within recommender systems into distinct categories, detailing the specific impacts and solutions relevant to each type. It highlights the complexities related to fairness and discrimination challenges, illustrating the relationships between data-driven, algorithmic, user interaction, and systemic biases.

# 3.1 Data-Driven Biases

Data-driven biases in recommender systems emerge from distortions within training datasets, crucially shaping algorithmic fairness. These biases manifest through minority group underrepresentation, skewed interaction patterns, and systemic disparities in data collection. The dataset composition influences model behavior, often perpetuating discriminatory outcomes, particularly affecting non-mainstream users [27]. This is intensified by latency-fairness trade-offs, where user fairness and delay optimization remain challenging [28].

Three primary categories of data-driven biases dominate research: sampling biases, measurement biases, and historical biases. Sampling biases result from disproportionate representation, skewing training data toward mainstream preferences. Measurement biases stem from systematic errors in data collection, distorting user preferences. Historical biases reflect societal inequalities embedded in observational data, entrenching discriminatory practices in algorithmic outputs. Minority providers suffer from sampling biases, creating feedback loops that marginalize them by imbalanced exposure [29].

The relationship between user activity levels and recommendation quality highlights another dimension of data-driven bias. Active users significantly influence model training, leading to fairness gaps where they receive more accurate recommendations. Inconsistencies in user ratings further exacerbate this, disadvantaging specific groups through miscalibrated recommendations [30]. Protected attributes like gender and age can lead to systematic discrimination in outputs [31].

Addressing these biases involves reconciling fairness objectives. User fairness often conflicts with organizational fairness, aiming for balanced content [12]. Low public understanding of fairness metrics complicates decision-making about trade-offs [13]. Algorithms prioritize one dimensionuser fairness, item fairness, or diversityneglecting others and causing inequitable outcomes [18]. Balancing engagement and exposure remains challenging [16].

Intersectional analysis reveals compounding effects from interacting bias dimensions, with singlesided fairness methods failing to address this complexity. Historical injustices encoded in AI systems from training data underscore causal pathway analysis necessity [25, 32]. Emerging research advocates for adaptive sampling techniques and synthetic data generation to address representation gaps, preserving data utility. Standardized bias assessment protocols must include statistical and qualitative evaluations, addressing fundamental metric misapplication that leads to flawed conclusions [33]. Causal inference frameworks promise robust fairness interventions at the data level, accounting for biases’ compositional nature in complex pipelines.

# 3.2 Algorithmic and Model Biases

Algorithmic and model biases emerge from recommendation algorithms’ design, optimization objectives, and computational constraints, independent of data characteristics. Biases arise from preferential item treatment, disparity amplification, and systematic user discrimination. Post-processing algorithms can inadvertently introduce discrimination by altering ranking distributions, illustrating the complex balance between interventions and fairness outcomes [15].

Research identifies three primary algorithmic bias categories: popularity, feedback loop, and representation biases. Algorithms favor popular items, reinforcing cycles of visibility that marginalize lesser-known items. Feedback loop biases optimize recommendations based on past interactions, perpetuating inequalities and limiting diverse exposure. Representation biases result from models inadequately reflecting varied user preferences, failing to cater broadly and marginalizing underrepresented groups. These biases affect satisfaction, decision-making, and fairness in AI systems [34, 29, 5, 6].

To further elucidate these concepts, Figure 3 illustrates the main algorithmic biases, challenges in achieving fairness, and directions for future research in fairness-aware recommendation systems. This visual representation emphasizes the intricate relationships among the identified biases and the ongoing challenges in the field.

Operationalizing fairness constraints involves trade-offs between competing objectives. Fairnessaware algorithms requiring protected attributes raise privacy concerns, introducing discrimination via bias mitigation processes. Balancing individual and group fairness metrics complicates algorithm design. Neural recommendation models’ distributed representations exacerbate bias isolation challenges; mainstream bias leads to suboptimal recommendations, with cost-sensitive learning methods often ineffective [9, 27, 1, 35].

Emerging research advocates for model-agnostic fairness interventions applicable across paradigms while preserving recommendation systems’ core functions. Future work must tackle scalability challenges of fairness-aware algorithms in industrial-scale systems, where constraints limit complex bias mitigation feasibility. Adaptive fairness mechanisms show promise, dynamically responding to behavior and societal norm shifts, addressing evolving algorithmic biases’ impact on individual and societal decision-making. Interdisciplinary research explores fairness’s normative claims within various contexts [36, 5, 37, 6].

![](images/c8cfbb61f4b16fe28b732e158edd8f9243f845e1252904b0c11f3b210e99bbb6.jpg)  
Figure 3: This figure illustrates main algorithmic biases, challenges in achieving fairness, and directions for future research in fairness-aware recommendation systems.

# 3.3 User Interaction and Stakeholder Biases

User interaction and stakeholder biases in recommender systems emerge from interactions between human behavior, platform choices, and competing interests, distorting recommendation outcomes. Biases arise from self-selection effects, strategic provider behavior, and platform design decisions privileging particular interactions. Feedback loops amplify initial user interaction biases through iterative model updates [15], creating cycles where certain content gains visibility while others remain underrepresented.

Three interaction bias categories dominate research: activity, conformity, and presentation biases. Activity bias occurs when active users disproportionately influence outcomes, creating gaps where they receive more accurate recommendations. Conformity bias arises from users engaging with mainstream content, homogenizing recommendations. Presentation bias results from design choices affecting user selection, independent of preferences [18].

Stakeholder biases reflect competing interests among platform operators, providers, and users, creating complex trade-offs. Operators balance engagement metrics with equitable outcomes, prioritizing business goals reinforcing existing biases. Providers can manipulate algorithms, creating false popularity signals distorting quality. Such dynamics create trade-off spaces where optimizing one stakeholder’s interests often impedes others [16].

User demographics’ interaction with recommendation quality reveals substantial bias dimensions. Demographic groups can differ in interaction patterns due to cultural, access, or historical factors, leading to differential algorithm treatment. This challenge intensifies with intersectional identities, where multiple characteristics create compounded biases beyond single-axis fairness approaches [8].

Emerging research focuses on interaction-aware metrics addressing diverse group behaviors, ensuring recommendation quality. This involves methodologies adapting to unique group behaviors and preferences, mitigating algorithm biases, promoting equitable outcomes in information dissemination [38, 7, 37, 8, 6]. Future work must address designing interfaces mitigating presentation bias without compromising user experience, and stakeholder-aware optimization frameworks balancing interests transparently. Participatory design methods promise recommendations reflecting diverse user values, addressing inherent platform power asymmetries.

# 3.4 Systemic and Intersectional Biases

Systemic and intersectional biases arise from entrenched societal inequalities and demographic interplays, disadvantaging marginalized groups. These biases reflect power asymmetries beyond technical limits, manifesting through underrepresentation and misrepresentation in outcomes. Insufficient diversity in AI research perpetuates exclusionary patterns [31], as AI technologies intersect with and exacerbate inequalities against marginalized groups [25].

Systemic biases in recommender systems fall into institutional, cultural, and historical categories. Institutional biases embed in platform policies favoring certain interactions or content types. Cultural biases reflect societal norms, amplifying mainstream preferences over minority ones. Historical biases encode discrimination patterns through generations, creating feedback loops perpetuating inequality. These dimensions interact multiplicatively, causing compounded disadvantages that single-axis fairness approaches overlook.

Operationalizing intersectional fairness faces unique challenges, requiring multiple attribute consideration and interaction analysis. Current metrics focusing on single dimensions neglect intersecting identity complexities. This limitation is evident in recommendation scenarios with compounded gender, race, and socioeconomic disadvantages. Algorithmic auditing lacks granularity to detect intersectional discrimination and frameworks for societal implication interpretation.

Research focuses on developing intersectional metrics accounting for multiplicative effects of demographic interactions. Future work must tackle privacy-preserving design tensions in granular analysis, engaging marginalized voices in fairness evaluations. Critical race theory and feminist technoscience perspectives promise nuanced frameworks beyond technical solutions, addressing systemic biases’ root causes.

# 4 Fairness Metrics and Evaluation Frameworks

Understanding fairness in recommender systems necessitates a comprehensive grasp of the metrics and evaluation frameworks that define this domain. Fairness discourse involves both consumer-side and producer-side perspectives, each with unique criteria and implications, crucial for designing metrics that ensure equitable outcomes for users, operators, and content providers by addressing the inherent biases in algorithmic systems [32, 39, 40, 6]. Beyond ethical compliance, fairness metrics are vital for fostering trust and transparency in algorithmic decision-making. As recommender systems increasingly shape user experiences, understanding these metrics is key to creating an inclusive digital environment.

The following subsection will categorize fairness notions, detailing frameworks that articulate fairness principles. This classification will clarify how various fairness concepts interact and guide the development of effective evaluation methodologies, offering a structured approach to assessing fairness in recommender systems. By organizing these notions, researchers can identify overlaps and divergences, leading to more nuanced fairness assessments and practical guidance for implementing fairness-aware algorithms.

# 4.1 Taxonomy of Fairness Notions

Fairness in recommender systems encompasses diverse notions, addressing equitable treatment across stakeholders. Research categorizes these notions by stakeholder group (consumer-side vs. producer-side), temporal dynamics (static vs. adaptive), and granularity (individual vs. group fairness). Consumer-side fairness ensures recommendations align with user preferences without discrimination, while producer-side fairness ensures content providers receive fair exposure [12]. This dichotomy highlights tensions between user-centric and provider-centric perspectives, particularly in music recommendation systems where algorithms like Most-pop exhibit high bias towards mainstream artists [14].

Static fairness applies fixed constraints, while dynamic mechanisms adapt to contextual changes. Static frameworks use predefined thresholds, whereas dynamic approaches adjust to user behaviors and system states. The model by [16] characterizes multi-sided fair recommendations, balancing user and item fairness. Dynamic models accommodate real-world complexities by recognizing user interaction fluidity.

Individual fairness ensures similar users receive comparable recommendations, while group fairness ensures equitable outcomes across demographics. User rating inconsistencies affect individual fairness, impacting recommendation quality [30]. Addressing these challenges requires understanding fairness across different analysis levels. Emerging notions emphasize ethical axioms, integrating individual agency and compliance in decision-making, recognizing that technical definitions often overlook power asymmetries [12].

This discussion is further illustrated in Figure 4, which presents a comprehensive taxonomy of fairness notions in recommendation systems. The figure categorizes these notions by stakeholder groups, temporal dynamics, and granularity, while also highlighting key studies that address these dimensions. Future research should develop unified frameworks accommodating multiple fairness notions while ensuring computational efficiency and applicability.

![](images/7348c6e8a913793e7d951dc04a069954f8d9ba81e23e01be46b34a36f7b26649.jpg)  
Figure 4: This figure illustrates the taxonomy of fairness notions in recommendation systems, categorizing them by stakeholder groups, temporal dynamics, and granularity, highlighting key studies that address these dimensions.

# 4.2 Consumer-Side and Producer-Side Fairness Metrics

Evaluating fairness in recommender systems requires distinct metrics for consumer-side and producer-side perspectives, reflecting their divergent objectives. Consumer-side fairness metrics, such as Disparate Impact and Accuracy Difference, assess bias across demographics [41]. Disparate Impact evaluates outcome ratios between protected and unprotected groups, while Accuracy Difference measures prediction accuracy gaps. Balanced Accuracy considers true positive and negative rates, offering nuanced performance assessments [42]. Careful metric selection fosters user trust and ensures fair system operation.

Producer-side metrics address visibility and exposure equity, crucial in two-sided marketplaces where algorithmic decisions affect economic opportunities. Metrics like Normalized Discounted Cumulative Gain (nDCG) fairness and Average Recommendation Popularity (ARP) quantify recommendation slot distribution across items or creators. Analysis shows systematic biases in platform visibility, with certain content categories receiving disproportionate exposure. Causal disparity analysis reveals sensitive attributes, like race, influencing outcomes, highlighting the need for nuanced understanding of exposure dynamics [29, 32]. Balancing consumer and producer fairness requires trade-off analysis frameworks modeling these interdependencies.

Operationalizing these metrics involves context-specific considerations, including recommendation domain nature and stakeholder concerns’ relative importance. Metrics must account for individual fairness (comparable recommendations for similar users) and group fairness (preventing disadvantage for protected classes). Metric selection shapes bias mitigation strategies’ effectiveness, as different combinations yield varied fairness assessments. Research highlights the nuanced, modelspecific nature of fairness evaluations, emphasizing a comprehensive approach to addressing AI fairness [42, 7, 6]. Future research should develop unified evaluation frameworks accommodating multiple stakeholder perspectives while ensuring computational efficiency across diverse scenarios.

# 4.3 Evaluation Frameworks and Methodologies

Contemporary fairness evaluation frameworks in recommender systems integrate technical metrics with stakeholder perspectives. The CPFair method exemplifies this trend, using an optimizationbased re-ranking approach addressing consumer and producer fairness constraints [4]. This framework extends beyond accuracy metrics, incorporating fairness-aware protocols ensuring quality recommendations and equitable outcomes. Emphasizing both dimensions ensures fairness is integral to the recommendation process.

Table 1 provides a comprehensive overview of key benchmarks utilized for evaluating fairness and bias detection methodologies within recommender systems and related domains, highlighting their scope and metric frameworks.

<html><body><table><tr><td>Benchmark</td><td>Size</td><td>Domain</td><td>Task Format</td><td>Metric</td></tr><tr><td>POI-Fairness[10]</td><td>618.621</td><td>Point-of-Interest Recommendation</td><td>Top-k Recommendation</td><td>NDCG,GCE</td></tr><tr><td>Clarify[41]</td><td>1,000,000</td><td>Bias Detection</td><td>Classification</td><td>Disparate Impact, Accu- racy Difference</td></tr><tr><td>AIF360[42]</td><td>59,842</td><td>Bias Mitigation</td><td>Binary Classification</td><td>Balanced Accuracy, Dis- parate Impact</td></tr><tr><td>UFR[38]</td><td>1,000,000</td><td>Recommender Systems</td><td>Fairness Evaluation</td><td>NDCG@10,F1@10</td></tr><tr><td>REVISE[43]</td><td>100,000</td><td>Computer Vision</td><td>Bias Detection</td><td>Object Counts,Gender Representation</td></tr><tr><td>URC[30]</td><td>1,000,209</td><td>Movie Recommendation</td><td>Recommendation Calibration</td><td>Kullback-Leibler Diver- gence,Precision@10</td></tr></table></body></html>

Table 1: This table presents a selection of representative benchmarks used for evaluating fairness, bias detection, and recommendation systems across various domains. It details the size, task format, and evaluation metrics for each benchmark, providing insights into their applicability and focus areas within the context of fairness and recommendation research.

Advancements introduce dynamic frameworks adapting to evolving conditions and priorities. The SCRUF-D framework employs a multi-agent architecture addressing multiple fairness concerns through specialized agents [44]. This approach enables real-time fairness evaluation across user fairness, item fairness, and diversity metrics, incorporating new criteria as they emerge. Stakeholder consultation ensures fairness definitions align with contextual and ethical considerations, addressing static metrics’ limitations. Such adaptability is crucial in the rapidly evolving digital landscape.

In-processing methodologies integrate fairness assessment into model training. The In-UCDS framework embeds fairness constraints within the learning process, enabling early bias detection and mitigation [35]. This approach enhances robustness against bias, effectively addressing active users’ and popular items’ unfairness [10]. Integrating fairness from the outset strengthens recommender systems against discrimination.

Emerging frameworks incorporate uncertainty quantification, addressing fairness assessment’s epistemic limitations in complex scenarios. Uncertainty modeling plays a critical role in ethical AI, accounting for incomplete information and ambiguous trade-offs [23]. These frameworks introduce confidence intervals and probabilistic guarantees, providing nuanced equity assessments under realworld conditions. This approach is valuable in high-stakes domains where fairness decisions carry significant consequences.

Comprehensive methodologies must address metric selection and interpretation challenges, where inappropriate measures can mislead performance conclusions. Surveys categorize evaluation approaches across domains, emphasizing context-specific metric selection aligned with application requirements [33]. User feedback mechanisms, demonstrated in frameworks assessing empathy and reliability, provide qualitative insights complementing quantitative measurements [45]. Future research should develop unified frameworks bridging technical metrics and human-centered methodologies, balancing competing fairness objectives in complex ecosystems. This holistic approach is crucial for advancing recommender systems and prioritizing fairness in algorithmic design.

# 5 Bias Mitigation Techniques

<html><body><table><tr><td>Category</td><td>Feature</td><td>Method</td></tr><tr><td>Pre-processing Techniques</td><td>Equity and Bias Management</td><td>URPF[46], ITFR[8]</td></tr><tr><td>In-processing Techniques</td><td>PaiesFocsedechnithds</td><td>A74O4]U5</td></tr><tr><td>Post-processing Techniques</td><td>Fairness Adjustment</td><td>PL-ICFW[29]</td></tr><tr><td rowspan="2">Emerging and Hybrid Approaches</td><td>Collaborative Systems</td><td>SCRUF-D[44]</td></tr><tr><td>Ethical and Bias Considerations Information Management</td><td>N/A[50], CDA[32],TDA[22] TS-AI[51]</td></tr></table></body></html>

Table 2: This table provides a comprehensive overview of bias mitigation techniques in recommender systems, categorized by intervention stages: pre-processing, in-processing, post-processing, and emerging hybrid approaches. Each category highlights specific features and methods, illustrating the range of strategies employed to address equity, bias, privacy, and ethical considerations in recommendation algorithms. The table serves as a detailed taxonomy, aiding in the understanding and comparison of these techniques within the context of fairness-enhancing methodologies.

Exploring bias mitigation techniques in recommender systems is essential for achieving equitable outcomes across diverse user groups and enhancing system fairness. This section categorizes strategies into three primary intervention stages: pre-processing, in-processing, and post-processing, each presenting unique methodologies and challenges. By examining these techniques systematically, we aim to provide a comprehensive understanding of their functionalities and implications, setting the stage for a detailed taxonomy of bias mitigation techniques. Table 2 presents this detailed taxonomy, categorizing the techniques into distinct intervention stages and highlighting their respective features and methods. Additionally, Table 5 offers an in-depth taxonomy of bias mitigation techniques, categorizing them by intervention stage and detailing their distinct characteristics and methodological approaches.

# 5.1 Taxonomy of Bias Mitigation Techniques

Bias mitigation techniques in recommender systems are organized along three dimensions: intervention stage, targeted bias type, and stakeholder perspective. This framework allows a comparative analysis of fairness-enhancing approaches, highlighting their strengths and limitations while addressing algorithmic bias complexities [39, 37, 9, 5, 6]. As illustrated in Figure 5, the taxonomy categorizes these techniques, providing a comprehensive framework for addressing algorithmic bias complexities. Intervention stages include pre-processing, which modifies input data to rectify biases before training; in-processing, which adjusts learning algorithms during training; and post-processing, which operates on model outputs to meet fairness criteria. Bias types targeted include demographic, content-based, and systemic biases arising from user-item interactions. The stakeholder perspective considers the distinct fairness requirements of users, content providers, and platform operators. Integrating these dimensions aids in navigating bias complexities, leading to more effective interventions and serving as a roadmap for future research in fairness in AI.

![](images/187d9fbd8f3e4f951029094ea7e237df19333305a71467f0eab85bca584fcacf.jpg)  
Figure 5: This figure illustrates the taxonomy of bias mitigation techniques in recommender systems, categorized by intervention stages, bias types, and stakeholder perspectives, providing a comprehensive framework for addressing algorithmic bias complexities.

# 5.2 Pre-processing Techniques

<html><body><table><tr><td>Method Name</td><td>Bias Mitigation</td><td>Methodology Diversity</td><td>Adaptability Requirements</td></tr><tr><td>URPF[46]</td><td>Upsampling Interactions</td><td>Fairness-aware Approaches</td><td>Adaptive Frameworks Needed</td></tr><tr><td>ITFR[8]</td><td>Sharpness-aware Loss</td><td>Collaborative Loss Balance</td><td>Adaptive Learning Process</td></tr><tr><td>TDA[22]</td><td>Targeted Data Augmentation</td><td>Data Augmentation</td><td>Augmentation Policy Design</td></tr></table></body></html>

Table 3: Overview of pre-processing techniques for bias mitigation, methodology diversity, and adaptability requirements in recommendation systems. This table highlights the URPF, ITFR, and TDA methods, emphasizing their approaches to fairness and adaptability in handling biased datasets.

Table 3 provides a comprehensive summary of various pre-processing techniques aimed at mitigating biases in recommendation systems, detailing their methodologies and adaptability requirements. Pre-processing techniques address biases at the data level before model training, focusing on representation imbalances and skewed interaction patterns. The Upsampling with Regularization for Provider Fairness (URPF) method adjusts relevance scores to enhance fairness for minority providers, preventing marginalization of niche creators [46]. Intersectional fairness approaches like the Intersectional Two-sided Fairness Recommendation (ITFR) framework balance treatment across intersectional groups using innovative loss functions [8]. Targeted Data Augmentation (TDA) introduces biases into training datasets to build model resilience against discriminatory patterns [22]. The effectiveness of these techniques depends on bias identification quality and protected attribute granularity, with trade-offs between reducing biases and preserving data utility. Future research should develop adaptive pre-processing frameworks that adjust intervention intensity based on dataset characteristics and application requirements, integrating causal inference for precise bias mitigation.

# 5.3 In-processing Techniques

<html><body><table><tr><td>Method Name</td><td>Fairness Techniques</td><td>Optimization Strategies</td><td>Challenges and Future Directions</td></tr><tr><td>DF[47]</td><td>Minority Indexes</td><td>Tailored Loss Function</td><td>Simplifying The Architecture</td></tr><tr><td>DRFO[48]</td><td>Fair Learning Framework</td><td>Tailored Loss Functions</td><td>Computational Complexity</td></tr><tr><td>In-UCDS[35]</td><td>Fairness Loss</td><td>Representation Gap</td><td>Training Time</td></tr><tr><td>CSM-BM[27]</td><td>User-weighting Approach</td><td>Loss Function Adjustment</td><td>Sample Selection Bias</td></tr><tr><td>PAM[49]</td><td>Privacy-adversarial Framework</td><td>Adversarial Training</td><td>Enhancing Robustness</td></tr></table></body></html>

Table 4: Overview of in-processing techniques for enhancing fairness in recommendation systems, highlighting the specific fairness techniques, optimization strategies, and challenges associated with each method. The table provides a comparative analysis of methods such as DeepFair, DRFO, InUCDS, CSM-BM, and PAM, focusing on their unique approaches to balancing recommendation quality and fairness.

Table 4 presents a comparative overview of various in-processing techniques employed to integrate fairness constraints in recommendation systems, detailing the methods, optimization strategies, and associated challenges. In-processing techniques modify learning algorithms to incorporate fairness constraints during training, offering direct control over bias pathways. DeepFair uses a tailored loss function to balance recommendation quality with equitable outcomes [47]. The Distributionally Robust Fair Optimization (DRFO) approach constructs ambiguity sets around sensitive attributes, ensuring robustness across distributions [48]. User-Constrained Dominant Sets (UCDS) adjusts user representations based on relational patterns for relative fairness [35]. Mainstream bias mitigation employs adaptive weighting strategies to prioritize non-mainstream users [27]. Privacy-preserving methods use adversarial learning to prevent recovery of sensitive attributes while maintaining accuracy [49]. Convex optimization frameworks ensure fairness through constrained optimization, effective in multi-stakeholder scenarios [18]. Practical challenges include computational complexity and potential conflicts between fairness objectives. Future research should focus on efficient optimization algorithms and adaptive fairness constraints.

# 5.4 Post-processing Techniques

Post-processing techniques adjust model outputs to ensure fairness without altering model architecture, offering flexibility in fairness implementation. The PL-ICFW algorithm adjusts exposure distribution based on item popularity patterns [29]. These techniques employ constrained optimization frameworks or heuristic approaches to maximize utility subject to fairness constraints. Challenges include maintaining recommendation relevance while satisfying fairness constraints, with advanced scoring functions integrating fairness considerations to balance competing objectives [9, 40, 18, 8, 6]. Emerging research focuses on adaptive frameworks that modify fairness constraints based on realtime monitoring and stakeholder feedback [37, 6]. Future work should address scalability limitations and enhance transparency through explainable AI techniques.

# 5.5 Emerging and Hybrid Approaches

Emerging and hybrid approaches combine multiple strategies or introduce novel mechanisms to address complex bias patterns. The SCRUF-D architecture uses a multi-agent design for dynamic fairness adjustments [44]. The ECCOLA method operationalizes ethical considerations throughout the recommendation pipeline [50]. Causal Disparity Analysis quantifies bias pathways for targeted mitigation [32]. Transparency schemes categorize AI system information to facilitate fairness assessment [51]. Hybrid approaches address unique challenges in non-personalized recommendation contexts [12]. Retrieval-augmented generation systems present fairness challenges requiring innovative solutions [52]. Targeted Data Augmentation (TDA) introduces biases during training for robust model performance [22]. Future research should focus on scalability, standardized evaluation protocols, and adaptive mechanisms for real-time fairness parameter adjustments, contributing to more trustworthy and equitable recommender systems [5, 37, 6, 7].

<html><body><table><tr><td>Feature</td><td></td><td></td><td>Pre-processing TechniquesIn-processing TechniquesPost-processing Techniques</td></tr><tr><td>Intervention Stage</td><td>Data Level</td><td>Algorithmic Level</td><td>Output Level</td></tr><tr><td>Targeted Bias Type</td><td>Demographic,Systemic</td><td>Sensitive Attributes</td><td>Item Popularity</td></tr><tr><td>Stakeholder Perspective</td><td>Users,Providers</td><td>Users,Multi-stakeholder</td><td>Users,Feedback</td></tr><tr><td colspan="4">Table 5: This table provides a comprehensive comparison of bias mitigation techniques across three intervention stages: pre-processing, in-processing,and post-processing. It highlights the specific features,targeted bias types,and stakeholder perspectives associated with each stage,ofering a structured overview of the methodologies employed to address algorithmic biases in recommender systems.</td></tr></table></body></html>

# 6 Ethical and Regulatory Considerations

The ethical and regulatory environment for AI in recommender systems is complex, shaped by varying principles and interpretations globally. This section explores foundational ethical guidelines that inform AI development, serving as benchmarks for fairness, accountability, and transparency in algorithmic decision-making. Understanding these frameworks reveals the shared aims and the complexities of applying them across diverse cultural and regulatory settings, thus framing the discussion on AI fairness.

# 6.1 Global Ethical Guidelines and Frameworks

International ethical guidelines for AI fairness emphasize addressing algorithmic biases while respecting diverse cultural and regulatory contexts. Global initiatives focus on transparency, justice, non-maleficence, responsibility, and privacy [20]. These principles highlight both a consensus on objectives and challenges in achieving uniform application across jurisdictions, impacting the implementation of fairness-aware systems. The DRESS-eAI framework showcases an integrated approach to ethical AI governance, embedding cross-functional perspectives throughout the AI lifecycle to ensure fairness [53]. This approach underscores the need for accountability beyond technical solutions, fostering a culture of ethical awareness in AI development.

Regional variations in ethical guideline implementation, such as the influence of the Blueprint for an AI Bill of Rights on U.S. federal agencies, highlight tensions between voluntary frameworks and enforceable regulations [54]. Bridging the gap between expert discourse and public understanding is crucial for informed engagement with AI technologies [55]. Emerging frameworks advocate for collaborative ethical design, engaging technologists, ethicists, and policymakers to integrate ethical considerations early in system development [56]. This participatory approach enhances ethical robustness and shared responsibility.

Operationalizing ethical principles in recommender systems involves challenges in metric selection and interpretation. Misapplication of fairness measures can lead to misleading conclusions about performance, emphasizing the need for context-specific evaluation protocols [33]. Explainability frameworks distinguishing between XAI and IAI provide guidance for balancing transparency with complexity [57]. Future efforts should focus on translating ethical principles into technical specifications while accommodating diverse application requirements and cultural values, ensuring ethical considerations are practically integrated into AI development.

# 6.2 Regulatory Compliance and Challenges

Regulatory compliance for algorithmic fairness in recommender systems is challenged by evolving governance frameworks and their application. Variations in scope and enforcement across jurisdictions create complex compliance requirements. The non-binding nature of frameworks like the AI Bill of Rights highlights the tension between ethical principles and enforceable standards [54]. Navigating this landscape requires understanding the interplay between regulatory expectations and operational realities.

Key compliance challenges include divergent interpretations of fairness, conflicting obligations across jurisdictions, and lack of standardized protocols. Global AI ethics guidelines show convergence in principles but divergence in definitions and enforcement [20]. This variability complicates compliance for multinational platforms. Static compliance strategies risk obsolescence as standards evolve, necessitating adaptive strategies.

Operationalizing fairness regulations faces technical challenges, such as metric selection and interpretation, without clear regulatory benchmarks. Intersectional fairness requirements are often overlooked, focusing on single attributes rather than interactions. Lack of standardized evaluation protocols complicates compliance verification. Addressing these challenges ensures compliance aligns with ethical principles.

Emerging regulatory paradigms call for adaptive compliance frameworks to accommodate evolving fairness standards. Continuous compliance monitoring systems can enhance alignment with standards, addressing dynamic challenges posed by regulatory variations and AI’s ethical complexities. This approach promotes responsible AI development by integrating fairness, transparency, and user agency into decision-making processes.

# 6.3 Stakeholder Responsibilities and Ethical AI Development

Ethical AI in recommender systems requires a multi-stakeholder framework delineating roles and responsibilities throughout development. Technical developers must implement fairness-aware algorithms, addressing challenges like candidate set imbalance [29]. Modular architectures should integrate ethical considerations without compromising functionality, recognizing that per-component fairness does not ensure overall equity [9].

Content providers and platform operators must mitigate risks through comprehensive stakeholder involvement, amplifying marginalized voices and maintaining transparency [58, 26]. Structured maturity models enable systematic assessment and improvement of ethical practices [24]. Engaging diverse stakeholders fosters an inclusive AI development approach prioritizing fairness and equity.

End-users contribute to ethical AI through engagement with interfaces and feedback mechanisms, aligning with responsible AI principles emphasizing accountability and transparency [59]. Users should influence fairness parameters, particularly where algorithmic decisions impact opportunities. Balancing user agency with technical complexity requires intuitive interfaces communicating ethical considerations.

Regulatory bodies and civil society organizations establish standards and oversight aligning technical implementations with societal values. They must balance prescriptive regulations with innovation-friendly environments, developing adaptive frameworks accommodating evolving capabilities while protecting rights. Collaborative governance models fostering dialogue among stakeholders are essential for enhancing ethical practices in recommendation systems, addressing fairness, transparency, and accountability. Integrating insights from various fields can create effective frameworks operationalizing ethical principles and facilitating best practices in AI governance, promoting responsible and equitable AI systems.

# 7 Challenges and Future Directions

Recommender systems face multifaceted challenges related to algorithmic fairness that necessitate a nuanced understanding of the interplay between fairness and system performance. This section delves into specific dimensions of these challenges and future directions to achieve equitable outcomes without sacrificing efficacy.

# 7.1 Trade-offs Between Fairness and System Performance

Designing recommender systems that balance fairness with performance metrics poses a complex optimization challenge. These trade-offs occur across recommendation accuracy, computational efficiency, and business objectives, requiring context-specific strategies. Fairness constraints can incur performance costs, particularly in diverse recommendation scenarios [7, 16]. Empirical studies show that addressing item fairness may reduce efficacy, highlighting the need for calibrated trade-offs. Adaptive weighting approaches can enhance performance for underserved groups while maintaining overall quality [27, 49]. Intersectional identities further complicate fairness, as compounded disadvantages often escape existing methods [31].

Dynamic mechanisms, such as multi-agent architectures, are crucial for balancing objectives through continuous monitoring and real-time balancing [44]. In-processing techniques offer equilibrium across diverse contexts, though scalability remains an issue for large-scale systems [35]. Nonpersonalized scenarios risk ideological biases despite diversity improvements [12]. Governance gaps, like those in the AI Bill of Rights adoption, add uncertainty to compliance and further complicate trade-off management [54]. Future research must develop adaptive frameworks that consider multidimensional fairness while maintaining practicality, possibly integrating causal inference to distinguish genuine performance limitations from spurious trade-offs [60].

# 7.2 Scalability and Practical Implementation

Implementing fairness-aware systems at scale involves substantial computational demands, exacerbated by dynamic fairness adjustments and large-scale data volumes [27, 44]. Main scalability bottlenecks include the computational cost of optimization, storage for fairness metadata, and latency from fairness constraints. Algorithms like TSFD Rank require significant resources [18], while methods such as In-UCDS face scalability limitations when applied to large user bases [35]. Privacy constraints complicate metadata access needed for fairness assessment due to varying jurisdictional requirements [49]. Integrated approaches are needed for holistic fairness within multi-stage recommendation pipelines [9].

Emerging modular frameworks, like SCRUF-D and CPFair, promise scalable implementations by distributing computations across components or using post-processing layers [44, 4]. Future research should focus on lightweight mechanisms minimizing computational overhead, leveraging distributed processing or edge computing for latency challenges, and developing standardized interfaces for fairness metadata to enhance interoperability across diverse recommendation platforms.

# 7.3 Interdisciplinary Collaboration and Ethical Integration

Ethical AI in recommender systems demands interdisciplinary collaboration that bridges technical and social domains. This participatory approach promotes mutual responsibility and dialogue among developers, domain experts, and communities [61]. Successful collaboration hinges on methodological integration, institutional support, and stakeholder engagement, addressing technical metrics and qualitative assessments while respecting moral pluralism across regions [20]. Barriers include divergent terminologies, conflicting evaluation criteria, and institutional silos.

Unified frameworks translating ethical principles into actionable specificationscovering all SDLC phasesare vital for implementing Responsible AI across contexts [62, 20]. Participatory design methods embed diverse stakeholder perspectives into system processes, enhancing ethical integration. Future efforts should develop standardized interdisciplinary protocols and translational roles to bridge gaps between technical and non-technical domains. Educational initiatives fostering interdisciplinary literacy can enhance collaboration in algorithmic fairness [63].

# 7.4 Dynamic and Adaptive Fairness

Dynamic fairness offers a paradigm where systems respond to evolving conditions and stakeholder priorities. SCRUF-D’s multi-agent design enables real-time fairness adjustments [44]. This is crucial in dynamic environments, where shifts in demographics or content distributions render static interventions ineffective [18]. Context-aware mechanisms, like the In-UCDS method, identify and respond to emerging fairness violations [35]. High-stakes decisions require stringent fairness guarantees, necessitating adaptable systems.

Feedback loops, such as those in CPFair, refine fairness constraints over time [4], aligning with responsible AI principles emphasizing continuous learning and adaptation [26]. Future research should explore adaptive algorithms and establish protocols for rigorous evaluation, ensuring equitable outcomes in large-scale systems. Hybrid approaches, combining rule-based and learning-based strategies, offer robust frameworks for balancing interpretability with flexibility. Transparency in reporting fairness dynamics allows transparent assessment of adaptive systems’ performance.

# 7.5 Future Research Directions

Advancing fairness in recommender systems requires exploration across dimensions addressing limitations and emerging challenges. Key areas include developing technical solutions for operationalizing fairness frameworks alongside real-world implementation barriers. Refining value taxonomies and learning mechanisms captures human complexities in diverse contexts [64]. Methodological advancements in fairness evaluation and metric composition need refinement for interpretability and effectiveness in analytics [33].

Intersectional fairness mechanisms and bias detection methods demand focused research, aiming to actively incorporate intersectional analysis and refine automation processes [31, 43]. Learning processes need to adapt to evolving ethical standards, exploring applications in new domains, and integrating user feedback for improved framework robustness [65, 21].

Standardized protocols for assessment that balance accuracy, diversity, and fairness while minimizing computational costs represent promising directions. Research should develop scalable architectures, including adaptive mechanisms responding to evolving norms [13], exploring novel aggregation rules for strategy-proofness alongside ethical constraints [51].

Transparency and uncertainty require investigation, aiming at standardizing transparency schemes and representing uncertainty in algorithms [23]. Accountability frameworks should identify trends influencing AI development, focusing on bias types and evaluation metrics for recommendation algorithms [25, 10]. Data augmentation techniques need refinement and application to broader datasets [22], ensuring adaptive systems consider context and user preferences while enhancing public understanding of algorithmic fairness [13].

# 8 Conclusion

The exploration of algorithmic fairness in recommender systems has revealed significant strides alongside enduring challenges. Notable advancements have been made in enhancing consumer-side fairness through innovative methodologies, yet the field grapples with the lack of consensus on evaluation metrics and the practical application of ethical principles. Intersectional fairness frameworks, such as ITFR, offer promising approaches to mitigate compounded disadvantages for marginalized groups while preserving recommendation accuracy. Fairness is increasingly recognized as a concern that transcends technical implementations, engaging ethical and societal dimensions. Despite progress in theoretical frameworks for responsible AI, challenges remain in integrating human values and achieving transparency in system development. Limited public awareness of ethical AI issues further complicates stakeholder engagement and accountability, while the tension between global ethical standards and local contexts necessitates adaptable approaches that respect cultural diversity.

The reconciliation of competing fairness objectives, especially the trade-offs between recommendation quality and equitable outcomes, remains a critical area for future research. There is a pressing need for dynamic fairness mechanisms that adapt to changing user behaviors and societal norms, standardized evaluation protocols inclusive of multi-stakeholder perspectives, and scalable implementations that ensure computational efficiency. Interdisciplinary approaches will be crucial in advancing the technical and ethical foundations of fairness-aware systems, aligning them with human values while delivering practical benefits across diverse domains.

Algorithmic fairness in recommender systems encompasses a complex interplay of ethical considerations, societal expectations, and technological progress. As the digital landscape evolves, the significance of fairness in recommendations will grow, impacting user trust and engagement. Future research should focus on developing frameworks that enhance transparency and promote inclusive practices reflecting diverse user experiences. Collaboration with stakeholders from various fields, including ethics, sociology, and technology, will be essential in crafting comprehensive solutions that address fairness nuances in real-world applications.

The pursuit of fairness in recommender systems is an ongoing journey marked by challenges and opportunities. Insights from this survey lay the groundwork for future exploration and innovation. By fostering a collaborative approach that values diverse perspectives, the field can progress towards recommender systems that are both effective and equitable, contributing to a more inclusive digital ecosystem where all users benefit from fair and unbiased recommendations.

# References

review on diversity, serendipity and fairness in recommender systems based on graph neural networks, 2023.   
[2] Dong Liu and Chenyang Yang. Caching at base stations with heterogeneous user demands and spatial locality, 2018.   
[3] Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, and Fosca Giannotti. Multi-perspective stance detection, 2024.   
[4] Mohammadmehdi Naghiaei, Hossein A. Rahmani, and Yashar Deldjoo. Cpfair: Personalized consumer and producer fairness re-ranking for recommender systems, 2022.   
[5] Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu, and Yongfeng Zhang. Fairness in recommendation: Foundations, methods and applications, 2023.   
[6] Yashar Deldjoo, Dietmar Jannach, Alejandro Bellogin, Alessandro Difonzo, and Dario Zanzonelli. Fairness in recommender systems: Research landscape and future directions, 2023.   
[7] Ludovico Boratto, Gianni Fenu, Mirko Marras, and Giacomo Medda. Consumer fairness in recommender systems: Contextualizing definitions and mitigations, 2022.   
[8] Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, and Shaoping Ma. Intersectional two-sided fairness in recommendation, 2024.   
[9] Xuezhi Wang, Nithum Thain, Anu Sinha, Flavien Prost, Ed H. Chi, Jilin Chen, and Alex Beutel. Practical compositional fairness: Understanding fairness in multi-component recommender systems, 2021.   
[10] Hossein A. Rahmani, Yashar Deldjoo, Ali Tourani, and Mohammadmehdi Naghiaei. The unfairness of active users and popularity bias in point-of-interest recommendation, 2022.   
[11] Andrés Domínguez Hernández and Vassilis Galanos. A toolkit of dilemmas: Beyond debiasing and fairness formulas for responsible ai/ml, 2023.   
[12] Aadi Swadipto Mondal, Rakesh Bal, Sayan Sinha, and Gourab K Patro. Two-sided fairness in non-personalised recommendations, 2020.   
[13] Veronica Kecki and Alan Said. Understanding fairness in recommender systems: A healthcare perspective, 2024.   
[14] Himan Abdollahpouri, Robin Burke, and Masoud Mansoury. Unfair exposure of artists in music recommendation, 2020.   
[15] Jurek Leonhardt, Avishek Anand, and Megha Khosla. User fairness in recommender systems, 2018.   
[16] Sophie Greenwood, Sudalakshmee Chiniah, and Nikhil Garg. User-item fairness tradeoffs in recommendations, 2024.   
[17] Dominik Kowald. Transparency, privacy, and fairness in recommender systems, 2024.   
[18] Lequn Wang and Thorsten Joachims. User fairness, item fairness, and diversity for rankings in two-sided markets, 2021.   
[19] Eren Kurshan, Jiahao Chen, Victor Storchan, and Hongda Shen. On the current and emerging challenges of developing fair and ethical ai solutions in financial services, 2021.   
[20] Anna Jobin, Marcello Ienca, and Effy Vayena. Artificial intelligence: the global landscape of ethics guidelines, 2019.   
[21] Yashar Deldjoo, Vito Walter Anelli, Hamed Zamani, Alejandro Bellogin, and Tommaso Di Noia. Recommender systems fairness evaluation via generalized cross entropy, 2019.   
[22] Agnieszka Mikoajczyk-Barea, Maria Ferlin, and Micha Grochowski. Targeted data augmentation for bias mitigation, 2023.   
[23] Nicholas Gray. Speculations on uncertainty and humane algorithms, 2024.   
[24] Ville Vakkuri, Marianna Jantunen, Erika Halme, Kai-Kristian Kemell, Anh Nguyen-Duc, Tommi Mikkonen, and Pekka Abrahamsson. Time for ai (ethics) maturity model is now, 2021.   
[25] Abhishek Gupta, Alexandrine Royer, Connor Wright, Falaah Arif Khan, Victoria Heath, Erick Galinkin, Ryan Khurana, Marianna Bergamaschi Ganapini, Muriam Fancy, Masa Sweidan, Mo Akif, and Renjie Butalid. The state of ai ethics report (january 2021), 2021.   
[26] Alexandra Olteanu, Michael Ekstrand, Carlos Castillo, and Jina Suh. Responsible ai research needs impact statements too, 2023.   
[27] Roger Zhe Li, Julián Urbano, and Alan Hanjalic. Mitigating mainstream bias in recommendation via cost-sensitive learning, 2023.   
[28] M. López-Sánchez, A. Villena-Rodríguez, G. Gómez, F. J. Martín-Vega, and M. C. AguayoTorres. Latency fairness optimization on wireless networks through deep reinforcement learning, 2022.   
[29] Amanda Bower, Kristian Lum, Tomo Lazovich, Kyra Yee, and Luca Belli. Random isn’t always fair: Candidate set imbalance and exposure inequality in recommender systems, 2022.   
[30] Masoud Mansoury, Himan Abdollahpouri, Joris Rombouts, and Mykola Pechenizkiy. The relationship between the consistency of users’ ratings and recommendation calibration, 2019.   
[31] Eduard Fosch-Villaronga and Gianclaudio Malgieri. Queering the ethics of ai, 2023.   
[32] Farnaz Kohankhaki, Shaina Raza, Oluwanifemi Bamgbose, Deval Pandya, and Elham Dolatabadi. Practical guide for causal pathways and sub-group disparity analysis, 2024.   
[33] Ryan S. Baker, Nigel Bosch, Stephen Hutt, Andres F. Zambrano, and Alex J. Bowers. On fixing the right problems in predictive analytics: Auc is not the problem, 2024.   
[34] Caitlin Kuhlman, Latifa Jackson, and Rumi Chunara. No computation without representation: Avoiding data and algorithm biases through diversity, 2020.   
[35] Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Weiming Liu, Jun Wang, Wenjie Cheng, and Yuyuan Li. In-processing user constrained dominant sets for user-oriented fairness in recommender systems, 2023.   
[36] Jessie Smith, Nasim Sonboli, Casey Fiesler, and Robin Burke. Exploring user opinions of fairness in recommender systems, 2020.   
[37] Di Jin, Luzhi Wang, He Zhang, Yizhen Zheng, Weiping Ding, Feng Xia, and Shirui Pan. A survey on fairness-aware recommender systems, 2023.   
[38] Hossein A. Rahmani, Mohammadmehdi Naghiaei, Mahdi Dehghan, and Mohammad Aliannejadi. Experiments on generalizability of user-oriented fairness in recommender systems, 2022.   
[39] Deepak P, Sanil V, and Joemon M. Jose. On fairness and interpretability, 2021.   
[40] Qinyi Chen, Jason Cheuk Nam Liang, Negin Golrezaei, and Djallel Bouneffouf. Interpolating item and user fairness in multi-sided recommendations, 2024.   
[41] Michaela Hardt, Xiaoguang Chen, Xiaoyi Cheng, Michele Donini, Jason Gelman, Satish Gollaprolu, John He, Pedro Larroy, Xinyu Liu, Nick McCarthy, Ashish Rathi, Scott Rees, Ankit Siva, ErhYuan Tsai, Keerthan Vasist, Pinar Yilmaz, Muhammad Bilal Zafar, Sanjiv Das, Kevin Haas, Tyler Hill, and Krishnaram Kenthapadi. Amazon sagemaker clarify: Machine learning bias detection and explainability in the cloud, 2021.   
[42] Christina Hastings Blow, Lijun Qian, Camille Gibson, Pamela Obiomon, and Xishuang Dong.

Comprehensive validation on reweighting samples for bias mitigation via aif360, 2023.

[43] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets, 2021.   
[44] Amanda Aird, Paresha Farastu, Joshua Sun, Elena tefancová, Cassidy All, Amy Voida, Nicholas Mattei, and Robin Burke. Dynamic fairness-aware recommendation through multiagent social choice, 2024.   
[45] Edward Y. Chang. Cocomo: Computational consciousness modeling for generative and ethical ai, 2023.   
[46] Ludovico Boratto, Gianni Fenu, and Mirko Marras. Interplay between upsampling and regularization for provider fairness in recommender systems, 2021.   
[47] Jesús Bobadilla, Raúl Lara-Cabrera, Ángel González-Prieto, and Fernando Ortega. Deepfair: Deep learning for improving fairness in recommender systems, 2020.   
[48] Tianhao Shi, Yang Zhang, Jizhi Zhang, Fuli Feng, and Xiangnan He. Fair recommendations with limited sensitive attributes: A distributionally robust optimization approach, 2024.   
[49] Yehezkel S. Resheff, Yanai Elazar, Moni Shahar, and Oren Sar Shalom. Privacy and fairness in recommender systems via adversarial training of user representations, 2018.   
[50] Ville Vakkuri, Kai-Kristian Kemell, and Pekka Abrahamsson. Eccola – a method for implementing ethically aligned ai systems, 2020.   
[51] Dario Garcia-Gasulla, Atia Cortés, Sergio Alvarez-Napagao, and Ulises Cortés. Signs for ethical ai: A route towards transparency, 2022.   
[52] Mengxuan Hu, Hongyi Wu, Zihan Guan, Ronghang Zhu, Dongliang Guo, Daiqing Qi, and Sheng Li. No free lunch: Retrieval-augmented generation undermines fairness in llms, even for vigilant users, 2024.   
[53] Anna Felländer, Jonathan Rebane, Stefan Larsson, Mattias Wiggberg, and Fredrik Heintz. Achieving a data-driven risk assessment methodology for ethical ai, 2021.   
[54] Darren Lage, Riley Pruitt, and Jason Ross Arnold. Who followed the blueprint? analyzing the responses of u.s. federal agencies to the blueprint for an ai bill of rights, 2024.   
[55] Kimon Kieslich, Marco Lünich, and Pero Doenovi. Ever heard of ethical ai? investigating the salience of ethical ai issues among the german population, 2022.   
[56] Esther Taiwo, Ahmed Akinsola, Edward Tella, Kolade Makinde, and Mayowa Akinwande. A review of the ethics of artificial intelligence and its applications in the united states, 2023.   
[57] Caesar Wu, Rajkumar Buyya, Yuan Fang Li, and Pascal Bouvry. Dataset | mindset $=$ explainable ai | interpretable ai, 2024.   
[58] Samson Tan, Araz Taeihagh, and Kathy Baxter. The risks of machine learning systems, 2022.   
[59] Yi Zhang, Mengjia Wu, Guangquan Zhang, and Jie Lu. Responsible ai: Portraits with intelligent bibliometrics, 2024.   
[60] Jiahao Chen, Victor Storchan, and Eren Kurshan. Beyond fairness metrics: Roadblocks and challenges for ethical ai in practice, 2021.   
[61] Mark Findlay and Josephine Seah. An ecosystem approach to ethical ai and data use: Experimental reflections, 2020.   
[62] Vita Santa Barletta, Danilo Caivano, Domenico Gigante, and Azzurra Ragone. A rapid review of responsible ai frameworks: How to guide the development of ethical ai, 2023.   
[63] Han Yu, Zhiqi Shen, Chunyan Miao, Cyril Leung, Victor R. Lesser, and Qiang Yang. Building ethics into artificial intelligence, 2018.

# Disclaimer:

SurveyX is an AI-powered system designed to automate the generation of surveys. While it aims to produce high-quality, coherent, and comprehensive surveys with accurate citations, the final output is derived from the AI’s synthesis of pre-processed materials, which may contain limitations or inaccuracies. As such, the generated content should not be used for academic publication or formal submissions and must be independently reviewed and verified. The developers of SurveyX do not assume responsibility for any errors or consequences arising from the use of the generated surveys.