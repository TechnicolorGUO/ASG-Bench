# Literature Review: Pre-trained Language Models in Biomedical Domain- A Systematic Survey

*Generated on: 2025-12-25 23:23:32*
*Topic Index: 5/10*

---

This research aims to systematically survey pre-trained language models in the biomedical domain, focusing on their historical evolution, key methodologies, prominent applications, and existing challenges. The review will synthesize state-of-the-art approaches, analyze performance across biomedical subdomains such as clinical text and scientific literature, and identify open problems and future research trajectories. It is structured to meet academic standards for publication in a top-tier journal, incorporating rigorous analysis and comprehensive referencing.# A Systematic Survey of Pre-trained Language Models in the Biomedical Domain

## Introduction and Research Motivation

The rapid proliferation of artificial intelligence, particularly through large-scale pre-trained language models (LLMs), has catalyzed a paradigm shift across numerous scientific and industrial domains. The biomedical field, characterized by an immense and continuously growing corpus of unstructured data from sources like electronic health records (EHRs), clinical notes, biomedical literature, and genomic databases, stands as one of the most promising yet challenging frontiers for these technologies. The core motivation for applying LLMs to biomedicine stems from the need to extract actionable insights, enhance diagnostic accuracy, accelerate drug discovery, and personalize patient care from this vast sea of information. Traditional natural language processing (NLP) methods have struggled with the complexity, ambiguity, and specialized vocabulary inherent in medical text. Pre-trained language models, however, offer a powerful solution by learning rich semantic representations directly from these massive datasets, enabling them to understand context, identify subtle patterns, and perform complex reasoning tasks that were previously intractable.

This systematic literature review aims to provide a comprehensive and rigorous analysis of the development, application, and challenges of pre-trained language models within the biomedical domain. The primary objective is to synthesize current knowledge, critically evaluate the state-of-the-art, and identify key research gaps and future directions. By examining the historical trajectory from foundational models to modern LLMs, this report will map the evolution of techniques and architectures designed to address the unique demands of biomedical data. Furthermore, it will conduct a deep dive into the diverse applications of these models, assessing their performance on critical tasks such as named entity recognition, relation extraction, question answering, and genomics analysis. This survey also places a significant emphasis on the multifaceted challenges confronting the field, including technical hurdles related to model scalability, data privacy concerns stemming from sensitive health information, and profound ethical dilemmas surrounding algorithmic bias and accountability. Through this holistic examination, the report seeks to equip researchers, clinicians, and policymakers with a clear understanding of the transformative potential and the critical obstacles associated with deploying AI in healthcare and life sciences.

## Key Concepts and Foundational Models

The successful adaptation of large language models to the biomedical domain rests upon a foundation of key concepts and architectural principles that distinguish them from their general-purpose counterparts. Central to this foundation is the **transformer architecture**, which replaced older recurrent neural network (RNN) models due to its superior ability to process long-range dependencies in text [[1]]. At its heart, the transformer uses a self-attention mechanism that allows every word in a sequence to attend to all other words, capturing contextual relationships more effectively than sequential models. For biomedical language models (BPLMs), this is crucial for understanding complex terminologies and syntactic structures. Core components include embedding layers that convert text tokens into numerical vectors, followed by a stack of transformer encoders that iteratively refine these representations [[1]].

Two fundamental training paradigms underpin the creation of BPLMs: **self-supervised learning (SSL)** and **transfer learning**. SSL enables models to learn from unlabeled data, which is abundant in the biomedical domain. The most common SSL technique is **masked language modeling (MLM)**, where the model learns to predict randomly masked tokens in a sentence based on the surrounding context [[11,25]]. This process forces the model to build a deep, contextualized understanding of the language. Another technique, used by GPT-family models, is **causal language modeling**, where the model predicts the next token in a sequence [[2]]. Once a model is pre-trained using SSL on a massive corpus, it can be adapted to specific downstream tasks through **fine-tuning**. This involves taking the pre-trained model and continuing its training on a smaller, labeled dataset for a particular task like named entity recognition (NER) or classification, allowing it to specialize its broad knowledge for a specific purpose [[1]].

Early attempts to apply standard NLP models to biomedicine revealed a significant "domain gap"—the specialized lexicon and syntax of medical texts were not well-captured by models trained on general web or book corpora. To bridge this gap, researchers developed **domain-specific pre-training**, where a base transformer architecture is further pre-trained specifically on large biomedical or clinical text corpora [[2]]. This approach established the blueprint for modern BPLMs. The first major model to follow this path was **BioBERT**, introduced in 2020 [[3,17]]. BioBERT was created by taking the popular BERTBASE architecture and performing continued pre-training on two massive biomedical corpora: PubMed abstracts (~4.5 billion words) and PMC full-text articles (~13.5 billion words) [[3]]. This process imbued the model with a deep understanding of biomedical terminology without altering its underlying structure [[2]]. Similarly, **ClinicalBERT** was developed in 2019 by pre-training a BERT model on clinical notes from the MIMIC-III database, making it highly effective at understanding the nuances of EHR data [[2,14]]. These pioneering models demonstrated conclusively that domain-specific pre-training was a necessary step to achieve state-of-the-art performance in biomedical NLP. They laid the groundwork for a family of subsequent models like SciBERT, BlueBERT, and MatSciBERT, which were pre-trained on scientific and materials science texts, respectively, confirming the broader principle of domain-adaptive pre-training [[19,24,26]].

## Historical Development and Milestones

The history of pre-trained language models in biomedicine can be traced as a series of distinct waves, each marked by key milestones that pushed the boundaries of scale, capability, and application. The initial phase, spanning from approximately 2018 to 2020, was defined by the introduction of foundational domain-specific models that proved the viability of adapting existing architectures to biomedical text. The release of **BioBERT** in early 2019 was a watershed moment, establishing a new standard for performance on biomedical NLP tasks [[3,17]]. By being pre-trained on the vast and diverse corpora of PubMed and PMC, BioBERT demonstrated significant improvements over its general-domain counterpart, achieving higher F1 scores in NER, relation extraction (RE), and question answering (QA) [[3,17]]. Around the same time, **ClinicalBERT** emerged, focusing on the unique linguistic characteristics of clinical notes from EHRs, proving that models needed to be tailored to specific sub-domains within biomedicine [[2,9]]. These models were computationally intensive, requiring extensive resources; for instance, BioBERT's pre-training took 23 days on eight NVIDIA V100 GPUs [[3,17]]. During this period, other notable models like **SciBERT** and **BlueBERT** further solidified the strategy of domain-adaptive pre-training [[19,24]]. The primary focus of this era was on enhancing performance within the constraints of the BERT architecture, which limited input sequences to 512 tokens and required substantial computational power for pre-training.

A second wave of innovation, beginning around 2020, sought to overcome the limitations of the BERT architecture, particularly its fixed input length. This led to the development of **long-sequence transformers** capable of processing much larger contexts, a critical need for analyzing entire clinical notes or long genomic sequences. **Clinical-Longformer** and **Clinical-BigBird** were among the first to bring this capability to the clinical domain, pre-trained on millions of clinical notes from the MIMAC-III dataset [[25]]. They leveraged novel attention mechanisms—such as sliding window attention with global attention heads—to extend the effective input length to 4,096 tokens while maintaining computational feasibility [[25]]. This architectural upgrade resulted in state-of-the-art performance across a range of downstream tasks, including NER and QA, outperforming earlier models like ClinicalBERT [[25]]. Concurrently, the rise of the GPT family of models began to signal a new direction. While GPT-based models are typically trained with causal language modeling, their sheer scale and powerful few-shot learning capabilities started to attract attention for zero-shot reasoning tasks in medicine [[2,4]]. However, during this period, fine-tuned BERT-based models still consistently surpassed GPT models on most traditional NLP benchmarks [[4]].

The third and current wave is characterized by the emergence of **Large Language Models (LLMs)** with billions or even trillions of parameters, representing a quantum leap in scale and capability. Models like **GPT-4**, **LLaMA 2**, and **Gemini** entered the biomedical arena, demonstrating remarkable abilities in complex reasoning, generation, and zero-shot learning [[4,13,27]]. GPT-4, for example, achieved state-of-the-art results on medical multiple-choice exams like MedQA without any task-specific fine-tuning, showcasing a level of general-purpose reasoning far beyond what previous models could offer [[4]]. This wave is also marked by a diversification of model types. Alongside massive open-source LLMs, highly specialized models have emerged, such as **Med-BERT**, which is pretrained not on text but on structured EHR diagnosis codes, creating embeddings that capture temporal disease progression [[28]]. In the genomics space, models like **Enformer** and **DNABERT** have been developed to analyze DNA sequences, with Enformer showing the ability to predict gene expression from genomic variants across long distances [[20]]. This trend towards specialization highlights a matured understanding that while scale is important, architectural and data choices must be carefully tailored to the specific biomedical problem at hand. The timeline below summarizes these key developments.

| Era | Representative Models | Key Innovation | Impact & Limitations |
| :--- | :--- | :--- | :--- |
| **Foundational Wave (2019-2020)** | BioBERT, ClinicalBERT, SciBERT [[2,3,24]] | Domain-adaptive pre-training on biomedical/corpora. | Established that domain-specific knowledge is crucial. High computational cost for pre-training. Limited to 512-token sequences [[3,25]]. |
| **Long-Sequence Wave (2021-2023)** | Clinical-Longformer, Clinical-BigBird [[25]] | Architectural extensions (sparse attention) to handle long input sequences (>4k tokens). | Enabled analysis of whole clinical documents. Outperformed short-sequence models on NER and QA. Required significant computational resources for training (~2 weeks) [[25]]. |
| **LLM & Specialization Wave (2023-Present)** | GPT-4, LLaMA 2, Med-BERT, Enformer, DNABERT [[4,20,28]] | Massive scale (trillions of parameters), zero/few-shot reasoning, and specialized data modalities (structured data, DNA). | Demonstrated superior reasoning and generative abilities. High costs and resource requirements. Persistent issues with hallucination and interpretability [[4,27]]. |

## Current State-of-the-Art Methods and Techniques

The contemporary landscape of biomedical pre-trained language models is dominated by sophisticated Large Language Models (LLMs) and increasingly specialized architectures that push the boundaries of both scale and domain adaptation. A critical analytical distinction exists between **generalist models**, such as GPT-4 and LLaMA 2, which exhibit broad reasoning capabilities, and **specialist models**, like BioBERT and Med-BERT, which are optimized for specific data types or tasks. The choice between these approaches often depends on the nature of the problem, balancing the raw power of generalist models against the efficiency and domain-awareness of specialist ones. Recent studies highlight a nuanced relationship between these two classes. For instance, a comparative evaluation of four prominent LLMs (GPT-3.5, GPT-4, LLaMA 2, and PMC LLaMA) against fine-tuned BERT and BART models revealed that while traditional fine-tuning remains superior for most information extraction tasks, LLMs excel in zero-shot reasoning [[4]]. In a study on medical question answering, GPT-4 achieved 71.56% accuracy on the MedQA benchmark in a zero-shot setting, significantly outperforming the best fine-tuned model (41.95%) [[4]]. This suggests that for tasks requiring flexible, multi-step reasoning, the vast latent knowledge of LLMs provides a decisive advantage.

However, the superiority of LLMs is not absolute. In many classic biomedical NLP tasks like NER and RE, fine-tuned BERT-based models often maintain an edge. A study comparing several LLMs and BERT variants for ophthalmic medication NER found that GPT-4 had the highest overall F1-score (0.962), but among BERT models, BioBERT performed best (0.875) [[27]]. Another study showed that traditional fine-tuning of domain-specific BERT or BART models yielded a macro-average F1 score of 0.6536, compared to just 0.5131 for fine-tuned LLMs across 12 BioNLP datasets [[4]]. This indicates that for tasks requiring precise, localized pattern matching, the high-resolution, token-level understanding honed during fine-tuning may still surpass the more generalized, holistic understanding of LLMs. This dichotomy suggests that the optimal approach is not to replace one class of models with another, but to strategically select or combine them based on the task's requirements.

Beyond the generalist-specialist debate, methodological advancements are focused on improving model efficiency, handling long-context data, and integrating diverse data types. Architectural innovations are central to this progress. **Clinical ModernBERT**, released in 2025, exemplifies this trend by incorporating advanced features like rotary positional embeddings (RoPE) and Flash Attention into a transformer encoder [[11,22]]. These modifications reduce inference latency by up to 60% compared to models like BioClinicalBERT, a critical factor for real-time clinical applications [[11]]. Its extended context length of 8,192 tokens further addresses the challenge of processing lengthy clinical notes [[11,23]]. This model builds upon the ModernBERT architecture, demonstrating a continuous effort to adapt state-of-the-art transformer optimizations for the biomedical domain [[11]]. Another key technique is **Retrieval-Augmented Generation (RAG)**, which aims to mitigate the hallucination problem common in LLMs [[30]]. RAG systems first retrieve relevant documents or facts from an external knowledge base (like PubMed) and then use this retrieved information to guide the generation of an answer. This ensures that outputs are grounded in factual, verifiable information, enhancing reliability and safety [[15,30]]. The DiRAG pipeline, for example, improved entity normalization accuracy by leveraging semantic similarity from BioBERT embeddings to find relevant UMLS concepts [[15]]. These techniques represent a concerted effort to make powerful LLMs safer and more trustworthy for deployment in high-stakes biomedical environments.

## Applications and Case Studies

Pre-trained language models have been applied to a vast and expanding array of tasks within the biomedical domain, transforming everything from basic information extraction to complex clinical decision support. Their applications span text-based analysis of EHRs and literature, as well as emerging roles in genomics and multimodal data integration. One of the most fundamental applications is **Named Entity Recognition (NER)**, which involves identifying and classifying entities like diseases, drugs, genes, and proteins in text. Models like BioBERT, ClinicalBERT, and their successors have become standard tools for this task [[10,15]]. For example, ClinicalBERT was integrated into the DeIDClinic framework for de-identifying protected health information (PHI) in clinical documents, achieving a high F1-score of 0.9732 [[21]]. More recent models have pushed performance even further; PubMedBERT achieves the highest F1-score (88.8%) among comparable models in medical entity extraction tasks [[5]], while GPT-4 has shown top-tier performance in highly specialized NER tasks like identifying ophthalmic medications [[27]]. Beyond simple extraction, models are used for **Relation Extraction (RE)**, which identifies semantic relationships between entities, such as protein-protein interactions or drug-disease associations. Here again, domain-specific models have proven essential, with BioBERT and ClinicalBERT achieving strong results on datasets like LLL and MADE [[10,18]].

In the clinical domain, LLMs are increasingly being deployed for **question answering (QA)** and clinical decision support. Medical multiple-choice exams like MedQA and PubMedQA serve as benchmarks for evaluating QA capabilities [[4,13]]. As noted, GPT-4 has demonstrated exceptional zero-shot performance on MedQA, highlighting its capacity for complex reasoning [[4]]. Other applications include summarizing clinical notes, generating discharge summaries, and simplifying complex medical texts for patient education [[4,13]]. Models like PubMed Text Summarization (MS^2) and PLOS Text Simplification are key benchmarks in these areas [[4]]. In addition to text, BPLMs are being integrated with other data modalities. The CheXpert dataset contains chest radiographs paired with free-text reports, supporting tasks that require correlating image findings with textual descriptions [[7]]. Similarly, the MIMIC-CXR dataset links thousands of chest X-rays with their corresponding radiology reports, enabling research in multimodal analysis [[7]]. The Human Protein Atlas (HPA) and The Cancer Genome Atlas (TCGA) integrate proteomic and genomic data with clinical information, providing fertile ground for LLMs to discover novel biological insights [[7,20]].

The table below presents a selection of case studies illustrating the breadth of applications and the performance of various models across different tasks. It showcases how different models can be superior depending on the specific problem, underscoring the importance of task-specific evaluation.

| Task / Application | Model(s) | Dataset / Context | Performance Metric(s) / Outcome | Citation |
| :--- | :--- | :--- | :--- | :--- |
| **Medical Entity Extraction** | PubMedBERT | Medical NER | F1-score: 88.8% (highest among BERT models); 79.1% in 10-shot setting. | `[[5]]` |
| **Entity Normalization** | BioBERT | ShARe/CLEF, NCBI, TAC2017ADR | Accuracy: 91.10% (ShARe/CLEF); 93.22% (TAC2017ADR). | `[[14]]` |
| **Clinical PHI De-identification** | ClinicalBERT | i2b2/UTHealth 2014 | F1-score: 0.9732; outperformed base BERT (0.935) and RoBERTa (0.963). | `[[21]]` |
| **Information Extraction (NER, RE)** | Fine-tuned BERT vs. LLMs | 12 BioNLP datasets | Macro-average F1: 0.6536 (fine-tuned) vs. 0.5131 (LLMs). Fine-tuned models excelled in RE (0.79 vs. 0.33). | `[[4]]` |
| **Reasoning (Medical QA)** | GPT-4 | MedQA | Zero-shot accuracy: 71.56%, surpassing the SOTA fine-tuned model (41.95%). | `[[4]]` |
| **Genome-Wide Variant Effects** | Enformer | Genomic sequences | Predicts effects from DNA sequences, integrating long-range interactions up to 100 kb. | `[[20]]` |
| **Cell-Type Annotation** | scRNA-seq LLMs | Single-cell transcriptomics data | Supports analysis of cellular heterogeneity and discovery of novel cell types. | `[[20]]` |
| **Structured EHR Analysis** | Med-BERT | Cerner Health Facts® | Improves AUC by 1.21–6.14% in cancer prediction tasks; enables small-data learning. | `[[28]]` |

These case studies reveal a dynamic field where models are not just passively reading text but actively participating in complex analytical workflows, from curating knowledge bases to generating personalized treatment plans.

## Challenges and Open Problems

Despite remarkable progress, the widespread adoption of pre-trained language models in biomedicine is hindered by a formidable set of technical, ethical, and practical challenges. The most immediate and pervasive issue is the **high computational and financial cost** associated with training and deploying state-of-the-art models [[2,4]]. Training a large model like BioBERT required 23 days on eight V100 GPUs [[3]], and fine-tuning LLMs like GPT-4 can be 60 to 100 times more expensive than its predecessor, GPT-3.5, especially for generative tasks [[4]]. This creates a significant barrier to entry for academic labs and smaller institutions, potentially widening the gap between well-funded research centers and others. Even inference latency remains a concern; while models like Clinical ModernBERT have made strides in optimization [[11]], the "black-box" nature of these models makes it difficult to guarantee real-time performance in fast-paced clinical environments [[2]].

A second major challenge lies in ensuring the **reliability and trustworthiness** of model outputs. LLMs are notoriously prone to **hallucinations**, where they generate fluent but factually incorrect or nonsensical information [[4]]. A study on LLaMA 2 zero-shot outputs found that 32% of responses contained hallucinated content [[4]]. In a medical context, this can have severe consequences. While traditional fine-tuned models cannot hallucinate in the same way, they can produce false positives or negatives [[27]]. Overconfidence is another related problem; a 2024 study found that 12 different LLMs exhibited high confidence in their answers regardless of whether they were correct or not [[13]]. This unreliability necessitates robust validation and verification pipelines, such as Retrieval-Augmented Generation (RAG), which cross-references model outputs with trusted knowledge bases to improve consistency and factual accuracy [[30]].

Perhaps the most critical challenges are those concerning **data privacy, ethics, and governance**. Medical data is inherently sensitive, and its use in training models raises significant privacy risks. Adversarial attacks can sometimes re-identify individuals from supposedly anonymized data [[30]]. This is compounded by regulatory complexities under frameworks like HIPAA and GDPR [[13]]. The reliance on cloud-based APIs for powerful models like GPT-4 further complicates data sovereignty and compliance [[27]]. Algorithmic bias is another profound ethical issue. Models trained on vast, uncurated internet data can inherit and amplify societal biases present in that data, leading to inequitable outcomes for certain patient populations [[13,30]]. Ensuring fairness, transparency, and accountability is paramount. This requires developing better evaluation frameworks that assess dimensions beyond pure accuracy, such as bias, toxicity, and user trust [[13]]. Finally, there is a systemic gap in institutional governance. Many hospitals lack the ethical audit protocols, adaptive IRBs, and monitoring tools necessary to safely deploy and steward these rapidly evolving technologies post-deployment [[29,30]]. Addressing these challenges requires a multi-pronged approach involving not only technological solutions but also robust policy-making, interdisciplinary collaboration, and a commitment to responsible AI development throughout the research lifecycle.

## Future Research Directions

To realize the full potential of pre-trained language models in biomedicine, the field must pursue several key research directions that address its most pressing challenges. A primary area of focus is the development of **more efficient and scalable models**. While scaling up parameters has proven effective, it is becoming unsustainable. Future work should concentrate on model compression techniques, parameter-efficient fine-tuning (PEFT) methods like LoRA, and designing lightweight yet powerful architectures that can run efficiently on edge devices or in low-resource settings [[15]]. Research into novel attention mechanisms that are less computationally intensive than the standard dot-product attention will be crucial. The success of Clinical ModernBERT in reducing latency by 60% through architectural optimizations like Flash Attention serves as a promising blueprint for this line of inquiry [[11]].

Improving **model reliability and interpretability** is non-negotiable for clinical deployment. Research must move beyond simple error rates to develop more robust evaluation frameworks that systematically test for bias, fairness, and robustness to adversarial inputs [[13,19]]. A major thrust should be on advancing techniques that ensure model outputs are faithful and accurate. This includes the continued development and integration of **Retrieval-Augmented Generation (RAG)** systems, which ground model predictions in verifiable evidence from trusted biomedical knowledge bases [[30]]. Additionally, there is a need for better explainability methods tailored to the biomedical domain, allowing clinicians to understand *why* a model made a particular recommendation, thereby fostering trust and facilitating human-in-the-loop workflows. Exploring hybrid approaches that combine the strengths of symbolic AI (e.g., expert systems, ontologies) with the pattern-recognition power of neural networks could yield systems that are both powerful and transparent [[15]].

Finally, the future of biomedical NLP lies in **multimodal and integrated analysis**. The field must evolve beyond processing single data types. Future models should be designed to seamlessly ingest and reason across multiple modalities, including text (EHRs, literature), images (radiology, pathology), genomic sequences, and laboratory results [[7,20]]. This will enable a more holistic understanding of a patient's condition. For example, a model could analyze a doctor's note (text), correlate it with a patient's MRI scan (image), and integrate this with their genetic predispositions (genomics) to generate a more comprehensive diagnostic hypothesis. Achieving this requires breakthroughs in cross-modal representation learning and unified architectures that can fuse disparate data streams. Furthermore, research into **federated learning** is critical for building models without compromising patient privacy. Federated learning allows models to be trained across decentralized hospital servers without exchanging raw patient data, addressing a key bottleneck in acquiring large, diverse datasets [[19,29]]. In summary, the path forward involves a strategic combination of efficiency, reliability, and integration, paving the way for AI systems that are not only intelligent but also practical, trustworthy, and safe for use in the complex world of medicine.

---

# References

1. AMMU: A survey of transformer-based biomedical ...
   URL: https://www.sciencedirect.com/science/article/pii/S1532046421003117
2. A systematic review of ethical considerations of large ...
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC12460403/
3. BioBERT: a pre-trained biomedical language representation ...
   URL: https://academic.oup.com/bioinformatics/article/36/4/1234/5566506
4. Benchmarking large language models for biomedical ...
   URL: https://www.nature.com/articles/s41467-025-56989-2
5. Pre-trained Language Models and Few-shot Learning for ...
   URL: https://arxiv.org/abs/2504.04385
6. Evaluation of GPT and BERT-based models on identifying ...
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC11101131/
7. Large Language Model Benchmarks in Medical Tasks
   URL: https://arxiv.org/html/2410.21348v3
8. Multi-model Comparison for Classification of Medical ...
   URL: https://ieeexplore.ieee.org/document/9945029/
9. Comparison of BERT implementations for natural language ...
   URL: https://www.sciencedirect.com/science/article/pii/S2352914822002763
10. BERT applications in natural language processing: a review
   URL: https://link.springer.com/article/10.1007/s10462-025-11162-5
11. An efficient and long context encoder for biomedical text
   URL: https://arxiv.org/html/2504.03964v1
12. A comprehensive evaluation of large Language models on ...
   URL: https://www.sciencedirect.com/science/article/pii/S0010482524002737
13. A Brief Review on Benchmarking for Large Language ...
   URL: https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.70010
14. BERT-based Ranking for Biomedical Entity Normalization - PMC
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC7233044/
15. Enhancing Clinical Named Entity Recognition via Fine ...
   URL: https://www.mdpi.com/2079-9292/14/18/3676
16. Accurate Clinical and Biomedical Named Entity ...
   URL: https://www.sciencedirect.com/science/article/pii/S2665963822000793
17. BioBERT: a pre-trained biomedical language representation ...
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC7703786/
18. Multiple features for clinical relation extraction: A machine ...
   URL: https://www.sciencedirect.com/science/article/pii/S1532046420300095
19. An in-depth evaluation of federated learning on biomedical ...
   URL: https://www.nature.com/articles/s41746-024-01126-4
20. Large language models in biomedicine and healthcare
   URL: https://www.nature.com/articles/s44387-025-00047-1
21. A Multi-Layered Framework for De-identification of Clinical ...
   URL: https://arxiv.org/html/2410.01648v1
22. An efficient and long context encoder for biomedical text
   URL: https://arxiv.org/abs/2504.03964
23. An efficient and long context encoder for biomedical text
   URL: https://chatpaper.com/paper/127241
24. MatSciBERT: A materials domain language model for text ...
   URL: https://www.nature.com/articles/s41524-022-00784-w
25. A comparative study of pretrained language models for ...
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC9846675/
26. [2412.08255] Accurate Medical Named Entity Recognition ...
   URL: https://arxiv.org/abs/2412.08255
27. Evaluating the Performance of Large Language Models for ...
   URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC12099357/
28. Med-BERT: pretrained contextualized embeddings on ...
   URL: https://www.nature.com/articles/s41746-021-00455-y
29. Toward Systemic Governance under Healthy China 2030
   URL: https://arxiv.org/html/2505.07205v1
30. Ethical and regulatory challenges of large language ...
   URL: https://www.thelancet.com/journals/landig/article/PIIS2589-7500(24)00061-X/fulltext
