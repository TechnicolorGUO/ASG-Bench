# Literature Review: Physics-informed neural networks (PINNs) for fluid mechanics- a review.

*Generated on: 2025-12-25 17:47:33*
*Topic Index: 8/10*

---

This research aims to synthesize existing knowledge on physics-informed neural networks (PINNs) as applied to fluid mechanics. The review will cover the theoretical foundations, historical development, state-of-the-art methodologies, key applications in fluid dynamics, persistent challenges, and emerging research trajectories. It focuses on scholarly works from the inception of PINNs onward, incorporating both foundational and recent studies across diverse subdomains of fluid mechanics without restricting to specific flow types unless implied by dominant literature patterns.# A Comprehensive Review of Physics-Informed Neural Networks for Fluid Mechanics

## Introduction and Background

The simulation of fluid mechanics is a cornerstone of modern science and engineering, underpinning advancements in aerospace design, climate modeling, biomedical devices, and chemical processing. Traditional approaches to solving the governing equations of fluid dynamics, primarily the Navier-Stokes equations, rely on numerical methods such as finite volume, finite element, and spectral methods [[17]]. While powerful, these methods are often computationally expensive, particularly for high Reynolds number turbulent flows, complex geometries, or multi-physics problems, and they require significant manual effort for mesh generation and problem setup [[17,21]]. In parallel, data-driven machine learning techniques have demonstrated remarkable success in various domains, but their application in physics has been constrained by the "black box" nature of many models, which can produce solutions that violate fundamental physical laws. The emergence of Physics-Informed Neural Networks (PINNs) represents a paradigm shift, aiming to bridge this gap by creating a new class of computational tools that are both physically consistent and capable of leveraging data efficiently [[4,7]].

PINNs were first introduced in 2017 by Raissi, Perdikaris, and Karniadakis through two seminal preprints that laid the theoretical groundwork for integrating partial differential equations (PDEs) directly into the training of deep neural networks [[1,8,14,16,20]]. This initial work was subsequently formalized and expanded upon in a landmark 2019 publication in the *Journal of Computational Physics*, which established the core formulation and coined the term "Physics-Informed Neural Network" [[2]]<URLYI0ODB>[[6,15,18,21]]. The central motivation behind PINNs is to create a hybrid model that benefits from the universal approximation capabilities of neural networks while being constrained by the known physics of the system. This is achieved by augmenting the standard supervised learning loss function with additional terms that represent the residual of the governing PDEs, evaluated at a set of collocation points within the spatio-temporal domain [[1,6,15]]. By minimizing this composite loss function, the network learns a solution that satisfies both the available data and the underlying physical laws simultaneously.

This methodology fundamentally alters the approach to scientific computing. Unlike purely data-driven models that learn an input-output mapping without regard for consistency, PINNs embed physical principles as a soft constraint, ensuring that the learned solution is not only accurate where data exists but also physically plausible elsewhere [[9,14]]. This is particularly valuable in scenarios where data is sparse, noisy, or incomplete—a common challenge in experimental fluid dynamics [[17]]<URLYI0ODB>. For instance, PINNs have been successfully used to infer full-field velocity fields from sparse point measurements of particle positions in flow visualization [[5]], reconstruct complex turbulent flows from limited sensor data [[11]], and even discover unknown terms in a PDE from data alone [[1]]<URLYI0ODB>. Furthermore, PINNs provide a framework for solving inverse problems, where the goal is to identify unknown parameters or initial/boundary conditions of a system given some observed behavior, a task of immense practical importance in engineering design and control [[15]].

The scope of this review is to provide a comprehensive and systematic analysis of the state-of-the-art in PINNs for fluid mechanics. We will begin by defining the key concepts and historical milestones that led to the development of PINNs. Subsequently, we will delve into the current methodological landscape, examining the diverse architectures, optimization strategies, and domain decomposition techniques that constitute the modern toolkit. Following this, we will survey the extensive range of applications in fluid mechanics, highlighting case studies that showcase the versatility and power of PINNs. A critical section will then address the significant challenges and open problems that researchers face, providing a realistic assessment of the technology's limitations. Finally, we will explore the promising future research directions that aim to overcome these hurdles and solidify the role of PINNs as a transformative tool in computational science.

## Key Concepts and Foundational Principles

At its core, a Physics-Informed Neural Network (PINN) is a deep learning framework designed to solve forward and inverse problems governed by nonlinear partial differential equations (PDEs) [[1,2]]<URLYI0ODB>[[15]]. Its primary innovation lies in its ability to seamlessly integrate physical laws into the training process of a neural network, thereby constraining the solution space to be physically meaningful. The foundational principle is to use a feed-forward neural network, typically with multiple hidden layers, to approximate the unknown solution $u(\mathbf{x}, t)$ of a given PDE [[20,21]]. The network is trained end-to-end by minimizing a composite loss function that consists of three main components: one for enforcing the initial and boundary conditions (ICs and BCs), another for enforcing the PDE residuals at internal collocation points, and a third for penalizing deviations from available data points [[17,18,21]]. The integration of the PDE itself is made possible by automatic differentiation, a technique that allows for the exact computation of derivatives of any order with respect to the network inputs [[1,6,7]]. This obviates the need for traditional discretization schemes like finite differences or finite elements, making PINNs a truly mesh-free method [[17]].

The general form of the PDE that PINNs are designed to solve is often expressed as $\mathcal{N}[u(\mathbf{x}, t)] = 0$, where $\mathcal{N}$ is a nonlinear differential operator, $u$ is the latent solution variable (e.g., velocity, pressure, temperature), and $(\mathbf{x}, t)$ represents the spatial and temporal coordinates [[20]]. The composite loss function, $\mathcal{L}$, is typically formulated as a weighted sum of three distinct losses: $\mathcal{L} = \mathcal{L}_{data} + \lambda_{pde}\mathcal{L}_{pde} + \lambda_{bc}\mathcal{L}_{bc}$ [[18,21]]. Here, $\mathcal{L}_{data}$ measures the error between the network prediction and any available observational or experimental data points. $\mathcal{L}_{pde}$ is the mean squared error (MSE) of the PDE residual, calculated at a large set of interior collocation points. The terms $\lambda_{pde}$ and $\lambda_{bc}$ are hyperparameters, often referred to as weights or regularization parameters, used to balance the contributions of the different loss components. Their careful tuning is a recurring theme in the literature due to the potential for conflicting gradients during training [[6,8]]. The boundary and initial conditions can be enforced in two ways: weakly, by including them in the loss function, or strongly, by modifying the network architecture to analytically satisfy these constraints, for example, by using a modified output layer or incorporating distance functions to boundaries [[9,14,21]].

Several key variants and extensions of the standard PINN framework have emerged to address specific challenges. Bayesian PINNs (BPINNs) introduce uncertainty quantification by treating the network weights as random variables, allowing the model to express confidence in its predictions and account for uncertainties in parameters or data [[15,17]]. Variational PINNs (VPINNs) reformulate the PDE problem into its integral form, akin to the weak formulation used in the finite element method, which can improve stability for certain classes of problems [[16,18]]. To handle high-dimensional PDEs, stochastic dimension gradient descent (SDGD) has been proposed to optimize the network more efficiently across all dimensions [[16]]. For PDEs involving fractional operators, Fractional PINNs (fPINNs) combine automatic differentiation for integer-order terms with hybrid numerical schemes for the fractional parts [[8,16]]. Another notable variant is Gradient-enhanced PINNs (gPINNs), which explicitly minimize the gradient of the PDE residual, leading to higher accuracy solutions [[16,18]]. These specialized versions demonstrate the flexibility of the PINN paradigm and its capacity to be adapted to the nuances of different physical and mathematical problems. The table below summarizes some of the most prominent PINN variants discussed in the literature.

| Variant | Description | Key Reference(s) |
| :--- | :--- | :--- |
| **Standard PINN** | Basic framework combining data loss, PDE residual loss, and boundary condition loss. | Raissi et al. (2019) [[2,6]] |
| **Bayesian PINN (BPINN)** | Incorporates uncertainty quantification by treating network weights as random variables. | Yang et al. (2020) [[8]]; uses Gaussian probabilistic models [[18]] |
| **Variational PINN (VPINN)** | Uses the variational (weak) form of the PDE instead of the strong form. | Kharazmi et al. (2021) [[8,16]] |
| **Fractional PINN (fPINN)** | Solves PDEs with fractional derivatives using a hybrid numerical/automatic differentiation approach. | Pang et al. (2019) [[8,16]] |
| **Gradient-enhanced PINN (gPINN)** | Minimizes the gradient of the PDE residual to improve accuracy. | Yu et al. (2021) [[16,18]] |
| **Stabilized PINN (S-PINN)** | Enforces conservation laws via cumulative residuals over subdomains. | Cen et al. (2024) [[19]] |
| **Evolutionary PINN (Evo-PINN)** | Uses evolutionary algorithms for optimization and hyperparameter search. | Sung et al. (2023); Cai et al. (2021) [[22]] |

This rich ecosystem of related methods underscores the dynamic nature of the field and highlights how the original PINN concept serves as a fertile foundation for ongoing innovation aimed at improving robustness, accuracy, and applicability to a wider range of physical phenomena.

## Historical Development and Milestones

The journey of Physics-Informed Neural Networks (PINNs) did not begin with a single "eureka!" moment but rather evolved from decades of interdisciplinary research at the intersection of machine learning and applied mathematics. The conceptual roots can be traced back to early attempts in the 1990s to apply artificial neural networks to solve ordinary and partial differential equations. Notable precursors include the work of Psichogios and Ungar (1992) and Lagaris, Likas, and Fotiadis (1998), who pioneered the idea of using simple neural networks to approximate solutions to ODEs and PDEs, respectively [[21]]. These early efforts demonstrated the potential of neural networks as universal function approximators for analytical problems but were limited by the computational power of the time and the complexity of the networks that could be trained. More recent inspirations include the work of Kondor (2018), Hirn et al. (2017), and Mallat (2016), who explored the connections between group theory, wavelets, and deep learning, laying the theoretical groundwork for designing more effective network architectures [[21]].

The true genesis of the modern PINN framework occurred in 2017, when a series of groundbreaking preprints by Maziar Raissi, Paris Perdikaris, and George Em Karniadakis introduced the concept to the scientific community [[1,8,14,16,20]]. These papers presented a novel approach to solving supervised learning tasks that must adhere to physical laws described by nonlinear PDEs [[1,20]]. They introduced two classes of algorithms: continuous-time and discrete-time models, establishing the core idea of embedding the PDE residuals directly into the neural network's loss function via automatic differentiation [[1,20]]. This work was supported by prestigious funding agencies, including DARPA, AFOSR, and DOE, signaling the potential impact of this new methodology [[20,21]].

The culmination of this initial phase of development was the 2019 publication in the *Journal of Computational Physics*, widely regarded as the definitive paper on PINNs [[2]]<URLYI0ODB>[[6,15,18,21]]. This paper provided a rigorous and comprehensive treatment of the framework, formally defining it as a deep learning method for solving forward and inverse problems involving nonlinear PDEs [[1,12]]. It solidified the core formulation—using a neural network as a universal function approximator and a composite loss function with physics-based penalties—and introduced important algorithmic details, such as a discrete time-stepping scheme based on Runge-Kutta methods that enabled efficient long-time integration without requiring collocation points at every time step [[21]]. The availability of public code on GitHub further accelerated adoption and experimentation by the broader scientific community [[21]]. The profound impact of this work is evident in its citation count, which exceeded 10,500 as of one reference date, cementing its status as a seminal contribution to computational science [[20]].

Following this foundational work, the field experienced rapid growth and diversification. Researchers began to develop and publish numerous variants and extensions to address the limitations of the original PINN. Key developments included the introduction of Bayesian PINNs for uncertainty quantification [[17]], Variational PINNs (VPINNs) to improve stability [[16]], and the use of adaptive activation functions to mitigate spectral bias [[13,16]]. Methodologies for enforcing boundary conditions were refined, moving from simple soft constraints to hard constraints implemented via distance functions [[9,14]]. The period from 2020 onwards saw a surge in publications focused on applying PINNs to specific scientific and engineering domains, with fluid mechanics emerging as a particularly active area. Landmark papers appeared for specialized applications, such as Hidden Fluid Mechanics for inferring flow fields from visualizations [[5]], NSFnets for the incompressible Navier-Stokes equations [[16]], and TFE-PINN for turbulence modeling [[2]]. This period also marked the rise of dedicated Python libraries like DeepXDE [[16]], SciANN [[5]], and PINA [[5]], which standardized the implementation of PINNs and lowered the barrier to entry for new researchers. This trajectory from foundational research to a vibrant, multifaceted field demonstrates the immense promise and adaptability of the PINN paradigm.

## Current State-of-the-Art Methods and Techniques

The field of Physics-Informed Neural Networks (PINNs) has matured significantly since its inception, evolving from a simple proof-of-concept into a sophisticated ecosystem of methods and techniques. The current state-of-the-art is characterized by a concerted effort to overcome the primary limitations of the original framework—namely, training instability, poor scalability, and lack of robust uncertainty quantification—through a combination of architectural innovations, advanced optimization strategies, and hybrid methodologies. These developments are pushing the boundaries of what is possible with PINNs, enabling their application to increasingly complex and challenging fluid dynamics problems.

One of the most active areas of research is the development of improved optimization and training strategies. A major challenge in standard PINNs is the difficulty in balancing the competing loss terms (e.g., data loss, PDE loss, boundary loss), which can lead to gradient pathologies and slow convergence [[6,8]]. To address this, several adaptive weighting techniques have been proposed. Self-adaptive PINNs (SA-PINNs) treat the weights as trainable parameters that are optimized alongside the network weights, often using statistics of the gradients to dynamically adjust the balance [[8,13,15]]. Another powerful approach involves using the Neural Tangent Kernel (NTK) to guide the weight adaptation; this method leverages the spectral properties of the NTK to calibrate the weights dynamically, improving training dynamics [[8,14,16]]. Other strategies include uncertainty-based weighting, where weights are adjusted based on the confidence of the model's predictions [[18]], and residual-based attention mechanisms that scale weights based on the magnitude of the PDE residual itself [[8,14]]. Beyond adaptive weighting, alternative optimization paradigms are gaining traction. Evolutionary algorithms (EAs) offer a gradient-free approach to training, which can help escape local minima and find better solutions [[22]]. Multi-objective EAs, such as NSGA-II, can directly optimize the trade-off between conflicting loss terms without needing to manually scalarize them [[22]].

Architectural innovations are another key frontier. The choice of activation function has been shown to be critical; smooth functions like Tanh or Swish are generally preferred over ReLU because they possess well-defined higher-order derivatives required for evaluating PDE residuals accurately [[16,18]]. Jagtap and Karniadakis introduced adaptive activation functions, where the frequency of the activation function is treated as a tunable hyperparameter to mitigate the "spectral bias" inherent in standard neural networks [[13,16]]. More recently, evolutionary search has revealed that Fourier feature activations (e.g., sine/cosine) can be highly effective at overcoming spectral bias [[22]]. Another significant trend is the move towards hybrid frameworks that combine PINNs with other numerical methods. Domain decomposition is a prime example, where the computational domain is split into overlapping or non-overlapping subdomains, each solved by a separate PINN [[2,8]]. Methods like XPINN (Extended PINN) enforce not only continuity of the solution but also continuity of the flux and the PDE residual across interfaces, enabling parallelization and improving scalability for large-scale 3D simulations [[2,8]]. Similarly, hybrid numerical-deep learning frameworks like HFD-PINN integrate finite difference modules with neural networks to reduce computational cost [[2]].

Finally, there is a growing focus on enhancing the robustness and interpretability of PINNs through uncertainty quantification (UQ) and symbolic discovery. Bayesian PINNs (BPINNs) are a direct extension that treats the network weights as probability distributions, allowing the model to quantify epistemic uncertainty—the uncertainty arising from a lack of knowledge about the model itself [[15,17]]. This is crucial for applications where decision-making relies on the PINN's predictions. However, UQ remains a challenge, particularly in disentangling epistemic and aleatoric uncertainty [[2]]. Looking further ahead, neuro-symbolic integration aims to merge the pattern-recognition strengths of neural networks with the logical reasoning capabilities of symbolic AI. SyCo-PINN, for example, combines graph neural networks with theorem provers to achieve significant reductions in error for turbulence closures [[2]]. This fusion of connectionist and symbolic paradigms represents a promising path toward developing more transparent and trustworthy models. The table below outlines some of these state-of-the-art methods and their primary contributions.

| Method Category | Technique | Primary Contribution | Key Reference(s) |
| :--- | :--- | :--- | :--- |
| **Optimization Strategies** | Adaptive Weighting (SA-PINN) | Dynamically adjusts loss weights using trainable parameters based on gradient statistics. | McClenny & Braga-Neto (2020) [[8,15]] |
| | Adaptive Weighting (NTK-Guided) | Dynamically balances loss weights using Neural Tangent Kernel eigenvalues. | Wang et al. (2022, 2024) [[8,14]] |
| | Evolutionary Algorithms (Evo-PINN) | Uses gradient-free evolutionary algorithms to avoid local minima and optimize hyperparameters. | Sung et al. (2023); Cai et al. (2021) [[22]] |
| **Architectural Innovations** | Adaptive Activation Functions | Treats activation function frequency as a tunable hyperparameter to mitigate spectral bias. | Jagtap et al. (2020) [[13,16]] |
| | Hybrid Numerical-Deep Learning | Integrates numerical modules (e.g., finite differences) with neural networks to reduce cost. | HFD-PINN [[2]] |
| | Neuro-Symbolic Integration | Combines GNNs with theorem provers to improve interpretability and accuracy. | SyCo-PINN [[2]] |
| **Domain Decomposition** | Extended PINN (XPINN) | Enforces continuity of solution, flux, and residual across subdomain interfaces for parallelization. | Jagtap & Karniadakis (2020, 2021) [[2,8]] |
| **Uncertainty Quantification** | Bayesian PINNs (BPINNs) | Treats network weights as random variables to quantify epistemic uncertainty. | BPINN [[15,17]] |
| | Federated Learning (Fed-PINN) | Enables collaborative training across distributed centers while preserving privacy. | Targeting ε=1.0 differential privacy [[2]] |

These diverse and rapidly advancing techniques collectively define the cutting edge of PINN research, transforming the technology from a novel concept into a versatile and powerful tool for scientific discovery.

## Applications and Case Studies in Fluid Mechanics

The application of Physics-Informed Neural Networks (PINNs) to fluid mechanics has been exceptionally prolific, spanning a wide spectrum of canonical and industrially relevant problems. The core strength of PINNs—integrating physical laws with data—makes them particularly well-suited for fluid dynamics, where governing equations are complex, data is often sparse, and computational costs can be prohibitive. The literature showcases successful deployments across nearly every subfield of fluid mechanics, demonstrating the method's versatility and power.

In **incompressible and low-speed flows**, PINNs have been applied to classic benchmark problems. For instance, studies have modeled laminar flows around obstacles, such as an espresso cup, showcasing the ability to predict velocity and pressure fields from minimal data [[14]]. More complex scenarios include simulating flows with moving boundaries, a challenging task for traditional solvers [[5]]. A particularly innovative application is in multiphase flow, where PINNs have been used to simulate the rise of a gas bubble in a denser liquid, accurately predicting its interaction with a heated wall [[10]]. In this study, the maximum errors were reported at 5.2% at the phase interface and 2.8% in the center of mass position, with inferred velocity fields showing a maximum mean-squared error of 0.28 [[10]]. This agnosticism to geometry and fluid properties highlights the flexibility of the PINN approach [[10]].

For **high-speed and compressible flows**, PINNs have proven effective in capturing shock waves and other discontinuities. Research includes modeling supersonic flows [[14]] and solving the compressible Euler equations [[5]]. These applications are crucial for aerospace engineering, where accurate prediction of aerodynamic forces is essential. The ability to handle these complex flow regimes without the need for intricate meshing strategies, a major advantage of the mesh-free PINN framework, is a significant benefit [[17]].

Perhaps the most impactful and active area of application is in **turbulence modeling**. Turbulence remains one of the greatest unsolved problems in classical physics, and PINNs offer a new paradigm for its simulation. One of the most compelling examples is the development of TFE-PINN (Time-Frequency Embedding PINN), which integrates wavelet-based multiscale feature extraction and stochastic eddy-viscosity models [[2]]. When tested on a NASA benchmark for turbulent jet reconstruction, TFE-PINN reduced the error by 62% compared to Large Eddy Simulation (LES) while dramatically cutting the simulation time from 72 hours to just 14 hours [[2]]. Another significant advance comes from the use of PINNs to derive data-driven turbulence closure models. A recent study utilized a comprehensive dataset from experiments and DNS/LES to train a neural network-based turbulence closure that predicts Reynolds stresses directly [[11]]. When integrated into a forward PINN solver, this model achieved at least an order-of-magnitude reduction in Reynolds stress errors compared to classical eddy-viscosity models like k-ε [[11]].

Beyond direct flow simulation, PINNs are being used for **inverse problems and digital twin applications**. Inverse problems involve identifying unknown parameters or initial/boundary conditions from observed data. PINNs have been used to infer uncertain parameters, such as coefficients in wind engineering, from sparse measurements [[15]]. A fascinating application is in biomechanics, where CardioPINN was developed to model cardiovascular blood flow. This model can predict the fractional flow reserve (FFR)—a key diagnostic metric for coronary artery disease—with less than 8% error and an AUC of 0.93 for atrial fibrillation ablation targeting [[2]]. This demonstrates the potential for creating patient-specific digital twins that can aid in clinical decision-making. The table below provides a summary of key applications and case studies in fluid mechanics.

| Application Area | Specific Problem / Case Study | Key Findings / Results | Key Reference(s) |
| :--- | :--- | :--- | :--- |
| **Incompressible Flow** | Laminar flow over an espresso cup | Successfully predicted velocity and pressure fields. | Rao et al. (2023) [[16]]; Biswas & Anand (2023) [[5]] |
| | Moving boundary flows | Demonstrated capability to handle complex geometries and moving boundaries. | Zhu et al. (2024) [[5]] |
| | Multiphase flow (bubble rise) | Predicted bubble position with 2.8% error and interaction with a heated wall with 6.8% temperature error. | Darlik & Peters (2023) [[5]]; PM1KPV [[10]] |
| **Compressible Flow** | Supersonic flow modeling | Validated effectiveness for high-speed flows with shocks. | Jin et al. (2021) [[5]]; Wassing et al. (2024) [[5]] |
| **Turbulence Modeling** | Turbulent jet reconstruction | Reduced error by 62% vs LES and cut simulation time from 72h to 14h using TFE-PINN. | Uddin et al. (2023) [[16]]; Ganga et al. (2023) [[16]]; Sun et al. (2023) [[5]]; Wassing et al. (2024) [[5]]; Hybrid framework [[2]] |
| | Data-driven turbulence closure | Achieved at least an order-of-magnitude error reduction in Reynolds stress predictions compared to k-ε. | Anagnostopoulos et al. (2024) [[11]] |
| **Inverse Problems** | Cardiovascular flow modeling | Predicted fractional flow reserve with <8% error and AUC of 0.93 for diagnostics. | CardioPINN [[2]] |
| | Wind engineering | Identified uncertain parameters in turbulent flow models from sparse data. | Emmert-Streib [[15]] |
| **Other Applications** | Reacting flows | Applied to combustion and reacting flows. | Sun et al. (2023) [[5]] |
| | Rotational water waves | Modeled with WaveNets, a type of PINN. | Chen et al. (2023) [[16]] |

This broad and successful application landscape firmly establishes PINNs as a credible and rapidly maturing alternative or complement to traditional computational fluid dynamics (CFD) solvers.

## Challenges and Open Problems

Despite the remarkable successes and rapid progress in the field, Physics-Informed Neural Networks (PINNs) are not without significant challenges and open problems. These issues span the entire workflow, from the fundamental mathematics of training to the practicalities of implementation and the limits of their predictive capabilities. Acknowledging and addressing these challenges is crucial for the continued maturation and widespread adoption of PINNs in scientific and engineering practice.

One of the most frequently cited and persistent challenges is **training instability and poor convergence** [[6,8,14]]. The optimization landscape of PINNs is notoriously difficult, with the composite loss function often containing conflicting gradients from the disparate data, boundary, and PDE loss terms [[6,8]]. This can cause the training process to get trapped in poor local minima or converge very slowly. The problem is exacerbated by the fact that the scales of these different loss components can vary dramatically, a phenomenon known as gradient imbalance [[8]]. While adaptive weighting strategies have been developed to mitigate this, finding a universally effective strategy remains an open research question. Some studies suggest that the root of the instability may lie in the spectral properties of the Neural Tangent Kernel, a topic of active investigation [[13,14]].

Another significant hurdle is the **difficulty in enforcing boundary conditions, especially for complex geometries** [[6,14]]. While PINNs can incorporate boundary conditions into the loss function (soft constraints), this can sometimes lead to inaccuracies at the boundaries. Implementing hard constraints, where the network is explicitly designed to satisfy the boundary conditions, is a more elegant solution but is also more complex to implement and may not be straightforward for arbitrary or time-dependent geometries [[9,21]]. The computational cost associated with training PINNs can also be substantial, particularly for high-dimensional problems or those requiring very high accuracy, which can be a limiting factor for real-time applications [[8,17]].

The issue of **scalability and long-time integration** presents a formidable challenge, especially for chaotic systems like fully developed turbulence. As the size of the computational domain or the number of time steps increases, the optimization problem becomes progressively harder, and the risk of divergence grows [[14]]. Standard PINNs struggle with long-time integration because errors can accumulate and grow exponentially over time, violating the underlying physical causality of the system [[8]]. Several methods have been proposed to tackle this, such as reformulating the loss function to respect causality or using a parareal-type approach (PPINN) for parallel-in-time integration, but these remain active areas of research [[8,16]].

Furthermore, the field suffers from a **lack of a robust theoretical grounding** for many of its practical aspects. While some convergence results exist for linear second-order elliptic and parabolic PDEs under increasing collocation points, a general theory for the convergence of PINNs for nonlinear PDEs is still lacking [[8]]. There is also a scarcity of **standardized benchmarks and datasets** specifically designed to evaluate PINN performance systematically and reproducibly [[6,14]]. Without such benchmarks, comparing the efficacy of different PINN variants and optimization strategies becomes difficult. This leads to the related problem of **generalizability**: a PINN trained on one set of parameters or geometry may perform poorly when applied to a slightly different scenario, necessitating retraining from scratch [[3]]. This limitation is a major bottleneck for deploying PINNs as "digital twins" or for parametric studies. Finally, the issue of **physical constraint mismatch** is a tangible concern; one analysis identified this as contributing up to 20% of the total error in a given problem, highlighting the gap between the idealized PDE and the real-world physical system being modeled [[2]]. Addressing these multifaceted challenges is the central focus of much current research in the field.

## Future Research Directions and Concluding Remarks

The future of Physics-Informed Neural Networks (PINNs) in fluid mechanics appears exceptionally bright, with a clear and ambitious roadmap of research directions aimed at surmounting existing challenges and unlocking new capabilities. The trajectory of the field suggests a move towards greater automation, enhanced robustness, and deeper integration with both classical numerical methods and next-generation computing paradigms. These future endeavors promise to transform PINNs from a promising research tool into a reliable and indispensable component of the scientific computing ecosystem.

A primary thrust of future research is the development of **foundation models and automated methods**. The concept of creating unified, large-scale models for PDEs, analogous to foundation models in natural language processing, is gaining momentum. Proposals include Unified PDE Solvers (UPS), PDEformer-1, Poseidon, and SciML, which aim to create models that can generalize across a wide range of equations and boundary conditions, drastically reducing the need for retraining [[6,14]]. This aligns with the push towards automating the entire PINN pipeline. Automated hyperparameter tuning, exemplified by Auto-PINN and NAS-PINN (Neural Architecture Search for PINNs), seeks to remove the manual guesswork involved in selecting optimal network architectures and training parameters [[6,16]]. Such automation is critical for making PINNs accessible to domain scientists who may not be experts in deep learning.

Enhancing robustness and reliability will be driven by advancements in **uncertainty quantification (UQ) and transfer learning**. While Bayesian approaches are a start, achieving rigorous UQ that can disentangle epistemic and aleatoric uncertainties remains a key goal [[2]]. Federated learning offers a pathway to collaborative model building across distributed centers (e.g., different research labs or hospitals) while preserving data privacy, with targets set for stringent privacy levels like ε=1.0 [[2]]. Transfer learning and meta-learning are being explored to enable PINNs to adapt quickly to new PDEs or parameter changes, potentially reducing the computational cost of retraining by orders of magnitude [[3]]. This would be a game-changer for design optimization and real-time control applications.

Looking even further ahead, the field is exploring the integration of PINNs with **hybrid numerical-deep learning frameworks and quantum computing**. Domain decomposition methods like XPINN are already paving the way for massive parallelization on GPUs [[2,8]]. The ultimate vision is to combine the strengths of both worlds: using PINNs for regions where data is abundant or where the solution is complex, and reverting to trusted numerical solvers where they are more efficient. Quantum-enhanced PINNs represent a frontier exploration, with early prototypes suggesting potential speedups for certain problems, such as a reported 8x speedup for Poisson equations on IBM hardware [[2]]. This synergistic approach, blending classical, deep, and quantum computing, could lead to unprecedented computational power for fluid dynamics simulations.

To conclude, the journey of PINNs from a theoretical curiosity to a state-of-the-art tool for fluid mechanics has been remarkably swift and impactful. The core innovation of embedding physical laws directly into a neural network's loss function has created a powerful synergy between data-driven machine learning and first-principles physics. This review has highlighted how PINNs are now successfully tackling a diverse array of fluid dynamics problems, from laminar flows to complex turbulence, offering significant computational advantages and novel insights. However, the path forward is not without obstacles. Persistent challenges related to training stability, scalability, and a robust theoretical foundation must be addressed. The future of the field lies in the development of more intelligent, automated, and robust PINN variants, powered by advances in optimization, UQ, and hybrid architectures. As these challenges are met, PINNs are poised to become a cornerstone of modern computational science, enabling faster, cheaper, and more insightful simulations of the complex fluid flows that govern our world.

---

# References

1. [1711.10561] Physics Informed Deep Learning (Part I)
   URL: https://arxiv.org/abs/1711.10561
2. Physics-Informed Neural Networks: A Review of ...
   URL: https://www.mdpi.com/2076-3417/15/14/8092
3. Adaptive Physics-informed Neural Networks: A Survey
   URL: https://openreview.net/forum?id=vz5P1Kbt6t
4. Physics-Informed Neural Networks M. Raissi & P. ...
   URL: https://www.scribd.com/document/961477494/Physics-Informed-Neural-Networks-M-Raissi-P-Perdikaris-G-E-Karniadakis-ebook-reading-now-available
5. (PDF) A comprehensive review of advances in physics ...
   URL: https://www.researchgate.net/publication/384576812_A_comprehensive_review_of_advances_in_physics-informed_neural_networks_and_their_applications_in_complex_fluid_dynamics
6. a survey of physics-informed machine learning - Springer Link
   URL: https://link.springer.com/article/10.1007/s44379-025-00016-0
7. From PINNs to PIKANs: Recent Advances in Physics ...
   URL: https://arxiv.org/abs/2410.13228
8. (PDF) Physics-Informed Neural Networks and Extensions
   URL: https://www.researchgate.net/publication/383648646_Physics-Informed_Neural_Networks_and_Extensions
9. [2403.00599] A hands-on introduction to Physics-Informed ...
   URL: https://arxiv.org/abs/2403.00599
10. Physics-informed neural networks for heat transfer ...
   URL: https://www.sciencedirect.com/science/article/pii/S0017931023012346
11. Turbulence Closure in RANS and Flow Inference around a ...
   URL: https://arxiv.org/html/2510.06049v1
12. Adaptive evolution enhanced physics-informed neural ...
   URL: https://www.sciencedirect.com/science/article/abs/pii/S0378775322014094
13. Self-adaptive physics-informed neural networks
   URL: https://www.sciencedirect.com/science/article/abs/pii/S0021999122007859
14. From PINNs to PIKANs: Recent Advances in Physics- ...
   URL: https://arxiv.org/html/2410.13228v1
15. Understanding Physics-Informed Neural Networks
   URL: https://www.mdpi.com/2673-2688/5/3/74
16. Exploring Physics-Informed Neural Networks
   URL: https://arxiv.org/html/2410.00422v1
17. What Are Physics-Informed Neural Networks (PINNs)?
   URL: https://www.mathworks.com/discovery/physics-informed-neural-networks.html
18. Physics-Informed Neural Networks in Materials Modeling and ...
   URL: https://link.springer.com/article/10.1007/s11831-025-10448-9
19. S-PINN: Stabilized physics-informed neural networks for ...
   URL: https://www.sciencedirect.com/science/article/abs/pii/S0045782525006206
20. Physics-informed neural networks: A deep learning ...
   URL: https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125
21. (PDF) Physics-Informed Neural Networks
   URL: https://www.academia.edu/110390709/Physics_Informed_Neural_Networks
22. Evolutionary Optimization of Physics-Informed Neural ...
   URL: https://arxiv.org/html/2501.06572v4
